# -*- coding: utf-8 -*-
"""RAG-STAT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14dXYpgi4f0c97nJlczjySDzUWA6nisQ7

## RAG System Using Llama2 With Hugging Face
"""

!pip install pypdf

!pip install -q transformers einops accelerate langchain bitsandbytes

!pip install sentence-transformers

!pip install deprecated==1.2.14 dirtyjson==1.0.8 h11==0.14.0 httpcore==1.0.2 httpx==0.26.0 llama_index==0.9.40 openai==1.10.0 tiktoken==0.5.2 typing-extensions==4.9.0

!pip install -U langchain-community

from llama_index import VectorStoreIndex,SimpleDirectoryReader,ServiceContext
from llama_index.llms import HuggingFaceLLM
from llama_index.prompts.prompts import SimpleInputPrompt

documents=SimpleDirectoryReader("/content/data").load_data()
documents

!huggingface-cli login

system_prompt="""
You are a Q&A assistant. Your goal is to answer questions as
accurately as possible based on the instructions and context provided.
"""
## Default format supportable by LLama2
query_wrapper_prompt = SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")

import torch
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
    model_name="meta-llama/Llama-2-7b-chat-hf",
    device_map="auto",
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={"torch_dtype": torch.float16 , "load_in_8bit":True}
)

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index import ServiceContext
from llama_index.embeddings import LangchainEmbedding

embed_model=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"))

service_context=ServiceContext.from_defaults(
    chunk_size=512,
    llm=llm,
    embed_model=embed_model
)

service_context

index=VectorStoreIndex.from_documents(documents,service_context=service_context)

index

query_engine=index.as_query_engine()

response = query_engine.query("How many diagonals can you draw in a decagon?")

print(response)

import pandas as pd

splits = {'train': 'data/train-00000-of-00001.parquet', 'val': 'data/val-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/rvv-karma/Math-QA/" + splits["train"])

df

# Filter the DataFrame by the 'Statistics' topic
filtered_df = df[df['topic'] == 'Statistics']

# Sample 2 rows from the filtered DataFrame
test_df = filtered_df.sample(n=30)

# Initialize a list to store the answers
answers = []

# Iterate over each row in the DataFrame
for index, row in test_df.iterrows():
    # Get the question
    question = row['question']

    # Use the RAG model to generate an answer
    answer = query_engine.query(question)

    # Store the generated answer
    answers.append(answer)

# Add the answers to the DataFrame
test_df['generated_answer'] = answers

# Display the updated DataFrame
print(test_df)

test_df

test_df.drop(columns="topic", inplace=True)

test_df.rename(columns={'sub_topic': 'context'}, inplace=True)

test_df

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Basic preprocessing: ensure all entries are strings
for column in ['context', 'question', 'answer', 'generated_answer']:
    test_df[column] = test_df[column].astype(str).str.lower()

# Debugging: Print the initial DataFrame
print("Initial DataFrame:")
print(test_df)

# Combine the relevant columns for vectorization
combined_series = pd.concat([test_df['context'], test_df['answer'], test_df['generated_answer']], ignore_index=True)

# Debugging: Print the combined series
print("\nCombined Series for Vectorization:")
print(combined_series)

# Vectorize using TF-IDF
vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(combined_series)
vectors_array = vectors.toarray()

# Debugging: Print the vectors
print("\nTF-IDF Vectors:")
print(vectors_array)

# Split the vectors
num_rows = len(test_df)
context_vectors = vectors_array[:num_rows]
answer_vectors = vectors_array[num_rows:2*num_rows]
generated_vectors = vectors_array[2*num_rows:]

# Ensure that the vectors are correctly sized
assert len(context_vectors) == num_rows, "Mismatch in context vectors length"
assert len(answer_vectors) == num_rows, "Mismatch in answer vectors length"
assert len(generated_vectors) == num_rows, "Mismatch in generated vectors length"

# Debugging: Print the split vectors
print("\nContext Vectors:")
print(context_vectors)
print("\nAnswer Vectors:")
print(answer_vectors)
print("\nGenerated Vectors:")
print(generated_vectors)

# Calculate similarities
faithfulness = [cosine_similarity([context_vectors[i]], [generated_vectors[i]])[0][0] for i in range(num_rows)]
answer_relevancy = [cosine_similarity([answer_vectors[i]], [generated_vectors[i]])[0][0] for i in range(num_rows)]

# Add similarities to DataFrame
test_df['faithfulness'] = faithfulness
test_df['answer_relevancy'] = answer_relevancy

# Debugging: Print the DataFrame with similarities
print("\nDataFrame with Similarities:")
print(test_df)

# Calculate context recall and precision
def compute_recall_precision(context, generated):
    context_terms = set(context.split())
    generated_terms = set(generated.split())

    common_terms = context_terms.intersection(generated_terms)

    recall = len(common_terms) / len(context_terms) if len(context_terms) > 0 else 0
    precision = len(common_terms) / len(generated_terms) if len(generated_terms) > 0 else 0

    return recall, precision

# Using try-except block to capture indexing issues
context_recall = []
context_precision = []

for i in range(num_rows):
    try:
        recall, precision = compute_recall_precision(test_df['context'].iloc[i], test_df['generated_answer'].iloc[i])
        context_recall.append(recall)
        context_precision.append(precision)
    except KeyError as e:
        print(f"KeyError at index {i}: {e}")

test_df['context_recall'] = context_recall
test_df['context_precision'] = context_precision

# Calculate answer correctness
threshold = 0.8
test_df['answer_correctness'] = test_df['answer_relevancy'] >= threshold

# Final DataFrame output
print("\nFinal DataFrame with all metrics:")
print(test_df)

import matplotlib.pyplot as plt

# Set up the figure and axis
plt.figure(figsize=(10, 6))

# Scatter plot 'answer_relevancy'
plt.scatter(test_df.index, test_df['answer_relevancy'], color='teal', alpha=0.7, marker='o')

# Add labels and title
plt.xlabel('Index')
plt.ylabel('Answer Relevancy')
plt.title('Scatter Plot of Answer Relevancy for Each Entry')

# Display the plot
plt.tight_layout()
plt.show()

# Calculate the overall performance
average_faithfulness = test_df['faithfulness'].mean()
average_answer_relevancy = test_df['answer_relevancy'].mean()
average_context_recall = test_df['context_recall'].mean()
average_context_precision = test_df['context_precision'].mean()
accuracy_answer_correctness = test_df['answer_correctness'].mean()

# Print the overall performance metrics
print("\nOverall Performance Metrics:")
print(f"Average Faithfulness: {average_faithfulness:.4f}")
print(f"Average Answer Relevancy: {average_answer_relevancy:.4f}")
print(f"Average Context Recall: {average_context_recall:.4f}")
print(f"Average Context Precision: {average_context_precision:.4f}")
print(f"Answer Correctness Accuracy: {accuracy_answer_correctness:.4f}")

# If you want a performance summary in percentage terms, you could format them as such:
performance_summary = {
    'Average Faithfulness (%)': average_faithfulness * 100,
    'Average Answer Relevancy (%)': average_answer_relevancy * 100,
    'Average Context Recall (%)': average_context_recall * 100,
    'Average Context Precision (%)': average_context_precision * 100,
    'Answer Correctness Accuracy (%)': accuracy_answer_correctness * 100
}

# Print the performance summary as percentages
print("\nPerformance Summary (in %):")
for metric, value in performance_summary.items():
    print(f"{metric}: {value:.2f}%")