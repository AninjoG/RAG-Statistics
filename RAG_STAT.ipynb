{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnUOq3dZeDK1"
      },
      "source": [
        "## RAG System Using Llama2 With Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHP9Gfafdq57",
        "outputId": "ac5bcd00-a317-4074-ec1c-fe47f82a8cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/292.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m286.7/292.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YYeHrwmHfbgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dadcb3-f2a0-4f79-8b3b-0fd030da6121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu1g-xwnfv9s",
        "outputId": "510193dc-ff4c-4ab8-a88d-bbe793a0465f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3vxTqdQ5bc_",
        "outputId": "38f6ceed-063c-4331-a6f9-51a543059d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deprecated==1.2.14\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson==1.0.8\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Collecting httpcore==1.0.2\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting httpx==0.26.0\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting llama_index==0.9.40\n",
            "  Downloading llama_index-0.9.40-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting openai==1.10.0\n",
            "  Downloading openai-1.10.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting tiktoken==0.5.2\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-extensions==4.9.0\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated==1.2.14) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpcore==1.0.2) (2024.8.30)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx==0.26.0) (3.7.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.26.0) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.26.0) (1.3.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index==0.9.40) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (3.10.5)\n",
            "Collecting dataclasses-json (from llama_index==0.9.40)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (2024.6.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.9.40) (8.5.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama_index==0.9.40)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.10.0) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (2.9.2)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.2) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index==0.9.40) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index==0.9.40) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index==0.9.40) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index==0.9.40) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index==0.9.40) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index==0.9.40) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index==0.9.40) (4.0.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx==0.26.0) (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index==0.9.40) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index==0.9.40) (1.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.10.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.10.0) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index==0.9.40) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index==0.9.40) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama_index==0.9.40) (3.1.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama_index==0.9.40)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama_index==0.9.40)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.9.40) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.9.40) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.9.40) (2024.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index==0.9.40) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama_index==0.9.40) (1.16.0)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index-0.9.40-py3-none-any.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: dirtyjson, typing-extensions, mypy-extensions, marshmallow, httpcore, deprecated, typing-inspect, tiktoken, httpx, dataclasses-json, openai, llama_index\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.5\n",
            "    Uninstalling httpcore-1.0.5:\n",
            "      Successfully uninstalled httpcore-1.0.5\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.2\n",
            "    Uninstalling httpx-0.27.2:\n",
            "      Successfully uninstalled httpx-0.27.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 httpcore-1.0.2 httpx-0.26.0 llama_index-0.9.40 marshmallow-3.22.0 mypy-extensions-1.0.0 openai-1.10.0 tiktoken-0.5.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install deprecated==1.2.14 dirtyjson==1.0.8 h11==0.14.0 httpcore==1.0.2 httpx==0.26.0 llama_index==0.9.40 openai==1.10.0 tiktoken==0.5.2 typing-extensions==4.9.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCw6F6WfKSSG",
        "outputId": "dc6836fc-dac5-4e76-a8d9-ebdf0e43dcb0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.125)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (0.26.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (3.10.7)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, pydantic-settings, langchain-community\n",
            "Successfully installed langchain-community-0.3.0 pydantic-settings-2.5.2 python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HutpiHgpgLBE"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcU-nQ_XgxWD",
        "outputId": "3552cf73-7ddc-454a-d169-c6619f8f0a3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='3b06b785-b86f-43a8-b7b3-ae364ac2bca2', embedding=None, metadata={'page_label': '1', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Foundations\\notfStatistics\\nLEONARD J.SAVAGE\\nLate Eugene Higgins Professor ofStatistics\\nYale University\\nSECOND REVISED EDITION\\nDOVER PUBLICATIONS, INC.\\nNEW YORK\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca63cfaa-6b6f-406d-a5e0-c498f3f9d56a', embedding=None, metadata={'page_label': '2', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Copyright ©1972byDoverPublications, Inc.\\nCopyright©1954byI.Richard Savage.\\nAllrightsreserved underPanAmerican andInter-\\nnational Copyright Conventions.\\nThisDoveredition, firstpublished in1972,isa\\nrevisedandenlarged version oftheworkoriginally\\npublished byJohnWiley&Sonsin1954.\\nInternational Standard BookNumber: 0-486-62349-1\\nLibrary ofCongress CatalogCardNumber: 79-188245\\nManufactured intheUnited StatesofAmerica\\nDoverPublications, Inc.\\n180Varick Street\\nNewYork,N.Y.10014\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='65a0730f-63e8-41fc-b447-18e93e0a747b', embedding=None, metadata={'page_label': '3', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TOMYFATHER\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c299c4ea-ffa2-4a1c-af46-e33101c560f8', embedding=None, metadata={'page_label': '4', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='202f378c-7a3d-476f-af6e-23615a098e6a', embedding=None, metadata={'page_label': '5', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface totheDover Edition\\nCONTINUING INTEREST HAS ENCOURAGED PUBLICATION OFASECOND\\nedition ofthis book. Because revising ittofitmy present thinking and\\nthenew climate ofopinion about the foundations ofstatistics would\\nobliterate rather than restore, Ihave limited myself inthe preparation\\nofthis edition much asthough dealing with thework ofanother.\\nThe objective errors that have come tomy attention, mainly through\\nthe generosity ofreaders, ofwhom Peter Fishburn hasmy special\\nthanks, have been corrected, ofcourse. Minor and mechanical ones, such\\nasaname misspelled oraninequality that had persisted inpointing in\\nthewrong direction, have been silently eliminated. Other changes are\\nconspicuous asadditions. They consist mainly ofthis Preface, Appendix\\n4:Bibliographic Supplement, and several footnotes identified asnew\\nbythe signt. Toenable you topursue themany new developments\\nsince 1954 according tothe intensity and direction of your own\\ninterests, anumber ofnew references leading tomany morearelisted in\\ntheBibliographic Supplement, and the principle advances known tome\\narepointed out innew footnotes orincomments onthenew references.\\nCitations tothe bibliography inthe original Appendix 3aremade\\nbyacompact, but otherwise ill-advised, letter and number code; those\\ntothenew Appendix 4aremade byanow popular system, which is\\neffective, informative, and flexible. Example: The historic papers (Borel\\n1924) and [D2] have been translated byKyburg and Smokler (1964).\\nThe following paragraphs are intended tohelp you approach\\nthis book with amore current perspective. Tosome extent, they will be\\nintelligible and useful even toanovice inthe foundations ofstatistics,\\nbut they are necessarily somewhat technical and will therefore take on\\nnew meaning ifyou return tothem asyour reading inthis book and\\nelsewhere progresses.\\nThe book falls into two parts. The first, ending with Chapter 7, is a\\ngeneral introduction tothe personalistic tradition inprobability and\\nutility. Were this part tobedone over, radical revision would not be\\nrequired, though Iwould now supplement the line ofargument center-\\ningaround asystem ofpostulates byother less formal approaches, each\\nconvincing initsown way, that converge tothe general conclusion that\\npersonal (or subjective) probability isagood key, and the best yet\\n1\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a153ca4d-6a70-47a0-b432-9e8865f919b6', embedding=None, metadata={'page_label': '6', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Iv PREFACE TOTHEDOVER EDITION\\nknown,toallourvalidideasabouttheapplications ofprobability. There\\nwouldalsobemanynewworkstoreportonandanalyzemorethoroughly\\nthancanbedoneinfootnotes.\\nTheoriginal aimofthesecondpartofthebook,beginning with\\nChapter8,1salltooplainly statedinthesecondcomplete paragraph on\\npage4.There,apersonalistic justification ispromised for thepopular\\nbodyofdevices developed bytheenthusiastically frequentistic schools\\nthatthenoccupied almostthewholestatistical sceneandstilldominate\\nit,though lesscompletely. Thesecondpartofthebookisindeeddevoted\\ntopersonalistic discussion offrequentistic devices, butforoneafter\\nanother itreluctantly admits thatjustification hasnotbeenfound.\\nFreudalonecouldexplainhowtherashandunfulfilled promise on\\npage4wentunamended through somanyrevisions ofthemanuscript.\\nToday, asIseeit,thetheoryofpersonal probability applied tosta-\\ntisticsshowsthatmanyoftheprominent frequentistic devices canat\\nbestleadtoaccidental andapproximate, notsystematic andcogent, suc-\\nGess,asIsexpanded upon,perhaps moreoptimistically, byPratt(1965).\\nAmongtheill-founded frequentistic devices areminimax rules,almost\\nalltail-area tests,tolerance intervals, and,inasortofclassbyitself,\\nfiducial probability.\\nIfIhavelostfaithinthedevices ofthefrequentistic schools, Ihave\\nlearnednewrespect forsomeoftheirgeneral theoretical ideas.Letme\\namplify firstinconnection withtheNeyman-Pearson school.While\\ninsisting onlong-run frequency asthebasisofprobability, thatschool\\nwiselyemphasizes theultimate subjectivity ofstatistical inference or\\nbehavior withintheobjective constraint of‘‘admissibility,’’ asin(Leh-\\nmann1958;Wolfowitz 1962).Butcarefulstudyofadmissibility leads\\nalmostinexorably totherecognition ofpersonal probabilities andtheir\\ncentral roleinstatistics (Savage 1961,Section 4;1962,pp.170-175),\\nsopersonalistic statistics appears asanatural latedevelopment ofthe\\nNeyman-Pearson ideas.\\nOneconsequence ofthissortofanalysis ofadmissibility istheex-\\ntremely important likelihood principle, acorollary ofBayes’theorem,\\nofwhichIwasnotevenawarewhenwritingthefirsteditionofthisbook.\\nThisprinciple, inferable from,though nominally atvariance with,\\nNeyman-Pearson ideas(Birnbaum 1962),wasfirstputforward by\\nBarnard (1947)andbyFisher (1955),members ofwhatmightbe\\ncalledtheFisherschooloffrequentists. Seealso(Barnard 1965;Bar-\\nnardetal.1962;Cornfield 1966).\\nTheviewsjustexpressed areevidently controversial, andifIhave\\npermitted myselfsuchexpressions as‘‘show’’ and‘‘inexorably,’’ they\\narenotmeantwithmathematical finality. Yet,controversial though\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f274e552-e313-4883-91f7-ecf8dc4e5d48', embedding=None, metadata={'page_label': '7', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='PREFACE TOTHE DOVER EDITION Vv\\nthey may be,they aretoday shared byanumber of statisticians, who\\nmay becalled personalistic Bayesians, orsimply personalists. This book\\nhasplayed—and continues toplay—a role inthe personalistic move-\\nment, but themovement itself has other sources apart from those from\\nwhich this book itself was drawn. One with great Impact onpractical\\nstatistics and scientific management isabook byRobert Schlaifer\\n(1959). This isawelcome opportunity tosay that hisideas were devel-\\noped wholly independently ofthe present book, and indeed ofother\\npersonalistic literature. They are infull harmony with the ideas in\\nthis book but aremore down toearth and less spellbound bytradition.\\nL.J.SAVAGE\\nYale Unwersity\\nJune, 1971\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e14020df-81ce-4bdb-9258-c6f74528b355', embedding=None, metadata={'page_label': '8', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c3ef2218-b016-48f3-814a-c3e2d434db83', embedding=None, metadata={'page_label': '9', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface tothe First Edition\\nABOOK ABOUT SOCONTROVERSIAL ASUBJECT ASTHE FOUNDATIONS\\nofstatistics may have some valuein theclassroom, asIhope this one\\nwill; but itcannot beatextbook, ormanual ofinstruction, stating the\\naccepted facts about itssubject, forthere scarcely areany. Openly, or\\ncoyly screened behind the polite conventions ofwhat wecall adisinter-\\nested approach, itmust, even more than other books, beanairing of\\nitsauthor’s current opinions.\\nOne whoso airs hisopinions has serious misgivings that (asmay be\\njudged from other prefaces) heoften tries tocommunicate along with\\nhisbook. First, helongs toknow, forreasons that arenot altogether\\nnoble, whether heisreally making avaluable contribution. Hisown\\nconceit, the encouragement offriends, and the confidence ofhispub-\\nlisher have given him hope, but heknows that the hopes ofothers in\\nhisposition have seldom been fully realized.\\nAgain, what hehas written isfarfrom perfect, even tohisbiased\\neye. Hehas stopped revising and called thebook finished, because\\none must sooner or later.\\nFinally, hefears that hehimself, and still more such public ashe\\nhas, will forget that thebook istentative, that anauthor’s most recent\\nword need notbehislast word.\\nThe application ofstatistics interests some workers inalmost every\\nfield ofempirical investigation—not only inscience, but also incom-\\nmerce and industry. Moreover, the foundations ofstatistics are con-\\nnected conceptually with many disciplines outside ofstatistics itself,\\nparticularly mathematics, philosophy, economics, and psychology—a\\nsituation that, incidentally, must augment the natural misgivings of\\nanauthor inthis field about hisown competence. Those who read in\\nthisbook may, therefore, bediverse inbackground and interests. With\\nthis consideration inmind, Ihave endeavored tokeep thebook asfree\\nfrom technical prerequisites asitssubject matter and itsrestriction to\\nareasonable size permit.\\nTechnical knowledge ofstatistics isnowhere assumed, but the reader\\nwho has some general knowledge ofstatistics will bemuch better pre-\\npared tounderstand and appraise this book. The books Statistics, by\\nL.H. C.Tippett, and On the Principles ofStatistical Inference by\\nvu\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7a076423-23f6-4569-afce-643040b4c7f9', embedding=None, metadata={'page_label': '10', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='vill PREFACE TOTHEFIRSTEDITION\\nA. Wald, listed intheBibliography attheendofAppendix 3,areshort\\nauthoritative introductions tostatistics, eitherofwhichwouldprovide\\nsomestatistical background forthisbook.ThebooksofTippettand\\nWaldaresodifferent intoneandemphasis thatitwouldbynomeans\\nbewasteful toreadthemboth,inthatorder.\\nAnybutthemostcasualreadershouldhavesomeformal preparation\\ninthetheory ofmathematical probability. Thoseacquainted with\\nmoderately advanced theoretical statistics willautomatically havethis\\npreparation; othersmayacquire it,forexample, byreading Theory of\\nProbability, byM.E.Munroe, orselected partsofAnIntroduction to\\nProbability TheoryandItsApplications, byW.Feller,according to\\ntheirtaste.InFeller’s book,athorough reading oftheIntroduction\\nandChapter 1,andacasualreading ofChapters 5, 7,and8wouldbe\\nsufficient.\\nTheexplicit mathematical prerequisites arenotgreat;ayearofcal-\\nculuswouldinprinciple bemorethanenough. But,inpractice, read-\\nerswithoutsometraining informallogicoroneoftheabstract branches\\nofmathematics usuallytaughtonlyaftercalculus will,Ifear,findsome\\nofthelongthough elementary mathematical deductions quiteforbid-\\nding.Forthesakeofsuchreaders, Itherefore takethelibertyof giv-\\ningsomepedagogical advicehereandelsewhere thatmathematically\\nmoremature readers willfindsuperfluous andpossibly irritating. In\\nthefirstplace, itcannotbetoostrongly emphasized thatalongmathe-\\nmaticalargument canbefullyunderstood onfirstreading onlywhenit\\nisveryelementary indeed, relative tothereader’smathematical knowl-\\nedge.Ifonewantsonlythegistofit,hemayreadsuchmaterial once\\nonly;butotherwise hemustexpecttoreaditatleastonceagain. Seri-\\nousreading ofmathematics isbestdonesittingboltuprightonahard\\nchairatadesk.Pencilandpaperarenearlyindispensable; forthere\\narealways figurestobesketched andstepsintheargumenttobeveri-\\nfiedbycalculation. Inthisbook,asinmanymathematical books,\\nwhenexercises areindicated, itisabsolutely essential thattheybe\\nreadandnearlyessential thattheybeworked, because theyconstitute\\npartoftheexposition, theexercise formbeingadopted where itseems\\ntotheauthorbestforconveying theparticular information athand.\\nTosomemathematicians, andevenmoretologicians, Imustsaya\\nwordofapology forwhattheymayconsider lapsesofrigor,suchas\\nusingthesamesymbolwithmorethanonemeaning andfailing todis-\\ntinguish uniformly between theuseandthemention ofasymbol; but\\ntheywillunderstand thattheselapsesaresacrifices towhatItaketo\\nbegeneral intelligibility andwillhave,Ihope,norealdifficulty inre-\\npairingthem.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dfe81277-5a09-4db3-b26f-1caad27834c6', embedding=None, metadata={'page_label': '11', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='PREFACE TOTHEFIRSTEDITION 1X\\nFewwillwishtoreadthewholebook;therefore introductions tothe\\nchapters andsections havebeensowritten asnotonlytoprovide orien-\\ntationbutalsotofacilitate skipping. Inparticular, safedetours are\\nindicated aroundmathematically advanced topicsandotherdigressions.\\nAfewwordsinexplanation oftheconventions, suchasthosebywhich\\ninternalandexternal references aremadeinthisbook,maybeuseful.\\nTheabbreviation §3.4meansSection 4ofChapter 3;withinChapter\\n3itself,thiswouldbeabbreviated stillfurtherto§4.Theabbreviation\\n(3.4.1)meansthefirstnumbered anddisplayed equation orotherex-\\npression in§3.4;withinChapter 3,thiswouldbeabbreviated still\\nfurther to(4.1)andwithin §3.4simply to(1).Theorems, lemmas,\\nexercises, corollaries, figures,andtablesarenamedbyasimilarsystem,\\ne.g.,Theorem 3.4.1,Theorem 4.1,Theorem 1.Incidentally, theproofs\\noftheorems areterminated withthespecialpunctuation mark@,a\\ndeviceborrowed fromHalmos’s Measure Theory.\\nSevenpostulates, Pl,P2,etc.,areintroduced overthecourse of\\nseveralchapters. Forreadyreference theseare,withsomeexplanatory\\nmaterial, reproduced ontheendpapers.\\nEntries intheBibliography attheendofAppendix 3aredesignated\\nbyaself-explanatory notation insquarebrackets. Forexample, the\\nworksofTippett, Wald,Munroe, Feller,andHalmos, already referred\\nto,are[T2],[W1], [M6], [F1],and[H2],respectively.\\nIoftenalludetoasetofkeyreferences toagiventopic.Thismeans\\nasetofexternal references intended toleadthereaderthatwishesto\\npursuethatparticular topictothefullestandmostrecentbibliographies;\\nithasnothing todowiththemeritorimportance oftheworksreferred to.\\nTechnical terms(except fornon-verbal symbols) thataredefined in\\nthisbookareprinted inboldfaceoritalics(depending ontheimpor-\\ntanceofthetermforthisbookorforestablished usage)inthecontext\\nwherethetermisdefined. Thesespecial fontsareoccasionaily used\\nforotherpurposes aswell.Termsaresometimes usedinformally—\\neveninunofficial definitions—before beingofficially defined. Eventhe\\nofficialdefinitions aresometimes ofnecessity veryloose,corresponding\\ntothewell-known principle that,inaformaltheory,sometermsmust\\ninstrictlogicbeleftundefined.\\nL.J.SAVAGE\\nUniversity ofChicago\\nApril,1954\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='64016060-3276-4393-823d-cb364a885328', embedding=None, metadata={'page_label': '12', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec5afa70-a0b5-421a-abd7-e72e398d7092', embedding=None, metadata={'page_label': '13', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Acknowledgement\\nIHAVEMANYFRIENDS, FEWOFWHOMSHAREMYPRESENT OPIN-\\nions,tothankforcriticism andencouragement. Though thelistseems\\nlong,Icannotrefrainfromexplicitly mentioning: I.Bross,A.Burks,\\nR.Carnap, B.deFinetti,M.Flood, I.J.Good,P.R.Halmos, O.Hel-\\nmer,C.Hildreth, T.Koopmans, W.Kruskal, C.F.Mosteller, I.R.\\nSavage,W.A.Wallis,andM.A.Woodbury. Wallisaschairman of\\nmydepartment andclosefriendhasparticularly encouraged meto\\nwritethebookandfacilitated mydoingsoinmanyways.Mrs.Janet\\nLowreyandMissLouiseForsyth typedandretyped anddidsomany\\notherpainstaking taskssowellthatitwouldbeinadequate tocall\\ntheirhelpsecretarial.\\nMyworkonthebookwasmadepossible byfourorganizations to\\nwhichIherewith express thanks. During theyears1950through 1954\\nIworkedonitattheUniversity ofChicago, wheretheworkwassup-\\nportedbytheOfficeofNavalResearch andtheUniversity itself,which\\nalsosupported itduringthesummer of1952.During theacademic\\nyear1951-52 Iworkedonitasaresearch scholar inFranceunderthe\\nFulbright Act(PublicLaw584,79thCongress), andduringthewhole\\nofthatyearasafellowoftheJohnSimon Guggenheim Memorial\\nFoundation.\\nL.J.S.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dad25c37-9f98-4ac5-b3aa-4a5c205ef340', embedding=None, metadata={'page_label': '14', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c91c58e9-54cc-45c3-8df7-5a6cdef855bb', embedding=None, metadata={'page_label': '15', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents\\nPostulates ofapersonalistic theory ofdecision .\\n1.INTRODUCTION\\n1.\\n2.\\n3.The role offoundations .\\nHistorical background ;\\nGeneral outline ofthis book ..End papers\\n2.PRELIMINARY CONSIDERATIONS ONDECISION INTHE FACE OFUNCERTAINTY\\n.Introduction .\\nThe person . . ;\\nThe world, and states oftheworld ;\\nEvents ; .\\n.Consequences, acts, and decisions\\n.The simple ordering ofacts with respect topreference |\\n.The sure-thing principle .\\n3.PERSONAL PROBABILITY\\n.Introduction .\\nQualitative personal probability\\n.Quantitative personal probability\\nSome mathematical details\\n.Conditional probability, qualitative and quantitative ,\\n.The approach tocertainty through experience\\nSymmetric sequences ofevents .\\n4,CritTIcCAL COMMENTS ONPERSONAL PROBABILITYOo\\nRm\\n&\\nNe.Introduction . ; ,\\n.Some shortcomings ofthe personalistic view .\\n.Connection with other views.\\n.Criticism ofother views.\\n.The role ofsymmetry inprobability. ,\\n.How canscience useapersonalistic view ofprobability?\\n5.UOTriiry\\nOoOF em&W bo.Introduction .\\n.Gambles\\n.Utility, and preference among -gambles\\n.The extension ofutility tomore general acts .\\n.Small worlds , ;\\n.Historical and critical comments onutility\\nxiii10\\n13\\n17\\n21\\n27\\n30\\n33\\n40\\n43\\n46\\n50\\n56\\n57\\n60\\n60\\n63\\n67\\n69\\n70\\n73\\n76\\n82\\n91\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1062d0fa-bf09-4b07-a115-3c813dd2f3fd', embedding=None, metadata={'page_label': '16', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='XIV CONTENTS\\n6.OBSERVATION\\n1.Introduction. .......... re\\n2.What anobservationis ........2...2..\\n3.Multiple observations, and extensions ofobservations and ofsets of\\nacts .... toe eeee\\n4.Dominance and admissibility Cnee\\n5.Outline ofthe design ofexperiments ..............\\n7.PARTITION PROBLEMS\\n.Introduction. ...... Cone ekee\\n.Structure of(twofold) partition problemsre\\n.The value ofobservation ........2..2.. Coeee\\n.Extension ofobservations, and sufficient statistics ..... Loe\\n.Likelihood ratios . 2. 2.2. 1. weee\\n.Repeated observations .. . Cok ee\\n.Sequential probability ratio proceduresre\\n.Standard form, and absolute comparison between observations Lone CON\\nOOF\\nWN\\n8.STATISTICS PROPER\\n1.Introduction. ...Ck\\n2.What isstatistics proper?Ck\\n3.Multipersonal problems .........2.2.2.2..08. 808848\\n4.The minimax theory ......... 2... 0. 0.ee ee eee\\n9.INTRODUCTION ToTHE MINIMAx THEORY\\n.Introduction. .........2... Coee eeee a\\n.The behavioralistic outlook .2...2... ee,\\n.Mixedacts .... 1... eee ee re\\n.Income andloss ,\\n.The minimax rule, and theprinciple ofadmissibility\\n.Illustrations oftheminimax rule . Los\\n.Objectivistic motivation oftheminimax rule Lo.\\n8.Loss asopposed tonegative income intheminimax rule\\n“IO\\nOm\\n©\\nNe\\n10.APERSONALISTIC REINTERPRETATION OFTHE MINIMAX THEORY\\n1.Introduction. . . .ee\\n2.Amodel ofgroup decision ;\\n3.The group minimax rule, and thegroup principle ofadmissibility\\n4,Critique ofthegroup minimax rule .\\n11.Tae PARALLELISM BETWEEN THE MINIMAX THEORY AND THE THEORY OF\\nTwo-PERsSON GAMES\\n1.Introduction. .2...2.\\n2.Standard games\\n3.Minimax play .... . Lone\\n4,Parallelism and contrast with the 1minimax theories Lone eeee\\n12.THe Matuematics orMinimax PROBLEMS\\n1.Introduction. ........2.... re\\n2.Abstract games .2... 11.ee ee ee105\\n106\\n111\\n114\\n116\\n120\\n121\\n125\\n128\\n134\\n140\\n142\\n148\\n154\\n154\\n154\\n156\\n158\\n159\\n162\\n163\\n164\\n165\\n168\\n169\\n172\\n172\\n173\\n174\\n178\\n178\\n180\\n180\\n184\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97cba32e-711c-42fe-bd88-83439daa77e3', embedding=None, metadata={'page_label': '17', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CONTENTS XV\\n3.Bilinear games. 2.2 12. 186\\n4,Anexample ofabilinear game. .........2.2.2..02848. 189\\n5.Bilinear games exhibiting symmetry ............46. 193\\n13.OBJECTIONS TOTHE Minimax RULES\\n1.Introduction. .2... 1.we eeee eeee 200\\n2.Aconfusion between loss and negative income .......... 200\\n3.Utility and theminimaxrule ..........2..2.2..2.2.. 201\\n4,Almost sub-minimax acts ......... 2.08. eee .. .203\\n5.The minimax rule does not generate asimple ordering ...... 205\\n14.THe Minimax THrory APPLIED TOOBSERVATIONS\\n1.Introduction. ............. re 208\\n2.Recapitulation ofpartition problems ..... .Loe eeee 208\\n3.Sufficient statistics .2... 2... Coe ee ee ewee we) 6212\\n4,Simple dichotomy, anexample.........2.2.2.2.2..048. 212\\n5.The approach tocertainty. .......... 2.2.48 802 ee 214\\n6.Cost ofobservation. . 2.2. 1wweeee 214\\n7.Sequential probability ratio procedures ........2.2.2.4.4. 215\\n8.Randomization ...... 1... ee ee ee eeee 216\\n9.Mixed acts instatistics .2... 2... 1ee eeee 217\\n15,Point ESTIMATION\\nl.Introduction. .2... 2we eeee 220\\n2.The verbalistic concept ofpoint estimation ........2.2.. 221\\n3.Examples ofproblems ofpoint estimation. .......2..2.. 221\\n4,Criteria that have been proposed forpoint estimates ....... 223\\n5.Abehavioralistic review ofthe criteria forpoint estimation ... .229\\n6.Abehavioralistic review, continued. .......2.2.2.2..4284. 234\\n7.Abehavioralistic review, concluded. ........2.2.2.2248. 244\\n16.TESTING\\n1.Introduction. .. 2... 2.ee 246\\n2.Atheory oftesting... 22... eeee ee eeee 247\\n3.Testing inpractice . .2... 1weeeee 252\\n17.INTERVAL ESTIMATION AND RELATED Topics\\n1.Estimates oftheaccuracy ofestimates ........2..2.2.. 257\\n2.Interval estimation and confidence intervals ....... .. . . 259\\n3.Tolerance intervals... 2... 1 1weee 262\\n4,Fiducial probability ...... ee ke 262\\nAPPENDIX 1.ExprcTED VALUE... ... 1... ee ee eeee 263\\nAPPENDIX 2.CoNvEX FUNCTIONS. ..........2 008. .. . .266\\nAppenpix 8. BrBuioGRaAPHIc MATERIAL ........... 2.8.8084 270\\nAPPENDIX 4.BIBLIOGRAPHIC SUPPLEMENT ...........-+.. 283\\nTECHNICAL SYMBOLS . . . 1 1 1we 299\\nAuTHor INDEX ... 1... eeee ee 301\\nGENERAL INDEX ... . 11 we we ee ee te we ew ekek 305\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a316bde1-069b-4d1d-8ee6-22bd71c46f5c', embedding=None, metadata={'page_label': '18', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbfe3f46-754f-4de7-baf5-2f6bd5a49f90', embedding=None, metadata={'page_label': '19', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 1\\nIntroduction\\n1Theroleoffoundations\\nItisoftenarguedacademically thatnosciencecanbemoresecure\\nthanitsfoundations, andthat,ifthereiscontroversy aboutthefoun-\\ndations, theremustbeevengreatercontroversy aboutthehigherparts\\nofthescience. Asamatter offact,thefoundations arethemostcon-\\ntroversial partsofmany, ifnotall,sciences. Physicsandpuremathe-\\nmaticsareexcellent examples ofthisphenomenon. Asforstatistics,\\nthefoundations include, onanyinterpretation ofwhich Ihaveever\\nheard,thefoundations ofprobability, ascontroversial asubject asone\\ncouldname.Asinothersciences, controversies overthefoundations\\nofstatistics reflectthemselves tosomeextentineveryday practice, but\\nnotnearlysocatastrophically asonemightimagine. Ibelieve that\\nhere,aselsewhere, catastrophe isavoided, primarily because inprac-\\nticalsituations commonsensegenerally savesallbutthemostpedantic\\nofusfromflagrant error. Itishardtojudge,however, towhatextent\\ntherelative calmofmodern statistics isduetoitsdomination bya\\nvigorous schoolrelatively wellagreedwithin itselfaboutthefoundations.\\nAlthough studyofthefoundations ofasciencedoesnothavethe\\nrolethatwouldbeassigned toitbynaivefirst-things-firstism, ithasa\\ncertaincontinuing importance asthescience develops, influencing, and\\nbeinginfluenced by,themoreimmediately practical partsofthescience.\\n2Historical background\\nTheconcept andproblem ofinductive inference havebeenpromi-\\nnentinphilosophyatleastsinceAristotle. Mathematical workonsome\\naspects oftheproblem ofinference datesbackatleasttotheearly\\neighteenth century. Leibniz issaidtobethefirsttopublish asugges-\\ntioninthatdirection, butJacobBernoulli’s posthumous ArsConjec-\\ntandi(1713)[B12]seemstobethefirstconcerted effort.{ Thismathe-\\n+Valuable information onthisandothertopicsoftheearlyphilosophic history of\\nprobability isattractively presented inKeynes’ treatise [K4],especially inChapters\\nVII,XXIII,andthebibliography.\\n]\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='043d744e-e0f5-4cb4-a09b-570f60647041', embedding=None, metadata={'page_label': '20', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2 INTRODUCTION [1.2\\nmaticalworkhasalwaysrevolved aroundtheconcept ofprobability;\\nbut,though therewasactiveinterest inprobability fornearlyacen-\\nturybeforethepublication ofArsConjectandi, earlieractivity seems\\nnottohavebeenconcerned withinductive inference.\\nInthepresent century therehasbeenandcontinues tobeextra-\\nordinary interest inmathematical treatment ofproblems ofinductive\\ninference. Forreasons Icannotandneednotanalyze here,thisac-\\ntivityhasbeenstrikingly concentrated intheEnglish-speaking world.\\nItisknownunderseveralnames,mostofwhichstresssomeaspectof\\nthesubject thatseemed ofoverwhelming importance atthemoment\\nwhenthenamewascoined. ‘Mathematical statistics,” oneofits\\nearliest names,isstillthemostpopular. Inthisname,‘‘mathematical’”’\\nseemstobeintended toconnote rational, theoretical, orperhaps mathe-\\nmatically advanced, todistinguish thesubjectfromthoseproblems of\\ngathering andcondensing numerical datathatcanbeconsidered apart\\nfromtheproblem ofinductive inference, themathematical treatment\\nofwhich isgenerally relatively trivial.Thename“statistical inference’’\\nrecognizes thatthesubject isconcerned withinductive inference. The\\nname“statistical decision” reflectstheideathatinductive inference is\\nnotalways, ifever,concerned withwhattobelieve inthefaceofin-\\nconclusive evidence, butthatatleastsometimes itisconcerned with\\nwhatactiontodecideuponundersuchcircumstances. Within this\\nbook,therewillbenoharminadopting theshortest possible name,\\n“statistics.”\\nItisunanimously agreedthatstatistics depends somehow onproba-\\nbility. But,astowhatprobability isandhowitisconnected with\\nstatistics, therehasseldombeensuchcomplete disagreement andbreak-\\ndownofcommunication sincetheTower ofBabel. Theremustbe\\ndozens ofdifferent interpretations ofprobability defended byliving\\nauthorities, andsomeauthorities holdthatseveral different interpreta-\\ntionsmaybeuseful,that1s,thattheconcept ofprobability mayhave\\ndifferent meaningful sensesindifferent contexts. Doubtless, muchof\\nthedisagreement ismerely terminological andwoulddisappear under\\nsufficiently sharpanalysis. Somebelieve thatitwould alldisappear,\\noreventhattheyhavethemselves alreadymadethenecessary\\nanalysis.\\nConsidering theconfusion aboutthefoundations ofstatistics, it is\\nsurprising, andcertainly gratifying, tofindthatalmosteveryone is\\nagreedonwhatthepurelymathematical properties ofprobability are.\\nVirtually allcontroversy therefore centersonquestions ofinterpreting\\nthegenerally accepted axiomatic concept ofprobability, thatis,ofde-\\ntermining theextramathematical properties ofprobability.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90881190-8039-4c3b-9d9a-5759f0fea408', embedding=None, metadata={'page_label': '21', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.2] HISTORICAL BACKGROUND 3\\nThe widely accepted axiomatic concept referred toiscommonly as-\\ncribed toKolmogoroff [K7] and goes byhisname. Itshould bemen-\\ntioned that there issome dissension from itonthe part ofasmall group\\nledbyvon Mises [V2]. There arealso afewminor technical variations\\nontheKolmogoroff system that aresometimes of interest; they will be\\ndiscussed in§ 3.4.\\nIwould distinguish three main classes ofviews onthe interpretation\\nofprobability, forthe purposes ofthis book, calling them objectivistic,\\npersonalistic, and necessary. Condensed descriptions ofthese three\\nclasses ofviews seem called forhere. Ifsome readers find these descrip-\\ntions condensed tothe point ofunintelligibility, letthem beassured\\nthat fuller ones will gradually bedeveloped asthebook proceeds.\\nObjectivistic views hold that some repetitive events, such astosses\\nofapenny, prove tobeinreasonably close agreement with themathe-\\nmatical concept ofindependently repeated random events, allwith the\\nsame probability. According tosuch views, evidence forthe quality\\nofagreement between the behavior ofthe repetitive event and the\\nmathematical concept, and forthe magnitude ofthe probability that\\napplies (incase any does), istobeobtained byobservation ofsome\\nrepetitions ofthe event, and from noother source whatsoever.\\nPersonalistic views hold that probability measures the confidence\\nthat aparticular individual has inthe truth ofaparticular proposition,\\nforexample, the proposition that itwill rain tomorrow. These views\\npostulate that the individual concerned isinsome ways “reasonable,”’\\nbut they donot deny the possibility that two reasonable individuals\\nfaced with thesame evidence may havedifferent degrees ofconfidence\\ninthe truth ofthesame proposition.\\nNecessary views hold that probability measures the extent towhich\\none set ofpropositions, out oflogical necessity and apart from human\\nopinion, confirms the truth ofanother. They are generally regarded\\nbytheir holders asextensions of logic, which tells when oneset ofprop-\\nositions necessitates the truth ofanother.\\nAfter what hasbeen said about the intensity and complexity ofthe\\ncontroversy over the probability concept, you must realize that the\\nshort taxonomy aboveis bound toinfuriate any expert onthe founda-\\ntions ofprobability, but Itrust itmay dothe less learned more good\\nthan harm.\\nThe great burst ofstatistical research intheEnglish-speaking world\\ninthe present century has revolved around objectivistic views onthe\\ninterpretation ofprobability. Aswill shortly beexplained, any purely\\nobjectivistic view entails asevere difficulty for statistics. This diffi-\\nculty isrecognized bymembers ofthe British-American School, ifI\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='78f44907-e564-4aab-b456-d72ff53c582b', embedding=None, metadata={'page_label': '22', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4 INTRODUCTION [1.3\\nmayusethatnamewithout itsbeingtaken tooliterally oratallna-\\ntionalistically, andisregarded bythemasagreat,though notinsur-\\nmountable, obstacle; indeed,someofthemseeitasthecentralproblem\\nofstatistics.\\nThedifficulty intheobjectivistic position isthis.Inanyobjecti-\\nvisticview,probabilities canapplyfruitfully onlytorepetitive events,\\nthatis,tocertain processes; and(depending ontheviewinquestion)\\nitiseithermeaningless totalkabouttheprobability thatagivenpropo-\\nsitionistrue,orthisprobability canbeonly1or0,according asthe\\nproposition isinfacttrueorfalse.Underneither interpretation can\\nprobability serveasameasure ofthetrusttobeputintheproposition.\\nThustheexistence ofevidence foraproposition cannever,onanob-\\njectivistic view,beexpressed bysayingthattheproposition istruewith\\nacertain probability. Again, ifonemustchooseamongseveralcourses\\nofactioninthelightofexperimental evidence, itisnotmeaningful, in\\ntermsofobjective probability, tocompute whichoftheseactions 1s\\nmostpromising, thatis,whichhasthehighestexpected income. Hold-\\nersofobjectivistic viewshave,therefore, norecourse buttoarguethat\\nitisnotreasonable toassignprobabilities tothetruthofpropositions\\nor tocalculate whichofseveral actions isthemostpromising, andthat\\ntheneedexpressed bytheattempt tosetupsuchconcepts mustbe\\nmetinotherways, ifatall.\\nTheBritish-American Schoolhashadgreatsuccess inseveral re-\\nspects.Thenumberofitsadherents hasrapidly increased. Ithascon-\\ntributed manyprocedures ofstrongintuitive appealand(onefeels)of\\nlasting worth. Thesehavefoundwidespread application inmany\\nsciences, inindustry, andincommerce. Thesuccess oftheschoolmay\\npragmatically betakenasevidence forthecorrectness ofthegeneral\\nviewonwhich itisbased. Indeed, anyonewhooverthrows thatview\\nmusteitherdiscredit theprocedures towhich ithasled,orshow,as\\nIhopetoshowinthisbook,thattheyareonthewholeconsistent with\\nthealternative proposed.\\nSome, Iamongthem,holdthatthegrounds foradopting anobjec-\\ntivisticviewarenotoverwhelmingly strong; thatthereareserious log-\\nicalobjections toanysuchview;and,mostimportant ofall,thatthe\\ndifficulty astrictly objectivistic viewmeetsinstatistics reflects real\\ninadequacy.\\n3General outline ofthisbook\\nThisbookpresents atheory ofthefoundations ofstatistics which is\\nbasedonapersonalistic viewofprobability derived mainlyfromthe\\nworkofBrunodeFinetti, asexpressed forexample in[D2].Thetheory\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2186fd42-3f48-4b38-87d7-67a2460968e2', embedding=None, metadata={'page_label': '23', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.3] GENERAL OUTLINE OFTHIS BOOK 5\\nispresented inatentative spirit, forIrealize that theserious blemishes\\ninitapparent tomearenot the only ones that will bediscovered by\\ncritical readers. Atheory ofthefoundations ofstatistics that appears\\ncontrary tothe teaching ofthemost productive statisticians will prop-\\nerly beregarded with extraordinary caution. Other views onproba-\\nbility will, ofcourse, bediscussed inthis book, partly fortheir own in-\\nterest and partly toexplain the relationship between the personalistic\\nview on which this book isbased and other views.\\nThe book isorganized into seventeen chapters, ofwhich the present\\nintroduction isthe first. Chapters 2—7 are, sotospeak, concerned with\\nthefoundations atarelatively deep level. They develop, explain, and\\ndefend acertain abstract theory ofthe behavior ofahighly idealized\\nperson faced with uncertainty. That theory isshown tohave asim-\\nplications atheory ofpersonal probability, corresponding tothe per-\\nsonalistic view ofprobability basic tothis book, and also atheory of\\nutility due, initsmodern form, tovon Neumann and Morgenstern\\n[V4].\\nThereisatransition, occurring inChapter 8and maintained through-\\noutthe rest ofthebook, toashallower level ofthe foundations of sta-\\ntistics; Imight say from pre-statistics tostatistics proper. Inthose\\nlater chapters, itisrecognized that the theory developed inthe earlier\\nones istoo highly idealized forimmediate application. Some compro-\\nmises have tobemade, and theappropriate ones aresought inananal-\\nysis ofsome oftheinventions andideas oftheBritish-American School.\\nItwill, Ihope, bedemonstrated thereby that the superficially incom-\\npatible systems ofideas associated ontheonehand with apersonalistic\\nview ofprobability and onthe other with the objectivistically inspired\\ndevelopments oftheBritish-American School doinfact lend each other\\nmutual support and clarification.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='571b2000-081e-488f-9b4a-2a07c3934ad3', embedding=None, metadata={'page_label': '24', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 2\\nPreliminary Considerations\\nonDecision in\\ntheFaceofUncertainty\\n1Introduction\\nDecisions madeinthefaceofuncertainty pervade thelifeofevery\\nindividual andorganization. Evenanimals mightbesaidcontinually\\ntomakesuchdecisions, andthepsychological mechanisms bywhich\\nmendecidemayhavemuchincommon withthosebywhichanimals\\ndoso.Butformalreasoning presumably playsnoroleinthedecisions\\nofanimals, littleinthoseofchildren, andlessthanmightbewishedin\\nthoseofmen.Itmaybesaidtobethepurpose ofthisbook,andin-\\ndeedofstatistics generally, todiscusstheimplications ofreasoning for\\nthemaking ofdecisions.\\nReasoning iscommonly associated withlogic,butitisobvious, as\\nmanyhavepointed out,thattheimplications ofwhatisordinarily\\ncalledlogicaremeager indeedwhenuncertainty istobefaced. Ithas\\ntherefore oftenbeenaskedwhether logiccannotbeextended, byprin-\\nciplesasacceptable asthoseoflogicitself,tobearmorefullyonun-\\ncertainty. Anattempt toextend logicinthiswaywillbebegunin\\nthischapter, differing intwoimportant respects frommost,butnot\\nall,otherattempts.\\nFirst,sincelogicisconcerned withimplications among propositions,\\nmanyhavethought itnatural toextend logicbysettingupcriteria for\\ntheextenttowhichoneproposition tendstoimply,orprovideevidence\\nfor,another. Itseemstomeobvious, however, thatwhatisultimately\\nwanted iscriteria fordeciding among possible courses ofaction;and,\\ntherefore, generalization oftherelation ofimplication seemsatbesta\\nroundabout method ofattack. Itmustbeadmitted thatlogicitself\\ndoesleadtosomecriteria fordecision, because whatisimpliedbya\\nproposition knowntobetrueisinturntrueandsometimes relevant to\\nmaking adecision. Shouldsomenotion ofpartialimplication bede-\\nmonstrably evenbetterarticulated withdecision thanisimplication it-\\n6\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7565cfc3-b828-4252-b3bd-4a5e30a2eda5', embedding=None, metadata={'page_label': '25', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.2] THE PERSON 7\\nself, that would beexcellent; buthow issuch anotion tobesought ex-\\ncept byexplicitly studying decision? Ramsey’s discussion in[R1] of\\nthe point atissue hereis especially forceful.\\nSecond, itisappealing tosuppose that, iftwo individuals inthesame\\nsituation, having thesame tastes and supplied with thesame informa-\\ntion, actreasonably, they will act inthesame way. Such agreement,\\nbelief inwhich amounts toanecessary (asopposed toapersonalistic)\\nview ofprobability, iscertainly worth looking for. Personally, Ibe-\\nlieve that itdoes not correspond even roughly with reality, but, hav-\\ning atthemoment nostrong argument behind my pessimism onthis\\npoint, Idonot insist onit.But Idoinsist that, until thecontrary be\\ndemonstrated, wemust beprepared tofind reasoning inadequate to\\nbring about complete agreement. Inparticular, the extensions of logic\\ntobeadduced inthis book will not bring about complete agreement;\\nand whether enough additional principles todoso,orindeed any addi-\\ntional principles ofmuch consequence, canbeadduced, Idonotknow.\\nItmay be,and indeed Ibelieve, that there isanelement indecision\\napart from taste, about which, like taste itself, there isnodisputing.\\nThe next four sections ofthis chapter build upaformal model, or\\nscheme, ofthe situation inwhich aperson isfaced with uncertainty ;\\nthe final two, interms ofthis model, motivate and state some ofthe\\nfew principles that seem tome entitled tobetaken aspostulates for\\nrational decision.\\n2The person\\nIamabout tobuild upahighly idealized theory ofthe behavior of a\\n“rational” person with respect todecisions. Indoing sowill, ofcourse,\\nhave toaskyou toagree with methat such and such maxims of behavior\\nare “rational.’’ Insofaras“rational’’ means logical, there isnolive\\nquestion; and, ifIaskyour leavethere atall, itisonly asamatter of\\nform.+ But our person isgoing tohave tomake uphismind insitua-\\ntions inwhich criteria beyond the ordinary ones oflogic will beneces-\\nsary. So,when certain maxims arepresented foryour consideration,\\nyou must ask yourself whether you try tobehave inaccordance with\\nthem, or,toput itdifferently, how you would react ifyou noticed your-\\nself violating them.\\n+The assumption that aperson’s behavior islogical is,ofcourse, farfrom vacuous.\\nInparticular, such aperson cannot beuncertain about decidable mathematical prop-\\nositions. This suggests, atleast tome, that thetempting program sketched byPolya\\n[P6] ofestablishing atheory ofthe probability ofmathematical conjectures cannot\\nbefully successful inthat itcannot lead toatruly formal theory, but deFinetti\\n[D5] seems more optimistic about theprogram.+\\n+Polya has greatly elaborated hisprogram, but not inthe direction ofseek-\\ning aformal theory. Acurious early work byCérésole (1915) issomewhat\\npertinent, and Hacking (1967) argues for the possibility ofineluding math-\\nematical uncertainty inaformal theory.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='49c3f887-6584-437d-a872-254856121d4a', embedding=None, metadata={'page_label': '26', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"8 PRELIMINARY CONSIDERATIONS ONDECISION [2.3\\nItisbrought outineconomic theorythatorganizations sometimes\\nbehave likeindividual people, sothatatheory originally intended to\\napplytopeoplemayalsoapplyto(ormayevenapplybetterto)such\\nunitsasfamilies, corporations, ornations. Inviewofthispossibility,\\neconomic theorists aresometimes reluctant tousetheword‘‘person,”’\\noreven“individual,” forthebehaving unitstowhichtheyrefer;but\\nforourpurpose “person” threatens noconfusion, though thepossi-\\nbilityofusingitinanextended sensemaywellbeborneinmind.\\n3Theworld,andstatesoftheworld\\nAformaldescription, ormodel, ofwhattheperson isuncertain about\\nwillbeneeded. Tomotivate thisformal description, letmebeginin-\\nformally byconsidering alistofexamples. Thepersonmightbeun-\\ncertainabout:\\n1.Whether aparticular eggisrotten.\\n2.Which,ifany,inaparticular dozeneggsarerotten.\\n3.Thetemperature atnooninChicago yesterday.\\n4.Whatthetemperature wasandwillbeintheplacenowcovered\\nbyChicago eachnoonfromJanuary 1,1a.p.,toJanuary 1,4000a.p.\\n5.Theinfinite sequence ofheadsandtailsthatwillresultfromre-\\npeated tossesofaparticular (everlasting) coin.\\n6.Thecomplete decimal expansion ofz.\\n7.Theexactandentirepast,present, andfuturehistory oftheuni-\\nverse,understood inanysense,however wide.\\nTheseexamples haveafewfeatures incommon, though, ifthereare\\nmorethanafew,it isadiscredit tomyimagination. Thus,ineach\\nthereissomeobjectaboutwhichtheperson isuncertain, anegg,a\\ndozeneggs,atemperature, asequence oftemperatures, etc.Eachob-\\njectadmits a certain classofdescriptions thatmightthinkably apply\\ntoit.Toillustrate, theeggofExample 1mightberottenornot;and\\nthetermsoftheexample aremeanttoexcludeanyotherdescription\\nfromconsideration, though, ofcourse,arealegghasmanyotherfea-\\ntures.Again,sinceanysubsetofthedozeneggs(including theextreme\\ncasesofallandnoneatall)mightberotten,thereare2'”descriptions\\nassociated withExample 2.ForExample 3andeachsubsequent one,\\nthereareaninfinitenumber ofdescriptions, though thearrayofde-\\nscriptions ismorecomplicated insomethaninothers,reaching theulti-\\nmateofcomplexity inExample 7.Example 6isalittleanomalous\\ninthatanything thepersondoesnotknowaboutthedescription of7\\nhecouldknowinprinciple bythinking sufficiently hardabout it,that\\nis,bylogicalone.Thispoint,banaltosomereaders, needsexplanation\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='63ef8989-0d7d-4e22-933a-608649ad6392', embedding=None, metadata={'page_label': '27', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"2.3] THE WORLD, AND STATES OFTHE WORLD 9\\nforothers. If,forexample, 7isunderstood tobethearea ofacircle of\\nunit radius, itfollows bylogic alone that 7isnot greater than the area\\nofasquare circumscribing the unit circle, that is,r<4.Byanelabo-\\nration ofthis method wzcan becomputed toany degree ofaccuracy,\\nandbyother purely logical methods many other facts about mcan be\\nestablished, such asthe fact that aisnot arational number.\\nInconnection with the concepts suggested bythe preceding para-\\ngraph, thefollowing nomenclature isproposed asbrief, suggestive, and\\ninreasonable harmony with the usages ofstatistics and ordinary dis-\\ncourse.\\nTerm Definition\\ntheworld the object about which theperson is\\nconcerned\\na,state (oftheworld) adescription oftheworld, leaving no\\nrelevant aspect undescribed\\nthetrue state (oftheworld) the state that does infact obtain, i.e.,\\nthetrue description oftheworld\\nInapplication ofthe theory, the question will arise astowhich world\\ntouse inagiven context. Thus, ifthe person isinterested inthe only\\nbrown egg inadozen, should that egg orthewhole dozen betaken as\\ntheworld? Itwill beseen asthe theory isdeveloped that inprinciple\\nnoharm isdone bytaking the larger oftwo worlds asamodel ofthe\\nsituation. One istherefore tempted toadopt, once and for all, one\\nworld sufficiently large, sayExample 7.The most serious objection to\\nthis isthat Example 7isvague, and some mathematical and philosophi-\\ncalexperience suggests that thevagueness cannot beremoved without\\nruining the universality oftheexample. Itmay also beadded that the\\nuse ofmodest little worlds, tailored toparticular contexts, isoften a\\nsimplification, the advantage ofwhich isjustified byaconsiderable\\nbody ofmathematical experience with related ideas.\\nThe sense inwhich theworld ofadozen eggs islarger than theworld\\noftheonebrown egg inthedozen isinsomerespects obvious. Itmay\\nbewell, however, toemphasize that astate ofthe smaller world corre-\\nsponds not toone state ofthe larger, but toaset ofstates. Thus,\\n“The brown egg isrotten” describes the smaller world completely, and\\ntherefore isastate ofit;but thesame statement leaves much about the\\nlarger world unsaid and corresponds toasetof2'!states ofit.Inthe\\nsense under discussion asmaller world isderived from alarger byneg-\\nlecting some distinctions between states, not byignoring somestates\\noutright. The latter sort ofcontraction may beuseful incase certain\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ec927a5-0833-414b-af11-b30819e3bab4', embedding=None, metadata={'page_label': '28', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10 PRELIMINARY CONSIDERATIONS ONDECISION (2.4\\nstatesareregarded bythepersonasvirtually impossible sothatthey\\ncanbeignored.\\n4Events\\nAneventisasetofstates.Forexample, inconnection withthe\\nworldofExample 2,thepersonmightwellbeconcerned withtheevent\\nthatexactly oneegginthedozen isrotten(aneventhaving 12states\\naselements), or,alittlelessacademically, thatatleastoneoftheeggs\\nisrotten(aneventhaving 2!”—1statesaselements, i.e.,allthestates\\nintheworldbutone).Inconnection withtheworldofExample 3,\\nthepersonmightbeconcerned withtheevent,havinganinfinitenum-\\nberofstates,thatthetemperature atnooninChicago yesterday was\\nbelowfreezing. Togiveafinalillustration, ofamoremathematical\\nflavor,consider inconnection withExample 5theeventthattheratio\\nofthenumber ofheadstotailsapproaches 3asthesequence progresses\\ntoinfinity.\\nInconnection withanygivenworld,therearetwoeventsthatare\\noftheutmost logicalimportance, though inordinary discourse itmay\\nseembanaleventomention theirexistence. Thesearetheuniversal\\nandthevacuous events. Theuniversal event,heretobesymbolized\\nbyS,istheeventhavingeverystateoftheworldaselement. Inso\\nfaras‘“‘world’”’ hasarealtechnical meaning, Sistheworld.Thevacu-\\nousevent,whichcanherebesafelyenoughsymbolized bythe0of\\narithmetic, istheeventhaving nostates aselements. Toillustrate, in\\nExample 1theeventthattheeggisrottenorgoodistheuniversal\\nevent,andthatitisbothrottenandgoodisthevacuous event.\\nIt1simportant tobeabletoexpress theideathatagiveneventcon-\\ntainsthetruestateamong itselements. English usageseemstooffer\\nnoalternative totheratherstuffyexpression, ‘‘theeventobtains.”’\\nThetheoryunderdevelopment makesnoformalreference totime.\\nInparticular, theconcept ofeventashereformulatedistimeless, though\\ntemporal ideasmaybeemployed inthedescription ofparticular events.\\nThus,itwouldnotbesaidthatLincoln’s assassination isaneventthat\\noccurred in1865andthatthenextreturnofHalley’s cometisonethat\\nwilloccurin1985,butthatLincoln’s assassination in1865andthe\\nreturn ofHalley’s comet in,butnotbefore, 1985areevents that\\nobtain.\\nModern mathematical usage,especially thatofabranch ofmathe-\\nmatics calledBoolean algebra, suggests thefollowing tableofdefini-\\ntionsinconnection withtheconcepts ofstateandevent. Someof\\nthesearesynonyms, othersabbreviations, andstillothersnewterms\\ncompounded outofold.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fee4e7ad-335f-4fc7-95de-3d11e6006356', embedding=None, metadata={'page_label': '29', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4] EVENTS 11\\nThough the notations introduced inTable 1are very elementary\\nand ofgreat utility, they arenot ordinarily taught except inconnec-\\ntion with logic orrelatively advanced mathematics. Asetofexercises\\nillustrating their use istherefore given below intheform ofanumbered\\nlist ofstatements. These statements aretrue whatever the sets A,B,\\nTABLE 1.MATHEMATICAL NOMENCLATURE PERTAINING TOSTATE AND EVENTS\\nTerm Definition\\n(Basic terms)\\nset event\\nA,B,C, generic symbols forevents\\ns,s,s” generic symbols forstates\\nS theuniversal event\\n0 thevacuous event\\n(Relations)\\nseéA. sisanelement ofA,1.e., astate inA.f\\nAC B(orBD A). Aiscontained inB,i.e., every element\\nofAisanelement ofB.\\nA=B. Aequals B,1.e., Aisthesameset asB,\\n1e.,AandBhave exactly thesame\\nelements.\\n(Constructs)\\nthecomplement ofAwith those elements ofSthat arenot inA\\nrespect toS\\n~A thecomplement ofAwith respect toS\\ntheunion ofthe A,’s\\nUA:AUBthose elements ofSthat are elements\\nofatleast one ofthe sets Aj,Ag, etc.\\ntheunion ofthe A,’s\\ntheunion ofAand B,1.e., those ele-\\nments ofSthat areelements ofAor\\nB(possibly ofboth)\\nthose elements ofSthat are elements\\nofeach ofthe sets Aj,Ag, etc.\\nthe intersection ofthe A,’s\\nthe intersection ofAand B,1.e., those\\nelements ofSthat are elements of\\nboth AandBthe intersection ofthe A;’s\\nNiAsANB\\n{Typographical note: The Porson font oftheGreek alphabet (a,B,y,6,«,f,-*-)\\nistheone almost always printed, atleast inAmerica, when mathematical constants\\nand variables aredenoted byGreek letters. The symbol eused inthis andsome other\\npublications todenote ‘element of’ is,however, the epsilon ofthe Vertical font\\n(a, 8,y,8,e,6,-°°). Some publications usethe special symbol €;and someuse ¢,\\nthePorson epsilon, presumably because ofitsresemblance to€.The latter usage\\nentails either using ¢fortwo different purposes orelse changing fonts inmid alphabet\\n(a,B,v,5,e¢,¢,°°) when constants and variables aredenoted byGreek letters.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a4270f40-6191-4b03-99d9-b9dc48f7e8c8', embedding=None, metadata={'page_label': '30', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12 PRELIMINARY CONSIDERATIONS ONDECISION (2.4\\nCmaybe.Mathematicians wouldforthemostpartverifythemby\\ntranslating themintoEnglishandappealing tocommonsense, though\\nincomplicated casesexplicit usemightbemadeofExercise 9.Dia-\\ngrams, calledVenndiagrams, inwhichsetsaresymbolized byareas,\\nasillustrated byFigure 1,areoftensuggestive.\\n ~(AUB)\"A|e@”\\nFigure 1 \\n   \\n   \\nIt1saremarkable andusefulfactthatanyuniversally validstate-\\nmentaboutsetsremainssoif,throughout, Uisinterchanged withN,\\n0withS,andCwithD.Thedualinthissenseofeachexercise should\\nbestudied alongwiththeexercise itself.Forexample, thedualof\\nExercise 7is:A>B,ifandonlyifA=AUB.Notethatthefirst\\npartsofExercises 1through 6aredualtothesecondparts.\\nItmayberemarked that,ifExercises 1-6aretakenasaxiomsand\\n7asadefinition, Exercises 8-21andalsotheduality principle follow\\nformally fromthem.Forexample, 10canbeproved thus:By7,if\\nAfBisA,thenACcB;but,by1,ANAisA;thereforeACA.\\nAgain,8canbeproved, using6,3,2,1,3,and6inthatorder,thus:\\n(1)ONA=(ANAA)NAH=(RKANA)NA\\n=nAN(ANA)=XANA=ANAA =O.\\nSuchformaldemonstration isfunandhelpsdevelop mathematical skill.\\nInthepresent exercises thenovice, however, shouldconsider itasa\\npossible supplement to,butnotasasubstitute for,demonstration by\\ninterpretation.\\nIftheexercises failtorenderthenotations familiar, itwouldbebest\\ntotalkwithsomeone towhomtheyarealready familiar orfailingthat,\\ntoreadinanyelementary bookwherethesubject istreated, forex-\\nample,Chapter II,‘‘TheBoole-Schroeder Algebra,” inthetextof\\nLewisandLangford [L7].\\nExercises illustrating Boolean algebra\\nLANA=A=AUA.\\n2(ANB)NC=AN(BNC); AUBUC=AU(BUOC.\\n(Thesefactsoftenrenderparentheses superfluous.)\\n83ANB=BNA;AUB=BUA.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3dd788bf-bb3d-45e8-a26e-d0ebab2c0cbb', embedding=None, metadata={'page_label': '31', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.5] CONSEQUENCES, ACTS,ANDDECISIONS 13\\n4AN(BUQO=(ANBUANC;AU(BNC=\\n(AUB)N (AUC).\\n5.SNA=A;0UA =A.\\n6.AN(~A)=0;AU(WA)=S.\\n7.ACB, ifandonlyifA=ANB.\\n8.ONA=O.\\n9.A=B,ifandonlyifAC BandBCA.\\n10.ACA.\\n11.(ANBCA.\\n12.1fACB,thn(ANC)C(BNC), and(AUC)C(BUC).\\n13.(AUB)CC,ifandonlyifACCandBCC.\\n144.0CACS.\\n15.AN(AUB)=A.\\n16.~(~A)=A.\\n17.~(4UB)=(~A)A(~B)(DeMorgan’s theorem).\\n18.~0=S.\\n19.AN(~AUB)=ANB.\\n20.ACB,ifandonlyif(~B)C(~A).\\n21.ACB,ifandonlyifAN(~B)=0.\\n22.~(U:A;)=N:(~A,;)(General DeMorgan’s theorem).\\n23.AU(();Bi)=1):(AUBY).\\n24.A1((1):B,)=1):(ANBp.\\n25.(Ui4s)U(UB)=Ui(4:UBp.\\n26.(1):Ad)U(11;Bs)=M)e7(AiUB)).\\n27.ACc(();B,), ifandonlyifACB;forevery i.\\n28.(1);ByCBC (U:B;)foreveryj.\\n5Consequences, acts,anddecisions\\nTosaythatadecision istobemadeistosaythatoneoftwoormore\\nactsistobechosen, ordecided on.Indeciding on anact,account\\nmustbetakenofthepossible statesoftheworld,andalsoofthecon-\\nsequences implicit ineachactforeachpossible stateoftheworld.A\\nconsequence isanything thatmayhappen totheperson.\\nConsider anexample. Yourwifehasjustbroken fivegoodeggsinto\\nabowlwhenyoucomeinandvolunteer tofinishmaking theomelet.\\nAsixthegg,whichforsomereasonmusteitherbeusedfortheomelet\\norwasted altogether, liesunbroken besidethebowl.Youmustde-\\ncidewhattodowiththisunbroken egg.Perhapsitisnottoogreatan\\noversimplification tosaythatyoumustdecideamongthree actsonly,\\nnamely, tobreakitintothebowlcontaining theotherfive,tobreakit\\nintoasaucerforinspection, ortothrow itawaywithout inspection.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc0a35b0-d9e8-47f6-ace1-1f458115635f', embedding=None, metadata={'page_label': '32', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14 PRELIMINARY CONSIDERATIONS ONDECISION [2.5\\nDepending onthestateoftheegg,eachofthesethreeactswillhave\\nsomeconsequence ofconcern toyou,saythatindicated byTable1.\\nTaBLE 1.ANEXAMPLE ILLUSTRATING ACTS,STATES,ANDCONSEQUENCES \\n \\n State\\nAct\\nGood Rotten\\nbreakintobowl six-eggomelet noomelet,andfivegoodeggs\\ndestroyed\\nbreakintosaucer|six-eggomelet,andasaucer|five-egg omelet,andasaucer\\ntowash towash\\nthrowaway five-egg omelet,andonegood|five-eggomelet\\neggdestroyed   \\nEventhelittleexample concerning theomeletsuggests howvaried\\nthethings, orexperiences, regarded asconsequences, canbe.They\\nmightingeneralinvolvemoney,life,stateofhealth,approvaloffriends,\\nwell-being ofothers,thewillofGod,oranything atallaboutwhichthe\\npersoncouldpossibly beconcerned. Consequences mightappropriately\\nbecalledstatesoftheperson, asopposed tostatesoftheworld.They\\nmightalsobereferred to,withsomeextension oftheeconomic notion\\nofincome, asthepossible incomes oftheperson. Inanyoneproblem,\\nthesetofconsequences envisaged willbedenoted byF,andtheindi-\\nvidualconsequences willbedenotedbyf, g,h,etc.Intheomeletex-\\nample,Fconsists ofthesixconsequences tabulated inTable1:six-egg\\nomelet;noomelet,andfivegoodeggsdestroyed; etc.\\nIftwodifferent actshadthesameconsequences ineverystate ofthe\\nworld,therewouldfromthepresent point ofviewbenopointincon-\\nsidering themtwodifferent actsatall.Anactmaytherefore beiden-\\ntifiedwithitspossible consequences. Or,moreformally, anactisa\\nfunction attaching aconsequence toeachstateoftheworld.Thenota-\\ntionfwillbeusedtodenoteanact,thatis,afunction, attaching the\\nconsequence f(s)tothestates.Thenotation fislogically abetter\\nnameforafunction thanthemorecustomary f(s)forexactly thesame\\nreasonthattheword“logarithm” isabettertermforlogarithm than\\n“logarithm ofx’wouldbe.Thenotational distinction involved hereis\\noftenjustifiably neglected inmathematical work,butwewillhavespe-\\ncialneedtoobserve it,atleastinconnection withacts,aswillsoonbe\\nexplained. Whenseveral actsaretobediscussed atonce,theymaybe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac4ca68a-8641-474f-8b6a-f8b5c3a1e17c', embedding=None, metadata={'page_label': '33', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.5] CONSEQUENCES, ACTS,ANDDECISIONS 15\\ndenotedbydifferent lettersthus:f,g,h;bytheuseofprimes thus:f,\\nf’,f’’;orbysubscripts thus:f;,f;.Thesetofallactsavailable ina\\ngivensituation willbedenoted byForasimilarsymbol. Intheex-\\nampleoftheomelet,Fhasthreeactsaselements. If,forexample, f\\ndenotes thefirstofthethreeactslistedinTable 1,thenfisdefined\\nthus:\\nf(good)=six-eggomelet;\\n(1)\\nf(rotten)=noomelet,andfivegoodeggsdestroyed.\\nTheargument mightberaisedthattheformaldescription ofdecision\\nthathasthusbeenerectedseemsinadequate because apersonmaynot\\nknowtheconsequences oftheactsopentohimineachstateofthe\\nworld.Hemightbesoignorant, forexample, asnottobesurewhether\\nonerotteneggwillspoilasix-egg omelet. Butinthatcasenothing\\ncouldbesimpler thantoadmitthattherearefourstatesintheworld\\ncorresponding tothetwostatesoftheeggandthetwoconceivable\\nanswers totheculinary question whether onebadeggwillspoilasix-\\neggomelet. Itseemstomeobvious thatthissolution worksinthe\\ngreatest generality, thoughathoroughgoing analysis mightnotbetriv-\\nial.Areaderinterested inthetechnicalities ofthispointorthatof\\nthesucceeding paragraph willfindanextensive discussion ofasimilar\\nproblem inChapter IIof[V4],wherevonNeumann andMorgenstern\\ndiscussthereduction ofageneralgametoitsreduced form.\\nAgain,theformaldescription mightseeminadequate inthatitdoes\\nnotprovide explicitly forthepossibility thatonedecisionmayleadto\\nanother. Thus, iftheomeletshouldbespoiledbybreaking arotten\\neggintoit,newquestions mightariseaboutwhattosubstitute for\\nbreakfast andhowtoappease yourjustifiably furious wife.But,Just\\nasinthepreceding paragraph anapparent shortcoming oftheproposed\\nmodeofdescription wasattributed toanincomplete analysis ofthe\\npossible states,hereIwouldsaythatthelistofavailable actsenvisaged\\ninTable 1isinadequate fortheinterpretation thathasjustbeenput\\nontheproblem. Wherethesingleact“‘breakintobowl”nowstands,\\nthereshouldbeseveral,suchas:“breakintobowl,andincaseofdis-\\nasterhavetoast,” ‘‘break intobowl,andincaseofdisaster takefamily\\ntoaneighboring restaurant forbreakfast.’’ Appropriate consequences\\nofthesenewactscaneasilybeimagined.\\nAshasjustbeensuggested, whatintheordinary wayofthinking\\nmightberegarded asachainofdecisions, oneleading totheotherin\\ntime,isintheformaldescription proposed hereregarded asasinglede-\\ncision.Toputitalittledifferently, itisproposed thatthechoiceofa\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e279080e-889b-452f-963e-303017b76564', embedding=None, metadata={'page_label': '34', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16 PRELIMINARY CONSIDERATIONS ONDECISION [2.5\\npolicy orplan beregarded asasingle decision. This point ofview,\\nthough not always insoexplicit aform, has played aprominent role\\ninthe statistical advances ofthe present century. For example, the\\ngreat majority ofexperimentalists, even today, suppose that the func-\\ntion ofstatistics and ofstatisticians istodecide what conclusions to\\ndraw from data gathered inanexperiment orother observational pro-\\ngram. But statisticians hold ittobelacking inforesight togather data\\nwithout aview tothemethod ofanalysis tobeemployed, that is,they\\nhold that the design and analysis ofanexperiment should bedecided\\nupon asanarticulated whole.\\nThe point ofview under discussion may besymbolized bythe prov-\\nerb, ‘‘Look before you leap,” and theone towhich itisopposed bythe\\nproverb, “You can cross that bridge when you cometo it.”’ When two\\nproverbs conflict inthis way, it1sproverbially true that there issome\\ntruth inboth ofthem, but rarely, ifever, can their common truth be\\ncaptured byasingle pat proverb. One must indeed look before he\\nleaps, insofarasthelooking isnotunreasonably time-consuming and\\notherwise expensive; but there are innumerable bridges one cannot\\nafford tocross, unless hehappens tocome tothem.\\nCarried toitslogical extreme, the“Look before you leap” principle\\ndemands that one envisage every conceivable policy forthegovernment\\nofhiswhole life (atleast from now on) initsmost minute details, in\\nthe light ofthevast number ofunknown states oftheworld, and decide\\nhere and now onone policy. This isutterly ridiculous, not—as some\\nmight think—because there might later because for regret, ifthings\\ndidnot turn out ashad been anticipated, but because thetask implied\\ninmaking such adecision isnot even remotely resembled byhuman\\npossibility. It1seven utterly beyond our powerto plan apicnic orto\\nplay agame ofchess inaccordance with the principle, even when the\\nworld ofstates and the setofavailable acts to beenvisaged areartifi-\\ncially reduced tothenarrowest reasonable limits.\\nThough the‘Look before you leap” principle ispreposterous ifcar-\\nried toextremes, Iwould nonethe less argue that itisthe proper sub-\\nject ofour further discussion, because tocross one’s bridges when one\\ncomes tothem means toattack relatively simple problems ofdecision\\nbyartificially confining attention tososmall aworld that the“Look\\nbefore you leap” principle canbeapplied there. Iamunable toformu-\\nlate criteria for selecting these small worlds and indeed believe that\\ntheir selection may be a matter ofjudgment and experience about which\\nitisimpossible toenunciate complete and sharply defined general prin-\\nciples, though something morewill besaid inthis connection in§5.5.\\nOnthe other hand, it is anoperation inwhich weall necessarily have\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc4ddb9a-83b7-442a-8ac1-b4f413a296aa', embedding=None, metadata={'page_label': '35', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.6] THE ORDER OF ACTS WITH RESPECT TOPREFERENCE 17\\nmuch experience, and one inwhich there isinpractice considerable\\nagreement.\\nInview ofthe ‘‘Look before you leap” principle, acts and decisions,\\nlike events, are timeless. The person decides ‘‘now”’ once forall; there\\nisnothing forhim towait for, because hisone decision provides forall\\ncontingencies. None the less, temporal modes ofdescription, though\\ntranslatable into atemporal ones, are often suggestive. Thus, there\\nwill beoccasion toanalyze and make frequent use ofthe idea ofdefer-\\nring adecision until anobservation relevant toithasbeen made.\\n6The simple ordering ofacts with respect topreference\\nOftwo acts fand g,itispossible that the person prefers ftog.\\nLoosely speaking, this means that, ifhe were required todecide between\\nfand g,noother acts being available, hewould decide onf.\\nThis procedure fortesting preference isnot entirely adequate, ifonly\\nbecause itfails totake account of,oreven define, the possibility that\\nthe person may not really have any preference between fand g,re-\\ngarding them asequivalent; inwhich case hischoice offshould not be\\nregarded assignificant. Ifthe person really does regard fand gas\\nequivalent, that is, ifhe isindifferent between them, then, ifforg\\nwere modified byattaching an arbitrarily small bonus toitsconse-\\nquences inevery state, the person’s decision would presumably befor\\nwhichever actwas thus modified. This test for indifference does not\\nprovide analtogether satisfactory definition, since itbegs the question\\ntosome extent bypostulating ineffect that the tester knows what con-\\nstitutes asmall bonus. Another attempted solution would betosay\\nthat the person knows by introspection whether hehas decided hap-\\nhazardly orinresponse toadefinite feeling ofpreference. This sort of\\nsolution seems tome especially objectionable, because Ithink itof\\ngreat importance that preference, and indifference, between fand gbe\\ndetermined, atleast inprinciple, bydecisions between acts and notby\\nresponse tointrospective questions. Inspite ofthe difficulty ofdis-\\ntinguishing between preference and indifference, Ithink enough has\\nbeen said forustoproceed toapostulational treatment ofthem.\\nThe very meaning ofthe relationship ofpreference that Ihave at-\\ntempted toestablish inthe preceding paragraph implies that the per-\\nsoncannot simultaneously prefer ftogand gtof. Inthe postulational\\ntreatment ofthe relationships ofpreference and indifference, itwill be\\ntechnically convenient towork with the relation ‘isnot preferred to’’\\nrather than directly with itscomplementary relation ‘‘ispreferred to.”’\\nThus, rather than say that itisimpossible that both fispreferred to\\ngand gtof, Imight say that, of any two acts fand g,fisnotpreferred\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e825589d-241b-4e50-bc66-9300f9f8527b', embedding=None, metadata={'page_label': '36', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18 PRELIMINARY CONSIDERATIONS ON DECISION [2.6\\ntogorgisnot preferred tof,possibly both. Again, the definition of\\npreference suggests that, iffisnot preferred tog,and gisnot preferred\\ntoh,then it is impossible that fshould bepreferred toh.\\nThe two assumptions just made about the relation “isnot preferred\\nto”’ issometimes expressed inordinary mathematical usage bysaying\\nthat the relation isasimple ordering among acts. Formally, arelation\\n<-among aset ofelements z,y,2---, iscalled asimple ordering, in\\nthis book, ifand only ifforevery x,y,and 2:\\n1.Hither x<-y, ory<>a.\\n2.Ifa <-y, and y<-z, then x<-z.\\nBorrowmg from arithmetic the suggestive abbreviation <for the re-\\nlation “isnot preferred to,’’ the assumption that <isasimple order-\\ningcan beexpressed formally byapostulate, thus:\\nPl The relation <isasimple ordering among acts.\\nItisnoteworthy that P1makes noexplicit reference tostates ofthe\\nworld. Except possibly formathematical refinements, titseems tome\\nthat noadditional postulates can beformulated without making such\\nreference—at any rate none will beinthis book.\\nP1by itself isnot very rich inconsequences, but one easily proved\\ntheorem following from itmay bementioned.\\nTHEOREM 1 IfFisafinite setofacts, there exist fandhinFsuch\\nthat for allginF\\nf<g<h.\\nTheorem 1isespecially relevant toapplication ofthe theory ofde-\\ncision, because Iinterpret the theory toimply that, ifFisfinite, the\\nperson will decide onanacthinFtowhich noother act inFispre-\\nferred, the existence ofatleast one such hbeing guaranteed bythe\\ntheorem.\\nItisoften appropriate toconsider infinite sets ofavailable acts. In\\neconomic contexts, forexample, itisgenerally aninappropriate com-\\nplication totake explicit account ofthe possibility that alltransactions\\nmust beinintegral numbers ofpennies. Ifinfinite sets ofavailable acts\\nare setupand interpreted without some mathematical tact, unrealistic\\nconclusions are likely tofollow. Suppose, forexample, that you were\\nfree tochoose any income, provided it bedefinitely less than $100,000\\nper year. Precisely which income would you choose, abstracting from\\nthe indivisibility ofpennies?\\n{For example, such topological assumptions about thespace with neighborhoods\\ndefined interms of<asconnectedness, local compactnesss, ordensity.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4337bed1-f9ee-4963-8489-092117ff93c6', embedding=None, metadata={'page_label': '37', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.6] THE ORDER OFACTS WITH RESPECT TOPREFERENCE 19\\nItissometimes convenient tosupplement the relation <byother\\nrelations derived from itinaccordance with the definitions inTable 1,\\nanalogous definitions being applicable toany simple ordering. Theas-\\nsumption ofsimple ordering, P1, has several implications for the de-\\nrived relations >,<,>,and =. These are generally strongly sug-\\ngested bythe properties ofthe corresponding relations inarithmetic.\\nTABLE 1.TABLE OFRELATIONS DERIVED FROM <\\nNew Relation Definition\\nf> g. g<i.\\nf<g,ie.,gispreferred tof. Itisfalse that g<f.\\nf>g. g<f.\\nf= g,ie., fisequivalent to(or f<g,andg<f.\\nindifferent with respect to)g.\\ngisbetween fand h. f<g<horh<g<f.\\nAfew such implications ofP1are listed below, with nointention of\\ncompleteness, asexercises for those who may not already befamiliar\\nwith the elementary properties ofsimple ordering.\\nExercises\\n1.The relation >isalso asimple ordering.\\n2.Allthe relations <, >, <,>,and =are transitive, that is,they\\ncan bevalidly substituted for<inthe second part ofthe definition of\\nsimple ordering.\\n3.Between any pair ofacts f,g,one and only one ofthe threerela-\\ntions <,=,and>holds.\\n4.Iff<g,and g=h,thenf <h.\\n5.Iff=g,then g=f.\\n6.For any f,f=f.\\n7.Atleast one ofthree acts f,g,hisbetween the other two. When\\ncan there bemore than one such?\\nTwo very different sorts ofinterpretations can bemade ofPland\\nthe other postulates tobeadduced later. First, Plcan be regarded as\\naprediction about thebehavior ofpeople, oranimals, indecision situa-\\ntions. Second, itcan beregarded asalogic-like criterion ofconsist-\\nency indecision situations. For us, the second interpretation isthe\\nonly one ofdirect relevance, but itmay befruitful todiscuss both,\\ncalling the first empirical and thesecond normative.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='200645c5-cfce-4ba8-a30f-85ffa822d11c', embedding=None, metadata={'page_label': '38', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20 PRELIMINARY CONSIDERATIONS ONDECISION [2.6\\nLogicitselfadmitsanempirical aswellasanormative interpreta-\\ntion.Thus, ifanexperimental subject believes certain propositions,\\nitistobeexpected thathewillalsobelieve theirlogicalconsequences\\nanddisbelieve thenegations oftheseconsequences. Thistheoryofhu-\\nmanpsychology hassomevalidity andisofgreatpractical utilityinour\\neveryday dealings withotherpeople,though itisverycrudeandap-\\nproximate. Foronething,peopleoftendomakeelementary mistakes\\ninlogic;morerefinedtheorieswouldattribute thesemistakes tosuch\\nthingsasaccident orsubconscious motivation. Foranother, ifany-\\nonewhobelieved theaxiomsofmathematics alsobelieved allthatthey\\nimplyandnothing thattheycontradict, mathematical studywouldbe\\nsuperfluous forhim;suchapersonwould, ashasbeenexplained, be\\nabletostatetheten-thousandth oranyotherterminthedecimal ex-\\npansion ofzondemand. Tosummarize, logiccanbeinterpreted asa\\ncrudebutsometimes handyempirical psychological theory.\\nTheprincipal valueoflogic,however, isinconnection withitsnorma-\\ntiveinterpretation, thatis,asasetofcriteriabywhichtodetect,with\\nsufficient trouble, anyinconsistencies theremaybeamong ourbeliefs,\\nandtoderivefromthebeliefswealready holdsuchnewonesascon-\\nsistency demands. Itdoesnotseemappropriate heretoattempt an\\nanalysis ofwhyandinwhatcontexts wewishtobeconsistent; it is\\nsufficient toalludetothefactthatweoftendowishtobeso.\\nAnalogously, P1together withthepostulates tobeadduced latercan\\nbeinterpreted asacrudeandshallow empirical theorypredicting the\\nbehavior ofpeoplemaking decisions. Thistheory ispractical insuitably\\nlimited domains, andeveryone infactmakesuseofatleastsomeas-\\npectsofitinpredicting thebehavior ofothers. Atthesametime,the\\nbehavior ofpeople isoftenatvariance withthetheory. Thedeparture\\nissometimes flagrant, inwhichcaseourattitude toward itismuchlike\\nthatweholdtowardslipinlogic,callingthedeparture amistake and\\nattributing ittosuchthingsasaccident andsubconscious motivation.\\nOr,thedeparture maybedetectable onlybyalongchainofargument\\norcalculation, thepossibilities becoming increasingly complicated as\\nnewpostulates arebrought tostandbesideP1.\\nPursuing theanalogy withlogic,themainuseIwouldmakeofP1\\nanditssuccessors isnormative, topolicemyowndecisions forconsist-\\nencyand,wherepossible, tomakecomplicated decisions depend on\\nsimpler ones.\\nHereit ismorepertinent thanitwasinconnection withlogicthat\\nsomething besaidofwhyandwhenconsistency isadesideratum, though\\nIcannotsaymuch. Suppose someone saystome,“Iamarational\\nperson,thatistosay,Iseldom, ifever,makemistakes inlogic.ButI\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db2359fc-050d-4c20-8387-18c9e38a9c9a', embedding=None, metadata={'page_label': '39', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7] THESURE-THING PRINCIPLE 21\\nbehave inflagrant disagreement withyourpostulates, because theyvio-\\nlatemypersonal taste,anditseemstomemoresensible tocatertomy\\ntastethantoatheoryarbitrarily concocted byyou.” Idon’tseehow\\nIcouldreallycontrovert him,butIwouldbeinclined tomatchhisin-\\ntrospection withsomeofmyown.Iwould,inparticular, tellhimthat,\\nwhenitisexplicitly brought tomyattention thatIhaveshownapref-\\nerenceforfascompared withg,forgascompared withh,andforhas\\ncompared withf,Ifeeluncomfortable inmuchthesamewaythatIdo\\nwhenitisbrought tomyattention thatsomeofmybeliefs arelogically\\ncontradictory. Whenever Iexamine suchatriple ofpreferences onmy\\nownpart,Ifindthatitisnotatalldifficult toreverseoneofthem.In\\nfact,Ifindoncontemplating thethreealleged preferences sidebyside\\nthatatleastoneamongthemisnotapreferenceatall,atanyratenot\\nanymore.\\nThere issometemptation toexplore thepossibilities ofanalyzing\\npreference amongactsasapartialordering, thatis,ineffecttoreplace\\npart1ofthedefinition ofsimpleordering bytheveryweakproposition\\nf<f,admitting thatsomepairsofactsareincomparable. Thiswould\\nseemtogiveexpression tointrospective sensations ofindecision orvacil-\\nlation,whichwemaybereluctant toidentify withindifference. My\\nownconjecture isthatitwouldproveablindalleylosingmuchinpower\\nandadvancing little,ifatall,inrealism; butonlyanenthusiastic ex-\\nploration couldshedreallightonthequestion.\\n7Thesure-thing principle\\nAbusinessman contemplates buyingacertain pieceofproperty. He\\nconsiders theoutcome ofthenextpresidential election relevant tothe\\nattractiveness ofthepurchase. So,toclarifythematter forhimself,\\nheaskswhether hewouldbuyifheknewthattheRepublican candidate\\nweregoingtowin,anddecidesthathewoulddoso.Similarly, hecon-\\nsiderswhether hewouldbuyifheknewthattheDemocratic candidate\\nweregoingtowin,andagainfindsthathewoulddoso.Seeingthathe\\nwouldbuyineitherevent,hedecides thatheshouldbuy,eventhough\\nhedoesnotknowwhicheventobtains, orwillobtain, aswewouldordi-\\nnarilysay.Itisalltooseldomthatadecision canbearrived atonthe\\nbasisoftheprinciple usedbythisbusinessman, but,exceptpossibly\\nfortheassumption ofsimpleordering, Iknowofnootherextralogical\\nprinciple governing decisions thatfindssuchreadyacceptance.\\nHaving suggested whatIshalltentatively callthesure-thing prin-\\nciple,letmegiveitrelatively formalstatement thus:Iftheperson\\nwouldnotpreferftog,eitherknowing thattheeventBobtained, or\\nknowing thattheevent~Bobtained, thenhedoesnotprefer ftog.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f75d2644-31ae-4516-9756-a64cae4a65f9', embedding=None, metadata={'page_label': '40', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='22 PRELIMINARY CONSIDERATIONS ONDECISION (2.7\\nMoreover (provided hedoesnotregardBasvirtually impossible) ifhe\\nwoulddefinitely prefergtof,knowing thatBobtained, and,ifhewould\\nnotpreferftog,knowing thatBdidnotobtain,thenhedefinitely pre-\\nfersgtof.\\nThesure-thing principle cannotappropriately beaccepted asapostu-\\nlateinthesensethatPlis,because itwouldintroduce newundefined\\ntechnical termsreferring toknowledge andpossibility thatwouldren-\\nderitmathematically uselesswithout stillmorepostulates governing\\ntheseterms. Itwillbepreferable toregardtheprinciple asalooseone\\nthatsuggests certainformalpostulates wellarticulated withP1.\\nWhattechnical interpretation canbeattached totheideathatf\\nwouldbepreferred tog,ifBwereknowntoobtain? Underanyrea-\\nsonable interpretation, thematterwouldseemnottodepend onthe\\nvalues fandgassumeatstatesoutside ofB.There is,then,noloss\\nofgenerality insupposing thatfandgagreewitheachotherexceptin\\nB,thatis,thatf(s)=g(s)forallse~B.Underthisunrestrictive as-\\nsumption, fandgaresurelytoberegarded asequivalent given~B;\\nthatis,theywouldbeconsidered equivalent, ifitwereknownthatB\\ndidnotobtain. Thefirstpartofthesure-thing principle cannowbe\\ninterpreted thus:If,afterbeingmodified soastoagreewithonean-\\notheroutside ofB,fisnotpreferred tog;thenfwouldnotbepreferred\\ntog,ifBwereknown. Thenotion willbeexpressed formally bysay-\\ningthatf<ggivenB.+\\nItisimplicit intheargument thathasjustledtothedefinition of\\nf<ggivenBthat,iftwoactsfandgaresomodified in~Bas toagree\\nwitheachother,thentheorderofpreference obtaining between the\\nmodified actswillnotdependonwhichofthepermitted modifications\\nwasactually carried out.Equivalently, iffandgaretwoactsthatdo\\nagreewitheachotherin~B,andf<g;then,iffandgaremodified\\nin~Binanywaysuchthatthemodified actsf’andg’continue to\\nagreewitheachotherin~B,itwillalsobesothatf’<g’.Thisas-\\nsumption ismadeformally inthepostulate P2belowandillustrated\\nschematically inFigure 1,akindofdiagram Ifindsuggestive inmany\\nsuchcontexts.\\nInFigure 1,thesetSofallstatessandthesetFofal]consequences\\nfarerepresented byhorizontal andvertical intervals respectively. In\\nanysuchdiagram anactf,beingafunction attaching avaluef(s) ¢«F\\ntoeachs¢Sisrepresented byagraph. Thisparticular diagram graphs\\ntwoactsfandgthatagreewitheachotherin~B,andtwootheracts\\nf’andg’thatalsoagreewitheachotherin~Bandarisebymodifying\\nfandgrespectively onlyin~B,thatis,actsagreeing withfandg\\nrespectively inB.\\n+Inthisedition, thecorresponding definition D1ontheendpapers has\\nbeenslightly strengthened tocompensate aninadvertent weakness intheend\\npaperversionofP2,pointed outtomebyPeterFishburn.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea5e0fd9-0ae8-49f8-922f-ad6f928be606', embedding=None, metadata={'page_label': '41', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7] THESURE-THING PRINCIPLE 23 \\nB(s,)=8\\'(s,)\\nf\\'\"(8,)=8(8)\\nF;\\nf(s,)=f\"(s))\\nf(8q)=g(85)\\n  \\n    fCty<2by \\\\\\nFigure 1\\nP2 Iff,g,andf’,g’aresuchthat:\\n1.in~B,fagreeswithg,andf’agreeswithg’,\\n2.inB,fagreeswithf’,andgagreeswithg’,\\n3.f<g;\\nthenf’<g’.\\nEachoftherelations ‘‘<givenB”’isnoweasilyseentobeasimple\\nordering, andtherelations ““>,<,>,=givenB”’aretobedefined\\nmutatis mutandis. Itisnoteworthy thoughobvious that,iff(s)=g(s)\\nforalls¢B,thenf=ggivenB.\\nItisnowpossible andinstructive togiveanatemporal analysis of\\nthefollowing temporally described decision situation: Thepersonmust\\ndecidebetween fandgafterhefindsout,thatis,observes, whetherB\\nobtains; whatwillhisdecision beifhefindsoutthatBdoesinfact\\nobtain?\\nAtemporally, thepersoncansubmit himself totheconsequences of\\nforelseofgforalls¢B,and,independently, hecansubmithimself to\\ntheconsequences offorelseofgforallse~B;whichalternative will\\nhedecideuponforthes’sinB?\\nFinally, describing thesituation notonlyatemporally butalsoquite\\nformally, thepersonmustdecideamongfouractsdefined thus:\\nhooagreeswithfonBandwithfon~B,\\nho;agreeswithfonBandwithgon~B,\\nhigagreeswithgonBandwithfon~B,\\nh,;agreeswithgonBandwithgon~B.\\nThequestion atissuenowtakesthisform.Supposing thatnoneof\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93ce012b-31a2-4388-adb6-bc1726aa41b0', embedding=None, metadata={'page_label': '42', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='24 PRELIMINARY CONSIDERATIONS ONDECISION [2.7\\nthefourfunctions ispreferred totheparticular oneh;;,is7=0,oris\\ni=1;thatis,doesh,;;agreewithfonBorwithgonB?\\nItisnothardtoseethat7canbe1, ifandonlyiff<ggivenB.In-\\ndeed,if7=1,ho;<h;;,whichmeansthatf<ggivenB.Arguing in\\ntheopposite direction, iff<ggivenB;thenhog<hyo,andhg;<hy.\\nSuppose now,fordefiniteness, hig<h,;,thennoneofthefourpossi-\\nbilities ispreferred toh,,;thisprovesthepointinquestion.\\nItmayfairlybesaidthatthepersonconsiders Bvirtually impossible,\\northatBisnull;ifandonlyif,forallfandg,f<ggivenB.Indeed,\\nifBisnullinthissense,thevaluesactstakeonelements ofBareirrele-\\nvanttoalldecisions.\\nSeveral trivialconclusions aboutnulleventsarelistedasacompound\\ntheorem, allcomponents butthelastofwhichhaveimmediate intuitive\\ninterpretations.\\nTHEOREM 1\\n1.Thevacuousevent,0,isnull.\\nBisnull,ifandonlyif,foreveryfandg, f=ggivenB.\\nIfBisnull,andB>C;thenisnull.\\nIf~Bisnull;f<ggivenB,ifandonlyiff<g.\\nf<ggivenS,ifandonlyiff<g.\\nIfSisnull,f=gforeveryfandg. OmoR\\noo\\nbo\\nComponent 6ofTheorem 1requires comment, because itcorresponds\\ntoapathological situation. IncaseSisnull,itisnotreallyintuitive\\ntosaythatS(andtherefore everyevent) isvirtually impossible. The\\ninterpretation isratherthatthepersonsimplydoesn’t carewhathap-\\npenstohim.Thisisimaginable, especially underasuitably restricted\\ninterpretation ofF’,butitisuninteresting andwillaccordingly beruled\\noutbyalaterpostulate, P5.\\nAfinitesetofeventsB;isapartition ofB;ifB;NB;=0,for?¥J,\\nand(J;B;=B.Withthisdefinition, itiseasilyprovedbyarithmetic\\ninduction that\\nTHEOREM 2IfB;isapartition ofB,andf<ggivenB;foreach1,\\nthenf<ggivenB.If,inaddition, f<ggivenB;foratleastoneJ,\\nthenf<ggivenB.\\nCOROLLARY |Theunionofanyfinitenumberofnullevents isnull.\\nTherearestillotherinteresting consequences ofTheorem 2,which\\nmaybemostconveniently mentioned informally. If,inTheorem 2,\\nB=S(or,moregenerally, if~Bisnull),itissuperfluous tosay“given\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2fbefd3b-cdc6-4559-999a-527eda35926b', embedding=None, metadata={'page_label': '43', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7] THESURE-THING PRINCIPLE 25\\nB”intheconclusions ofthetheorem. Iff=ggivenB;foreach1,\\nthenf=ggivenB.Somuchfortheconsequences ofP2.\\nActsthatareconstant, thatis,actswhoseconsequences areinde-\\npendent ofthestateoftheworld,areofspecial interest. Inparticular,\\ntheylead toanatural definition ofpreference amongconsequences in\\ntermsofpreference among acts.Following ordinary mathematical us-\\nage,f=gwillmeanthatfisidentically g,thatis,foreverys,f(s)=g.\\nAformal definition ofpreference amongconsequences cannowcon-\\nveniently beexpressed thus.Foranyconsequences gandg’,g<q’;\\nifandonlyif,whenf=gandf’=9’,f<f’.\\nInthesamespirit,meaning canbeassigned tosuchexpressions as\\nf<g,g<fgivenB,etc.,andIwillfreelyusesuchexpressions without\\ndefining themexplicitly. Inparticular, f<ggivenBhasanatural\\nmeaning, butonethatisrendered superfluous bythenextpostulate,\\nP3.\\nIncidentally, it isnowevidenthowawkward forusitwouldbeto\\nusef(s)forf;because f(s)<g(s)isastatement abouttheconsequences\\nf(s)andg(s),whereas f<gisastatement aboutacts,andwewill\\nhavefrequent needforbothsortsofstatements.\\nSuppose thatf=g,andf’=g’,andthatg<g’,isitreasonable to\\nadmitthat,forsomeB,f>f’givenB?Thatdepends largelyonthe\\ninterpretation wechoosetomakeofourtechnical terms,asanexample\\nhelpstobringout.+\\nBeforegoingon apicnicwithfriends, aperson decides tobuya\\nbathing suitoratennisracket,nothavingatthemoment enoughmoney\\nforboth. Ifwecallpossession ofthetennisracketandpossession of\\nthebathing suitconsequences, thenwemustsaythattheconsequences\\nofhisdecision willbeindependent ofwherethepicnic isactually held.\\nIfthepersonprefers thebathing suit,thisdecision wouldpresumably\\nbereversed, ifhelearned thatthepicnicwerenotgoingtobeheld\\nnearwater. Thusthequestion whether itcanhappen thatf>f’\\ngivenBwouldbeansweredintheaffirmative. But,undertheinterpre-\\ntationof‘act’?and“consequence” Iamtryingtoformulate, thisis\\nnotthecorrectanalysis ofthesituation. Thepossession ofthetennis\\nracketandthepossession ofthebathing suitaretoberegardedasacts,\\nnotconsequences. (Itwouldbeequivalent andmoreinaccordance\\nwithordinary discourse tosaythatthecoming intopossession, orthe\\nbuying, ofthemareacts.)Theconsequences relevant tothedecision\\naresuchasthese:arefreshing swimwithfriends, sittingonashadeless\\nbeachtwiddling abrand-new tennisracketwhileone’sfriendsswim,\\netc.Itseemsclearthat,ifthisanalysis iscarriedtoitslimit,theques-\\ntionatissuemustbeanswered inthenegative; andItherefore propose\\n+Theroleofsuchfreedom throughout science isbrilliantly discussed by\\nQuine(1951).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1fe7c431-26f0-4007-b4c4-23521f6be234', embedding=None, metadata={'page_label': '44', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"26 PRELIMINARY CONSIDERATIONS ON DECISION [2.7\\ntoassume the negative answer asapostulate. The postulate isso\\ncouched asnot only toassert that knowledge ofanevent cannot estab-\\nlish anew preference among consequences orreverse anold one, but\\nalso toassert that, ifthe event isnot null, nopreference among conse-\\nquences can bereduced toindifference byknowledge ofanevent.\\nP3 Iff=g,f'=g’,andBisnot null; then f<f’given B,ifand\\nonly ifg<g’.\\nApplying Theorem 2,itisobvious that\\nTHEOREM 3 IfB;isapartition ofB;and if(for all7and s)f;<gi,\\nf(s) =fi,and g(s) =g;when s¢B;; then f<ggiven B. If,inaddi-\\ntion, f;<g;forsome 7forwhich B;isnot null, then f<ggiven B.\\nTheorem 3islogically equivalent toP3inthepresence ofP1and P2,\\nand Theorem 3can aseasily begiven anintuitive basis asthe postulate\\nP3. Therefore theassumption ofP3asapostulate instead ofTheorem\\n31sonly amatter oftaste.\\nTheorem 3has been widely accepted bythe British-American School\\nofstatisticians, special emphasis having been given toit,inconnection\\nwith hisnotion ofadmissibility, bythe lateAbraham Wald. Ibelieve,\\naswill bemore fully explained later, that much ofitsparticular sig-\\nnificance for that school stems from the implication that, ifseveral\\ndifferent people agree intheir preferences among consequences, then\\nthey must also agree intheir preferences among certain acts.\\nThis brings the present chapter toanatural conclusion, since the\\nfurther postulates tobeproposed can bemore conveniently introduced\\ninconnection with the uses towhich they areput inlater chapters.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b270660-7fb8-4443-841d-8ff28701dfd3', embedding=None, metadata={'page_label': '45', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 3\\nPersonal Probability\\n1Introduction\\nIpersonally consider itmoreprobable thataRepublican president\\nwillbeelected in1996thanthatitwillsnowinChicago sometimeinthe\\nmonthofMay,1994.Buteventhislatespringsnowseemstomemore\\nprobable thanthatAdolfHitler isstillalive.Many,aftercarefulcon-\\nsideration, areconvinced thatsuchstatements aboutprobability toa\\npersonmeanprecisely nothing, oratanyratethattheymeannothing\\nprecisely. Attheopposite extreme, othersholdthemeaning tobeso\\nself-evident astobeunanalyzable. Anintermediate position’istaken\\ninthischapter, whereaparticular interpretation ofprobability toa\\nperson isgivenintermsofthetheory ofconsistent decision intheface\\nofuncertainty, theexposition ofwhichwasbeguninthelastchapter.\\nMuchasIhopethatthenotionofprobability definedhereisconsistent\\nwithordinary usage, itshouldbejudgedbythecontribution itmakes\\ntothetheoryofdecision, notbytheaccuracy withwhich itanalyzes\\nordinary usage.\\nPerhaps thefirstwaythatsuggests itselftofindoutwhichoftwo\\neventsa person considers moreprobable issimplytoaskhim.Itmight\\nevenbeargued, though Ithinkfallaciously, that,sincethequestion\\nconcerns whatisinsidetheperson’s head,therecanbenoothermethod,\\njustaswehavelittle, ifany,accesstoaperson’s dreamsexceptthrough\\nhisverbalreport. Attempts todefinetherelative probability ofapair\\nofeventsintermsoftheanswers peoplegivetodirectinterrogation\\nhasjustifiably metwithantipathy frommoststatistical theorists. In\\nthefirstplace,manydoubtthattheconcept “moreprobable tome\\nthan” isanintuitive one,opentonoambiguity andyetadmitting no\\nfurther analysis. Even iftheconcept weresocompletely intuitive,\\nwhichmightjustify directinterrogation asasubjectworthy ofsome\\npsychological study,whatcouldsuchinterrogation havetodowiththe\\nbehavior ofapersoninthefaceofuncertainty, exceptofcoursefor his\\nverbalbehavior underinterrogation? Ifthestateofmindinquestion\\nisnotcapable ofmanifesting itselfinsomesortofextraverbal behavior,\\n27\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3a89ed61-9107-4ea2-b0b2-d2e658206217', embedding=None, metadata={'page_label': '46', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='28 PERSONAL PROBABILITY [3.1\\nitisextraneous toourmain interest. If,onthe other hand, itdoes\\nmanifest itself through more material behavior, that should, atleast\\ninprinciple, imply the possibility oftesting whether aperson holds\\none event tobemore probable than another, bysome behavior express-\\ning, and giving meaning to,hisjudgment. Itwould, inshort, bepref-\\nerable, atleast inprinciple, tointerrogate the person, not literally\\nthrough hisverbal answer toverbal questions, but rather inafigurative\\nsense somewhat reminiscent ofthat inwhich ascientific experiment is\\nsometimes spoken ofasaninterrogation ofnature. Several schemes of\\nbehavioral, asopposed todirect, interrogation have been proposed.\\nThe one introduced below was suggested tomebyapassage ofdeFi-\\nnetti’s (onpp.5-6 of[D2]), though thepassage itself does notempha-\\nsize behavioral interrogation.\\nTo illustrate the scheme, our idealized person has just taken two\\neggs from hisicebox and holds them unbroken inhishand. Wewonder\\nwhether hethinks itmore probable that thebrown one isgood than\\nthat the white one is. Our curiosity being real, we are prepared to\\npay, ifnecessary, tohave itsatisfied. We therefore address him thus:\\n‘“We seethat you areabout toopen those eggs. Ifyou will besoco-\\noperative astoguess that one orthe other egg isgood, wewill pay you\\nadollar, should your guess prove correct. Ifincorrect, you and we\\nare quits, except that wewill inany event exchange your two eggs for\\ntwo ofguaranteed goodness.”’ Ifunder these circumstances theperson\\nstakes hischance forthe dollar onthebrown egg, itseems tome to\\ncorrespond well with ordinary usage tosay that itismore probable to\\nhim that thebrown one isgood than that thewhite one is.Though,\\nofcourse, Ihope foryour agreement onthis analysis ofordinary usage,\\nIrepeat that itisnot really fundamental tothesubsequent argument,\\nasindeed nosuch lexicographical point could be; for the utility ofa\\nconstruct ordefinition depends only secondarily onthe aptness ofthe\\nexpression interms ofwhichit iscouched.\\nThere isamode ofinterrogation intermediate between what Ihave\\ncalled thebehavioral and the direct. One can, namely, ask theperson,\\nnothow hefeels, but what hewould doinsuch and such situation.\\nInsofar asthe theory ofdecision under development isregarded as\\nanempirical one, theintermediate mode isacompromise between econ-\\nomy and rigor. But, inthe theory’s more important normative inter-\\npretation asaset ofcriteria ofconsistency forustoapply toourown\\ndecisions, the intermediate mode seems tome tobejust the nght\\none.\\nThough itentails digression from themain theme, some readers may\\nbeinterested inafew words about actual experimentation onstrictly\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b570aa8-f1bf-4609-b213-17a68854ac43', embedding=None, metadata={'page_label': '47', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.1] INTRODUCTION 29\\nempirical behavioral interrogation. Somekeyreferences bearing on\\nthesubject are[M4],[R3],and[W8].\\nInthefirstplace,alittlereflection showsthatanexperiment inwhich\\nhumansubjects arerequired todecideamongactualactsmaybevery\\nexpensive intime,money,andeffort,especially iftheconsequences en-\\nvisaged areexpensive toprovide, apointdiscussed indetailin[W8].\\nQuestions ofmorality, andevenoflegality, toward thesubjectmay\\nfurthercomplicate theinvestigation. Forexample, Mosteller andNo-\\ngee,asdescribed inSection3Bof[M4],madecertain thateverysub-\\njectinoneexperiment oftheirswouldbefinancially benefited, though\\ntheykeptthissecurity secretfromthesubjects.\\nThere isalsoadifficulty inprinciple. Suppose thatIwishtodis-\\ncoveraperson’s preferences amongseveralacts—three actsf,g,andh\\naresufficient tobringoutthedifficulty. IfIingoodfaithofferhimthe\\nopportunity todecideamongallthree,andhedecidesonf;thenthere\\nisnofurther possibility ofdiscovering whathispreference wasbetween\\ngandh. Suppose, forexample, thatahotmanactually prefersaswim,\\nashower,andaglassofbeer,inthatorder.Oncehedecides on,and\\nthereby becomes entitled to,theswim,hecannolongerappropriately\\nbeaskedtodecidebetween showerandbeer.Anaiveattempt todoso\\nwouldresultinhisdeciding between aswimandshowerontheone\\nhand,andaswimandbeerontheother—an altogether different situa-\\ntionfromtheoneintended.\\nThedifficulty cansometimes bemetbyspecialdevices. Forexample,\\ntheinvestigator mightwaitforadifferent but“similar” occasion. But\\nW.AllenWallishasmentioned tomeaninteresting andverygeneral\\ndevice,whichwillnowbedescribed, withhispermission. t\\nSuppose thatthehotmanisinstructed torankthethreeactsin\\norder,subject totheconsideration thattwoofthemwillbedrawnat\\nrandom (e.g.,bycarddrawing ordicerolling), andthatheisthento\\nhavewhichever ofthesetwoactshehasassigned thelowerrank.He\\nisthuscalledontoselectoneofsixacts,thatis,oneofthesixpossible\\nrankings. Ifhedoes,forexample, selecttheranking {swim,shower,\\nbeer}, itfollows easilyfromthetheoryofdecision thusfardeveloped\\nthatforhimswim>shower>beer,barring thefarfetched possibility\\nthatheregardsoneormoreofthethreedrawings asvirtually impossi-\\nbleandprovided thathispreference amongthethreeactsswim,shower,\\nbeergivenanyofthethreedrawingsisthesameashisoriginal prefer-\\nence.Theinvestigator couldinpractice designthedrawing insucha\\n|IhavesinceseenthissamedeviceusedbyM.Allais.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='64c48f51-5ee3-4f61-a58e-4b0c98d75907', embedding=None, metadata={'page_label': '48', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='30 PERSONAL PROBABILITY [3.2\\nwayastobewellsatisfied thattherequired “irrelevance” obtained, ex-\\nceptforvery“superstitious” people. Thisendsthepresent digression on\\nactualbehavioral interrogation.\\nThepurpose ofthischapter istoexplore theconcept ofpersonal\\nprobability |thatwasindicated intheexample aboutthetwoeggs.\\nTheconcept willbeputonaformalbasisin§2byintroducing twonew\\npostulates, P4andP5,tobeusedinconjunction withP1-3.Thiswill\\nleadtoaformalanalysis ofthenotionthatoneeventisnomoreprob-\\nablethananother. Several deductions aboutthisnotionreminiscent\\nofmathematical properties ordinarily attributed toprobability willbe\\nmade;butonlyin§3,afteradjunction ofstillanother postulate, P6,\\ncanthenotionbeconnected quantitatively withwhatmathematicians\\nordinarily callmathematical probability. Section 4isdevoted tosome\\nmathematically technical criticisms of thenotion ofpersonal proba-\\nbility,whichcansafelybeskipped orskimmedbythosenotinterested\\ninsuchmatters. Section 5discusses conditional personal probability;\\n6,theapproach tocertainty through alongsequence ofconditionally\\nindependent relevant observations; and7,anextension oftheconcept\\nofasequence ofindependent events, particularly interesting fromthe\\nviewpoint ofpersonal probability.\\n2Qualitative personal probability\\nWhenIspokeintheintroductory section ofoffering theperson a\\ndollar ifhisguessabouttheeggproved correct, itwastacitlyassumed\\nthathisguesswouldnotbeaffectedbytheamount oftheprizeoffered.\\nThatseemstomecorrect inprinciple. Itwould, forexample, seemun-\\nreasonable forthepersonwiththetwoeggstoreverse hisdecision if\\ntheprizewerereduced fromadollartoapenny.Hemightreverse\\nhimself ingoingfromapennytoadollar,because hemightnothave\\nfound itworthhistrouble togivecarefulconsideration fortoosmalla\\nprize. Ithinktheanomaly canbestbemetbydeliberately pretending\\nthatconsideration coststhepersonnothing, thoughthatisfarfromthe\\ntruthinactualcomplicated situations. Itmight,ontheotherhand,\\nbestimulating, anditiscertainly morerealistic, tothinkofconsidera-\\ntionorcalculation asitselfanactonwhichthepersonmustdecide.\\nThough Ihavenotexplored thelatterpossibility carefully, Isuspect\\nthatanyattempt todosoformally leadstofruitless andendless re-\\ngression.\\n+Theterm“personal probability’? wassuggested tomeorallybyThornton C.\\nFry.Someothertermssuggested forthesameconcept are“subjective probability,”\\n“psychological probability,’ and“degree ofconviction.”\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='24c11999-f6f9-45b5-b8bd-fff6482eb2ab', embedding=None, metadata={'page_label': '49', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"3.2] QUALITATIVE PERSONAL PROBABILITY 31\\nToofferaprizeincaseAobtainsmeanstomakeavailable totheper-\\nsonanactf4suchthat\\nfa(s)=f fors¢A,\\n1\\nm) fa(s)=f’ forse~A,\\nwheref’<f.Theassumption thatonwhichoftwoeventstheperson\\nwillchoosetostakeagivenprizedoesnotdepend ontheprizeitself\\nisexpressed bythefollowing postulate, whichlooksformidable only\\nbecause itcontains fourdefinitions like(1).Thereadermayfindit\\nhelpful tographaninstance ofthepostulate inthespiritofFigure\\n2.7.1.\\nP4sff,f’,9,9';A,B;fa,fe,ga,Searesuchthat:\\n1. i<f, g<9;\\n2a. fa(s)=f, ga(s)=9forseA,\\nfats)=f, ga(s)=9forse~A;\\n2b. fa(s)=f, ga(s)=9fors¢B,\\nfas)=f’,gas)=9'forse~B;\\n3. fa<fa;\\nthenga<gp.\\nInthelightofP4,itwillbesaidthatAisnotmoreprobable than\\nB,abbreviated A<B;ifandonlyifwhenf’<fandfa,fgaresuch\\nthat\\nfa(s)=fforseA,fa(s)=f' forse~A,\\nfa(s)=fforseB, fa(s)=f’forse~B;\\nthenfA<fp.\\nTheassumption thatthereisatleastoneworth-while prizeisin-\\nnocuous; for,though acontext failingtosatisfy itmightarise,sucha\\ncontextwouldbetootrivialtomeritstudy. Itherefore propose the\\nfollowing postulate.\\nP5There isatleastonepairofconsequences f,f’suchthatf’<f.\\nAlltheimplications tobededuced fromP1-—5forsometimetocome\\narethemselves implications ofthethreeeasilyestablished conclusions,\\nwhichareintroduced bythefollowing definition andtheorem.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4672aace-7e33-4614-80af-04c1a753587c', embedding=None, metadata={'page_label': '50', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"32 PERSONAL PROBABILITY [3.2\\nArelation <-between eventsisaqualitative probability; ifandonly\\nif,foralleventsB,C,D,\\n1.<+isasimpleordering,\\n2.B<-C, ifandonlyifBUD<-C UD,providedBND=\\nCND=0,\\n3.0<:-B,0<-S.\\nItmaybehelpful toremark thatthesecondpartoftheabovedefini-\\ntionsays,ineffect,thatitwillnotaffecttheperson’s guesstooffer\\nhimaconsolation prizeincaseneitherBnorCobtains, butDhappens\\nto.\\nTHEOREM 1Therelation<asapplied toevents isaqualitative\\nprobability.\\nYouwillhavenodifficulty inproving thatTheorem 1followsfrom\\nP1-5.Theorem 1hasmanyconsequences ofthesortonewouldexpect\\nif<meant“notmoreprobable than”inanysensehavingthemathe-\\nmatical properties ordinarily attributed tonumerical probability. This\\nisillustrated bythefollowing listofexercises, whichshouldnotonly\\nbeprovedformally, butalsointerpreted intuitively. Oneeasyexercise\\nnotincluded inthelistbelow,because itisnotstrictly aconsequence\\nofTheorem 1alone, istoshowthatB=0,ifandonlyifBisanull\\nevent.\\nExercises\\nLifbBcc,thnO<B<CC<S.\\n2a.IfBND=C flD=0;thenB<C,ifandonlyifBUD<\\nCUD.\\n2b.1f0<C,andBNC=0;thnB< BUC.\\n3.IfB<C,then~C<~B;andconversely. Hint:DrawaVenn\\ndiagram ofthefourfold partitionBNC,~BNC,BN~C,~BN\\n~C.\\n4a.IfB<CrandCN D=0;thn BUD<CUD.\\n4b.IfB <0;thenBUC=C,andB=0.\\n4c.fS<BsthnBNC=C,andB=S.\\n44.BUD<CUD,andBN D=0;thenB< C.\\n5a.IfB,<Ch,Bo<Co,andC1MC2=QO;thenB,UBo<CiU\\nC..Hint:ExhibitByandC,intheformBz=Bo’UQ,Ci=Cy’UQ\\nwithBy’,Cy’,Qdisjoint. Justifythefollowing calculation, stepbystep.\\nB,UBy’<C,UBo’=C,'UBeg<Cy’UCs,\\nwhenceB,UBy,<C,UCo.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bcd9c10a-4e67-4241-95ad-48b32213cba2', embedding=None, metadata={'page_label': '51', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3] QUANTITATIVE PERSONAL PROBABILITY 33\\n5b.IfB,UB,<C;UC,andB,NBe=0;thenBy<C, or\\nBo<Co.\\n6.IfB<~BandC>~C,thenB<C;equality holding inthe\\nconclusion, ifandonlyifitholdsinbothpartsofthehypothesis.\\n3Quantitative personal probability\\nAsIhavesaid,theexercises terminating thepreceding section sug-\\ngestaclosemathematical parallelism between personal probability and\\nthemathematical properties ordinarily attributed toprobability, though\\nthepostulates assumed thusfardonot(ascouldeasilybedemonstrated)\\nmakeitpossible todeducefromthisparallelism theunambiguous as-\\nsignment ofanumerical probability toeachevent. But,if,forexample\\n(following deFinetti [D2]),anewpostulate asserting thatScanbe\\npartitioned intoanarbitrarily largenumber ofequivalent subsetswere\\nassumed, itispretty clear(anddeFinetti explicitly showsin[D2)])\\nthatnumerical probabilities couldbesoassigned. Itmightfairlybe\\nobjected thatsuchapostulate wouldbeflagrantly adhoc.Onthe\\notherhand,suchapostulate couldbemaderelatively acceptable by\\nobserving thatitwillobtain if,forexample, inalltheworldthereisa\\ncointhattheperson isfirmlyconvincedisfair,thatis,acoinsuchthat\\nanyfinitesequence ofheadsandtails isforhimnomoreprobable than\\nanyothersequenceofthesamelength;thoughsuchacoinis,tobesure,\\naconsiderable idealization.\\nAftersomegeneralandabstract discussion ofthemathematical con-\\nnectionbetween qualitative andquantitative probability, apostulate,\\nP6,willbeproposed, which,though logically actually stronger thanthe\\nassumption thattherearepartitions ofSintoequivalent events,seems\\ntomeeveneasiertoaccept. OnceP6isaccepted, therewillscarcely\\nagainbeanyneedtoreferdirectly toqualitative probability.\\nTobeginwith,letmesayprecisely whatismeant, inthepresent\\ncontext, byaprobability measure, thisbeingthestandard termfor\\nwhatIwouldhereotherwise prefertocallaquantitative probability,\\nandwhatitmeansforaprobability measure tobeinagreement with\\naqualitative probability.\\nAprobability measure onasetSisafunction P(B)attaching to\\neachBCSarealnumbersuchthat:\\n1.P(B)>0foreveryB.\\n21f&BNC=0,P(B UC)=P(B)+PC).\\n3.P(S)=1.\\nThisdefinition, orsomething verylike it,isattherootofallordinary\\nmathematical workinprobability.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8706d533-75c4-4ccf-8431-166da7b98022', embedding=None, metadata={'page_label': '52', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='34 PERSONAL PROBABILITY [3.3\\nIfScarriesaprobability measurePandaqualitative probability\\n<+suchthat,foreveryB,C,P(B)<P(O), ifandonlyifB<-C;\\nthenP(strictly) agreeswith<-.IfB<-+C impliesP(B)<P(C),\\nthenPalmostagreeswith<-.Thisterminology isobviously con-\\nsistent inthat,ifPagrees,thatis,strictly agrees,with<:,Palsoal-\\nmostagreeswith<-.Itisalsoeasilyseenthat,ifPagreeswith<,,\\nthenknowledge ofPimpliesknowledge of<-.But,ifPonlyalmost\\nagreeswith<-,itmayhappen, asexamples in§4show,thatP(B)=\\nP(C),thoughB<:C,sothatknowledge ofPmayimplyonlyimperfect\\nknowledge of<.-.\\nTherestofthissection ismainlyastudyofqualitative probabilities\\ngenerally, withaviewtodiscovering interesting conditions underwhich\\nthereisaprobability measure thatagrees, eitherstrictly oralmost,\\nwithagivenqualitative probability. Theseconditions suggest anew\\npostulate governing thespecial qualitative probability <.Thework\\nisnecessarily rathertediousandburdened withdetail. Itwill,there-\\nfore,bewiseformostreaders toskimoverthematerial, omitting the\\nproofsbutnoticing themoreobvious logicalconnections amongthe\\ntheorems anddefinitions. Somemaythenfindthemselves sufficiently\\ninterested inthedetailstoreturnandreadorsupplytheproofs,asthe\\ncasemayrequire. Othersmaysafelygoforward. Here,aselsewhere,\\ntechnical termsofinterest forthemoment onlyareintroduced with\\nitalicsratherthanboldface.\\nAnn-foldalmostuniform partition ofBisann-foldpartition ofB\\nsuchthattheunionofnorelements ofthepartition ismoreprobable\\nthanthatofanyr+1elements.\\nTHEOREM 1Ifthereexistn-foldalmostuniform partitions ofBfor\\narbitrarily largevaluesofn,thenthereexistm-foldalmostuniform par-\\ntitionsforeverypositive integerm.\\nProor. LetB;,7=1,---,n,beann-foldalmostuniform partition\\n(ofB)withn>m?.Usingtheeuclidean algorithm, letnbewritten\\nn=am-+b,whereaand6areintegers suchthatm<aand0O<b<\\nm.NowletC;,7=1,---,m,beanym-foldpartition suchthateach\\nC;;istheunionofaora+1oftheB,’s.TheunionofanyroftheC;’s,\\nr<m,istheunionoffromarto(a+1)roftheB,’sandtheunionof\\nr+1oftheC;’sisthatoffroma(r+1)to(a+1)(r+1)oftheB;,’s.\\nSincer<mc<a, (a+1ljr=ar+r<ar+a=a(r+1).@\\nTHEOREM 2Ifthereexistn-foldalmostuniform partitions ofSfor\\narbitrarily largevaluesofn,thenthereisoneandonlyoneprobability\\nmeasurePthatalmostagreeswith<-.Furthermore, foranyp,0<p\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d92980c6-6d83-4fd7-a0c1-6085f03ccf58', embedding=None, metadata={'page_label': '53', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3] QUANTITATIVE PERSONAL PROBABILITY 35\\n<1,anyBCS,andtheuniquePjust defined, thereexistsCCB\\nsuchthatP(C)=pP(B).t\\nProor. Theproofisbroken intoasequence ofeasysteps,left,for\\nthemostpart,tothereader. Thesestepsaregroupedinblocks,only\\nthelaststepineachbeingneeded intheproofoflatersteps.\\n1.Thereexistn-foldalmostuniform partitions ofSforeveryposi-\\ntiven.\\n2a.Ifpi,---,Pnarerealnumbers suchthat0<p,<po<---< Mn,\\nand2p;=1;then\\n(1) p< r/n,r=leyn.\\n1\\n2b.Iffurther\\nr+1 n\\nDdPi=»Piforr=1,--::,n—1;\\n1 n—r+l1\\nthen\\n2)Lp=(r—1)/n, andDYmS(r+1)/n.\\n1n—r-+1\\n2c.Thesumofanyrofthep,’sliesbetween (r—1)/nand(r+1)/n.\\n2d.IfPalmostagreeswith<-,andC(r,n)denotes hereandlater\\ninthisproofanyunionofrelements ofanyn-foldalmostuniform par-\\ntition(notnecessarily thesamefromonecontext toanother), then\\n(3) (r—1)/nS$P(C(r,n))S(+1)/n.\\n3.Letk(B,n)denotethelargestinteger r(possibly zero)suchthat\\nsomeC(r,n)1snotmoreprobable thanB.Thefunction k(B,n)is\\nwell-defined, and0<k(B,n)<n.\\n4a.ForanyPthatalmostagreeswith<-,\\n(4) (k(B,n)—1)/n<P(B)S(k(B,n)+2)/n.\\n4b.AtmostonePcanalmostagreewith<:-\\n5a.IfB;andC;aren-foldpartitions (notnecessarily almostuniform)\\nsoindexed thatB,<-By<----<-B,,andCy>:C2,>:+--+>Cn;\\nthen\\n(5)UB>UC,1r=0,---,n-1.\\n+Technical note:Themathematical essence oftheterminal conclusion ofthis\\ntheorem, andotherconclusions related toit,aregivenbySobczyk andHammer\\n[S15].Itmightbeconjectured, inanalogy withcountably additive measures, that\\nthisconclusion meansonlythatPisnon-atomic, butthatconjecture isfalse[N5].+\\n+Akeyreference forfurther information onthestructure offinitely addi-\\n-tivemeasures is(Dubins 1969).Sustained useoffinitely additive prohahility\\nisillustrated in(Dubins andSavage1965).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4544ec96-9a56-489a-a13f-25fec463822d', embedding=None, metadata={'page_label': '54', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='36 PERSONAL PROBABILITY (3.3\\n5b.Ifinaddition thetwopartitions arealmostuniform, then\\nr r+2\\n(6) Ucis-UB,r=1,---,n—2.\\n1\\nr+2\\nProot.YBi>-UBi>Uer>-U cs\\noc.Theunionofanyrelements ofonealmostuniform n-foldparti-\\ntionisnotmoreprobable thantheunionofanyr+2elements ofan-\\nother.\\n5d.IfBMC=0,then\\n(7)K(B,n)+k(C,n)—-2<SkKBUC,n)<kB,n)+k,n)+1.\\n6a.IfaC(r,m)isnotmoreprobable thanaC(s,n),then\\nr—2 s+2 10(s(tm nmn\\n(Consider anmn-fold almostuniform partition, andusetheeasilyes-\\ntablished factthattheunionofany¢+2elements ofanalmostuni-\\nformpartition isactually moreprobable thanthatofany¢elements.)\\nk(B,m)_kB,n) 13coy2ys.m nN m nNmn\\n  \\n6b.  \\n  \\n6c.It1smeaningful todefineP(B)by\\nk(B,n) \\nn>©n\\nthatis,thelimitexists.\\n7.P(B),asjustdefined, isaprobability measure, andtheonlyone\\nthatalmostagreeswith<:.\\n8a.Thereexisttwoinfinite sequences ofsetsC,andD,contained\\ninBsuchthat:\\n1.C,ADz=\\n2.ChCcChai;andDyCcDn41,\\n3.P(Ca)=pP(B)—n™,\\n4.P(Da)>(1—p)P(B)—n=\\n8b.P(UnCn)>eP(B),P(UnDn)>(1—»)P(B),and(UnCx)A\\n(U,Dn)=0.\\n8c.P(UnCn)=pP(B).@\\nAfewtechnical termsoflocalized interest onlyarenowintroduced.\\nIfandonlyif,foreveryB>-0,thereisapartition of S,noelement of\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='af936f21-236a-4946-a3ff-f3271f9e1fac', embedding=None, metadata={'page_label': '55', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3] QUANTITATIVE PERSONAL PROBABILITY 37\\nwhichisasprobable asB;<:isfine.+Bandarealmostequiva-\\nlent,writtenB<=-C;ifandonlyifforallnon-nullGandAsuchthat\\nBNG=CNH=0, BUG>-C andCUHAH>-B. Itisobvious\\nthatequivalent eventsarealsoalmostequivalent. Finally, ifandonly\\nifeverypairofalmostequivalent eventsareequivalent, <:istight.\\nTHEOREM 3\\nHyp. <-isfine.\\nCoNcL. 1.IfB>-0, andC>-0;thereexistsDCC suchthat\\n0<-D<.-B.\\n2,.1fB<=-G,C=-H, andBNC=GNH=0; thenBUC\\n<=GUH.\\n38.WB=-C,G=H,BUCz=-GUHA,andBNC=GNA =0;\\nthenB=-G.\\n4,Anypartition ofSintoalmostequivalent events isanalmostuni-\\nformpartition.\\n5.Anyeventcanbepartitioned intotwoalmostequivalent events.\\n6.Anyeventcanbepartitioned into2”almostequivalent events,\\nforanynon-negative integer n.\\n7.ThereexistsoneandonlyonePthatalmost agreeswith<-.\\nForanyB,p(0<p<1),andtheuniquePjustdefined, thereex-\\nistsCCBsuchthatP(C)=pP(B). IfB>-0,P(B)>0. Finally,\\nB=-C,ifandonlyifP(B)=P(C).\\nProor. Thepartsoftheconclusion are soarranged thateachiseasy\\ntoproveinthelightofitspredecessors, butproofsforParts3and5\\naregivenbelow. Itmayberemarked thatallpartsaretrivialconse-\\nquences ofthelastoneandhavetherefore relatively littleimportance in\\nthemselves.\\nPart3.Suppose, forexample,BUE<-G, BNEHE=0, and\\nE>-0;andconsider twocases:\\n(a)IfBUC<-S,itmaybeassumed without lossofgenerality\\nthatCNEF=0,whence(BUC)UE>-GU 4H.Therefore, C>-H.\\nLetEbepartitioned intotwonon-null events£;and£2;then(since\\nitisabsurd tosuppose thatthepartofGoutside ofCisnull,which\\nwouldimplyC>-G>-BUE)thereisinGanEL’suchthatC|E’\\n=O0<-E’<-H,. NowCUEH’>- HUE’>-G>-(BU £,)U&,,\\nwhenceC>:BUE),whichisabsurd.\\n(b)IfBUC=-S,itcan(setting asidetheeasyspecialcaseCMG\\n=-(Q)beshownsuccessively that:HUG=-S; C<-BUE <:-G,\\nwhereE>-0andECCNG; (BNAUEK<-(G4NC; (CNA)\\n<-(GMB);andHUE<:G,whichestablishes acontradiction.\\n+Inthefirstedition, thisdefinition wasatrifletooweak,aspointed outby\\nMalcolm Pike.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='648bdf94-5938-4253-a988-8004ac6d332a', embedding=None, metadata={'page_label': '56', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='38 PERSONAL PROBABILITY (3.3\\nPart5.Thereexistsasequence ofthreefold partitions ofB,say\\nC,,Dn,andG,,suchthat:\\n1.C,UG,>+Dn,andD,UG,>:Ch,\\n2.Cn41~Cn,Dn4—Dp,andGn41CcGn,\\n3.~Ga4i MNGr2+Gr41;whence G,-contains twodisjoint events\\neachatleastasprobable asG,,41.\\nForanyH>-0,G,<:Hforsufficiently largen,asmaybeseenby\\nconsidering somem-foldpartition noelement ofwhich ismoreprobable\\nthanH,andlettingnbesuchthat2\"—’>m.IfG,weremoreprobable\\nthanHandtherefore moreprobable thaneachelementofthepartition,\\nitwouldfollowthattheunionofallelements ofthepartition, namely\\nS,islessprobable thanG,,whichwouldbeabsurd.\\nThetwoeventsBy=UnCn,Bo=(UnDn)U(FnGr)partitionB\\nintherequired fashion.@\\nCorROLLaRY 1If<-isbothfineandtight;theonlyprobability\\nmeasure thatalmostagreeswith<-strictly agreeswithit,andthere\\nexistpartitions ofSintoarbitrarily manyequivalent events.\\nTHEOREM 4<-isbothfineandtight, ifandonlyif,foreveryB<-C,\\nthereexistsapartition ofStheunionofeachelement ofwhichwithB\\nislessprobable thanC.\\nTheproofof thistheorem iseasy.\\nInthelightofTheorems 3and4,Itentatively propose thefollowing\\npostulate, P6’,governing therelation<among events,andthereby\\ntherelation<amongacts.\\nP6’ IfB<C,thereexistsapartition ofStheunionofeachele-\\nmentofwhichwithBislessprobable thanC.\\nItseemstomerathereasier tojustifytheassumption ofP6’,which\\nsaysineffectthat<isbothfineandtight,thantojustifytheassump-\\ntion,whichwasmadebydeFinetti [D2]andbyKoopman [K9],[K10],\\n[K11]incloselyrelated contexts, thatthereexistpartitions ofSinto\\narbitrarily manyequivalent events,though logically P6’implies that\\nassumption andsomewhat more.Suppose, forexample, thatyouyour-\\nselfconsiderB<C,thatis,thatyouwoulddefinitely ratherstakea\\ngaininyourfortune onCthanonB.Consider thepartition ofyour\\nownworldinto2”eventseachofwhichcorresponds toaparticular\\nsequence ofnheadsandtails,thrownbyyourself, withacoinofyour\\nownchoosing. Itseemstomethatyoucouldeasilychoosesucha\\ncoinandchoosensufficiently largesothatyouwouldcontinue topre-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='264f578e-08ef-48b7-99a5-da8b7c4131c3', embedding=None, metadata={'page_label': '57', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3] QUANTITATIVE PERSONAL PROBABILITY 39\\nfertostakeyourgainonC,ratherthanontheunionofBandanypar-\\nticularsequence ofnheadsandtails.Foryoutobeabletodoso,you\\nneedbynomeansconsider everysequence ofheadsandtailsequally\\nprobable.\\nItwould,however, bedisingenuous nottomention thatsomewho\\nhaveworkedonacloselyrelatedconcept ofprobability, notablyKeynes\\n[K4]andKoopman [K9],[K10],[K11],wouldobjecttoP6’precisely\\nbecause itimplies thattheagreement between numerical probability\\nandqualitative probability isstrict.Koopman, forexample, holds\\nthat,if4d>BandA B,thenA1snecessarily moreprobable than\\nB,thoughthenumerical probability ofAmaywellbethesameasthat\\nofB.Thus, ifamarksman shootsatawall,itislogically contradictory\\nthathisbulletshould fallnowhere atall,butitislogically consistent\\nthataprescribed mathematically idealpointonthebulletshouldstrike\\naprescribed mathematically ideallineonthewall.Sincetheeventof\\ntheprescribed pointhittingaprescribed lineislogically possible, Koop-\\nmanwouldinsistthattheevent ismoreprobable thanthevacuous\\nevent,namely thatthebulletgoesnowhere, thoughthenumerical proba-\\nbilityofbothevents iszero.IdonottakedirectissuewithKoopman,\\nbecause heispresumably talkingaboutasomewhat different concept\\nofprobability fromtheparticular relation <;butIdonotthinkit\\nappropriate tosuppose thatthepersonwoulddistinctly ratherstakea\\ngainonthelinethanonthenullset.Theissueisnotreallyeitheran\\nempirical oranormative one,because thepointandlineinquestion\\naremathematical idealizations. Ifthepointandlinearereplaced bya\\ndotandaband,respectively, then,ofcourse,nomatterhowsmallthe\\ndotandbandmaybe,theprobability oftheonehitting theotheris\\ngreaterthanthatofthevacuous event.Butitseemstomeentirely\\namatter oftaste,conditioned bymathematical experience, todecide\\nwhatidealization tomakeifthedotandbandarereplaced bytheirideal-\\nizedlimits.Somuchforhairsplitting.\\nAsfarasthetheory ofprobability perseisconcerned, postulate P6’\\nisallthatneedbeassumed, butinChapter 5aslightly stronger assump-\\ntionwillbeneededthat bears onactsgenerally, notonlyonthosevery\\nspecialactsbywhichprobability isdefined. Therefore, Iamaboutto\\npropose apostulate, P6,thatobviously implies P6’andwilltherefore\\nsupersede it.Thisstronger postulate seemstomeacceptable forthe\\nsamereasonthatP6’itselfdoes.\\nP6 Ifg<h,andfisanyconsequence; thenthereexistsaparti-\\ntionofSsuchthat,ifgorhissomodified onanyoneelement ofthe\\npartition astotakethevaluefateverysthere,othervaluesbeingun-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4dfbf23b-a272-4aac-852a-e61aa495e386', embedding=None, metadata={'page_label': '58', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='40 PERSONAL PROBABILITY (3.4\\ndisturbed; thenthemodified gremains lessthanh,orgremainsless\\nthanthemodified h,asthecasemayrequire.\\n4Somemathematical details\\nAretherequalitative probabilities thatarebothfineandtight,that\\narefinebutnottight,thataretightbutnotfine,thatareneither fine\\nnortightbutdohaveoneandonlyonealmostagreeing probability\\nmeasure? Examples answering allthesequestions intheaffirmative\\nwillbeexhibited inthissection.\\nToindicate adifferent topicthatwillalsobetreated here,thoseof\\nyouwhohavehadmorethanelementary experience withmathematical\\ntreatments ofprobability knowthatit isnotusualtosuppose, ashas\\nbeendonehere,thatallsetshaveanumerical probability, butrather\\nthatasufficiently richclassofsetsdoso,theremainder beingconsid-\\neredunmeasurable. Again, itisusualtosuppose that,ifeachofan\\ninfinitesequence ofdisjoint setsismeasurable, theprobability oftheir\\nunion isthesumoftheirprobabilities, thatis,probability measures\\naregenerally assumed tobecountably additive. Butthetheorybeing\\ndeveloped heredoesassume thatprobability isdefined forallevents,\\nthatis,forallsetsofstates,anditdoesnotimplycountable additivity,\\nbutonlyfiniteadditivity. Thepresent sectionnotonlyanswers the\\nquestions raisedinthepreceding paragraph, butalsodiscusses there-\\nlationofthenotions oflimiteddomain ofdefinition andofcountable\\nadditivity tothetheory ofprobability developed here.Thegeneral\\nconclusions of thisdiscussion are:First,thereisnotechnical obstacle\\ntoworking withalimiteddomain ofdefinition, and,exceptforexposi-\\ntorycomplications, itmighthavebeenmildlypreferable tohavedone\\nsothroughout. Second, itisalittlebetternottoassume countable\\nadditivity asapostulate, butratherasaspecialhypothesis incertain\\ncontexts. <Adifferent andmuchmoreextensive treatment ofthese\\nquestions hasbeengivenbydeFimetti [D4].\\nFinally, beforeentering uponthemaintechnical workofthis sec-\\ntion,oneeasyquestion abouttherelation between qualitative and\\nquantitative probability willbeanswered andseveralasyetunanswered\\noneswillberaised.\\nAretherequalitative probabilities without anystrictly agreeing meas-\\nure?Yes,because anyqualitative probability thatisfinebutnot\\ntightiseasilyshowntoprovide anexample. Itis,however, anopen\\nquestion, stressed bydeFinetti [D5],whether aqualitative probability\\nonafiniteSalwayshasstrictly agreeing measure. Itwouldalsobe\\ntechnically interesting toknowabouttheexistence ofalmostagreeing\\nmeasures inthesamecontext.t+\\n+Eventhishassincebeenanswered inthenegative byKraft, Pratt,and\\nSeidenherg (1959).Seealso(Fishburn 1970,pp.210-211).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ec0ad30-6bad-41a5-9a75-d223887f9f15', embedding=None, metadata={'page_label': '59', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4] SOMEMATHEMATICAL DETAILS 41\\nThematters tobetreated intherestof thissection arerathertech-\\nnicalmathematically, and,though Iwouldnotdeletethemaltogether,\\nitdoesnotseemjustifiable tolaythenecessary groundwork forpre-\\nsentingtheminanelementary fashion. Somemay,therefore, findit\\nnecessary toskiptherestofthissection altogether, ortoskimitrather\\nlightly.\\nItiswellknownthattheredoesnotexistacountably additive proba-\\nbilitymeasure defined foreverysubset oftheunitinterval, agreeing\\nwithLebesgue measure onthosesetswhereLebesgue measure isde-\\nfined,andassigning thesamemeasure toeachpairofcongruent setst\\n(Problem (b),p.276of[H2]).Ontheotherhand,theredoexistfinitely\\nadditive probability measures agreeing withLebesgue measure onthose\\nsetsforwhichLebesgue measure isdefined, andassigning thesame\\nmeasure toeachofanypairsofcongruent sets; cf.p.32of[B4].The\\nexistence ofsuchmeasures shows,amongotherthings, thatafinitely\\nadditive measure neednotbecountably additive. Again, callingsuch\\nafinitelyadditive extension ofLebesgue measurePanddefiningB<-C\\ntomeanP(B)<P(C),weseeanexample ofaqualitative probability\\nthatisbothfineandtight.\\nAnexample ofaqualitative probability thatistightbutnotfinemay\\nbeconstructed bytakingforStwounitintervals, S;andSe,ineach\\nofwhichfinitely additive extensions ofLebesgue measure, P;andPz,\\naredefined. Thegeneric setBinthisexample istherefore partitioned\\nintoB;=Bf)8,andBy=Bff)So,respectively. Forthisexample,\\nletB<-C; if,andonlyifP,(B,)<P,(C,), orelseP,(B,)=P,(C;),\\nandP2(Be)<Pe(Co). This<:isnotfine,because, forexample, S\\ncannotbepartitioned intoeventsnoneofwhich ismoreprobable than\\nSo.Ontheotherhand, itiseasilyseentobetight.\\nNext,takeStobetheunionofS,;andS.withthemeasures ofP,\\nandP.asdefined inthepreceding example, butmodifythedefinition\\nof<-,sayingB<-C; ifandonlyifP,;(B,;)+Pe(Be)<Pi(Cy)+\\nP2(C2), orelseP1(B1)+P2(Bo)=Pi(Ci)+Pe(Ce),andPi(Bi)<\\nP,(C,). Thisisanexample ofaqualitative probability thatisfinebut\\nnottight.\\nCombining theideasofthetwopreceding examples, itiseasytoex-\\nhibitaqualitative probability thatisneither finenortightbutissuch\\nthatScanbedivided intoarbitrarily manyequally probable events.\\nThusallthequestions raisedintheopening paragraph ofthissection\\nareanswered intheaffirmative.\\n+§.Ulam(1930)provesthatanynonatomic, countably additive probability\\nmeasure definedonallthesubsets oftheunitinterval isinconsistent withthe\\neontinuum hypothesis.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ed43a437-f4b0-4e1e-be13-b2009983d87c', embedding=None, metadata={'page_label': '60', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='42 PERSONAL PROBABILITY [3.4\\nTogetafeeling forthequestion whetherliterally allsetsshouldbe\\nregarded asmeasurable, suppose thatSisacubeofunitvolumeand\\nthattheprobability measurePthatstrictly agreeswith<issuchthat\\ntheprobability ofaparallelepiped isequaltoitsvolume. Itfollows\\nthattheprobability ofanysethavingJordan content isitsJordan\\ncontent, but,ifasethasnotJordan content, acontinuum ofpossibili-\\ntiesisstillopen.Though otherpossibilities areconceivable, itisnot\\nunnatural toconsider anidealized personforwhomthenumerical prob-\\nabilityattached toeachBorelset,oreveneachLebesgue measurable\\nset,isitsLebesgue measure. Togofurtherandtakeseriously compari-\\nsonsbetween setsthatarenotLebesgue measurable, orevenbetween\\nthosethatarenotBorelmeasurable, seemstometobewithout any\\nimplication bearingonreality. Isuppose itmightbeargued,onthe\\ncontrary, thatthereisnofeature ofrealitythatcanproperly beinter-\\npretedbypostulating thattheperson isabletocompare onlysetsfrom\\nasufficiently narrow field,sothatitissimplerandmoreelegant toad-\\nmitallsets.Thequestion seemstobeoneoftaste,butthefollowing\\nremark illustrates whatIconsider anawkwardness insupposing proba-\\nbilitytobeattached toallsets.Itwouldseem, atfirstglance,thatthe\\npersonshouldbeable,ifheissoconstituted, toregard allpairsofgeo-\\nmetrically congruent setsforwhichhemakesanycomparison atallas\\nequivalent, butthefamous Banach-Tarski paradox [B5]showsthat\\nthiscannotbedoneifallsetsareregarded asmeasurable. Ithinkita\\nlittlemoregraceful toabstainfromcomparison between themorebi-\\nzarresetsthantogiveup,orevenmuchmodify,myeveryday notions\\naboutthesymmetry ofsuchprobability problems associated with\\ngeometry.\\nIfoneisunwilling toinsistoncomparison between everypairof\\nsets,orevents; then,inthesamespirit, itisinappropriate toinsiston\\ncomparison between everypairofacts.Allthathasbeen,oristobe,\\nformally deduced inthisbookconcerning preferences amongsets,could\\nbemodified, mutatis mutandis, sothattheclass of eventswouldnot\\nbetheclassofallsubsets ofS,butratheraBorelfield,thatis,ao-alge-\\nbra,onS;thesetofallconsequences wouldbeameasurable space,\\nthatis,asetwithaparticular o-algebra singled out;andanactwould\\nbea measurable function fromthemeasurable spaceofeventstothe\\nmeasurable spaceofconsequences. Indeed, thewholethingcouldbe\\ndoneforabstract o-algebras without reference tosetsatall,andthis\\nmighthavesomeactualadvantage, sinceitwouldmakepossible the\\nidentification ofeventswithpropositions inalmostanyformallanguage,\\nevenoneunable toformulate atallthecomplete descriptions Icall\\nstates.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6745ab69-eef0-424b-add7-2b3ad3e36da1', embedding=None, metadata={'page_label': '61', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.5] CONDITIONAL PROBABILITY 43\\nItmayseempeculiar toinsistono-algebras asopposed tofinitely\\nadditive algebras eveninacontextwherefinitely additive measures are\\nthecentral object,butcountable unionsdoseemtobeessential tosome\\nofthetheorems of§3—forexample, theterminal conclusions ofTheo-\\nrem3.2andPart5ofTheorem 3.3.\\nSomuchofthemodern mathematical theory ofprobability depends\\nontheassumption thattheprobability measures athandarecountably\\nadditive thatoneisstrongly tempted toassume countable additivity,\\noritslogicalequivalent, asapostulate tobeadjoined toP1-6.*+ButI\\naminclined toagreewithdeFinetti [D2],[D4]andKoopman [K9],\\n[K10],[K11]that,however convenient countable additivity maybe,\\nit,likeanyotherassumption, oughtnotbelistedamongthepostulates\\nforaconcept ofpersonal probability unlessweactually feelthatits\\nviolation deserves tobecalledinconsistent orunreasonable. Iknowof\\nnoargument leading totherequirement ofcountable additivity, and\\nmanyofushaveastrongintuitive tendency toregardasnatural proba-\\nbilityproblems aboutthenecessarily onlyfinitely additive uniform\\nprobability densities ontheintegers, ontheline,andontheplane. It\\ntherefore seemsbetternottoassumecountable additivity outright asa\\npostulate, buttorecognize itasaspecial hypothesis yielding, where\\napplicable, alargeclassofusefultheorems.\\n5Conditional probability, qualitative andquantitative\\nConditional preferences amongactsinthelightofa given eventwere\\nintroduced in§2.7.Sincetherelation<amongeventshasbeende-\\nfinedintermsofthecorresponding relationamong acts,wemaywell\\nexpecttoattachmeaning tostatements oftheformB<CgivenD,\\nprovided thatDisnotnull.Thenaturalwaytodosoistotakeapair\\nofactsfandgthattestwhetherB<C(asprescribed bythedefinition\\nof<between actsin§2)andsaythatB<CgivenD,ifandonlyif\\nf<ggivenD.Sincethereismorethanonepairofactsf,gbywhich\\ntheproposition B<Ccanbetested,itisatfirstsightconceivable that\\nnotallsuchpairswouldbeinthesameordergivenD,whichwouldfrus-\\ntratetheproposed definition of<givenD.However,itiseasilyseen\\nthatforanyf,gtestingB<C,f<ggivenD(Dnotnull)isequiva-\\nlenttoBND<CND. Thusitisseennotonlythattheproposed\\ndefinition isunambiguous, butalsothatitisexpressible intermsof\\nprobability comparisons among sets,without directreference toacts\\natall,and,stillfurther, thatthepostulates P1-6applytothecondi-\\ntionalpreference relation<givenDamong acts.Thispreamble sufh-\\ncientlymotivates thefollowing definition andeasytheorem aboutquali-\\ntativeprobability relations generally.\\n1Carried outbyVillegas (1964).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc9a43a7-cd5b-4bef-837d-8bca37ea0c48', embedding=None, metadata={'page_label': '62', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='44 PERSONAL PROBABILITY [3.5\\nIf<-+isaqualitative probability, and0<-D;thenB<-Cgiven\\nD,ifandonlyifBND<-CND.\\nTHEOREM IIf<-isaqualitative probability, thensois<+given\\nD.Ifinaddition <:isfineortight,then<:givenDiscorrespondingly\\nfineortight.\\nIf<:isfine,then,foranyDthatisnotnull,thereexists,inviewof\\nTheorem 3.3,oneandonlyoneprobability measure P(B|D),the\\n(conditional) probability ofBgivenD,thatalmostagreeswith<-.\\nBut,justasonewouldexpectfromthetraditional studyofnumerical\\nprobability, andasmaybeeasily verified,P(BMD)/P(D) considered\\nasa function ofBforfixedDisaprobability measure thatalmost\\nagreeswith<-+givenD.Therefore,\\n(1) P(B|D)=P(BND)/P(D).\\nAswasexplained in§ 2.7,preference among actsgivenBcansug-\\ngestively beexpressed intemporal terms. Analogously, thecomparison\\namongevents given Band,therefore, conditional probability givenB\\ncanbeexpressed temporally. ThusP(C |B)canberegarded asthe\\nprobability thepersonwouldassigntoCafterhehadobserved thatB\\nobtains. Itisconditional probability thatgivesexpression inthetheory\\nofpersonal probability tothephenomenon oflearning byexperience.\\nInaccordance withestablished usage,apairofevents B, Carecalled\\nindependent ifP(BNMC)=P(B)P(C). Moregenerally, asetofevents\\narecalledindependent, ifforeveryfinitesetofthem,sayB,,---,Bn,\\n(2) P(f)\\\\:B,)=I]:PB).\\nObviously, ifDisnotnull,BandDareindependent; ifandonlyif\\nP(B|D)=P(B),inwhichcaseDmayfairlybecalledirrelevant toB.\\nThenotions ofindependence andirrelevance have,sofarasIcan\\nsee,noanalogues inqualitative probability; thisissurprising andun-\\nfortunate, forthesenotionsseemtoevokeastrongintuitive response.\\nTheabsence oftheseanalogues istraceable totheabsence ofaqualita-\\ntiveanalogue forpropositions oftheformP(B |C)<PG |H).Work-\\ningunderarather different motivation fromthatwhichguides this\\nbook,B. O.Koopman [K9],[K10],and[K11]hasdeveloped asystem of\\nqualitative possibility inwhich itismeaningful tocompareBgivenC\\nwithGgivenH.Itistruealsothatforqualitative probability, evenas\\nitisdefined here,someinterconditional comparisons mightbenatu-\\nrallydefined. If,forexample, B<-~B givenCand~G<:Ggiven\\nH,itwouldnotbeunreasonable toestablish theconvention thatB\\ngivenC<:-GgivenH.Thissortofextension isnot,however, highly\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c3d23d89-9671-44cc-be36-f36f773db6ba', embedding=None, metadata={'page_label': '63', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"3.5] CONDITIONAL PROBABILITY 45\\npertinent tomypurpose, forhereIhavelittleinterest inqualitative\\nprobabilities, exceptasafoundation forquantitative probability.\\nThefollowing partition formula iswellknownandeasytoprove:\\n(3) P(C)=2)P(CBj)P(B;)\\nwhereB,isapartition ofSintonon-null sets.If,further, C'isnotnull,\\nitisalsotrivialtoderivethecelebrated Bayes’ rule(ortheorem),\\nP(C|B,P(B:)\\nP(C)\\n_P(C|B)P(B)\\nEPC]B)P(B)(4) P(B;|C) = \\n \\nIllustrations oftheseformulas arefoundinallelementary texbooks on\\nprobability, aswellasinlatersections ofthisbook.\\nFinally, ifneitherBnorisnull,\\nP(B|C)P(C|B)P(BNC)\\nP(B) P(C)  P(B)P(C) (5)\\nwhichmaybegiventhesuggestive reading: Knowledge ofC'modifies\\ntheprobability ofBbythesamefactorbywhichknowledge ofBmodi-\\nfiestheprobability ofC.\\nTheconcept ofrandom variable entersintoalmostanydiscussion of\\nprobability. Experts arefairlywellagreedonthefollowing definition.\\nArandom variable isafunction xattaching avaluex(s)insomeset\\nXtoeverysinasetSonwhichaprobability measurePisdefined. f\\nSuchanStogether withthemeasurePiscalledaprobability space.\\nReal-valued random variables arethemostfamiliar, though ingen-\\neralthevaluesXcanbethingsofanysort. If,forexample, xandy,\\nwithvalues inXandY,respectively, arerandom variables onthe\\nsamemeasure space,anewrandom variable z={x,y}isdefinedby\\nsetting z(s)={x(s),y(s)}.Thevaluesofzarethuselements ofwhat\\niscalledX*Y(readthecartesian product ofXandY),thesetof\\nordered pairswithfirstelement inXandsecond inY.Thesamesort\\nofthingcanbedone,ofcourse, forordered n-tuples andalsoforinfinite\\nsequences ofrandom variables.\\n+Inmanyapplications ofthetheoryofprobability, notallsubsets ofSor ofX\\nareconsidered measurable. Itisthenrequired aspartofthedefinition ofrandom\\nvariable thatxbemeasurable, i.e.,thatforeverymeasurable YCX,thesetof\\ns’ssuchthatz(s)eYbemeasurable.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c30fd55a-0b2e-410d-b007-a5d11f68f5e0', embedding=None, metadata={'page_label': '64', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='46 PERSONAL PROBABILITY [3.6\\nTwo random variables xand y defined onthesame measure space S\\narecalled (statistically) independent; ifand only if,forevery XpCX\\nand YoCY,thetwo events (i.e., subsets ofS)defined bythe condi-\\ntions z(s) «Xo and y(s) «Yo, respectively, are independent.t The\\nextension ofthis definition from pairs toany number ofrandom variables\\nisobvious.\\n6The approach tocertainty through experience\\nIn§3,thetheory ofpersonal probability was, from thepurely math-\\nematical point ofview, reduced tothat ofprobability measures, asub-\\nject that has been elaborately studied, more orless explicitly, forcen-\\nturies. Any mathematical problem concerning personal probability is\\nnecessarily aproblem concerning probability measures—the study of\\nwhich iscurrently called bymathematicians mathematical probability\\n—and conversely. The particular outlook and interpretation implicit\\ninapersonalistic concept ofprobability leads, however, toproblems\\nthat, though perfectly meaningful formathematical probability, might\\nnot otherwise have been emphasized. This section and the succeeding\\none each briefly discuss one such problem. These two problems are\\nselected from among many possibilities for the insight they provide\\ninto the concept ofpersonal probability.\\nBefore studying these problems, it is necessary tobeconversant with\\nthe material inAppendixes 1and 2,which isused inthe immediate\\nsequel and often throughout the rest of this book.\\nAswas brought out in§5,the person learns byexperience. The\\npurpose ofthe present section istoexplore with amoderate degree of\\ngenerality how hetypically becomes almost certain ofthe truth, when\\nthe amount ofhis experience increases indefinitely. To be specific,\\nsuppose that the person isabout toobserve alarge number ofrandom\\nvariables, allofwhich are independent given B;foreach 7,where the\\nB;are apartition ofS. Itistobeexpected intuitively, and will soon\\nbeshown, that under general conditions the person isvery sure that\\nafter making the observation hewill attach aprobability ofnearly 1to\\nwhichever element ofthe partition actually obtains.\\nTo describe the situation formally, letB;beapartition ofSwith\\nP(B;) =B(1). Let x,,r=1,2,---, beasequence ofrandom variables,\\neach taking ononly afinite number ofvalues (which can without loss\\nofgenerality bethought ofasintegers). The restriction toafinite set\\nofvalues could beremoved, but todosowould raise problems ofmathe-\\nmatical technique that, however interesting, are rather beside the point\\n+Where not allsets are measurable, Xoand Yomust, ofcourse, berequired to\\nbemeasurable.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='331dd789-2071-4ece-a089-820b91c43707', embedding=None, metadata={'page_label': '65', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.6]APPROACH TOCERTAINTY THROUGH EXPERIENCE 47\\nofthisbook.Let xdenotethefirstnoftherandom variables x,.Itis\\ntobeborneinmindthatxdependsonn,so,strictlyspeaking, itshould\\nbewritten x(n).Theassumption that,givenB,thex,’sallhavethe\\nsamedistribution isexpressed by\\n(1) P(z,(s)=2,|Bi)=&(z,|2),\\nwhere(2, |1)isdefinedbythecontext. Combining (1)withtheas-\\nsumption thatthex,’sareindependent givenB,,\\n(2)P(x|B,=pP(x(s)={a1,«++,tn} |BY=IIkz, |0),\\nr=]\\nwhereaconventional symbolhasbeenusedforequalbydefinition.\\nThesehypotheses havingbeenlaiddown, itfollowsfromBayes’ rule\\nandthepartition formula (5.3)and(5.2),that\\n_PG |Bs)P(B,) \\n 3 P(B; (3) (B;2)Pw)\\nand Pe)\\n(4) P(x)=276IL&@, |4).\\nInconnection with(3),itmaybeobserved inpassing that,iftheapriori\\nprobability, 6(7),ofB;is0,then,nomatterwhatvaluexisobserved,\\ntheaposteriori probability ofB,;,P(B;|x),isalso0.Thisisanex-\\nampleofthegeneral principle that,ifsomeevent isregarded asvir-\\ntuallyimpossible, thennoevidence whatsoever canlenditcredibility.\\nSimilarly, (3)implies theequally common-sense principle that,ifan\\nobservation zxisvirtually impossible onthehypothesis (i.e.,given)\\nB,,andxisobserved, thenB;becomes virtually impossible aposteriori.\\nItisparticularly interesting tocompare theprobability oftwoele-\\nmentsofthepartition, sayB,andBzfordefiniteness, inthelightofz.\\nP(B,|2)_B(1)&(@;|1)\\n(5)P(Bo|x) B(2) +&(2,|2)\\n_B(1)R’(a,~B(2)°; (er)\\n=BO)ve),\\nB(2)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f084253-d3ea-401b-b0a2-67fb533651ee', embedding=None, metadata={'page_label': '66', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"48 PERSONAL PROBABILITY [3.6\\nwhere self-explanatory abbreviations have been introduced. Equation\\n(5) ismeaningless, ifboth thenumerator and denominator of itsleft-\\nhand side vanish. Ifthedenominator alone vanishes, the fraction may\\nproperly beregarded asinfinite. This will happen; ifand only ifBg is\\nnull, and B, isnot null given x.That is,itwill happen ifand only if\\nB(1)#0,B(2) =0,orifB(1)¥0,and R(x) =~.\\nInmodern statistical usage, R’(x,) and R(z) are the likelihood ratios\\nofB,toBzgiven x,and z,respectively, quantities ofimportance in\\nmany theoretical contexts.\\nIfaperson contemplates making the observation x,that is,finding\\nout thevalue ofx(s) forthe sthat isthe true state ofthe world, itmay\\nproperly beasked howprobable heconsiders itthat Rwill turn out to\\nhave aparticular value. Itwill beshown, barring two banal excep-\\ntions, that, fornsufficiently large, the probability, given B,, that Fis\\ngreater than any preassigned number isalmost 1.The possibility\\nP(B,) =0istobeexcepted, forthen the conditional probability in\\nquestion ismeaningless. The other exception occurs when E(x,|1)=\\nE(x, |2)forevery 7,,that is,when thecommon distribution ofx,given\\nB, isthesame asitisgiven Bo; forthen observation ofx,issimply\\nirrelevant indistinguishing B,from Bg, or,alittle more technically, x,\\nisirrelevant to B,given B,UBo,and\\n(6) P(R'(x,) =1|By)=1.\\nFormally, itistobedemonstrated that, unless P(B,) =0,or(6)\\nholds,\\n(7) limP(R(z)>p|Bi)=1 #for0<p<-.\\nThe problem isquite simple when account istaken ofthe fact that\\nR(x) istheproduct ofnrandom variables, R’(x,), that areindependent\\ngiven B,. Inattacking the problem, two casesare tobedistinguished,\\naccording asthere are orarenot values ofxthat have positive proba-\\nbility given B,but zero probability given Bo.\\nItisinpractice rather fortunate tofind instances ofthe first case,\\nforthen (7)applies with avengeance. Indeed, suppose that\\n(8) P(R'(zr) <@|By)=4, <1.\\nThen\\n(9) P(R=©|B,) =1@”,\\nwhich obviously approaches 1with increasing n.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='79c8136f-41bd-4fed-b1b5-32840f00df83', embedding=None, metadata={'page_label': '67', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"3.6]APPROACH TOCERTAINTY THROUGH EXPERIENCE 49\\nThesecond case,namely¢=1,ismoreinteresting. Sincemuchis\\nknownaboutsumsofidentically distributed independent random varia-\\nbles,itisnatural toinvestigate\\n(10) logR(x)=2)logR’(z,),\\nthereby replacing aproduct byasum.Itiseasilyseenfromthedefi-\\nnitionofR’(z,)thatP(R’(z,)>0|B,)=1,so,inthecasenowat\\nhand,thefunctions logR’(z,)areindependent realbounded random\\nvariables.\\nLetting\\n(11) I=E(logR’(z,) |Bi),\\ntheweaklawoflargenumbers {implies that,forany«>0,\\n(12) limP(logR(x)>n(UI—e)|By)=1,\\nequivalently,i\\n(13) limP(R(x)>e*@-°? |By)=1.\\nTheobjective willtherefore beachieved, ifit isdemonstrated that\\nI>0unless (6)holds.But\\n(14) IE(logR’(z,) |Bi)\\n—logE(R’~*(z,) |Bi)\\n=—log1=0,\\nIV\\nasmaybeargued thus:Theinequality intheabovecalculation isas-\\nsignedasExercise 8inAppendix 2,together withthefactthatequality\\ncanholdin(14)ifandonlyifR’—!(x,) isconstant withprobability\\nonegivenB;.Buttheexpected valueof#’~'(x,) givenB,isequalto\\n1,as(14)assertsandasmaybeeasily verifiedfromthedefinition of\\nR’~'(x,). So,barring theexceptions provided for,IJ>0,andthe\\ndemonstration of(7)iscomplete.\\nBeforetheobservation, theprobability thattheprobability given x\\nofwhichever element ofthepartition actually obtains willbegreater\\nthanais\\n(15) dB)P(P(B; |x)>a|Bi),\\nwheresummation isconfined tothose7’sforwhich6(7)#0.Applica-\\ntionof(14)(extended toarbitrary pairsof7’s)showsthatthecoefficients\\n|Forthedefinition ofthislaw,see,ifnecessary, p.191ofFeller’sbook[F1].\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ce630a7-f3cd-4857-a71c-8da351c37f70', embedding=None, metadata={'page_label': '68', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='50 PERSONAL PROBABILITY [3.7\\nofeach@(z)inthequantity (15),andtherefore thequantity itself,ap-\\nproaches |as7increases; provided onlythatnotwofunctions £(x, |1)\\nand&(x, |2’)arethesame,if6(2)and@(2’)arebothdifferent fromzero.\\nTosummarize informally, ithasnowbeenshownthat,withtheob-\\nservation ofanabundance ofrelevant data,theperson isalmost cer-\\ntaintobecome highlyconvinced ofthetruth,andithasalsobeenshown\\nthathehimself knowsthistobethecase.\\nItmayberemarked, forthosefamiliar withcertain theorems, that\\nmanyrefinements of(7)anditsconsequences couldbeworked outby\\napplication ofthestronglawoflargenumbers, thecentral limittheo-\\nrem,andthelawoftheiterated logarithm toR’(x,).\\nThequantity Jiscoming tobecalledtheinformation ofthedistri-\\nbution ofx,givenB,withrespect tothedistribution ofx,givenBo.\\nMoregenerally, ifPandQareprobability measures, confined (forsim-\\nplicity) toafinitesetXwithelements x;theinformation ofPwith\\nrespecttoQisdefinedby\\nP(x)P(x)log——- (16) 2X(x)logOz)\\nThisusagestemsfromworkofClaudeShannon incommunication en-\\ngineering, agoodaccount ofwhichisgivenin[S11];andalsofrominde-\\npendent workofNorbert Wiener inarelatedcontext [W10].Theideas\\nofShannon andofWiener, though concerned withprobability, seem\\nratherfarfromstatistics. Itis,therefore, something ofanaccident\\nthattheterm“‘information’’ coinedbythemshouldbenotaltogether\\ninappropriate instatistics. Thesituation isstillfurther confused, be-\\ncause,aslongagoas1925,R.A.Fisheremphasized animportant no-\\ntion,whichhecalled“information,” inconnection withthetheory of\\nestimation (Paper 11,Theory ofstatistical estimation in[F6]).Atfirst\\nglance, Fisher’s notionseemsquitedifferent fromthatofShannon and\\nWiener, but,asamatter offact,hisisalimiting formoftheirs.A\\nusefulbutrathertechnical exposition relating theseveral sensesof‘‘in-\\nformation” isgivenbyKullback andLeibler [K15],andIreturntothe\\ntopicin§15.6.+\\n7Symmetric sequences ofevents\\nAproblem oftenposedbystatisticians istoestimate fromasequence\\nofobservations theunknown probability pthatrepeated trialsofsome\\nsortaresuccessful. Onanobjectivistic view,thisproblem isnatural\\nandimportant, foronsuchaviewtheprobability thatacoinfallsheads,\\nforexample, isaproperty ofthecointhatcanbedetermined byex-\\nperimentation withthecoinandinnootherway.Butonapersonalistic\\n|Seealso(Kullback 1961).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9191699c-90b9-409b-bb64-23fcc2be7785', embedding=None, metadata={'page_label': '69', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.7] SYMMETRIC SEQUENCES OFEVENTS 51\\nviewofprobability, strictly interpreted, noprobability isunknown to\\nthepersonconcerned, or,atanyrate,hecandetermine aprobability\\nonlybyinterrogating himself, notbyreference totheexternal world.\\nThissituation hasbeeninterpreted toimplythatthepersonalistic\\nviewiswrong, oratanyrateinadequate, because itapparently cannot\\nevenexpressoneofthemostnaturalandtypicalproblemsofstatistics.\\nThusfarinthisbook,Ihavenotarguedagainst thepossibility ofde-\\nfiningsomeusefulnotion ofobjective probability, buthavecontented\\nmyselfwithpresenting aparticular notion ofpersonal probability.\\nTherefore, atthispointitmightbetempting toseekadualistic theory\\nadmitting bothobjective andpersonal probabilities insomekindofar-\\nticulation withoneanother. DeFinetti [D3]hasshown,however,\\nthatit isnotnecessary todoso,thatthenotionofacoinwithunknown\\nprobability pcanbereinterpreted intermsofpersonal probability\\nalone.\\nThepresent section isdevoted tooutlining thisdevelopment dueto\\ndeFinetti. Intheorganization ofthebookasawhole, itplaysnologi-\\ncallyessential part;itis,rather,adigression intended togiveaclearer\\nunderstanding ofthenotion ofpersonal probability, especially inrela-\\ntiontoobjectivistic views.Theideaspresented herearebutafrag-\\nmentofthoseonthesamesubject in[D2].\\nLetx,beasequence ofrandom variables takingonlythevalues0\\nand1.Thex,’sare,toallintentsandpurposes, asequenceofevents,\\ntherthofwhich istheeventthatz,(s)=1.Tosaythattheseevents\\nareindependent, eachoccurring withprobability p,istosaythatthe\\nprobability ofanyfinitepattern, 71,---,2n,initiating thesequence\\nz,(s)isgivenbytheformula\\n(1) P(a,(s)=tyr,r=1,a)n|Dp)=pil—p)\"%,\\nwhereyisthenumberof1’samongthe2,’sforr=1,---,n.\\nMixtures, inacertain sense,ofsequences ofrandom variables are\\noftenofinterest, astheyalready havebeeninthepreceding section.\\nSuppose, tobeexplicit, thattheworld ispartitioned byB;andthat,\\ngivenB,,thex,’sareindependent withP(z,(s)=1|B;)havingsome\\nfixedvaluep(t).Thentheunconditional probability ofaparticular\\ninitialsequence isamixture oftheprobabilities givenby(1)thus:\\n(2)P(a,(s)=a;r=1,+++,n)=mup(z)*(1—p(t))”4P(B,).\\nItisnatural togeneralize (2)formally thus:\\n8)Play(s)=ar=1,+n)=fh—pam On),\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f1ea68e7-b07d-4e1a-ae6f-264cca6b7da5', embedding=None, metadata={'page_label': '70', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='52 PERSONAL PROBABILITY [3.7\\nwhereMisaprobability measure ontherealnumbers intheinterval\\n(0,1].\\nItisnoteworthy thatequation (8),understood toapplyforeveryn,\\nisequivalent tothecondition thattheprobability thateverynofeach\\nprescribed setofnofthex,’stakesthevalue1is\\n(4) fp\"dM(p).\\nThisfollowsbyarithmetic induction fromtheobvious formula\\n(5)Pla,-(s)=t%3r= 1,---,n)\\n=P(a,(s)=tr5r=1,+++,0;tn41(8)=0)\\n+P@(s)=2,37=1,+++,0;tn4i(s)=1),\\nwhichapplies toanysequence ofrandom variables takingononlythe\\nvalues0and1.\\nEquation (3)canverywellhaveaninterpretation insuchtermsthat\\nthemeasureMisnotmerelyanabstract probability measure, butis\\nactually apersonal probability. Thus, ifpisarandom variable that\\nis(foragivenperson) distributed according toM,and,ifforeachp\\ntheconditional distribution ofthex,’sgivenpisindependent, with\\nP(z,(s)=1)=p;then(8)obtains. Strictly speaking, thenotionof\\nconditional probability asitoccursinthepreceding sentence isusedin\\nasomewhat widersensethanhasbeendefined inthisbook,forthe\\nprobability ofanyparticular pwilltypically bezero.Atleastfor\\ncountably additive measures, thenecessary extension ofconditional\\nprobability andconditional expectation ispresented byKolmogoroff in\\n[K7];itisaconcept ofthegreatest valueinadvanced mathematical\\nstatistics andinprobability generally.\\nHowever, inmostcontexts whereobjectivists speakofanunknown\\nprobability p,there is,sofarasanexclusively personalistic viewof\\nprobability isconcerned, nounknown parameter thatcanplaytherole\\nofpin(3).\\nExamination ofsituations inwhich“unknown’’ probability isap-\\npealed to,whetherJustifiably ornot,showsthat,fromthepersonalistic\\nstandpoint, theyalways refertosymmetric sequences ofeventsinthe\\nsenseofthefollowing definition. Thesequence ofrandom variables\\nx,,takingonlythevalues0and1,isasymmetric ¢sequence, ifandonly\\niftheprobability thatanybofthez,(s)’sequal 1andanycother\\nz,(s)’sequal0depends onlyontheintegers bandc.\\n¢DeFinettiusestheFrenchwordfor“equivalent.’’t\\n+Heandothersnowprefer“exchangeable.” Theconceptseemstohavebeen\\nfirstsuggested hvJulesHaag(1928).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='045c18c8-c07c-41d7-a626-4608919703c6', embedding=None, metadata={'page_label': '71', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.7] SYMMETRIC SEQUENCES OFEVENTS 53\\nItiseasytoverifythatanymixture ofindependent sequences inthe\\nsenseof(3)isasymmetric sequence. DeFinettihasdiscovered that\\ntheconverseisalsotrue.Theseconclusions canbeformally summarized\\nthus:\\nTHEOREM 1Asequence ofrandom variables x,,takingonlythe\\nvalues0and1,issymmetric, ifandonlyifthereexistsaprobability\\nmeasureMontheinterval [0,1]suchthattheprobability thatanypre-\\nscribednofthe2,(s)’sequal1isgivenby(4).Twosuchmeasures,\\nandM’,mustbeessentially thesame,f inthesensethat,ifBisasub-\\ninterval of[0,1],thenM(B)=M’(B).\\nConsidering thatdeFinettihaspublished aproofofTheorem 1in\\n[D2]basedontheFourier integral, thatanyproofofitmustberather\\ntechnical, andthatthetheorem isnotthebasisofanyformalinference\\nlaterin thisbook,itseemsbestnottoproveithere.f\\nItisTheorem 1thatmakes itpossible toexpress propositions re-\\nferringtounknownprobabilities inpurelypersonalistic terms. If,for\\nexample, a statistician weretosay,“Idonotknowthepofthiscoin,\\nbutIamsureit isatmostonehalf,”thatwouldmeaninpersonalistic\\nterms,‘Iregard the sequence oftossesofthiscoinasasymmetric se-\\nquence, themeasureMofwhichassigns unitmeasure totheinterval\\n(0,3]...Thiscondition onMmeansinturnthatforeverynthe(per-\\nsonal)probability ofnconsecutive heads isatmost2~”,asiseasily\\nverified. Idonotinsistthatpropositions couched intermsofaficti-\\ntiousunknown probability arebad,ifunderstood assuggestive abbrevi-\\nations,butonlythatthemeaningfulness ofsuchpropositions doesnot\\nconstitute aninadequacy ofthepersonalistic viewofprobability.\\nThemathematical concept ofprobability measure or,atriflemore\\ngenerally, bounded measure isfundamental tomathematics generally.\\nProbability measures, oftenunderothernames, are,therefore, em-\\nployed inmanyparts ofpureandappliedmathematics completely un-\\nrelated toprobability proper. Forexample, thedistribution ofmass\\ninanotnecessarily rigidbodyisexpressed byabounded measure that\\ntellshowmuchofthebodyisineachregionofspace.Wemust,there-\\nfore,notbesurprised if,eveninstudying probability itself,wecome\\nacrosssomeprobability measures usednottomeasure probability\\ntTechnical note:If‘probability measure’”’ werehereunderstood tomeanacount-\\nablyadditive probability measure ontheBorelsetsof[0, 1],thetheorem wouldre-\\nmaintrue,andtheessential uniqueness of4wouldbecome trueuniqueness.\\n¢Technical note:Theorem 1canbeprovedveryquicklyandnaturally byapply-\\ningthetheoryoftheHausdorff moment problem (pp.8-9of[S13])toM,butthis\\nmethod doesnotseemtogeneralize readily.+\\n+Newandgeneralmethods areinHewittandSavage (1955)andRyll-\\nNardzewski (1957).ForrelatedworkseeBiihlmann (1960),Freedman (1962,\\n1963),Milier-Gruzewska (1949,1950),andRényiandRévész (1963).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='65b9a719-84c5-41d1-bd9b-26e4256983b2', embedding=None, metadata={'page_label': '72', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"54 PERSONAL PROBABILITY (3.7\\nproperbutonlyforauxiliary purposes. Intheeventthatpisnotac-\\ntuallyanunknown parameter, themeasureMpresented byTheorem 1\\nseemsatfirstsighttobesuchapurelyauxiliary measure, but,asamatter\\noffact,Mdoesmeasure certain interesting probabilities, atleastap-\\nproximately. Forexample,letting\\n1n\\n(6) Ln=»Lry\\nnm1\\nitcanbeshownthat\\n(7) limP(z,(s)<6)=M(p<8).\\nInwords,thepersonconsiders theaverage ofanylargenumberoffu-\\ntureobservations tobedistributed approximately thewaypisdis-\\ntributed byM.Thisisanextension oftheordinary weaklawoflarge\\nnumbers, proved in[D2]alongwithacorresponding extension ofthe\\nstronglaw.\\nIfthefirstntermsofasymmetric sequence areobserved, howdoes\\ntherestofthesequence appeartotheperson inthelightofthisobser-\\nvation? Inthefirstplace,italsoisasymmetric sequence butgenerally\\nofastructure different fromthatoftheoriginal sequence, asmaybe\\nshownthus:Let\\n(8) my,m—y)=ptP(a-(s)=ty37=1,---,0),\\nasonemayforasymmetric sequence. Then\\n(9)P(tg(s)=%3dqqEnt+l, --+>n+m| z,(s)=2,r=1,---,n)\\n_P(xp(s)=2p,p=1,°-:,;n+m)\\nP(2,(s)=2,r=1,+++,n)\\nrytz,(n—y)+(m—2))\\n- my,2—Y) |\\nwherezisthenumberof1’samongthez,’s,g=n+1,-°--:,;n+™m.\\nEquation (9)showsthatthesequence x,,g>7,giventhat2z,(s)=2,,\\nr=1,-°--, n,isanewsymmetric sequence characterized by\\nry+2,(ny)+(m—2))\\nTY,n—y)\\nThemeasure M’associated withthenewsequence is,according to\\nTheorem 1,essentially determined bythecondition that  \\n(10) n'(z,m—2)=pf \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c5ef006e-bc7e-421b-ac7f-60c4fd98e219', embedding=None, metadata={'page_label': '73', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.7] SYMMETRIC SEQUENCES OFEVENTS 55\\n(11)fp™dM\"(p)=n’(m,0)\\nx(m+Yn—y)\\nT(Y,n—y)\\nfp™tu(1—p)\"-YdM(p) \\n \\nT(Y,n—y)\\ny1—n—y=fr? ame.\\nTY,n—y)\\nEquation (11)makes itplausible that,exceptfortheslightambiguity\\npermitted byTheorem 1,M’isdefined (forBorelsetsB)by\\n(12)M(B)=77y,n-y)JDY(1—p)\"-¥dM(p),\\nandthiscaninfactbedemonstrated withsomeappealtoslightly ad-\\nvancedmethods pertaining totheHausdorff moment problem (pp.8-9\\nof[S13}).\\nItisnoteworthy that,ifM(B)=0,thenM’(B)=0also.Inthe\\neventthatpreally isanunknown parameter, thismeansthat,ifthe\\nperson isvirtually certain thatthetruepisnotinB,noamount of\\nevidence canalterthatopinion.\\nEquation (12)showsthatM’isgenerally different fromM.Indeed,\\nforfixedn>1,M’isclearlythesameasMforeveryyforwhich\\nr(y,n—y)>0,ifandonlyifMassignsthemeasure 1tosomeone\\nvalueofp.Thatis,thepersonregardsevidence drawnfromasym-\\nmetricsequence asirrelevant tothefuturebehavior ofthesequence,if\\nandonlyifattheoutsetheregards thesequence notmerely assym-\\nmetricbutalsoasindependent.\\nItcanbeshownthatthepersonregards itashighlyprobable that,\\nifheobserves asufficiently longsegment ofasymmetric sequence, the\\ncontinuation ofthesequence willthenbeoneforwhichtheconditional\\nvariance ofp,\\n(13) fvdM\\'(p)—{fpano}\\nwillbesmall.Intheeventthatpisreallyanunknown parameter, this\\nimpliesthattheperson isverysurethatafteralongsequenceofobser-\\nvationshewillassignnearlyunitprobability totheimmediate neigh-\\nborhood ofthevalueofpthatactually obtains—a parallel totheap-\\nproachtocertainty discussed in§6.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b01a8e8-ce82-4ac6-8c4a-a45b22c67a23', embedding=None, metadata={'page_label': '74', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 4\\nCriticalComments\\nonPersonal Probability\\n1Introduction\\nItismytentative viewthattheconcept ofpersonal probability in-\\ntroduced andillustrated inthepreceding chapter is,exceptpossibly\\nforslightmodifications, theonlyprobability concept essential tosci-\\nenceandotheractivities thatcalluponprobability. Ipropose inthis\\nchapter todiscuss theshortcomings Iseeinthatparticular personal-\\nisticviewofprobability, which,forbrevity, shallherebecalledsimply\\n“thepersonalistic view’;topointoutbrieflytherelationships between\\nitandotherviews;tocriticize otherviewsinthelightofit;andtodis-\\ncussthecriticisms holders ofotherviewshaveraised, ormaybeex-\\npectedtoraise,againstit.\\nFromthestandpoint ofstrictlogicalorganization suchcritical re-\\nmarksaresomewhat premature, because thepersonalistic viewitself\\ninsiststhatprobability isconcerned withconsistent actionintheface\\nofuncertainty. Consequently, untilthetheory ofsuchactionhasbeen\\ncompletely outlined inlaterchapters, theviewtobecriticized cannot\\nevenbeconsidered tohavebeenwhollypresented. Practically, how-\\never,itSeemswisenottoconfine criticalcomments totheonepartof\\nthetextthatlogicmaysuggest asappropriate, butrathertotouchon\\ncriticism fromtimeto time, evenatthecostofsomerepetition. Thus,\\nsomeofwhatistobesaidherehasalready beensaidintheintroductory\\nchapter andelsewhere, andsomeofitwillbesaidagain.\\nViewsotherthanthepersonalistic viewaretobediscussed here,but\\nitcannotbetoodistinctly emphasized thattheaccount givenofthem\\nwillbeverysuperficial.t Onefunction ofdiscussing otherviewsisto\\nprovide thereaderwithatleastsomeorientation inthelargeanddi-\\nversified bodyofideaspertaining tothefoundation ofstatistics that\\n|Muchmoreextensive comparative material isgivenbyKeynes [K4],byNagel\\n[N1],andbyCarnap [C1].Koopman [K12]shouldalsobementioned in thiscon-\\nnection.\\n56\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c9e48ea1-8118-48e3-87af-2b1cdde70581', embedding=None, metadata={'page_label': '75', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2] SHORTCOMINGS OFTHEPERSONALISTIC VIEW o¢\\nhavebeenaccumulated. <Alessobvious, butIthinknolessimportant\\nandlegitimate, function istocastnewlightonthepersonalistic view,\\nespecially forthosewhoalready hold,ortendtohold,otherviews.\\n2Someshortcomings ofthepersonalistic view\\nIcananswer, tomyownsatisfaction, somecriticisms ofthepersonal-\\nisticviewthathavebeenbrought tomyattention. Thesepointsare\\ndiscussed laterinthechapter, butinthissection Istateanddiscuss\\nasClearlyasIcanthosethatIfindmoredifficult andconfusing to\\nanswer.\\nAccording tothepersonalistic view,theroleofthemathematical\\ntheory ofprobability istoenable the personusingittodetectincon-\\nsistencies inhisownrealorenvisaged behavior. Itisalsounderstood\\nthat,havingdetected aninconsistency, hewillremove it.Anincon-\\nsistency istypically removable inmanydifferent ways,amongwhich\\nthetheorygivesnoguidance forchoosing. Silenceonthispointdoes\\nnotseemaltogether appropriate, sotheremayberoomtoimprove the\\ntheoryhere.Consider anexample: Theperson findsoninterrogating\\nhimself aboutthepossible outcome oftossing aparticular coinfive\\ntimesthatheconsiders eachofthethirty-two possibilities equally\\nprobable, soeachhasforhimthenumerical probability 1/32.Healso\\nfindsthatheconsiders itmoreprobable thattherewillbefourorfive\\nheadsinthefivetossesthanthatthefirsttwotosseswillbothbeheads.\\nNow,reference tothemathematical theoryofprobability soonshows\\nthepersonthat,iftheprobability ofeachofthe thirty-two possibilities\\nis1/32,thentheprobability offourorfiveheadsoutoffiveis6/32,\\nandtheprobability thatthefirsttwotosseswillbeheads is8/32,so\\nthepersonhascaughthimself inaninconsistency. Thetheorydoesnot\\ntellhimhowtoresolvetheinconsistency; thereareliterally aninfinite\\nnumberofpossibilities amongwhichhemustchoose.\\nInthisparticular example, thechoicethatfirstcomestomymind,\\nandIimaginetoyours,istoholdfasttotheposition thatallthirty-two\\npossibilities areequally likelyandtoaccepttheimplications ofthat\\nposition, including theimplication thatfourorfiveheadsoutoffive\\nislessprobable thantwoheadsoutoftwo.Idonotthinkthatthereis\\nanyJustification forthatchoiceimplicit intheexample asformally\\nstated,butratherthatinthesortofactualsituation ofwhichtheex-\\nample isacrudeschematization theregenerally areconsiderations not\\nincorporated intheexample thatdoJustify, oratanyrateelicit,the\\nchoice.\\nToapproach thematter inasomewhat different way,thereseemto\\nbesomeprobability relations aboutwhichwefeelrelatively ‘‘sure”’as\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e0454283-5fd9-4a34-a0c9-5defef45f8f7', embedding=None, metadata={'page_label': '76', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='58 CRITICAL COMMENTS ONPERSONAL PROBABILITY [4.2\\ncompared withothers.Whenouropinions, asreflected inrealoren-\\nvisaged action, areinconsistent, wesacrifice theunsureopinions tothe\\nsureones.Thenotionof‘‘sure’”’and‘‘unsure”’ introduced hereisvague,\\nandmycomplaint isprecisely thatneitherthetheoryofpersonal proba-\\nbility,asit isdeveloped inthisbook,noranyotherdeviceknowntome\\nrenders thenotion lessvague+ There issometemptation tointroduce\\nprobabilities ofasecondordersothatthepersonwouldfindhimself\\nsayingsuchthingsas‘“‘theprobability thatBismoreprobable thanC\\nisgreaterthantheprobability thatFismoreprobable thanG.’”’But\\nsuchaprogram seemstomeetinsurmountable difficulties.\\nThefirstofthese—pointed outtomebyMaxWoodbury—is this.\\nIftheprimary probability ofaneventBwerearandom variable b\\nwithrespect tosecondary probability, thenBwouldhavea“‘composite”’\\nprobability, bywhichImeanthe(secondary) expectation ofb.Com-\\npositeprobability wouldthenplaytheallegedly villainous rolethat\\nsecondary probability wasintended to obviate, andnothingwouldhave\\nbeenaccomplished.\\nAgain,oncesecondorderprobabilities areintroduced, theintroduc-\\ntionofanendlesshierarchy seemsinescapable. Suchahierarchy seems\\nverydifficult tointerpret, anditseemsatbesttomakethetheoryless\\nrealistic, notmore.\\nFinally, theobjection concerning composite probability wouldseem\\ntoapply,evenifanendlesshierarchy ofhigherorderprobabilities were\\nintroduced. Thecomposite probability ofBwouldherebethelimit\\nofasequence ofnumbers, £,(£y,_1(--: Ee(Pi(B))---)), alimitthat\\ncouldscarcely bepostulated nottoexistinanyinterpretable theoryof\\nthissort.Thereadermaywishtoevaluate forhimself thearguments\\ninfavorofsuchahierarchy putforwardbyReichenbach (Chapter 8,\\n[R2]),takingproperaccount ofthedifferences, between Reichenbach’s\\noverallview,andhismathematical theory, ofprobability ononehand\\nand,ontheother,thepersonalistic viewandmeasure-theoretic mathe-\\nmatical theorythatarethebasisofmycritique ofhigherorderproba-\\nbilities.\\nTheinterplay between the‘‘sure”’and“‘unsure” isinterestingly ex-\\npressedbydeFinetti (p.60,[D2])thus:“Thefactthatadirectestimate\\nofaprobability isnotalways possible isjustthereasonthatthelogi-\\ncalrulesofprobability areuseful.Thepractical objectoftheserules\\nissimplytoreduceanevaluation, scarcely accessible directly, toothers\\nbymeans ofwhichthedetermination isrendered easierandmore\\nprecise.”\\nItmaybeclarifying, especially forsomereadersundertheswayof\\ntheobjectivistic tradition, tomention that,ifaperson is“sure”that\\n+Onetempting representation oftheunsure istoreplace theperson’s single\\nprobability measurePbyasetofsuchmeasures, especially aconvex set.Some\\nexplorations ofthisareDempster (1968),Good(1962),andSmith(1961).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='972219e2-19c5-43bc-8958-7382466908fa', embedding=None, metadata={'page_label': '77', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2] SHORTCOMINGS OFTHEPERSONALISTIC VIEW 59\\ntheprobability ofheadsonthefirsttossofacertainpenny is4,itdoes\\nnotatallfollowthatheconsiders thecoinfair.Hemight,totakean\\nextreme example, beconvinced thatthepenny isatrickonethatal-\\nwaysfallsheadsoralwaysfalls tails.\\nLogic,towhichthetheoryofpersonal probability canbecloselypar-\\nalleled, issimilarly incomplete. Thus, ifmybeliefsareinconsistent\\nwitheachother,logicinsiststhatIamendthem,withouttellingmehow\\ntodoso.Thisisnotaderogatory criticism oflogicbutsimplyapart\\nofthetruismthatlogicaloneisnotacomplete guidetolife.Sincethe\\ntheoryofpersonal probability ismorecomplete thanlogicinsomere-\\nspects, itmaybesomewhat disappointing tofindthatitrepresents no\\nimprovement intheparticular direction nowinquestion.\\nAsecond difficulty, perhaps closely associated withthefirstone,\\nstemsfromthevagueness associated withJudgments ofthemagnitude\\nofpersonal probability. Thepostulates ofpersonal probability imply\\nthatIcandetermine, toanydegreeofaccuracy whatsoever, theproba-\\nbility(forme)thatthenextpresident willbeaDemocrat. Now,itis\\nmanifest thatIcannot reallydetermine thatnumber withgreataccu-\\nracy,butonlyroughly. Since,asiswidelyrecognized, alltheinterest-\\ningandusefultheories ofmodern science, forexample, geometry, rela-\\ntivity,quantum mechanics, Mendelism, andthetheoryofperfectcom-\\npetition, areinexact; itmaynotatfirstsightseemdisquieting thatthe\\ntheory ofpersonal probability should alsobesomewhat inexact. As\\nwillimmediately beexplained, however, thetheory ofpersonal proba-\\nbilitycannot safelybecompared withordinary scientific theories in\\nthisrespect.\\nIamnotfamiliar withanyserious analysis ofthenotionthatatheory\\nisonlyslightly inexact orisalmost true,though philosophers ofscience\\nhaveperhaps presented some.Evenifvalidanalyses ofthenotion\\nhavebeenmade,oraremadeinthefuture, fortheordinary theories of\\nscience, itisnottobeexpected thatthoseanalyses willbeimmediately\\napplicable tothetheory ofpersonal probability, normatively inter-\\npreted; because thattheory isacodeofconsistency forthepersonap-\\nplying it,notasystem ofpredictions abouttheworldaroundhim.\\nThedifficulty experienced in§2.6withdefining indifference seems\\ncloselyassociated withthedifficulty aboutvagueness raisedhere.\\nAnother difficulty withthetheory ofpersonal probability (or,more\\nproperly, withthatlargertheory ofthebehavior ofaperson inthe\\nfaceofuncertainty, ofwhichthetheory ofpersonal probability isa\\npart)isthatthestatement ofthetheory isnotyetnecessarily complete.\\nThusweshallinthenextchaptercomeuponanother proposition that.\\ndemands acceptance asapostulate, and,sinceeventhisleavestheper-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0cecc6f5-6fed-4bc3-be64-8aae789172b7', embedding=None, metadata={'page_label': '78', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='60 CRITICAL COMMENTS ONPERSONAL PROBABILITY [4.4\\nsonagreatdealoffreedom, thereisnotellingwhensomeone willcome\\nuponstillanother postulate thatclamors tobeadjoined totheothers.\\nStrictly speaking, thisisnotsomuchanobjection tothetheoryasa\\nwarning aboutwhattoexpectofitsfuturedevelopment.\\n3Connection withotherviews\\nAllviewsofprobability areratherintimately connected withonean-\\nother.Forexample, anynecessary viewcan beregarded asanextreme\\npersonalistic viewinwhichsomanycriteria ofconsistency havebeen\\ninvoked thatthereisnoroleleftfortheperson’s individual judgment.\\nAgain, objectivistic viewscanberegarded aspersonalistic viewsac-\\ncording towhichcomparisons ofprobability canbemadeonlyforvery\\nspecialpairsofevents,andthenonlyaccording tosuchcriteria thatall\\n(right-minded) peopleagreeintheircomparisons.\\nFromadifferent standpoint, personalistic viewslienotbetween, but\\nbeside,necessary andobjectivistic views;forbothnecessary andobjec-\\ntivisticviewsmay,1ncontrast topersonalistic views,becalledobjective\\ninthattheydonotconcern individual judgment.\\n4Criticism ofotherviews\\nItwillthrowsomelightonthepersonalistic viewtosaybrieflyhow\\nsomeotherviewsseemtocompare unfavorably withit.\\nItisoneofmyfundamental tenetsthatanysatisfactory account of\\nprobability mustdealwiththeproblem ofactioninthefaceofuncer-\\ntainty. Indeed, almosteveryone whoseriously considers probability,\\nespecially ifhehaspractical experience withstatistics, doessooneror\\nlaterdealwiththatproblem, though oftenonlytacitly. Evensome\\npersonalistic viewsseemtometooremotefromtheproblem ofaction,\\nordecision. Forexample, deFinetti in[D2]givestwoapproaches to\\npersonal probability. Ofthese,oneisalmost exactly liketheview\\nsponsored here,exceptonlythatthenotion‘‘moreprobable than”’ is\\nsupposed tobeintuitively evident totheperson,without reference to\\nanyproblem ofdecision. Theotherismoresatisfactory inthisre-\\nspect,beingcouched intermsofbetting behavior, butitseemstome\\nasomewhat lesssatisfactory approach thantheonesponsored here,be-\\ncauseitmustassumeeither thatthebetsareforinfinitesimal sumsor—\\nanticipating thelanguage ofthenextchapter—that theutilityofmoney\\nislinear.Thetheoryexpressed byKoopmanin[K9],[K10],and[K11]\\nandthatexpressed byGoodin[G2]arebothpersonalistic viewsthat\\ntendtoignoredecision, oratanyratekeepitoutoftheforeground;\\nbutthepersonalistic viewexpressed byRamsey in[R1],liketheone\\nsponsored here,takesdecision as fundamental. Ifanynecessary view\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9027b442-6288-4a76-be8b-c90aec3f39a3', embedding=None, metadata={'page_label': '79', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4] CRITICISM OFOTHER VIEWS 61\\ncanbeformulated atall, itmightwellbepossible toformulate itin\\ntermsofdecision, but,sofarasIknow,thenotion ofdecision hasnot\\nappeared fundamental totheholders ofanynecessary view. Itseems\\nfairtosaythatobjectivistic views,bytheirverynature,mustinprin-\\ncipleregard decision assecondary toprobability, ifrelevant atall.\\nYet,theobjectivist A.Waldhasdonemorethananyoneelsetopopu-\\nlarizethenotionofdecision.\\nAshasalready beenindicated, fromtheposition ofthepersonalistic\\nview,thereisnofundamental objection tothepossibility ofconstruct-\\ninganecessary view,butitismyimpression thatthatpossibility has\\nnotyetbeenrealized, and,thoughunable toverbalize reasons, Icon-\\njecturethatthepossibility isnotreal.Twoofthemostprominent en-\\nthusiasts ofnecessary viewsareKeynes, represented by[K4],andCar-\\nnap,whohasbegunin[C1] to statewhathehopeswillproveasatis-\\nfactory necessary (ornearlynecessary) viewofprobability. Keynes\\nindicated intheclosingpagesof[K4]thathewasnotfullysatisfied\\nthathehadsolvedhisproblem andevensuggested thatsomeelement\\nofobjectivistic viewsmighthavetobeaccepted toachieve asatisfac-\\ntorytheory,and Carnap regards [C1]asonlyasteptowardtheestab-\\nlishment ofasatisfactory necessary view,intheexistence ofwhichhe\\ndeclares confidence. Thatthesemenexpressanydoubtatallaboutthe\\npossibility ofnarrowing apersonalistic viewtothepointwhere itbe-\\ncomesanecessary one,aftersuchextensive andcarefullabordirected\\ntowardproving thispossibility, speaksloudlyfortheirintegrity; atthe\\nsametimeitindicates thatthetasktheyhavesetthemselves, ifpossi-\\nbleatall,isnotahightone.\\nKeynes, writing in1921ofwhatareherecalledobjectivistic views,\\ncomplained, ‘‘Theabsence ofarecentexposition ofthelogicalbasisof\\nthefrequency theorybyanyofitsadherents hasbeenagreatdisadvan-\\ntagetomeincriticizing it.”(Chap.VIII,Sec.17,of[K4]). Ibelieve\\nthathiscomplaint applies asaptlytomyposition todayastohisthen,\\nthough Icannot pretend tohavecombed theintervening literature\\nwithanything likethethoroughness Keynes himselfwouldhaveem-\\nployed. Reichenbach, tobesure,presents ingreatdetailaninterest-\\ningviewthatmustbeclassified asobjectivistic [R2],butitseemsfar\\nremoved fromthosethatdominate modern statistical theoryandform\\nthemainsubject ofthefollowing discussion. Whatever objectivistic\\nviewsmaybe,theyseem,toholders ofnecessary andpersonalistic\\nviewsalike, subject totwomajorlinesofcriticism. Inthefirstplace,\\nobjectivistic viewstypically attach probability onlytoveryspecial\\nevents. Thus,onnoordinary objectivistic viewwould itbemeaning-\\nful,letalonetrue,tosaythatonthebasisoftheavailable evidence it\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea2ed0e6-f8de-4dce-97a9-eece60a72c43', embedding=None, metadata={'page_label': '80', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='62 CRITICAL COMMENTS ONPERSONAL PROBABILITY [4.4\\nisveryimprobable, thoughnotimpossible, thatFrance willbecome a\\nmonarchy withinthenextdecade.Manywhoholdobjectivistic views\\nadmitthatsucheveryday statements mayhaveameaning, butthey\\ninsist,depending ontheextremity oftheirpositions, thatthatmeaning\\nisnotrelevant tomathematical concepts ofprobability oreventosci-\\nencegenerally. Thepersonalistic viewclaims,however, toanalyze\\nsuchstatements intermsofmathematical probability, anditconsiders\\nthemimportant inscienceandotherhuman activities.\\nSecondly, objectivistic viewsare,andIthinkfairly,charged with\\ncircularity. Theyaregenerally predicated ontheexistence innature\\nofprocesses thatmay,toasufficient degreeofapproximation, berep-\\nresented byapurelymathematical object,namelyaninfinitesequence\\nofindependent events. Thisidealization issaid,bytheobjectivists\\nwhorelyonit,tobeanalogous tothetreatment ofthevagueandex-\\ntendedmarkofacarpenter’s pencilasageometrical point,which isso\\nfruitful incertaincontexts. When itispointed outtotheobjectivist\\nthatheusestheverytheoryofprobability indetermining thequality\\noftheapproximation towhichherefers,heretortsthattheapplied\\ngeometer—a fictitious character whosereputation forsolidity in science\\nisunquestioned—likewise usesgeometry indetermining thequality of\\nhisapproximations. Letthegeometer thenbechallenged, andhere-\\nplieswithathreefold reference toexperience, saying, “Itisacommon\\nexperience thatwithsufficient experience onedevelops goodjudgment\\nintheuseofgeometry andthenceforth generally experiences success in\\nthepredictions hebasesonit.’’‘“‘Now,’’ saystheobjectivist, ‘‘the\\ngeometer’s answer ismyanswer.”’ Butitseemstocriticsofobjectivistic\\nviewsthat,thoughthegeometer maybeentitled tomakeasmanyallu-\\nsionstoexperience ashepleases, theprobabilist isnotfreetodoso,\\nprecisely becauseitisthebusiness oftheprobabilist toanalyze thecon-\\nceptofexperience. He,therefore, cannotproperly support hisposition\\nbyalluding toexperience untilhehasanalyzed thatconcept, though\\nhecan,ofcourse, alludetoasmanyexperiences ashewishes.\\nTwosortsofmixedviewscallforspecialcomment here.\\nFirst,some(amongthemCarnap [C1];Koopman [K9],[K10],and\\n[K11];andNagel[N1])holdthattwoprobability concepts playarole\\nininference, anobjectivistic oneandapersonalistic oranecessary one.\\nThisdualism istypically justified asnecessary totheanalysis ofsuch\\naconcept asthatofacoinwithunknown probability offallingheads.\\nBut,as§3.7explains, deFinettihasprovided asatisfactory analysis\\nonthebasisofpersonal probability alone.\\nSecond,others—for example, vanDantzig [V1]andFéraud[F2]—\\nfindingtheconventional objectivistic viewscircular forthereasons I\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='568958fb-0ea4-4136-a0ed-0bd6309b68f5', embedding=None, metadata={'page_label': '81', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5] THE ROLE OFSYMMETRY INPROBABILITY 63\\nhave cited, trytobreak the circle byrelatively isolated use ofsubjec-\\ntive ideas. Very crudely, itseems tobetheir position that inany one\\ncontext it is allowable foraperson toact asthough some one event of\\nsufficiently small (objective) probability, chosen athis discretion, were\\nimpossible. Quite apart from the relatively technical question of\\nwhether any consistent mixed view of this kind can beconstructed,\\nholders ofpersonalistic and necessary views alike criticize them asun-\\nnecessarily timid, forthey embrace subjective ideas, but only gingerly.\\n&The role ofsymmetry inprobability\\nAnimportant and highly controversial question inthe foundations\\nofprobability iswhether and, ifso,how symmetry considerations can\\ndetermine the probabilities ofatleast some events.\\nSymmetry considerations have always been important inthestudy\\nofprobability. Indeed, early work inprobability was dominated by\\nthenotion ofsymmetry, for itwas usually either concerned with, ordi-\\nrectly inspired by, symmetrical gambling apparatus such asdice or\\ncards. Toillustrate those classical problems, suppose that agambler is\\noffered several bets concerning the possible outcome ofrolling three\\ndice, where it is tobeunderstood that refraining from any bets atall\\nmay beamong the available “bets.” Which ofthe available bets\\nshould thegambler choose? Perhaps Idistort history somewhat inin-\\nsisting that early problems were framed interms ofchoice among bets,\\nformany, ifnotmost, ofthem were framed interms ofequity, that is,\\nthey asked which oftwo players, ifeither, would have the advantage\\ninahypothetical bet. But, especially from the point ofview ofthe\\nearlier probabilists, such aquestion ofequity istantamount toaques-\\ntion ofchoice among bets, fortoask which oftwo “‘equal”’ betters has\\ntheadvantage istoaskwhich ofthem has the preferable alternative,\\naswas pointed out quite explicitly byD.Bernoulli in[B10].\\nIneffect, the classical workers recommended the following solution\\ntothe problem ofthree dice, with corresponding solutions toother\\ngambling problems:\\n1.Attach equal mathematical probabilities toeach ofthe216 (=6°)\\npossible outcomes ofrolling thethree dice. (There are 6°possibilities,\\nbecause the first, second, and third dice can each show anyof sixscores,\\nallcombinations being possible.)\\n2.Under themathematical probability established inStep 1,com-\\npute theexpected winnings (possibly negative) ofthegambler foreach\\navailable bet.\\n3.Choose abet that has the largest expected winnings among those\\navailable.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02d3b20c-bf23-41cf-922f-11b950172f62', embedding=None, metadata={'page_label': '82', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='64 CRITICAL COMMENTS ONPERSONAL PROBABILITY [4.5\\nAtpresent itisappropriate torefrainfromcriticisms oftheuse\\nmadeofexpected winnings untilthenextchapter andtoconcentrate\\ndiscussion onthenotionthatthe216possibilities shouldbeconsidered\\nequally probable, whichcanconveniently bedonebydrastically reduc-\\ningtheclassofbetsconsidered tobeavailable. Say,fordefiniteness,\\nthattheonlybetstobeconsidered aresimplyeven-money betsofone\\ndollar,thatthetripleofscoresfallsinapreassigned subsetofthe216\\npossibilities. Whenattention isfocusedonthisrestricted classofbets,\\nthetotalrecommendation isseentoimplythattheprobability measure\\ndefined inthefirststepoftherecommendation beadopted astheper-\\nsonalprobability ofthegambler. Toputitdifferently, agamblerwho\\nadoptstherecommendation willholdthe216possibleoutcomes equally\\nprobable notonlyinsomeabstract sense,butalsointhesenseofper-\\nsonalprobability asdefined in§3.2.\\nThenotionthatthe216possibilities shouldberegarded asequally\\nprobable isfamiliar toeveryone; foritistakenforgranted wherever\\ngentlemen gamble aswellasinthestandard high-school algebra courses,\\nwhereitservestoillustrate thetheoryofcombinations andpermutations.\\nTraditionally, theequality oftheprobabilities wassupposed tobe\\nestablished bywhatwascalledtheprinciple ofinsufficient reason, {\\nthus:Suppose thatthereisanargument leading totheconclusion that\\noneofthepossible combinations ofordered scores,say{1,2,3},is\\nmoreprobable thansomeother,say{6,3,4}.Thentheinformation\\nonwhichthathypothetical argument isbasedhassuchsymmetry as\\ntopermitacompletely parallel, andtherefore equally valid,argument\\nleading totheconclusion that{6,3,4}ismoreprobable than{1,2,3}.\\nTherefore, itwasasserted, theprobabilities ofallcombinations must\\nbeequal.\\nTheprinciple ofinsufficient reasonhasbeenand,Ithink,willcon-\\ntinuetobeamostfertileideainthetheory ofprobability; butitisnot\\nsosimpleasitmayappear atfirstsight,andcriticism hasfrequently\\nandjustlybeenbrought against it.Holders ofnecessary viewstypi-\\ncallyattempt toputtheprinciple onarigorous basisbymodifying it\\ninsuchawayastotakeaccount ofsuchcriticism. Holders ofpersonal-\\nisticandobjectivistic viewstypically regardthecriticism asnotalto-\\ngetherrefutable, sotheydonotattempt toestablish aformalpostulate\\ncorresponding totheprinciple butcontent themselves—as Ishallhere\\n—with exhibiting anelement oftruthinit.\\nOneofthefirstcriticisms isthattheprinciple isnotstrictly applicable\\nforapersonwhohashadanyexperience withtheapparatus inques-\\n+Perhaps whatIherecalltheprinciple ofinsufficient reasonshouldbecalledthe\\nprinciple ofcogentreason. SeeSection3of[B15]forthedistinction involved.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dccf14ad-a03c-4683-8e7e-26f0918d3e68', embedding=None, metadata={'page_label': '83', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5] THE ROLE OFSYMMETRY INPROBABILITY 65\\ntion, oreven with similar apparatus. Thus, attempts touse the prin-\\nciple, asIhave stated it,toprove that there isnosuch thing asarun\\nofluck atdice, asactually played, are invalid. The person may have\\nhad relevant experience, directly orvicariously, not only with gambling\\napparatus itself, but also with people who make andhandle it, including\\ncheaters.\\nItisnot always obvious what thesymmetry ofthe information isin\\nasituation inwhich one wishes toinvoke the principle ofinsufficient\\nreason. For example, d’Alembert, anotherwise great eighteenth-cen-\\ntury mathematician, issupposed tohave argued seriously that theprob-\\nability ofobtaining atleast one head intwotosses ofafair coin is2/3\\nrather than 3/4. (Cf. [T3], Art. 464.) Heads, ashesaid, might appear\\nonthe first toss, or, failing that, itmight appear onthe second, or,\\nfinally, might not appear oneither. D’Alembert considered the three\\npossibilities equally likely.\\nItseems reasonable tosuppose that, ifthe principle ofinsufficient\\nreason were formulated and applied with sufficient care, the conclusion\\nofd’Alembert would appear simply asamistake. There are, however,\\nmore serious examples. Suppose, totake afamous one, that itisknown\\nofanurn only that itcontains either two white balls, two black balls,\\norawhite ball and ablack ball. The principle ofinsufficient reason has\\nbeen invoked toconclude that thethree possibilities areequally proba-\\nble, sothat inparticular the probability ofone white and one black\\nball isconcluded tobe1/3. But the principle has also been applied to\\nconclude that there arefour equally probable possibilities, namely, that\\nthe first ball iswhite and the second also, that the first iswhite and the\\nsecond black, etc. Onthat basis, the probability ofone white and one\\nblack ball is,ofcourse, 1/2. Personally, Idonot try toarbitrate be-\\ntween thetwo conclusions but consider that the existence ofthe pair\\nofthem reflects doubt onthenotion that aperson’s knowledge relevant\\ntoany matter admits any full and precise description interms of\\npropositions heknows tobetrue and others about which heknows\\nnothing.\\nMost holders ofpersonalistic views donot find the principle ofin-\\nsufficient reason compelling, because they envisage the possibility that\\naperson may consider one event more probable than another without\\nhaving any compelling argument for his attitude. Viewed practically,\\nthis position isclosely associated with the first criticism oftheprinciple\\nofinsufficient reason, forthe holder ofapersonalistic view typically\\nsupposes that the person isunder the influence ofexperience, and pos-\\nsibly even biologically determined inheritance, that expresses itself in\\nhisopinions, though not necessarily through compelling argument.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='734e3b2e-2547-43d9-91c4-2f3921e3b054', embedding=None, metadata={'page_label': '84', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='66 CRITICAL COMMENTS ONPERSONAL PROBABILITY [4.5\\nHolders ofpersonalistic viewsdoseesometruthintheprinciple of\\ninsufficient reason,because theyrecognize thattherearefrequently par-\\ntitions oftheworld,associated withsymmetrical-looking gambling ap-\\nparatus andthelike,thatmanyanddiverse people allconsider (very\\nnearly) uniform partitions. Aswasillustrated inthepreceding sec-\\ntion,weoftenfeelmore‘‘sure’”’aboutprobabilities derivedfromthe\\njudgment thatsuchpartitions areuniform thanwedoaboutothers.\\nSuchpartitions are,moreover, veryimportant inthattheyprovide\\nsomeeventstheprobability ofwhichtodiversepeople isinagreement.\\nThough theeventsconcerned areoftenofnoimportance inthemselves,\\nagreement aboutthemcan,through thestatistical invention ofran-\\ndomization, contribute toagreement aboutallsortsofissuesopento\\nempirical investigation. Widespread though theagreement aboutthe\\nnearuniformity ofsomepartitions is,holders ofpersonalistic views\\ntypically donotfindthecontexts inwhichsuchagreement obtains\\nsufficiently definable toadmitofexpression inapostulate.\\nHolders ofpurelyobjectivistic viewsseenosense atallintheoriginal\\nformulation oftheprinciple ofinsufficient reason, forituses‘“proba-\\nbility”?inamanner theyconsider meaningless. Buttheytooseean\\nelement oftruthintheprinciple, whichtheyconsider tobeestablished\\nasapartofempirical physics. Thus,forexample, theyregard itasan\\nexperimental fact,admitting someexplanation intermsoftheoretical\\nphysics, thatthreedicemanufactured withreasonable symmetry will\\nexhibiteachofthe216possible patterns withnearlyequalfrequency,\\nifrepeatedly rolledwithsufficient violence onasuitable surface.\\nHolders ofpersonalistic viewsagreethatexperiments or,moregen-\\nerally,experiences determine toalargeextentwhenpeopleemploy the\\nideaofinsufficient reason. Thus,though experiments withgambling\\napparatus, quiteapartfromgambling itself,haveafascination that\\nperhaps exceeds theirrealinterest, suchexperiments arenotaltogether\\nworthless. Ontheonehand,theyprovide strongevidence thataper-\\nsoncannotexpecttomaintain asymmetrical attitude towardanypiece\\nofapparatus withwhichhehashadlongexperience, unless heisvir-\\ntuallyconvinced attheoutsetthatthepossible statesoftheapparatus\\nareequally probable andindependent fromtrialtotrial.Tosayitin\\nthemorefamiliar andsometimes morecongenial language ofobjective\\nprobability, longexperiments withcoins,dice,cards,andthelikehave\\nalwaysshownsomebias,andoftensomedependence fromtrialtotrial.\\nOntheotherhand(andthishastheutmost practical importance), it\\nhasbeenshownthat,withskillandexperience, gambling apparatus, or\\nitsstatistical equivalent, can bemanufactured inwhichthebiasand\\nthedependence fromtrialtotrialareextremely small. Thisimplies\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b366af0-830b-43ad-8589-d7bb63ebd2a1', embedding=None, metadata={'page_label': '85', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.6] HOW CAN SCIENCE USE APERSONALISTIC VIEW? 67\\nthat groups ofvery diverse people canbebrought toagree that repeated\\ntrials with certain apparatus arenearly uniform and nearly independent.\\nThus certain methods ofobtaining random numbers and other outcomes\\nofuniform and independent trials, which are vital tomany sorts of\\nexperimentation, have justifiably found acceptance with the scientific\\npublic. Astimulating account ofpractical methods ofobtaining ran-\\ndom numbers, and random samples generally, isgiven byKendall in\\nChapter 8(Vol. I)of[K2].\\n6How can science use apersonalistic view ofprobability?\\nItisoften argued byholders ofnecessary and objectivistic views alike\\nthat that ill-defined activity known asscience orscientific method con-\\nsists largely, ifnot exclusively, infinding out what isprobably true,\\nbycriteria onwhich allreasonable men agree. The theory ofproba-\\nbility relevant toscience, they therefore argue, ought tobeacodifica-\\ntion ofuniversally acceptable criteria. Holders ofnecessary views say\\nthat, just asthere isnoroom fordispute astowhether one proposition\\nislogically implied byothers, there can benodispute as to theextent\\ntowhich one proposition ispartially implied byothers that arethought\\nofasevidence bearing on it,fortheexponents ofnecessary viewsre-\\ngard probability asageneralization ofimplication. Holders ofobjec-\\ntivistic views say that, after appropriate observations, two reasonable\\npeople can nomore disagree about the probability with which trials\\ninasequence ofcoin tosses areheads than they can disagree about the\\nlength ofastick after measuring itbysuitable methods, forthey con-\\nsider probability anobjective property ofcertain physical systems in\\nthe samesense that length isgenerally considered anobjective property\\nofother physical systems, small errors ofmeasurement being contem-\\nplated inboth contexts. Neither the necessary nor the objectivistic\\noutlook leaves any room forpersonal differences; both, therefore, look\\nonany personalistic view ofprobability as,atbest, anattempt topre-\\ndict some ofthe behavior ofabnormal, oratany rate unscientific,\\npeople.\\nIwould reply that the personalistic view incorporates alltheuniver-\\nsally acceptable criteria for reasonableness inJudgment known tome\\nand that, when anycriteria that may have been overlooked arebrought\\nforward, they will bewelcomed into the personalistic view. ‘The cri-\\nteria incorporated inthe personalistic view donot guarantee agreement\\non allquestions among allhonest and freely communicating people,\\neven inprinciple. That incompleteness, ifone will call itsuch, does not\\ndistress me, for Ithink that atleast some ofthedisagreement wesee\\naround usisdue neither todishonesty, toerrors inreasoning, nor to\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89cfe9ba-de4f-419e-9f46-52c36b6918c9', embedding=None, metadata={'page_label': '86', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='68 CRITICAL COMMENTS ONPERSONAL PROBABILITY [4.6\\nfriction incommunication, though theharmful effectsofthelatterare\\nalmostincapable ofexaggeration.\\nAswasmentioned inconnection withsymmetry, therearepartitions\\nthatdiverse people allconsider nearlyuniform, thoughnotcompelled\\ntothatagreement byanypostulate ofthetheory ofpersonal proba-\\nbility.Ashasalsobeenmentioned andaswillbeexplained later(es-\\npecially in§14.8),through thestatistical invention ofrandomization,\\nagreement aboutpartitions pertaining togambling apparatus ofnoim-\\nportance initselfcanbemadetocontribute toagreement inevery\\npartofempirical science.\\nAnother mechanism thatbringspeoplehavingsome,butnotall,\\nopinions incommon intomorecomplete agreement wasillustrated in\\n§§3.6-7. Indeed, itwasthereshownthatincertaincontexts anytwo\\nopinions, provided thatneither isextreme inatechnical sense,areal-\\nmostsuretobebrought veryclosetooneanother byasufficiently\\nlargebodyofevidence.\\nIthasbeencountered; Ibelieve, that,ifexperience systematically\\nleadspeoplewithopinions originally different toholdacommonopinion,\\nthenthatcommon opinion, anditonly,isthepropersubject ofscien-\\ntificprobability theory. Therearetwoinaccuracies inthisargument.\\nInthefirstplace,theconclusion ofthepersonalistic viewisnotthat\\nevidence bringsholders ofdifferent opinions tothesameopinions, but\\nrathertosimilar opinions. Inthesecond place, itistypically trueof\\nanyobservational program, however extensive butprescribed inad-\\nvance,thatthereexistpairsofopinions, neither ofwhichcanbecalled\\nextreme inanyprecisely defined sense,butwhichcannotbeexpected,\\neitherbytheirholders oranyotherperson, tobe brought intoclose\\nagreement aftertheobservational program.\\nIhave,atleastonce,hearditobjected against thepersonalistic view\\nofprobability that,according tothatview,twopeoplemightbeof\\ndifferent opinions, according asoneispessimistic andtheotheropti-\\nmistic. Iamnotsurewhatposition Iwouldtakeinabstract discussion\\nofwhether thatalleged property ofpersonalistic viewswouldbeob-\\njectionable, butIthinkitisclearfromtheformaldefinition ofqualita-\\ntiveprobability thattheparticular personalistic viewsponsored here\\ndoesnotleaveroomforoptimism andpessimism, howeverthesetraits\\nbeinterpreted, toplayanyroleintheperson’s judgmentofprobabilities.\\n+See(Fisher1934),p.287.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c96584c1-d9c6-4a0e-9bad-eace78cf8884', embedding=None, metadata={'page_label': '87', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 5\\nUtility\\n1Introduction\\nThe postulates P4—-6, introduced inChapter 3,have already led to\\nsimplification ofthe relation <insofarasitapplies toacts ofaspecial\\nbut important form. Indeed, through the introduction ofnumerical\\nprobability, those special comparisons have been reduced toordinary\\narithmetic comparison ofnumbers insuch away that many relations\\namong acts are deducible bysimple and systematic arithmetic calcula-\\ntion. Inthis chapter itwill beshown that the arithmetization ofcom-\\nparison among acts can, with the introduction ofone mild new postu-\\nlate, beextended tovirtually allpairs ofacts.\\nThis far-reaching arithmetization ofcomparison among acts is\\nachieved byattaching anumber U(f) toeach consequence finsuch a\\nway that f<gifandonly iftheexpected value ofU(f) isnumerically\\nless than orequal tothat ofU(g), provided only that the real-valued\\nfunctions U(f) and U(g) are essentially bounded. The provision can\\nfail tobemet only ifthere exist acts that are, sotospeak, distinctly\\npreferable toany fixed reward ordistinctly worse than anyfixed punish-\\nment.\\nAfunction Uthat thus arithmetizes the relation ofpreference among\\nacts will becalled autility. Itwill beshown that the multiplicity of\\nutilities isnot complicated, every utility being simply related toevery\\nother. Ihave chosen touse thename “utility” inpreference toany\\nother, inspite ofsome unfortunate connotations this name has incon-\\nnection with economic theory, because itwas adopted byvonNeumann\\nand Morgenstern when in[V4] they revived the concept towhichit re-\\nfers, inamost stimulating way. Their treatment hasbeen ofsuch wide-\\nspread interest that the introduction ofaname other than “utility” at\\nthe present time would cause more confusion than itcould alleviate.\\nThe next three sections areconcerned with the technical exploration\\nofthe utility concept. Ithink readers interested inthe details will find\\nitbest toread these sections twice asaunit, inthe fashion Ihave been\\nrecommending forother material inwhich definitions and propositions\\n69\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6b5aa54-c90a-420a-a5e4-abdb0b850c20', embedding=None, metadata={'page_label': '88', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"70 UTILITY [5.2\\nareinterlarded withproofs; otherswillbecontent withacursory read-\\ning,omitting proofs.\\nTaking advantage ofthesimplicity afforded bytheintroduction of\\nutility, Itryin§5tomakesomeprogress withtheproblem, pointed\\noutin§2.5,ofspecifying criteria fortheconstruction of‘‘smallworlds.”\\nFinally, §6brieflyreportsthehistory oftheutilityidea.Aseparate\\ncritical section isnotnecessary, because thecriticisms ofthetheoryof\\nutilityknown tomeareincorporated conveniently intothehistorical\\nsection.\\n2Gambles\\nBefore discussing utility, itisexpedient toestablish certain facts,\\nthefirstbeingthatatleastamongaratherrichclassofacts,namely\\nactsconfined withprobability onetoafinitenumber ofconsequences,\\npreference depends onlyontheprobability distribution oftheconse-\\nquences oftheacts.\\nTHEOREM 1\\nHyp. l.fi,--+,fnarenelements ofF,n=>1.\\n2.pi,°**,Pnarenumbers suchthatZp;=1.\\n3.gandhareactssuchthat\\nP(g(s)=fi)=P(h(s)=fi)=Pi)i=Torey n,\\nCONCL. g=h.\\nProor. Thetheorem isobvious forn=1.Itwillbeprovedbyin-\\nduction, supposing henceforth thatn>1.\\nLetBdenotetheintersection ofthetwoeventsthatg(s)=f,and\\nh(s)¥fn,andletCdenotetheintersection ofthetwoeventsthat\\nh(s)=fnandg(s)#fn.ItiseasytoseethatP(B)=P(C).Ccan\\nbepartitioned intoCo,Ci,---,Cr—1,whereCoisanulleventandC;,,\\n}=1,---,n—1,istheintersection ofC'withtheeventthatg(s)=f;.\\nByrepeated application ofConclusion 7ofTheorem 3.3.3,Bcanbe\\npartitioned intoeventsBo,Bi,---,Bn—,suchthatP(B;)=P(C)),\\n1=0,---,n—-—1.\\nLetgo=g,anddefineg;,,stepbystepforz=0,---,m—2thus:\\n(1) gi4i(S)=fn fors¢Cy41,\\n=Sian forS¢Bean,\\n=g;(s) elsewhere.\\nItiseasilyseenfromthefactsofconditional probability thatg;41=\\ng:givenB;,,;UC;41,anditisevenmoreobvious thatgi41=g;given\\n~(BiziUCi4i). Therefore gi41=gi,soQn_1=g.Furthermore,\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d099123e-a6e7-4b26-af13-0145e98f4d0e', embedding=None, metadata={'page_label': '89', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.2] GAMBLES 71\\nP(gi4i(s) =f3)=P(g(s) =f3)=93,80Pn—i(s) =fi)=7,J=1,\\n--+,n. Thus g,_1 1snot only equivalent togbut also satisfies thehy-\\npothesis ofthetheorem relative toh,soitwill suffice toprove the theo-\\nrem forgn—; and hinplace ofgand h.\\nNow gn—1 has been constructed toequal f,inC,except onanull set.\\nTherefore gn; =hgiven CUD,where Disthe subset of~C on\\nwhich Zn-1 =h= fn:\\nItremains only toshow that g,_; =hgiven~(C UD). If~(C UD)\\nisnull, that istrue automatically; henceforth concentrate onthe less\\ntrivial situation. If~(C UD)is not null, then <given ~(C UD)\\nsatisfies allthe postulates assumed thus far, and therefore the conse-\\nquences fi,++, fa—1; thenumbers p,;’=p;/(1 —pn), =1,°+-,n —1;\\nthe acts g,—; and h;and the relation <given ~(C UD)satisfy the\\nhypothesis ofthetheorem foracase inwhich it is supposed already to\\nhave been proved. @\\nInthis chapter the notation 2p;f; will denote the class ofallacts f-\\nforwhich there exist partitions B;ofSsuch that P(B;) =p;and f(s) =\\nf;for s¢B;. Here the f,’s are afinite sequence ofconsequences (not\\nnecessarily distinct), and the p,’s acorresponding sequence ofnon-\\nnegative real numbers such that 2p;=1.Inview ofConclusion 7of\\nTheorem 3.3.3, such aclass ofacts, which will inthis chapter bere-\\nferred toasagamble and denoted by f,g,h,orthe like, always has at\\nleast one element. Theorem 1says, ineffect, that theperson regards\\nallelements ofany gamble asequivalent. Toput itdifferently, ifthe\\nevents B;ofapartition have the probabilities p;,and ifthe act fis\\nsuch that the consequence f;will befall the person incase B;occurs,\\nthen thevalue offisindependent ofhow thepartition B;ischosen.\\nGambles can bemixed, inasense, tomake new gambles, thus: Let\\nf;beafinite sequence ofgambles,\\n(2) f;=mupighis,\\nand o;acorresponding sequence ofnon-negative real numbers such\\nthat 20;=1.The mizture ofthe f;’swith weights o;,denoted 2ao;f;, is\\ndefined by\\n(3) Zot =DIoj| pisfiil\\nt J\\n=x(o;p55) fej,\\nwhich ismeaningful, the f;;’s being consequences and the (¢;p;;)’s being\\nnumbers such that 2(0;p;;) =1.Such mixtures areexemplified byan\\ninsurance policy inwhich the benefit isanannuity payable during the\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3c9e97ca-3454-4d4b-9d2e-1c401181e8fc', embedding=None, metadata={'page_label': '90', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='72 UTILITY [5.2\\nlifeofthebeneficiary, andbyalottery inwhichtheprizesaretickets\\ninotherlotteries.\\nInviewofTheorem 1,itisnatural tosaythatf<gmeansthat,for\\neveryactfintheclassofactscorresponding tof,f<g.Corresponding\\ndefinitions aretobeunderstood forf<g,f<g,f<g,ete.\\nTHEOREM 2Iff,g,andharegambles, and0<p<1;thenpf+\\n(1—p)h<pg+(1—p)h,ifandonlyiff<g.\\nProor. Letf,g;f;,g;;andB;,C;beacts,consequences, andparti-\\ntionssuchthatfandgareamongtheactsrepresented byfandg,re-\\nspectively, withf(s)=f;fors¢B;andg(s)=g;fors¢Cj.\\nConstruct D;;CB;NMC;suchthatP(D,;)=pP(B; NMC;),andlet\\nD=UD,;. ThenP(D)=p,P(B;|D)=P(B,,andP(C;|D)=\\nP(C;).\\nWhatistobeprovedis,ineffect,thatf<ggivenD,ifandonlyif\\nf<g.InviewofTheorem 1itisclearthatwhether thatissoornot\\nforfandgdoesnotdependontheparticular choiceofD;so,withan\\nobvioustemporary extension ofterminology, itistobeprovedthatf<g\\ngivenp,ifandonlyiff<g.\\nIff=ggivenaforevery0<a<1,thereisnothing toprove.\\nOtherwise itcanbeassumed without lossofgenerality that,forsome\\nQo,f<ggivenQo.\\nInviewofTheorem 2.7.2,ifa+8<1,f>ggivena,andf>g\\ngiven8;thenf>ggiven(a+@),andsimilarly f>ggivena/2.\\nMaking useofP6andTheorem 2.7.2,itcaneasilybeshownthat,for\\nanyasufficiently closetoao,f<ggivena.\\nThepreceding threeparagraphs implythat,inthecaseathand,\\nf<ggivenaforeverya,0<a<1.@\\nTHEOREM 3Iff<g,and0<o<p<1, thenpf+(1—p)g<\\nof+(1—o)g.\\nProor. Inviewoftheimmediately verifiable identities,\\npf+(1—p)g=(9p—oFf+[1—(9p—o)]X\\n|g (1—p)—__—__—__—— f+—-—_ g:1-—o 1-—o (4) (p ) (9 o)\\nof+(1—o)g=(p—o)g+[1—(p—o)]X\\na (1—p) fp>\",ote Ti@-~?9\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='798954e1-3e0f-4e60-9977-5dd5e6a8ab17', embedding=None, metadata={'page_label': '91', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3] UTILITY, ANDPREFERENCE AMONG GAMBLES 73\\nthistheorem isaspecialcaseofTheorem 2;unlessp=1,ando=0,\\ninwhichcaseitistrivial.@\\nTHEOREM 4Iff,<fandf;<g<fo,thenthereisoneandonly\\nonepsuchthatpf,+(1—p)f,=g.\\nProor. Itfollowsimmediately fromTheorem 3andtheprinciple of\\ntheDedekind cutfthatthereisoneandonlyoneppsuchthat\\nof,+(1—a)fo<g, ifo>po\\n(5).of,+(1—a)f,>g, ifao<po.\\nAccording to(5),nonumber, exceptpossibly po,cansatisfytheequiv-\\nalencedemanded bythetheorem.\\nFinally, using(5)andP6(muchasitwasusedintheproofofTheo-\\nrem2),itfollowsthatpodoesindeedsatisfytheequivalence. @\\n3Utility,andpreference amonggambles\\nTheideaofutilitycanmostconveniently beintroduced inconnec-\\ntionwithgambles or,equivalently, actsthatwithprobability oneare\\nconfined toafinitenumber ofconsequences, thus:Autility isafunction\\nUassociating realnumbers withconsequences insuchawaythat,if\\nf=Yp,f;andg=2o,g;;thenf<g,ifandonlyif2p,U(f,)<Do;U(g;).\\nWriting U[f]for2p;U(f;), thecondition takestheformU[f]<U[g].\\nSimilarly, itisconvenient tounderstand that, for anactf,\\n(1) Ulf]=E(U()).\\nInthisnotation thefollowing obvious theorem givesaslightly different\\ncharacterization ofutility.\\nTHEOREM 1Areal-valued function ofconsequences, U,isautility;\\nifandonlyiff<gisequivalent toU[f]<U[g],provided fandgare\\nbothwithprobability oneconfined toafinitesetofconsequences.\\nDothepostulates thusfarassumed guarantee thatanyutilities exist\\natall?CanTheorem 1beextended toanevenwiderclassofacts?\\nDoesagreatdiversity ofutilities exist,ordoestherelation<practi-\\ncallydetermine thefunction U?Thesequestions, herementioned in\\ntheorderinwhichtheymostnaturally arise,aremanifestly ofgreat\\nimportance inunderstanding utility. Fortechnical reasons, theywill\\n|Cf.,ifnecessary, anyintroduction tothetheoryoftherealnumbers forexplana-\\ntionofthisprinciple, e.g.,Chapter IT of[G3].\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='279c411e-8493-4ebb-83d7-6aae70e22425', embedding=None, metadata={'page_label': '92', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"74 UTILITY [5.3\\nbeanswered inadifferent order—the thirdfollowed bythefirstinthis\\nsection,andthesecondinthenextsection.\\nIfthereisautilityatall,thereissurelymorethanone,because a\\nutilityplusaconstant andautilitytimesapositive constant arealso\\nobviously utilities; thus:\\nTHEOREM 2IfUisautility,andp,oarerealnumbers withp>0;\\nthenU’=pU+aisalsoautility.\\nCoROLLARY 1Ifthereexistsautility,andiff<g;thenthereex-\\nistsautilityUforwhichU(f)andU(g)areanypreassigned pairof\\nnumbers, provided U(f)<U(g).\\nTheorem 2saysthatanyincreasing linearfunction ofautility isa\\nutility. Thenexttheorem saysthat,conversely, any two utilities are\\nnecessarily increasing linearfunctions ofoneanother.\\nTHEOREM 3IfUandU’areutilities, thereexistnumbers panda\\nsuchthatU’=pU+a,p>0.\\nProor. Thefirststepoftheproofwillbetodemonstrate thefol-\\nlowingidentity forthetwoutilitiesUandU’andforanythreeconse-\\nquences f,g,h.\\n1 1 1\\n(2) U(f)Ug)Uh)=9.\\nUf)Ug)U'(h)\\nIfanytwooftheconsequences f, g,hareequivalent, twocolumns of\\nthedeterminant inquestion areequal,andtherefore thedeterminant\\nvanishes. Itcanbeassumed, then,thatnotwooff,g,andhareequiv-\\nalent;andthereisnolossin generality, asmaybeseenbypermuting\\ncolumns, inassuming f<g<h. Theorem 2.4nowpermits thecon-\\nclusion thatthereisapsuchthatpf+(1—p)h=g.Therefore,\\n1=pl+(1—p)1\\n(3) U(g)=pU(f)+(1—p)U(h)\\nU'(g)=pU'(f)+A—p)U’(h).\\nThusthemiddlecolumn ofthedeterminant islinearly dependent on\\ntheothertwo,sothedeterminant vanishes, aswasasserted.\\nNowletgandhbeanyfixed pairofconsequences suchthatg<h,\\ntheexistence ofsuchapairbeingassuredbyP5.Equation (2)canbe\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56dee72d-2e14-473d-aca8-7320de28cede', embedding=None, metadata={'page_label': '93', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3] UTILITY, ANDPREFERENCE AMONG GAMBLES 75\\nsuccessively rewritten, wherefisanarbitrary consequence, thus:\\n(4)1U(g)U\\'(h) —U(h)U\"(g)] —U(F)[U\\'(A) —Ug)\\n+U\\'(Ff)[U(h) —U(g)]=9,\\nU’(h)—U\\'g) U(g)U\\'(h) —U(h)U\\'G)5 U\\'(f)= U(f)— ;\\n©) W)U(h)—U(g)W) U(h)—Ug)  \\nwhichprovesthetheorem; forU’(h)—U’(g)andU(h)—U(g)are\\nbothpositive.@\\nCOROLLARY 2IfUandU’areutilities suchthat,forsomeg<A,\\nU(g)=U’(g)andU(h)=U’(h);thenUandU’arethesame,thatis,\\nforeveryf,U(f)=U’(f).\\nTosummarize, ifthereisautilityatall,thereareaninfinitenumber,\\nbutthearrayofutilities isnotcomplicated; forallcanbegenerated\\nfromanyonebyincreasing lineartransformations.\\nTurnnowtothequestion ofexistence.\\nTHEOREM 4Thereexistsautility.\\nProor. VonNeumann andMorgenstern proveessentially thistheo-\\nrem,aswellasthepreceding one,intheappendix of[V4].Thefollowing\\nproofistheirs,expressed, astheteacherusedtosay,inmyownwords.\\nForthisproofonly,certain specialnomenclature isintroduced. A\\nsetofgambles Fisconvex; ifandonlyif,forevery f,g¢Fandp,0<p\\n<1,pf+(1—p)geF.Aninterval Iofgambles isthesetofallgam-\\nblesfsuchthat,forsomefixedgandh(whichdetermine theinterval),\\ngsf<h. Ahyper-utility Vonaconvex setFisareal-valued func-\\ntionofthegambles ofF,suchthatf<g,ifandonlyifV(f)<V(g),\\nandsuchthatV(of+(1—p)g)=pV(F)+(1—p)V(g).\\nThefollowing remarks aboutthisspecialnomenclature areobvious\\nandwillberepeatedly usedintheproof,without explicit reference.\\nThesetofallgambles isconvex. Theintersection oftwoconvex sets\\nisconvex. Everyinterval isconvex. There isaninterval containing\\nanyfinitesetofgambles. Ifthereisahyper-utility onthesetofall\\ngambles, itisautilitywhenconfined toconsequences.\\nBythesamemethod thatledtotheproofsofTheorems 2and3,\\nifthereisahyper-utility onFcontaining gandh,withg<h,thenthere\\nisoneandonlyonehyper-utility VonFsuchthatV(g)=0andV(h)\\n=],\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e103b504-f430-47c4-8469-99bcd1f2ca55', embedding=None, metadata={'page_label': '94', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='76 UTILITY [5.4\\nIf|isthe interval determined byg<h,then, according toTheorem\\n2.4, there isforevery finJaunique number, call itV(f), such that\\n(6) =(1— Vif))g +V(Ah.\\nByrepeated use ofTheorem 2.2, itfollows forany f,f’¢Ithat\\n(7) pf+(1—p)f pi(1—ViF))g +Vif)A}\\n+(1—pil —ViF))g +V(F)A}\\n{1—[oV(F) +(1—p)V(Fg\\n+[oV(F) +(1—p)V(F)IA,\\nsoVisahyper-utility onthe convex set I.\\nFrom here oninthis proof, letg,h be afixed pair ofconsequences with\\ng<h. Making use ofthe preceding two paragraphs, there isaunique\\nhyper-utility assigning the values 0and 1togand h,respectively, on\\nany one interval containing gand h.The intersection oftwo such in-\\ntervals isaconvex setcontaining gand h,and on theintersection the\\nhyper-utilities associated with thetwo intervals areboth hyper-utilities\\nattaching 0and 1togand h,respectively; they must, therefore, be\\nequal toone another ontheintersection.\\nAny gamble fisanelement ofsome interval containing gand h.\\nLet V(f) bethe common value assigned tofby allthe hyper-utilities\\nthat aredefined onintervals containing f,g,and hand that assign the\\nvalues 0and 1togand h,respectively. Since there isalways atleast\\none such interval forany gamble f,the function Visdefined forall\\ngambles.\\nThe proof will becomplete when itisshown that Visahyper-utility\\nfortheconvex set ofallgambles. Let fand f’beany two gambles and\\npanumber, 0<p<1. There isaninterval containing f,f’,g,h,and\\npf+(1—p)f’.. Inthat interval the function Visahyper-utility.\\nTherefore V(of+(1—p)f’) =pV(f) +(1—p)V(F’) andVf)<V(f),\\nifand only iff<f.@\\n4The extension ofutility tomore general acts\\nThe requirement that anact have only afinite number ofconse-\\nquences may seem, from apractical point ofview, almost norequire-\\nment atall. To illustrate, thenumber oftime intervals that might\\npossibly bethe duration ofahuinan lifecan beregarded asfinite, if\\nyou agree that the duration may aswell berounded tothe nearest\\nminute, orsecond, ormicrosecond, and that there isalmost nopossi-\\nbility ofitsexceeding athousand years. More generally, itisplausible\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a9216119-6f2e-4236-a3f9-1deaccba7986', embedding=None, metadata={'page_label': '95', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.4] THE EXTENSION OFUTILITY TOMORE GENERAL ACTS 77\\nthat, nomatter what set ofconsequences isenvisaged, each conse-\\nquence can bepractically identified with some element ofasuitably\\nchosen finite, though possibly enormous, subset. Itmight therefore\\nseem oflittle ornoimportance toextend the concept ofutility toacts\\nhaving an infinite number ofconsequences. Ifthat argument were\\nvalid, itcould easily beextended toreach the conclusion that infinite\\nsets are irrelevant toallpractical affairs, and therefore toallparts of\\napplied mathematics. But itisone ofthe most profound lessons of\\nmathematical experience that infinite sets, tactfully handled, can lead\\ntogreat simplification ofsituations that could, inprinciple, but only\\nwith enormous difficulty, betreated interms offinite sets. How diffi-\\ncult itwould betostudy geometry ifone madeat the outset the ‘‘sim-\\nplifying assumption’ that toallintents and purposes atmost 10!:°°\\npoints inspace can bediscriminated from one another! Again, itis\\ngenerally more convenient and fruitful tothink ofthe annual cash in-\\ncome ofanindividual orfirm asacontinuous variable with aninfinite\\nnumber ofpossible values than asadiscrete variable confined tosome\\nlarge finite number ofvalues, even ifitisknown that theincome must\\nbesome integral number of cents less, say, than 10°.\\nOne way toextend the concept ofutility toacts with an infinite\\nnumber ofconsequences would betopostulate: IfU[f] and U[g] both\\nexist (the values +o and—o being regarded aspossible); f<g,if\\nand only ifU[f]<U[g]. Iseenoserious objection tomaking this as-\\nsumption outright, though itmight becomplained that theassumption\\nismotivated more bygeneral mathematical intuition and experience\\nthan by intuitive standards ofconsistency among decisions, which I\\nhave tried totake asmy sole guide thus far. Astatement almost as\\nstrong asthe one inquestion can, however, bederived onadjoining a\\nnew postulate, P7, more inthe spirit ofPl-6. That rather technical\\nprogram will becarried out inthe next several paragraphs. Those not\\ninterested can safely skip tothe paragraph following Corollary 1on\\npage 80.\\nSuppose that every possible consequence ofthe actgisatleast as\\nattractive totheperson asthe act fconsidered asawhole; then itseems\\ntome within the spirit ofthe sure-thing principle toconclude that\\nf<g;thesame mightas fairly have beensaid forthe relations >,and\\nalso forthetwo relations <given Band>given B. This ideais for-\\nmalized inthe following postulate, which, according tothe conven-\\ntions ofmathematical double-talk, istobeinterpreted astwo proposi-\\ntions—one having <and the other >throughout.\\nP7 Iff<(>) g(s) given Bforevery s¢B,then f<(>) ggiven B.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00610d53-9e66-4e4e-a6f0-b667c8b0bd89', embedding=None, metadata={'page_label': '96', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='78 UTILITY [5.4\\nAttention has been called tothemathematically useful fact that, if\\nP1-6 apply toarelation <,then they also apply toany relation <\\ngiven B,provided Bisnot null. Itisobvious that the sameis true for\\nP1-7, afact that will be used often. Itisalso noteworthy that P1-7\\nobviously imply the propositions that arise ifinthem every instance\\nofthe sign<isreplaced by>and every instance of>isreplaced by\\n<. Therefore inany deduction from P1-7 every instance ofthe signs\\n<and >can bereversed toproduce adeduction that may becalled\\nthesymmetric dual ofthe original deduction. This remark, alegitimate\\nchild ofthe principle ofinsufficient reason, has not been important\\nheretofore, because almost alldeductions thus farmade have been their\\nown symmetric duals. Since that will not besoofsome ofthelemmas\\ninthe present section, much needless writing and thinking can besaved\\nbyagreeing atthe outset that, once aresult isproved, itand itssym-\\nmetric dual may beused asifboth had been explicitly proved.\\nBefore going towork with P7,some may wish toseeanexample of\\namathematical structure satisfying P1-6 but not satisfying P7. More-\\nover, understanding ofsuch anexample will domuchto clarify theuses\\ntobemade ofP7. Toconstruct the example, begin byletting Sbea\\nset carrying afinitely additive probability measure Punder which S\\ncan bepartitioned into subsets ofarbitrarily small probability. Let\\nthe setofconsequences bethehalf-open interval ofnumbers 0<f<1.\\nLet U(f) =f,Ulf]=E(f), and\\n(1) Vif]=limP{f(s) 21—¢}.\\nSince the probability in(1)decreases with e,there isnoquestion about\\nthe existence ofthe limit. Now let W(f] =U[f]+V/[f], and define\\nf<g tomean that W[f] <Wlg]. Checking postulates P1-6, itwill\\nbefound that the<thus defined satisfies them all,and that what has\\nhere been called U(f) isindeed autility for<. But if,forexample,\\nthere isanfsuch that U[f]=V[f]=4,P7 isviolated, ascanbeseen\\nbycomparing ftothe act that, foreach s,takes asvalue themaximum\\nof2and f(s). Whether there canbesuch anf,may,so farasIknow,\\ndepend onthe choice ofSand P. But, ifthe positive integers aretaken\\nasS,andPissochosen that though theprobability ofany one integer\\nis0the probability ofthe set ofeven integers is1/2, apossibility as-\\nsured bythenote toSection 3ofChapter IIonp.231 of[B4], the func-\\ntion equal to0attheodd integers and equal to(1—1/n) ateach even\\nnis such anf. Finite, asopposed tocountable, additivity seems tobe\\nessential tothis example; perhaps, ifthe theory were worked out ina\\ncountably additive spirit from the start, little ornocounterpart ofP7\\nwould benecessary.\\n+Fishburn (1970, Exercise 21,p.213) has suggested anappropriate weak-\\nening ofP7.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8071a753-32dd-4bc4-a17b-7d70e2d3f204', embedding=None, metadata={'page_label': '97', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.4] THE EXTENSION OFUTILITY TOMORE GENERAL ACTS 79\\nSeveral lemmas depending onP7arenow tobeproved preparatory\\ntoproving that U[f] governs preference foravery large class ofacts.\\nItistobeunderstood throughout the section thatUisany fixed utility.\\nThe truth ofeach lemma is intuitively clear, inthe sense that each could\\njustifiably beaccepted asapostulate ifneed be. Since they are also\\neasy toprove and ofsecondary interest, condensed proofs will suffice.\\nLEMMA 1 If,forevery consequence h,f<h,andg <h;then= g.\\nProor. Consider inthe light ofP7that f<g(s) and g<f(s) for\\nevery Ss.@\\nLEMMA 2 Ifthere exists aconsequence fpsuch that f<fo,and if\\nU(f(s)) <Uoforevery s,then there exists agamble gsuch that f<g\\nand U[g]<Uo.\\nProor. IfU(fo) <Uo,then gcan betaken toconsist offoalone.\\nOtherwise, letf;beany consequence such that U(f,) <Uoand letg\\nbetheunique mixture offoand f;such that U(g) =Uo.@\\nLEMMA 3\\nHyp. 1.The B,’s, 1=1,---, n,areapartition, and the U,’s are\\ncorresponding numbers.\\n2.fisanactsuch that U(f(s)) <U;for s«B,.\\n3.fisagamble such that f<f.\\nCoNCL. Ulf]<2U;P(B;).\\nProor. Ifthelemma were false, itwould befalse even forsome f<f.\\nThen itmay beassumed, modifying fifneed bebymeans ofP6and\\nLemma 1,that there exists foreach 7anf;such that f<f;given B,.\\nNow,inview ofLemma 2,there exists foreach 7ag;such that f<g;\\ngiven B;and U[g,;] <U;. Let g=2P(B;)g:, and observe that f<\\nf<g. Therefore, U[f]<U[g] =2P(B,;)U(g:) <2P(B,)U;. @\\nAn act will becalled bounded ifitsutility is,according toordinary\\nmathematical usage, anessentially bounded random variable; the no-\\ntion isput inamore formal andself-contained way asfollows: Abounded\\nact isanact fsuch that, forsome two numbers Upand U;,P{Uo <\\nU(f(s)) <U,;}=1.The definition isclearly not dependent onthe\\nchoice ofU.\\nTHEOREM 1 Iffand garebounded, then f<g,ifand only if\\nUlf]<Ulg].\\nProor. Ifthere exist gand hsuch that g<f<Ah,then thereis,\\nbyTheorem 2.4, amixture fofgand hsuch that f=f.The null event\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0a2e0148-3e30-4eea-b90b-823c47df72fc', embedding=None, metadata={'page_label': '98', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='80 UTILITY [5.4\\nonwhichU(f(s)) isnotbetween UpandU,;mayaswellbedisregarded;\\ntherestcanbepartitioned inton+1eventsB;definedbythecondition\\nthats¢B; ifandonlyifV;_;<U(f(s))<Vi,7=1, ---,n+1,\\nwhere\\n(2)vi=|(1-*)uo+2 0).i=0,---,n4+1.\\nnN nL\\nApplying Lemma3anditssymmetric dual,\\n(3) 2V;_1P(B,) <U[f]<2VP(B)).\\nSimilarly, according toExercise 3ofAppendix 1,\\n(4) 2V;-1P(Bi) <Ulf]<2V:P(B,).\\nTherefore\\n(5) |Ulf]—UIFl|<2(Vi—Vin)P(B,)=(Ui—Uo)/n,\\nwhence U(f)=U(f).\\nToconsider theremaining case,suppose thatthebounded actfex-\\nceeds (isexceeded by)everyconsequence; callitforthemoment big\\n(little).According toLemma1,allbig(and,dually, alllittle)actsare\\nequivalent tooneanother. Furthermore, itis,forexample, easilyseen\\nthat,ifanactisbig,thenfore>0,\\n(6) P{U(I(s))2supU(f)—J=1.\\n(Somemaybemorefamiliar withthenotation ‘““LUB” and“GLB,”\\nread“leastupperbound” and‘‘greatest lowerbound,” thanwiththe\\ncorresponding “‘sup”and“‘inf,”’read‘‘supremum”’ and“infimum.” If\\neventheseoldertermsarenotfamiliar, seeExercise 4ofAppendix 2.)\\nTherefore, iftherearebig(little)acts,theyallhavethesameexpected\\nutility,namely supU(f)(infU(f)).\\nSuppose nowthatf<g.Itispossible thatfandgarebothlittle;\\nthatfislittle,andgisequivalent tosomegamble; thatfislittleand\\ngbig;thatfandgareeachequivalent tosomegamble; thatfisequiva-\\nlenttosomegamble, andgisbig;or,finally, thattheyarebothbig.\\nIneachofthesecases,asimpleargument showsthatU/[f]<Ulg].\\nTheconverse arguments aresimilar.@\\nCoroLuary 1Iffandgarebounded, andP(B)>0,thenf<g\\ngivenB,ifandonlyifE(U(f)—U(g)|B)<0.\\nItwouldbepossible toexploreunbounded actsforwhichexpected\\nutilityexiststoseewhether expected utilitygoverns preferences among\\nevensuchactsunderpostulates P1—7orundersomeextension ofthem+\\n+PeterFishburn (1970,pp.194,206-207) andIhavesincediscovered to\\nmysurprise thatthesepostulates implybounded utility,whichputsthenext\\nseveralparagraphs inanewlight.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c870392-0ac2-4c52-89e2-b9b3fb405a86', embedding=None, metadata={'page_label': '99', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.4]THEEXTENSION OFUTILITY TOMOREGENERAL ACTS 81\\nIdonotthink,however, thatthequestion issufficiently interesting to\\nwarrant attention here,especially sincethereissomereason, firststated\\nbyGabriel Cramerinaletterpartially reproduced in[B10],topostulate\\nthatthereareupperandlowerbounds toutility, inwhich caseallacts\\nwouldnecessarily bebounded.\\nEvenwithout P7,thepostulates imply, inthefollowing sense,that\\nnogamblehasinfiniteorminusinfinite utility.\\nAnactfhasinfinite(minus infinite) utility; ifandonlyif,forsome\\ng<(>)h andforevery«>0,thereisaBwithP(B)<e¢andsuch\\nthattheactequaltofonBandto gon~Bexceeds(isexceeded by)h.\\nAgamble oraconsequence wouldbesaidtohaveinfinite (minus tn-\\nfinite)utility, ifoneoftheactscorresponding toithadinfinite (minus\\ninfinite) utility.\\nIndeed,Theorem 2.4,adeduction fromP1-6,obviously impliesthat\\ntherearenoinfinite orminus infinite gambles orconsequences. It\\nmay,however, bementioned thatPascalheldthat,inJustthesense\\nathand,salvation isaninfiniteconsequence ({P2],pp.189-191). Again,\\nitisoftensaid,ineffect,thattheutilitytoapersonofimmediate death\\nisaconsequence ofminusinfinite utility,butcasualobservation shows\\nthatthisisnottrueofanyone—at leastnotofanyonewhowouldcross\\nthestreettogreetafriend. Inthesamevein,medicine oftengiveslip\\nservicetotheideathatthedeathofapatient isofminusinfinite utility,\\nand,ofcourse, doctors dogotogreatlengths tokeeptheirpatients\\nalive;butadoctorwhotooktheideatooseriously wouldmakeanui-\\nsanceofhimselfandsoonfindhimselfwithnopatients totreasure.\\nIftheutilityofconsequences isunbounded, sayfromabove,{ then,\\neveninthepresence ofPI-7,acts(though notgambles) ofinfinite\\nutilitycaneasilybeconstructed. Mypersonal feeling isthat,theo-\\nlogicalquestions aside,therearenoactsofinfinite orminus infinite\\nutility,andthatonemightreasonably sopostulate, whichwouldamount\\ntoassuming utilitytobebounded.\\nJustifiable though itmightbe,thatassumption wouldentailacer-\\ntainmathematical awkwardness inmanypractical contexts. Forex-\\nample,aswillbediscussed atgreaterlengthinChapter 15,itsometimes\\nseemsreasonable tosuppose thatthepenalty foractingasthough a\\nparticular unknown numberwerefiinstead ofitstruevalue,u,ispropor-\\ntionalto6?=(u—f)*.But,ifthepossiblevaluesofuareunbounded,\\nthensoarethepossible valuesof6,soutility isheretakentobeun-\\nbounded. Onclosescrutiny ofsuchanexample onealwaysfindsthat\\n|Thatis,if,foreveryV,thereisaconsequence fsuchthatV<U(f).This\\nmanner ofspeaking ispermissible; because inviewofTheorem 3.3,ifoneutility is\\nbounded,allare.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2ab3866e-0e18-42c5-9ebe-6cea4fdffb0d', embedding=None, metadata={'page_label': '100', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='82 UTILITY [5.5\\nitisnotreallyreasonable toassume thepenalty evenroughly propor-\\ntionalto6”forlargevaluesof5”,butratherthatlargevaluesaresoim-\\nprobable thattheerrormadeinmisappraising thepenalty associated\\nwiththemisnegligible compared tothesaving insimplicity resulting\\nfromthemisappraisal. Iftheassumption ofbounded utilityweremade\\npartofthetheory ofpersonal probability, thenanyexample inwhich\\nunbounded utility isusedformathematical simplicity wouldbeincon-\\ntradiction tothepostulates. Ipropose, therefore, nottoassumebounded\\nutilityformally, buttoremember thatproblems involving unbounded\\nutilityaretobehandled cautiously.\\nTotakestockofthechapter thusfar,utilityhavingbeenestablished,\\nitisnowsuperfluous toconsider thatconsequences maybeofallsorts,\\nsincethepostulates implythatinvirtually everycontext aconsequence\\nisadequately characterized byitsutility,someoneutilityfunction\\nhavingbeenchosenfromthelinearfamily ofpossibilities. Therefore,\\nunlessthecontrary isclearlyindicated, f,g,andAwillhenceforth mean\\nnotexactly consequences inthesenseusedtodate,butratherreal\\nnumbers measuring utility inunitstobecalledutiles. Correspondingly,\\nanactfwillhenceforth beunderstood tobeareal-valued random varia-\\nble.Theentiretheory ofpreference, atleastforbounded acts,can\\nnowbesummarized bythefollowing résumé:\\nRf<ggivenB,ifandonlyifP(B)=0,orE(f—g|B)<0.\\nFromnowon,thoughnotformulated asapostulate, it istobeassumed\\nwithout further quibbling thatRholds,provided onlythatE(f)and\\nE(g)existandarefinite;noattempt willbemadetocompare actsfor\\nwhichtheexpected valuedoesnotexistorisinfinite.\\nIfaperson isfreetodecideamongasetFofacts,hewillpresumably\\nchooseonetheexpectation ofwhich isv(F),where\\n(7) v(F)=supE(f),\\nprovided thatsuchaoneexists.Thisprovision mustbementioned,\\neventhough asetFforwhichv(F)=©will,byconvention, notbe\\nconsidered togiverisetoavaliddecision problem;for,ifFisinfinitein\\nnumber, theremaybenoactinFwithexpectation quiteasgreatas\\nv(F).Nonetheless, v(F)may,inasense,beregarded asthevalueor\\nutilityofthesetofactsF,asisdiscussed inthepenultimate paragraph\\nof§6.5.\\n5Smallworlds\\nAllusion wasmadeinthepenultimate paragraph of§2.5totheprac-\\nticalnecessity ofconfining attention to,orisolating, relatively simple\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93639390-3570-415a-9223-4a88f6db8b74', embedding=None, metadata={'page_label': '101', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.5] SMALLWORLDS 83\\nsituations inalmost allapplications ofthetheory ofdecision developed\\ninthisbook.Aswasmentioned there,Ifinditdifficult tosaywith\\nanycompleteness howsuchisolated situations areactually arrived at\\nandjustified. Thepurpose ofthepresent section istotakesomesteps\\ntowardthesolution ofthatproblem or,atanyrate,tosettheproblem\\nforthasclearlyasIcan.Thissection, though importantforacritical\\nevaluation ofthethesisofthisbook,isnotessential toacasualreading.\\nMaking anextreme idealization, whichhasinprinciple guidedthe\\nwholeargument ofthisbookthusfar,apersonhasonlyonedecision\\ntomakeinhiswhole life.Hemust,namely, decidehowtolive,and\\nthishemightinprinciple doonceandforall.Though many,likemy-\\nself,havefoundtheconcept ofoveralldecision stimulating, it iscer-\\ntainlyhighlyunrealistic andinmanycontexts unwieldy.t Anyclaim\\ntorealismmadebythisbook—or indeedbyalmostanytheory ofper-\\nsonaldecision ofwhichIknow—is predicated ontheideathatsomeof\\ntheindividual decision situations intowhichactualpeopletendtosub-\\ndividethesinglegranddecision dorecapitulate inmicrocosm themech-\\nanismoftheidealized granddecision. Oneapplication ofthetheory\\nofutilitytooverall decisions has,however, beenattempted byMilton\\nFriedman in[F11].\\nTheproblem ofthissection istosayasclearlyaspossible whatcon-\\nstitutes asatisfactory isolated decision situation. Thegeneralmethod\\nofattackIpropose tofollow, for wantofabetterone,istotalkinterms\\nofthegrandsituation—tongue incheek—andinthosetermstoanalyze\\nanddiscuss isolated decision situations. Ihopeyouwillbeableto\\nagree,asthediscussion proceeds, thatIdonotleantooheavily onthe\\nconcept ofthegranddecision situation.\\nConsider asimpleexample. Jonesisfacedwiththedecision whether\\ntobuyacertainsedanforathousand dollars, acertainconvertible also\\nforathousand dollars, ortobuyneitherandcontinue carless. The\\nsimplest analysis, andtheonegenerally assumed, isthatJonesisde-\\ncidingbetween threedefiniteandsureenJoyments, thatofthesedan,\\ntheconvertible, orthethousand dollars. Chanceanduncertainty are\\nconsidered tohavenothing todowiththesituation. Thissimpleanal-\\nysismaywellbeappropriate insomecontexts; however, itisnotdiffi-\\nculttorecognize thatJonesmustinfacttakeaccount ofmanyuncer-\\ntainfuturepossibilities inactually making hischoice.Therelative\\ntUnrealistic thoughtheconcept is,itwouldbeamistake, arisingoutofelliptical\\npresentation, tosuppose thattheconcept predicates thechoiceofacomplete life-\\nlongpolicybynew-born babies. Ifapersoneverreached suchalevelofmaturity\\nastobeabletomakealifelong choiceforhislifefromthattimeon,hewouldthen\\nbecomeapersontowhomtheconcept couldbeliterally applied.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='471be58b-a30b-495b-b821-0cbfdad69aba', embedding=None, metadata={'page_label': '102', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='84 UTILITY [5.5\\nfragility oftheconvertible willbecompensated onlyifJones’shopeto\\narrange alongvacation inawarmandscenicpartofthecountry ac-\\ntuallymaterializes; Joneswouldnotbuyacaratallifhethought it\\nlikelythathewouldimmediately befacedbyafinancial emergency\\narisingoutofthesickness ofhimself orofsomememberofhisfamily;\\nhewouldbegladtoputthemoneyintoacar,oralmostanydurable\\ngoods, ifhefearedextensive inflation. Thisbringsoutthefactthat\\nwhatareoftenthought ofasconsequences (thatis,sureexperiences of\\nthedeciding person) inisolated decision situations typically areinre-\\nalityhighlyuncertain. Indeed, inthefinalanalysis, aconsequence is\\nanidealization thatcanperhaps neverbewellapproximated. Ithere-\\nforesuggest thatwemustexpectactswithactually uncertain conse-\\nquences toplaytheroleofsureconsequences intypicalisolated decision\\nsituations.\\nSuppose now,toelaborate theexample, thatJonesispresented with\\nachoicebetween tickets inseveral different lotteries suchthat,which-\\neverhechoosesandwhatever ticketsaredrawn,hewillwineither\\nnothing, thesedan,theconvertible, orathousand dollars. Noneof\\nthesefourconsequences—not even‘“‘nothing’’—is actually asurecon-\\nsequence inthestrictsense,asIthinkyouwillnowunderstand. I\\npropose toanalyze Jones’s present decision situation intermsofa\\n“small world.”’Themorecolloquial Greekword,microcosm, willbe\\nreserved foraspecialkindofsmallworldtobedescribed later.Tode-\\nscribethestateofthesmallworldistosaywhichprizeisassociated\\nwitheachoftheticketsofferedto Jones. Thesmall-world actsactually\\navailable toJonesareacceptance ofoneoranother ofthetickets.\\nThegeneric small-world actisanarbitrary function takingasitsvalue\\noneofthefoursmall-world consequences according towhichsmall-\\nworldstateobtains.\\nItwillbenoticed thatthesmall-world statesareinfacteventsin\\nthegrandworld,thatindeedtheyconstitute apartition ofthegrand\\nworld. Ifthereareaninfinitenumber ofsmall-world states,asindeed\\ntheremustbe,ifthesmallworldistosatisfythepostulates P1-7,then\\nthepartitic.. inquestion becomes an infinite partition.t Thesecon-\\nsiderations leadtothefollowing technical definitions.\\nLetthegrandworldSbe,asalways, asetwithelementss,s’,---\\nThegrand-world consequences Fmayaswellbetakentobeabounded\\n+Technical note:Itismathematically moregeneralandelegantnottoinsistthat\\nthesmallworldhavestatesatall,butrathertospeakofaspecial classofeventsas\\nsmall-world events. Thisclassshouldbeclosedundercomplements andfiniteunions.\\nInshort,thesmall-world events,andthereby thesmallworlditself,constitute a\\nBoolean subalgebra oftheBoolean algebra ofthegrand-world events.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aa82a060-0e55-4620-876e-e904c439b2c4', embedding=None, metadata={'page_label': '103', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"5.5] SMALLWORLDS 85\\nsetofrealnumbers. Thegrand-world acisarethenreal-valued func-\\ntionsf,g,h,---.Thepreference ordering between actsisdetermined\\nbythecondition thatf<gifandonlyif\\nwheretheexpected valueindicated in(1)isderived fromaprobability\\nmeasurePcharacteristic ofthegrandworldor,tobemoreexact,of\\ntheperson’s attitude towardthegrandworld.\\nTheconstruction ofasmallworldSfromthegrandworldSbegins\\nwiththepartition ofSintosubsets, orsmall-world states5,8’,---(not\\nnecessarily finiteinnumber). Throughout thistechnical discussion, it\\nwillbenecessary tobearinmindcertaindoubleinterpretations such\\nasthat5isbothanelement ofSandasubsetofS.Strictly speaking, a\\nsmall-world eventBinSisacollection ofsubsets ofSandnotitselfa\\nsubsetofS.However, theunionofalltheelements ofB,regarded as\\nsubsets of S,isaneventinS;callit[B].\\nThesmallworld,asImeantodefine it,isdetermined notonlyby\\nthedefinition ofastate,butalsobythedefinition ofsmall-world con-\\nsequences. Asmall-world consequence isagrand-world act.AsetF'of\\ngrand-world acts,regarded assmall-world consequences, isthuspartof\\nthedefinition ofanygivensmallworld. Itwillbemathematically\\nsimplest, andcostlittleifanything ininsight, tosuppose thattheele-\\nmentsofFarefiniteinnumber. Theywillbe denoted f,g,h,---;\\nand,whenthesmall-world consequence fisrecognized asagrand-world\\nact,f(s) will denotethegrand-world consequence offatthegrand-\\nworldstates.\\nAsmall-world actfis,ofcourse,afunction fromsmall-world states&\\ntosmall-world consequences f.Inthisisolated technical discussion, we\\nwillhobblealongwiththenotations f(8)forthesmall-world conse-\\nquenceattached to§byf,andf(s;8)forthegrand-world consequence\\nattached to-sbyf(§)recognized asagrand-world act.Eachsmall-\\nworldactfgivesrisetoauniquegrand-world actf,defined thus:\\n(2) f(s)=prf(s;8(s)),\\nwhereS(s)meansthatsmall-world state§ofwhichthegrand-world\\nstatesisanelement.\\nThedistinction between fandf,likesomeotherdistinctions Ihave\\nthought itworthwhiletomakeinthepresent complicated context, is\\nperhaps pedantic. Atanyrate,it istobeunderstood aspartofthe\\ndefinition ofasmallworldthatf<&ifandonlyiff<&,thatis,in\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9e4be98a-2781-4bd0-8bc8-9bf235f735ef', embedding=None, metadata={'page_label': '104', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='86 UTILITY (5.5\\nviewof(1),ifandonlyifE(f)<E(%).Inthisconnection, itisuseful\\ntonotethat\\n(3) E()=>>E@|F(5(s))=HPUG(s)) =&)\\nkeF\\n=DEf(3(s))=k)PFE(s))=&).\\nk\\nItmaybeadvantageous toreview (3),andthereby thewholetechni-\\ncaldefinition ofasmallworld,intermsofanexample. Asmall-world\\nact,typifiedbythepurchase ofalottery ticket,amounts toaccepting\\ntheconsequences ofoneofseveralordinary grand-world actsaccording\\ntowhichelement ofapartition doesinfactobtain. Forexample, the\\nparticipant inalotterymaydriveawayacar,leadawayagoat,face\\nafiringsquad, orremain inthestatusquo,according tothetermsof\\nthelotteryandaccording towhichtickethehasinfactdrawn. Letting\\ntheexample ofthelotterystandforthegeneral situation, theexpected\\nutility ofalottery ticketcanbecomputed bythepartition formula\\n(3.5.3)fromtheconditional expectation associated witheachticket,\\nwhich iswhat(8)does.\\nItmayfairlybesaidthatalottery prizeisnotanact,butratherthe\\nopportunity tochoosefroma numberofacts.Thusacashprizeputs\\nitspossessor inaposition tochooseamongmanypurchases hecould\\nnototherwise afford. Ibelievethatanalysis tobemorenearlycorrect,\\nbutitismorecomplicated; and,ifonethinksofeachsetofactsmade\\navailable byalottery prizeasrepresented byabestactofthatset,\\nthemorecomplicated analysis seemssuperfluous, atleastinafirst\\nattack.\\nAsmallworldiscompletely satisfactory fortheusetowhichImean\\ntoputit,ifandonlyifititselfsatisfies thesevenpostulates andleads\\nto—more technically, agreeswith—a probability Psuchthat\\n(4) P(B)=P({B))\\nforallBCSandhasautilityUsuchthat\\n(5) U(f)=E(f)\\nforallfeF. Forthepresent context, callsuchacompletely satisfac-\\ntorysmallworldamicrocosm; ifthesmallworldsatisfies thepostulates,\\nbutdoesnotnecessarily admitPasitsprobability norUasautility,\\ncallitapseudo-microcosm.\\nTodisplay thecircumstances underwhichasmallworld isapseudo-\\nmicrocosm, I shall brieflycomment oneachofthepostulates inthe\\nformgivenontheendpapersof thisbook,referring tothemhereas\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bce5b0f0-cfe5-4169-bd51-f5d9717777af', embedding=None, metadata={'page_label': '105', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.5] SMALLWORLDS 87\\nP1-7,asopposed toP1-7,toemphasize thattheyareherebeingcon-\\nsideredwithrespect to8andF.\\nPlSimple ordering.\\nAutomatically satisfied. Indeed itisdirectly impliedbyP1.\\nP2Conditional preference welldefined.\\nAutomatic.\\nP3Conditional preference doesnoteffectconsequences.\\nRequires exactlythat,foreveryf,9¢F,andBCS,either:\\na. f<Ggiven[B], ifandonlyiff<g,or\\nb. h<kgiven[Bl],foreveryh, keF.\\nIntheseinequalities theelements ofF’areofcourseinterpreted as\\ngrand-world acts.\\nP4Qualitative personal probability welldefined.\\nRequires exactlythat,iff<gandhg<hg,where\\nha(s)=gforse[B]\\n=f forse~[B]°\\n(6)\\nha(s)= fors¢[C] g\\n=fforse~[C];\\nh thenh’s<h’a,whereh’gandh’garedefinedintermsoff’,9’,f’<9’,\\ninanalogy with(6). _\\nThispostulate isautomatic incaseFf’hasatmosttwoelements.\\nP5Thepersonhassomedefimte preference.\\nRequiresf<gforsomef,g¢F.\\nP6Partition ofworldsintotinyevents.\\nItisclearthatthispostulate isnotautomatic, thatis,itisnotim-\\npliedbythevalidity ofP1-7forthegrandworld. Itisnotevenm-\\npliedbyP1-7together withP1-5,though inthepresence ofallthese\\nP6couldundoubtedly beweakened. Thereseemstobelittletogain\\ninthepresent contextbyreducing P6tosuchminimal terms,norby\\nexpressing it,asP1-5havebeenexpressed, ingrand-world termsalone;\\nforP6doesnotlenditselfeasilytosuchtreatment, though itwouldbe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c3d61777-2239-4edd-91a2-787db2dc7cac', embedding=None, metadata={'page_label': '106', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='88 UTILITY (5.5\\neasytodecide inanyinstance whether P6obtained without undue\\nreference tothegrandworld.\\nP7Strongformofsure-thing principle.\\nAutomatic, inviewoftheexplicit assumption thatFhasonlya\\nfinitenumberofelements.\\nTosummarize, asmallworld isapseudo-microcosm, ifandonlyif\\nitsatisfies P3-6.Thepossibility ofenlarging anarbitrary smallworld\\ninsuchawayastosatisfythoseconditions hasalreadybeenimplicitly\\ndiscussed inconnection withP3-6.Torecallthearguments thatwere\\nadduced, onemightreviewtheexample abouttheeggin§3.1,and\\nthefurther discussion ofthatexample intheopening paragraph of\\n§3.2;theremark in§3.2,introducing P5;andtheexample aboutthe\\ncoinfollowing P6’in§3.3.\\nItisencouraging topossess thearguments Justcitedtending toshow\\nthatanysmallworldcanwithout overwhelming difficulty beembedded\\ninasomewhat largersmallworldthatisapseudo-microcosm. Apseudo-\\nmicrocosm is,however, completely satisfactory, onlyifitisactually a\\nmicrocosm, thatis,onlyifitleadstoaprobability measure anda\\nutilitywellarticulated withthoseofthegrandworld.Theproblem of\\ndeciding underwhatcircumstances thatoccurs ismuchfacilitated by\\nthefactthattheprobability measure andautilityofapseudo-micro-\\ncosmcanbewrittendownexplicitly, asthenextfewparagraphs show.\\nTostudytheproblem, suppose thesmallworld isapseudo-micro-\\ncosm.Then,inviewofP5,letg,hbeelements ofFsuchthatg<h,\\nandlet\\n~ Ec(h—g[B))-\\n7 B)= ~ P((B (7) Q(B) =prBho ([B]) \\na)f(06~g(s)}dP(s).\\nByusingP3tocheckthepositivity, itiseasilyverified thatQisaprob-\\nabilitymeasure onS.Theprobability measure Qagreeswiththere-\\nlation<between small-world events, which iseasilyverified onre-\\nwriting (3)forthespecialsmall-world actfzthattakesthevalueh\\nfor§<¢Bandgfor§<~Bthus:\\nE(A|(B)P(B)+E@|~{B)P(~(Bp\\nEch—g|(B)P(B)+E@\\n=E(h—9)Q(B)+E(@).(8) E(tz)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b034b77-14ed-4ed1-a5b6-a813150c3d85', embedding=None, metadata={'page_label': '107', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.5] SMALLWORLDS 89\\nSincegandAareessentially arbitrary, therearemanywaystocon-\\nstructaprobability measure thatagreeswiththerelation<between\\nsmall-world events, but,inthepresence ofP1-6,allofthemmust(in\\nviewofCorollary 3.3.1)bethesameasQ.Thatconsideration leadsto\\ntheformula\\n(9) EG—f’|(B)P(B))=EG—FQ)\\nforallf,f/eFandBCS.\\nUsing(9)andrecalling thatU(f)hasbeendefinedasE(f),(3)can\\nberewritten thus:\\n(10)E(@)=EG)+reE(k—g|F(&(s))=K)PF(&(s)) =&)\\n=XU(kK)Q(F(8) =&).\\nThequestion whether agivenpseudo-microcosm isreallyamicro-\\ncosmisthequestion whether Q(B)=P([B])andwhetherisautility\\nforthepseudo-microcosm. Theanswer tothesecondpartisimmediate\\nand,Ithink,somewhat surprising, for(10)showsthatforanypseudo-\\nmicrocosm Uisindeedautility.\\nUnfortunately, thecondition Q(B)=P([B]) isnotalsoautomatic.\\nThepossibility ofitsfailingtobesatisfied isillustrated bythefollowing\\nsimplemathematical example. LetSbetheunitsquare0<2,y<1,\\nandlet\\n1 ]\\n(11) E(f)=fffla, y)dxdy.\\n00\\nItisofnorealmoment thattheintegral in(11),ifunderstood inthe\\nLebesgue orRiemann sense, isnotdefined forallbounded functions.\\nLettheelements ofSbethevertical linesegments, x=constant.\\nFinally, suppose thattheelements ofFconsist ofthefunction zeroand\\nanyfinitenumber ofnon-negative multiples ofafixedpositive function\\nh=h.ItiseasytoverifythatSasthusdefined isapseudo-microcosm\\nandthat\\n(12) QB)=fal’)de\\nwhere ;\\n[rena\\n(13) q(x’)=—— \\n] 1]fh(2,y)dedy\\nO 0\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4151a654-4501-4f6b-9a5b-a1aec0a67e45', embedding=None, metadata={'page_label': '108', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='90 UTILITY [5.5\\nUnlessqis1foreveryx’,whichwillnotatalltypically bethecase,S\\nisnotreallyamicrocosm.\\nThegeneral condition thatapseudo-microcosm beamicrocosm—i.e.,\\nthatQ(B)=P({B])—is evidently, inviewof(9),\\n(14) E(f—|(Bl)=EG-f)\\nforeveryf,f’<FandeveryBforwhichP([B])>0.Incidentally,\\nthatcondition alonepractically implies thatasmallworldS,notnéces-\\nsarilyassumed tobeapseudo-microcosm, isarealmicrocosm. More\\nexactly, itimplies allthepostulates P1-7,exceptP6;anditimplies\\nthattheprobability measurePagreeswiththerelation<between\\nsmall-world events. Also,ifasmallworld isapseudo-microcosm, itis\\nenoughthat(14)shouldholdforsomepairoffunctions forwhichthe\\nright-handsideoftheequation doesnotvanish.\\nEquation (14)is,however, unsatisfactory inthatitseemsincapable\\nofverification without taking thegrandworldmuchtooseriously.\\nSomeconsolation mayderivefromthefactthatiffandf’areconstants\\ntheyautomatically satisfy (14).Twosuchabsolute, orgrand-world,\\nconsequences would suffice, for,ashasJustbeenremarked, itissuffi-\\ncientthat(14)besatisfied fortwomaterially different small-world\\nconsequences, inthepresence ofP1-7(which areverifiable without\\nanydetailed knowledge ofthegrandworld). Itmust,however, bead-\\nmitted, ashasalready beenmentioned, thattheveryideaofagrand-\\nworldconsequence takesthegrandworldprettyseriously—a point\\nforcedintomyreluctant mindbyaconversation withFrancesco Bram-\\nbilla.\\nIfeel,ifImaybeallowed tosayso,thatthepossibility ofbeingtaken\\ninbyapseudo-microcosm thatisnotarealmicrocosm isremote, but\\nthedifficulty Ifindindefining anoperationally applicable criterion is,\\ntosaytheleast,ground forcaution.\\nTherecertainly seemtobecasesinwhichonecouldconfidently as-\\nsume(14),though thusfarformal analysis ofthesourceofsuchse-\\ncurityescapesme.Consider, forexample, alottery inwhichnumbered\\ntickets aredrawnfromadrum. Itseemsclearthatforanordinary\\npersontheoutcomeofthelottery isutterlyirrelevant tohislife,except\\nthrough therulesofthelottery itself.Inothertermsequally loose,\\nthevalueofathousand dollars, orofacar,toapersonwouldnotordi-\\nnarilydepend atallonwhatnumbers weredrawninalottery, unless\\nthepersonhimself (orperhaps someotherpersonororganization with\\nwhomhehadsomedegree ofcontact) heldtickets inthelottery.A\\nmoreprecise formulation, whichdoesindeedimply(14),isthatthe\\neventsthatrepresent theoutcome ofthelottery areallstatistically\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b73a810d-58bc-418d-a1a5-7255c4cbed8b', embedding=None, metadata={'page_label': '109', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6]HISTORICAL ANDCRITICAL COMMENTS ONUTILITY 91\\nindependent ofthegrand-world acts,orfunctions, thattypically enter\\nasprizesinalottery. Thissuggests oncemorethatitwouldbedesir-\\nable,ifpossible, tofindasimplequalitative personal description ofin-\\ndependence between events. (Compare thefirstparagraph after\\n(3.5.2).)\\n6Historical andcriticalcomments onutility\\nAcasualhistorical sketchoftheconcept ofutilitywillperhapshave\\nsomeinterest ashistory. Atanyrate,mostofthecritical ideasper-\\ntainingtoutilitythatIwishtodiscuss findtheirplacesinsuchasketch\\nasconveniently asinanyotherorganization Icandevise.Muchmore\\ndetailed material onthehistory ofutility, especially insofarasthe\\neconomics ofriskbearing isconcerned, istobefoundinArrow’s review\\narticle [A6].Stigler’s historical study[S18]emphasizes thehistory of\\nthenowalmostobsolete economic notionofutilityinriskless situations,\\nanotion stillsometimes confused withtheoneunderdiscussion.\\nAswasmentionedin§4.5,theearliestmathematical studies ofprob-\\nabilitywerelargely concerned withgambling, particularly withthe\\nquestion ofwhichofseveral available cashgambles ismostadvanta-\\ngeous.Earlyprobabilists advanced themaximthatthegamblewith\\nthehighestexpected winnings isbestor,intermsofutility,thatwealth\\nmeasured incashisautilityfunction. Somesensecanbeseeninthat\\nmaxim,whichwillherebecalledbyitstraditional though misleading\\nname,theprinciple ofmathematical expectation. First,ithasoftenbeen\\narguedthattheprinciple follows forthelongrunfromtheweaklawof\\nlargenumbers, applied tolargenumbers ofindependent bets,ineach\\nofwhichonlysumsthatthegambler considers smallaretobewonor\\nlost.Second, Daniel Bernoulli, who,in[B10],wasoneofthefirstto\\nintroduce ageneral ideaofutilitycorresponding tothatdeveloped in\\nthepreceding threesections, madethefollowing analysis oftheprinci-\\nple,whichjustifies itsapplication inlimited butimportant contexts.\\nIftheconsequences ftobeconsidered areallquantities ofcash,it is\\nreasonable tosuppose thatU(f)willchangesmoothly withchanges in\\nf.Therefore, ifaperson’s present wealth isfo,andhecontemplates\\nvarious gambles, noneofwhichcangreatly change hiswealth, the\\nutilityfunction can,forhisparticular purpose, beapproximated byits\\ntangent atfo,thatis,\\n(1) U(f)~U(fo)+(F—fo)U\"(fo),\\nalinearfunction off.Sinceaconstant termisirrelevant toanycom-\\nparison ofexpected values, theapproximation amounts toregarding\\nutilityasproportional towealth, thatis,tofollowing theprinciple of\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a7b0b17c-19d8-4c10-a2ce-1209823d3a15', embedding=None, metadata={'page_label': '110', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='92 UTILITY [5.6\\nmathematical expectation. SofarasIknow,theonlyotherargument\\nfortheprinciple thathaseverbeenadvanced isoneconcerning equity\\nbetween twoplayers. AsBernoulli says,thatargumentisirrelevant at\\nbest;andneither oftherelevant arguments justifies categorial accept-\\nanceoftheprinciple. Nonetheless,theprinciple wasatfirstsocate-\\ngorically accepted thatitseemed paradoxical tomathematicians ofthe\\nearlyeighteenth century thatpresumably prudent individuals reject\\ntheprinciple incertain realandhypothetical decision situations.\\nDanielBernoulli (1700-1782), inthepaper[B10],seemstohave\\nbeenthefirsttopointoutthattheprinciple isatbestaruleofthumb,\\nandhetheresuggested themaximization ofexpected utilityasamore\\nvalidprinciple. Daniel Bernoulli’s paperreproduces portions ofalet-\\nterfromGabriel Cramer toNicholas Bernoulli, whichestablishes\\nCramer’s chronological priority totheideaofutilityandmostofthe\\nothermainideasofBernoulli’s paper.ButitisBernoulli’s formulation\\ntogether withsomeoftheideasthatwerespecifically histhatbecame\\npopularandhavehadwidespread influence tothepresent day.Itis\\ntherefore appropriate toreviewBernoulli’s paperinsomedetail.\\nBeingunable toreadLatin,IfollowtheGerman edition [B11].\\nBernoulli beginsbyremindinghisreaders thattheprinciple ofmathe-\\nmatical expectation, though butweakly supported, hadtheretofore\\ndominated thetheory ofbehavior inthefaceofuncertainty. Hesays\\nthat,thoughmanyarguments hadbeengivenfortheprinciple, they\\nwereallbasedontheirrelevant ideaofequityamongplayers. Itseems\\nhardto believe thathehadneverheardtheargument justifying the\\nprinciple forthelongrun,eventhough theweaklawoflargenumbers\\nwasthenonlyinitsmathematical infancy. ArsConjectandi [B12],then\\nafairlyup-to-date andmosteminent treatise onprobability, doesseem\\ntogiveonlytheargument aboutequity,andthatincountless forms.\\nThistreatisebyDaniel’s uncle,Jacob(=James)Bernoulli (1654-1705),\\nincidentally, contains thefirstmathematical advance towardtheweak\\nlaw,proving itforthespecialcaseofrepeated trials.\\nManyexamples showthattheprinciple ofmathematical expecta-\\ntionisnotuniversally applicable. DanielBernoulli promptly presents\\none:“Tojustifytheseremarks, letussuppose apauperhappenstoac-\\nquirealottery ticketbywhichhemaywithequalprobability win\\neithernothing or20,000ducats. Willhehavetoevaluate the worth\\noftheticketas10,000ducats;andwouldhebeactingfoolishly, ifhe\\nsolditfor9,000ducats? ”’\\nOtherexamples occurlaterinthepaperasillustrations oftheuse\\noftheutilityconcept. Thusaprudent merchant mayinsure hisship\\nagainst lossatsea,though heunderstands perfectly wellthatheis\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9f125327-dc7b-4a1a-8aaf-1efe38e29e7c', embedding=None, metadata={'page_label': '111', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6]HISTORICAL ANDCRITICAL COMMENTS ONUTILITY 93\\nthereby increasing theinsurance company’s expected wealth,andto\\nthesameextentdecreasing hisown.Suchbehaviorisinflagrant vio-\\nlationoftheprinciple ofmathematical expectation, andtoonewhoheld\\nthatprinciple categorically itwouldbeasabsurd toinsureastothrow\\nmoneyawayoutright. Buttheprinciple isneither obvious norde-\\nducedfromotherprinciples regarded asobvious;soitmaybechallenged,\\nandmustbe,because everyone agreesthatit isnotreallyinsaneto\\ninsure.\\nBernoulli citesathird,nowveryfamous, example illustrating that\\nmenofprudence donotinvariably obeytheprinciple ofmathematical\\nexpectation. Thisexample, knownastheSt.Petersburg paradox (be-\\ncauseofthejournal inwhichBernoulli’s paperwaspublished) hadear-\\nlierbeenpublicized byNicholas Bernoulli, andDanielacknowledges\\nitasthestimulus thatledtohisinvestigation ofutility. Suppose, to\\nstatetheSt.Petersburg paradox succinctly, thatapersoncouldchoose\\nbetween anactleaving hiswealth fixedatitspresentmagnitude orone\\nthatwouldchangehiswealthatrandom,increasing itby(2”—f)dol-\\nlarswithprobability 2~”foreverypositive integer n.Nomatterhow\\nlargetheadmission feefmaybe,theexpected income oftherandom\\nactisinfinite, asmayeasilybeverified. Therefore, according tothe\\nprinciple ofmathematical expectation, therandom actistobepre-\\nferredtothestatusquo.Numerical examples, however, soonconvince\\nanysincerepersonthathewouldpreferthestatusquoiffisatall\\nlarge. Iffis$128,forexample, thereisonly1chance in64thata\\npersonchoosing therandom actwillsomuchasbreakeven,andhe\\nwillotherwise loseatleast$64,ajeopardy forwhichhecanseekcom-\\npensation onlyintheprodigiously improbable winning ofaprodigiously\\nhighprize.\\nAppealing tointuition, Bernoulli saysthatthecashvalueofaper-\\nson’swealth isnotitstrue,ormoral,worthtohim.Thus,according to\\nBernoulli, thedollarthatmightbeprecious toapauperwouldbenearly\\nworthless toamillionaire—or, better,tothepauper himselfwereheto\\nbecome amillionaire. Bernoulli thenpostulates thatpeopledoseek\\ntomaximize theexpected valueofmoralworth,orwhathasbeencalled\\nmoralexpectation.\\nOperationally, themoralworthofaperson’s wealth, sofarasitcon-\\ncernsbehavior inthefaceofuncertainty, isjustwhatIwouldcallthe\\nutility ofthewealth,andmoralexpectation isexpectation ofutility.\\ntDanielreferstothisNicholas Bernoulli ashisuncle,but,inviewofdatesmen-\\ntionedinthelastsectionofDaniel’s paperandthegenealogy inChapter 8of[B9],\\nIthinkhemusthavemeanthiseldercousin(1687-1759), perhaps using“uncle’’ as\\natermofdeference.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='25fc3e63-1a63-4a7d-aa01-d8c6127b42ae', embedding=None, metadata={'page_label': '112', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='94 UTILITY (5.6\\nItseemsmystical, however, totalkaboutmoralworthapartfrom\\nprobability and,havingdoneso,doublymystical topostulate thatthis\\nundefined quantity servesasautility. Theseobvious criticisms have\\nnaturally ledmanytodiscredit theveryideaofutility,but§§2-4\\nshow(following vonNeumann andMorgenstern) thatthereisamore\\ncogent,thoughnotaltogether unobjectionable, pathtothatconcept.\\nBernoulli argued, elaborating theexample ofthepauperandthe\\nmillionaire, thatafixedincrement ofcashwealth typically results in\\naneversmaller increment ofmoralwealthasthebasiccashwealth to\\nwhichtheincrement applies isincreased. Headmitted thepossibility\\nofexamples inwhichthislawofdiminishing marginal utility,asithas\\ncometobecalledintheliterature ofeconomics, might fail.Forex-\\nample,arelatively smallsummightbeprecious toawealthy prisoner\\nwhorequired ittocomplete hisransom. ButBernoulli insisted that\\nsuchexamples areunusual andthatasageneral rulethelawmaybe\\nassumed. Inmathematical terms,thelawsaysthatutilityasafunc-\\ntionofmoney isaconcave (i.e.,thenegative ofaconvex) function.f\\nItfollowsfromthebasicinequality concerning convexfunctions (Theo-\\nrem1ofAppendix 2)thataperson towhomthelawofdiminishing\\nmarginal utilityapplies willalways preferthestatusquotoanyfair\\ngamble, thatis,toanyrandom actforwhichthechangeinhisexpected\\nwealth iszero,andthathewillalwaysbewilling topaysomething in\\naddition toitsactuarial, orexpected, valueforinsurance againstany\\nlosstohimself. Thelawofdiminishing marginal utilityhasbeenvery\\npopular, andfewwhohaveconsidered utilitysinceBernoulli havedis-\\ncarded it,orevenrealized thatitwasnotnecessarily partandparcel\\noftheutilityidea.Ofcourse,thelawhasbeenembraced eagerlyand\\nuncritically bythosewhohaveamoralaversion togambling.\\nBernoulli wentfurtherthanthelawofdiminishing marginal utility\\nandsuggested thattheslopeofutilityasafunction ofwealthmight,\\natleastasaruleofthumb,besupposed, notonlytodecrease with,but\\ntobeinversely proportional to,thecashvalueofwealth. This,he\\npointed out,isequivalent topostulating thatutility isequaltothe\\nlogarithm (toanybase)ofthecashvalueofwealth.Tothisday,no\\notherfunction hasbeensuggested asabetterprototype forEveryman’s\\nutilityfunction. Nonetheless,asCramer pointed outinhisaforemen-\\ntioned letter,thelogarithm hasaseriousdisadvantage; for,iftheloga-\\nrithmweretheutilityofwealth, theSt.Petersburg paradox couldbe\\n{Oftenthemeanings of‘‘convex” and‘“‘concave’”’ asapplied tofunctions arein-\\nterchanged. Afunction isherecalledconvex ifitappears convex, intheordinary\\nsenseoftheword,whenviewedfrombelow.Suchafunction is,ofcourse,alsocon-\\ncavefromabove,whence theconfusion. Cf.Appendix 2.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f2ad49e0-b366-4bc9-b2a0-7eb55a15a4cc', embedding=None, metadata={'page_label': '113', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6]HISTORICAL ANDCRITICAL COMMENTS ONUTILITY 95\\namended toproduce arandom actwithaninfiniteexpected utility\\n(i.e.,aninfiniteexpected logarithm ofincome) that,again,noonewould\\nreallyprefertothestatusquo.Totakealesselaborate example, sup-\\nposethataman’stotalwealth, including anappraisal ofhisfuture\\nearningpower,wereamillion dollars. Ifthelogarithm ofwealthwere\\nactually hisutility,hewouldassoonasnotflipacointodecidewhether\\nhiswealthshouldbechanged totenthousand dollars—roughly $500\\nperyear—or ahundred million dollars. Thisseemspreposterous to\\nme.Atanyrate,Iamsureyoucanconstruct anexample alongthe\\nsamelinesthatwillseempreposterous toyou.Cramer therefore con-\\ncluded,andIthinkrightly, thattheutility ofcashmustbebounded,\\natleastfromabove. Itseemstomethatagoodargument canalsobe\\nadduced forsupposing utilitytobebounded frombelow, for,however\\nwealthmaybeinterpreted, weallsubject ourtotalwealthtoslight\\njeopardy dailyforthesakeofalargeprobability ofavoiding more\\nmoderate losses.Butthelogarithm isunbounded bothfromabove\\nandfrombelow; so,though itmightbeareasonable approximation to\\naperson’s utilityinamoderate rangeofwealth, itcannotbetaken\\nseriously overextreme ranges.\\nBernoulli’s ideaswereaccepted wholeheartedly byLaplace [L1],who\\nwasveryenthusiastic abouttheapplications ofprobability toallsorts\\nofdecision problems. Itismycasualimpression, however, thatfrom\\nthetimeofLaplace untilquiterecently theidéaofutilitydidnot\\nstrongly influence eithermathematical orpractical probabilists.\\nForalongperiodeconomists accepted Bernoulli’s ideaofmoral\\nwealth asthemeasurement ofaperson’s well-being apartfromany\\nconsideration ofprobability. Though “utility” ratherthan‘moral\\nworth” hasbeenthepopularnameforthisconceptamong English-\\nspeaking economists, itismyimpression thatBernoulli’s paperisthe\\nprincipal, ifnotthesole,sourceofthenotionforalleconomists, though\\nthepaperitselfmayoftenhavebeenlostsightof.Economists werefor\\natimeenthusiastic abouttheprinciple ofdiminishing marginal utility,\\nandtheysawwhattheybelieved tobereflections ofitinmanyaspects\\nofeveryday life.Whyelse,toparaphrase AlfredMarshall (pp.19,\\n95of[M2]),doesapoormanwalkinarainthatinduces arichmanto\\ntakeacab?\\nDuringtheperiodwhentheprobability-less ideaofutilitywaspopu-\\nlarwitheconomists, theyreferred notonlytotheutilityofmoney,\\nbutalsototheutilityofotherconsequences suchascommodities (and\\nservices) andcombinations (or,better,patterns ofconsumption) ofcom-\\nmodities. Thetheory ofchoiceamongconsequences wasexpressed by\\ntheideathat,amongtheavailable consequences, apersonprefersthose\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb15544c-7bd8-4c1b-a0e4-37485a11c9db', embedding=None, metadata={'page_label': '114', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='96 UTILITY [5.6\\nthat have the highest utility forhim. Also, the idea ofdiminishing\\nmarginal utility was extended from money toother commodities.\\nThe probability-less idea ofutility ineconomics has been completely\\ndiscredited inthe eyes ofalmost alleconomists, the following argument\\nagainst it—originally advanced by Pareto inpp. 158-159 and the\\nMathematical Appendix of[Pl]—being widely accepted. Ifutility is\\nregarded ascontrolling only consequences, rather than acts, itisnot\\ntrue—as itiswhen acts, oratleast gambles, are considered and the\\nformal definition in§ 3, isapplied—that utility isdetermined except\\nfor alinear transformation. Indeed, confining attention toconse-\\nquences, any strictly monotonically increasing function ofone utility\\nisanother utility. Under these circumstances there islittle, ifany,\\nvalue intalking about utility atall, unless, ofcourse, special economic\\nconsiderations should render oneutility, orsay alinear family ofutili-\\nties, ofparticular interest. That possibility remains academic todate,\\nthough oneattempt toexploit itwasmade byIrving Fisher, asisbriefly\\ndiscussed inthe paragraph leading toFootnote 155 of[S18]. Inpar-\\nticular, utility asafunction ofwealth can have any shape whatsoever\\ninthe probability-less context, provided only that the function inques-\\ntion isincreasing with increasing wealth, the provision following from\\nthe casual observation that almost nobody throws money away. The\\nhistory ofprobability-less utility has been thoroughly reported byStig-\\nler[S18].\\nWhat, then, becomes ofthe intuitive arguments that led tothe no-\\ntion ofdiminishing marginal utility? To illustrate, consider the poor\\nman and the rich man inthe rain. Those ofuswho consider diminish-\\ning marginal utility nonsensical in this context think itsufficient to\\nsay simply that itisacommon observation that rich men spend money\\nfreely toavoid moderate physical suffering whereas poor men suffer\\nfreely rather than make corresponding expenditures ofmoney; inother\\nterms, that the rate ofexchange between circumstances producing phys-\\nical discomfort andmoney depends on thewealth oftheperson involved.\\nInrecent years there has been revived interest inBernoulli’s ideas\\nofutility inthe technical sense of§§2—4, that is,asafunction that, so\\ntospeak, controls decisions among acts, oratleast gambles. Ramsey’s\\nessays in[R1], which inspirit closely resemble thefirst five chapters of\\nthis book, present arelatively early example ofthis revival ofinterest.\\nRamsey improves onBernoulli inthat hedefines utility operationally\\ninterms ofthe behavior ofaperson constrained bycertain postulates.\\nRamsey’s essays, though now much appreciated, seem tohave had\\nrelatively little influence.\\nBetween thetime ofRamsey and that ofvonNeumann and Morgen-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6505566-4e9a-4779-a722-092a857cf1d5', embedding=None, metadata={'page_label': '115', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6] HISTORICAL AND CRITICAL COMMENTS ON UTILITY 97\\nstern there was interest inbreaking away from the idea ofmaximizing\\nexpected utility, atleast sofar aseconomic theory was concerned (ef.\\n(T1a]). This trend was supported bythose who said that Bernoulli gives\\nnoreason forsupposing that preferences correspond tothe expected\\nvalue ofsome function, and that therefore much more general possi-\\nbilities must beconsidered. Why should not the range, the variance,\\nand the skewness, not tomention countless other features, ofthe dis-\\ntribution ofsome function join with theexpected value indetermining\\npreference? The question was answered bytheconstruction ofRamsey\\nand again bythat ofvonNeumann and Morgenstern, which hasbeen\\nslightly extended in§§2-4; itissimply amathematical fact that, al-\\nmost any theory ofprobability having been adopted and the sure-thing\\nprinciple having been suitably extended, the existence ofafunction\\nwhose expected value controls choices can bededuced. That does not\\nmean that asatheory ofactual economic behavior thetheory ofutility\\nisabsolutely established and cannot beoverthrown. Quite the con-\\ntrary, itisatheory that makes factual predictions many ofwhich can\\neasily beobserved tobefalse, but the theory may have some value in\\nmaking economic predictions incertain contexts where the departures\\nfrom ithappen not tobedevastating. Moreover, asIhave been argu-\\ning, itmay have value asanormative theory.\\nVon Neumann and Morgenstern initiated among economists and, to\\nalesser extent, also among statisticians anintense revival of interest\\ninthe technical utility concept bytheir treatment ofutility, which ap-\\npears asadigression in[V4].\\nThe von Neumann-Morgenstern theory ofutility has produced this\\nreaction, because itgives strong intuitive grounds for accepting the\\nBernoullian utility hypothesis asaconsequence ofwell-accepted maxims\\nofbehavior. Togive readers ofthis book some idea ofthevon Neu-\\nmann-Morgenstern theory, Imay repeat that the treatment ofutility\\nasapplied togambles presented in§3 isvirtually copied from their\\nbook [V4]. Indeed, their ideas onthis subject areresponsible foralmost\\nallofmy own. One idea now held byme that Ithink von Neumann\\nand Morgenstern donot explicitly support, and that sofarasIknow\\nthey might not wish tohave attributed tothem, isthe normative in-\\nterpretation ofthe theory.\\nOfcourse, much ofthenew interest inutility takes theform ofcriti-\\ncism and controversy. The greater part of this discussion that hascome\\ntomy attention has not yet been published. Alist ofreferences lead-\\ning tomost ofthat which hasis [B7], [W14], [S1], [C4], [F13], [A2].\\nIshall successively discuss each ofthe recent major criticisms ofthe\\nmodern theory ofutility known tome. My method in each case will\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d09b39a7-a453-4752-a6b8-eb78dec5272c', embedding=None, metadata={'page_label': '116', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='98 UTILITY [5.6\\nbefirst tostate the criticism inaform resembling those inwhichit is\\ntypically put forward, regardless ofwhether Iconsider that form well\\nchosen. Iwill then discuss the criticism, elaborating itsmeaning and\\nindicating itsrebuttal, when there seems tometobeone.\\n(a)Modern economic theorists have rigorously shown that there is\\nnomeaningful measure ofutility. More specifically, ifany function U\\nfulfills the role ofautility, then sodoes any strictly monotonically in-\\ncreasing function ofU. Itmust, therefore, beanerror toconclude that\\nevery utility isalinear function ofevery other.\\nThis argument has been advanced with aseriousness that issurpris-\\ning, considering that itconcedes little intelligence orlearning tothe\\nproponents ofthe utility theory under discussion and considering that\\nit,results, aswill immediately beexplained, from the baldest sort ofa\\nterminological confusion. Tobefair, Imust goontosay that Ihave\\nnever known theargument tobe defended long inthe presence ofthe\\nexplanation Iam aboutto give.\\nInordinary economic usage, especially prior tothework ofvon Neu-\\nmann and Morgenstern, autility associated with gambles would pre-\\nsumably besimply afunction Uassociating numbers with gambles in\\nsuch away that f<g,ifand only ifU(f)<U(g); though economic\\ndiscussion ofutility was, prior tovonNeumann and Morgenstern, al-\\nmost exclusively confined toconsequences rather than togambles or\\ntoacts. Itisunequivocally true, asIhave already brought out, that\\nany monotonic function ofautility inthis wideclassical sense isitself\\nautility. What von Neumann and Morgenstern have shown, and\\nwhat hasbeen recapitulated in§3,isthat, granting certain hypotheses,\\nthere exists atleast one classical utility Vsatisfying the very special\\ncondition\\n(2) V(af+Bg)=aV(F) +BV(g),\\nwhere fand gareany gambles and a,6arenon-negative numbers such\\nthata+ 6=1.Furthermore, ifImay forthemoment call aclassical\\nutility satisfying (2) avon Neumann-Morgenstern utility, every von\\nNeumann-Morgenstern utility isanincreasing linear function ofevery\\nother. Toput the point differently, the essential conclusion ofthevon\\nNeumann-Morgenstern utility theory isthat (2)can besatisfied bya\\nclassical utility, but notbyvery many. The confusion arises only be-\\ncause vonNeumann and Morgenstern use thealready pre-empted word\\n“utility” forwhat Ihere call ‘von Neumann-Morgenstern utility.”\\nInretrospect, that seems tohave been amistake intactics, but one of\\nnolong-range importance.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36c0f690-1125-47a2-a568-a610d2ab62dd', embedding=None, metadata={'page_label': '117', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6] HISTORICAL AND CRITICAL COMMENTS ON UTILITY 99\\n(b)The postulates leading tothe von Neumann-Morgenstern con-\\ncept ofutility arearbitrary and gratuitous.\\nSuch aview can, ofcourse, always beheld without the slightest fear\\nofrigorous refutation, but acritic holding itmight perhaps bepersuaded\\naway from itbyareformulation ofthe postulates that hemight find\\nmore appealing than the original set, orbyilluminating examples. In\\nparticular, Pl-7 are quite different from, but imply, the postulates of\\nvon Neumann and Morgenstern. Incidentally, themain function of\\nthevon Neumann-Morgenstern postulates themselves istoput the es-\\nsential content ofDaniel Bernoulli’s “postulate” into aform that is\\nless gratuitous inappearance. Atleast one serious critic, who had at\\nfirst found the system ofvon Neumann and Morgenstern gratuitous,\\nchanged hismind when the possibility ofderiving certain aspects of\\nthat system from the sure-thing principle was pointed out tohim.\\n(c)The sure-thing principle goes too far. For example, iftwo lot-\\nteries with cash prizes (not necessarily positive) arebased onthesame\\nsetoflottery tickets and soarranged that the prize that will beassigned\\ntoany ticket bythe second lottery isatleast asgreat asthe prize as-\\nsigned tothat ticket bythe first lottery, then there isnodoubt that\\nvirtually any person would find aticket inthe first lottery not prefer-\\nable tothesame ticket inthe second lottery. If,however, the prizes\\nineach lottery arethemselves lottery tickets, such that the prize asso-\\nciated with any ticket inthe first lottery isnot preferred bytheperson\\nunder study tothe prize associated with thesameticket bythesecond\\nlottery, the conclusion that the person will not prefer aticket inthe\\nfirst lottery tothesameticket inthesecond isnolonger compelling.\\nThis point resembles the preceding one inthat the intuitive appeal\\nofanassumption can atmost beindicated, not proved. Idothink it\\ncogent, however, tostress inconnection with this particular point that\\nacash prize istoalarge extent alottery ticket inthat the uncertainty\\nastowhat will become of aperson ifhehasagift ofathousand dollars\\nisnot inprinciple different from the uncertainty about what will be-\\ncome ofhim ifheholds alottery ticket ofconsiderable actuarial value.\\nPerhaps anadherent tothe criticism inquestion would thinkit rele-\\nvant toreply thus: Though cash sums are indeed essentially lottery\\ntickets, asum ofmoney isworth atleast asmuch toaperson asasmaller\\nsum, inapeculiarly definite and objective sense, because money can,.\\nifone desires, always bequickly and quietly thrown away, thereby\\nmaking any sum available toaperson who already has alarger sum.\\nBut Ihave never heard that reply made, nordoIhereplead itscogency.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8c1212bb-0a11-432d-b00f-e5dd908e4c8d', embedding=None, metadata={'page_label': '118', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='100 UTILITY [5.6\\n(d)Anactualsystematic deviation fromthesure-thing principle and,\\nwithit,fromthevonNeumann-Morgenstern theoryofutility,canbe\\nexhibited. Forexample, apersonmightperfectly reasonably preferto\\nsubsistonapacketofArmyKrationspermealthanontwoouncesof\\nthebestcaviarpermeal.Itisthentobeexpected, according tothe\\nsure-thing principle, thatthepersonwouldprefertheKrations toa\\nlottery ticketyielding theKrations withprobability 9/10andthe\\ncaviardietwithprobability 1/10.Thatexpectation isnodoubtful-\\nfilled,ifthelottery isunderstood todetermine theperson’s year-long\\ndietonceandforall.But,iftheperson isabletohaveateachmeala\\nlotteryticketofferinghimtheKrations orthecaviarwiththeindicated\\nprobabilities, itisnotatallunlikely, granting thathelikescaviarand\\nhassomestorage facilities, thathewillpreferthis“lottery diet.’”’This\\nconclusion isindefiance oftheprinciple that‘‘thetheoryofconsumer\\ndemandisastatictheory.” (Cf.[W14].)\\nIadmitthatthetheoryofutility isnotstaticintheindicated sense,\\nastheforegoing example conclusively shows. Butthereisnotthe\\nslightest reasontothinkofalotteryproducing eitherasteady dietof\\ncaviarorasteady dietofKrations asbeingthesamelottery asone\\nhavingamultitude ofdifferent prizesalmost allofwhicharemixed\\nchronological programs ofcaviarandKrations. Thefactthata theory\\nofconsumer behavior inriskless situations happens tobestaticinthe\\nrequired sense(undercertainspecialassumptions aboutstorability and\\nthelinearity ofprices) isnoargumentatallthatthetheoryofconsumer\\nbehavior inriskycircumstances shouldbestaticinthesamesense(as\\nImention inanoteappended to[W14)).\\n(e)IfthevonNeumann-Morgenstern theoryofutility isnotstatic,\\nitisnotsubject torepeated empirical observation andistherefore\\nvacuous. (Cf.[W14].)\\nIthinkthediscussion in§3.1ofhowtodetermine thepreferences of\\nahotmanforaswim,ashower,andaglassofbeer,andthediscussion\\nin§5ofthepracticality ofidentifying pseudo-microcosms aresteps\\ntowardshowinghowthetheorycanbeputtoempirical testwithout\\nmaking repeated trialsonanyoneperson.\\n(f)Casual observation showsthatrealpeoplefrequently andfla-\\ngrantlybehave indisaccord withtheutility theory,andthatinfactbe-\\nhaviorofthatsortisnotatalltypically considered abnormal orir-\\nrational.\\nTwodifferent topicscallfordiscussion underthisheading. Inthe\\nfirstplace,itisundoubtedly truethatthebehavior ofpeopledoesoften\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9cb0c38f-d1be-43e0-9318-5cad81473b31', embedding=None, metadata={'page_label': '119', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6]HISTORICAL ANDCRITICAL COMMENTS ONUTILITY 101\\nflagrantly departfromthetheory. Nonetheless, alltheworldknows\\nfromthelessons ofmodern physics thatatheory isnottobealtogether\\nrejected becauseitisnotabsolutely true.Itseemsnotunreasonable to\\nsuppose, andexamples couldeasilybecitedtoconfirm, thatintheex-\\ntremely complicated subject ofthebehavior ofpeopleverycrudetheory\\ncanplayausefulroleincertaincontexts.\\nSecond,manyapparent exceptions tothetheorycanbesoreinter-\\npretedasnottobeexceptions atall.Forexample, afliermaybeob-\\nserveddoingastuntthatriskshislife,apparently fornothing. That\\nseemstobeincomplete violation ofthetheory; but,ifinaddition itis\\nknownthattheflierhasarealandpractical needtoconvince certain\\ncolleagues ofhiscourage, thenheissimplypaying foradvertising with\\ntheriskofhislife,which isnotinitselfincontradiction tothetheory.\\nOr,suppose thatitwereknown moreorlessobjectively thattheflier\\nhasaneedtodemonstrate hisowncourage tohimself. Thetheory\\nwouldagainberescued, butthistimeperhaps notsoconvincingly as\\nbefore. Ingeneral, thereinterpretation needed toreconcile various\\nsortsofbehavior withtheutilitytheory issometimes quiteacceptable\\nandsometimes sostrained astolaywhoever proposes itopentothe\\ncharge oftryingtosavethetheorybyrendering ittautological. The\\nsamesortofthingarisesinconnection withmanytheories, andIthink\\nthereisgeneralagreement thatnohard-and-fast rulecanbelaiddown\\nastowhenitbecomes inappropriate tomakethenecessary reinterpre-\\ntation. Forexample, thelawoftheconservation ofenergy (orits\\natomicagevariant, thelawoftheconservation ofmassandenergy)\\nowesitssuccess largely toitsbeinganexpression ofremarkable and\\nreliable factsofnature,buttosomeextentalsotocertainconventions\\nbywhichnewsortsofenergyaresodefined astokeepthelawtrue.\\nAstimulating discussion ofthisdelicate pointinconnection withthe\\ntheoryofutility isgivenbySamuelson in[S1].\\n(g)Introspection aboutcertainhypothetical decision situations sug-\\ngeststhatthesure-thing principle and,withit,thetheory ofutility\\narenormatively unsatisfactory. Consider anexample basedontwode-\\ncisionsituations eachinvolving twogambles. f\\nSituation 1.Choosebetween\\nGamble 1.$500,000 withprobability 1;and\\nGamble 2.$2,500,000 withprobability 0.1,\\n$500,000 withprobability 0.89,\\nstatusquowithprobability 0.01.\\n+Thisparticular example isduetoAllais[A2].Another interesting example was\\npresented somewhat earlierbyGeorges Morlat [C4].\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bc597cd1-d1ef-42b4-a486-f5aa51484d41', embedding=None, metadata={'page_label': '120', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='102 UTILITY (5.6\\nSituation 2.Choosebetween\\nGamble 3.$500,000 withprobability 0.11,\\nstatusquowithprobability 0.89;and\\nGamble 4.$2,500,000 withprobability 0.1,\\nstatusquowithprobability 0.9.\\nManypeoplepreferGamble 1toGamble 2,because, speaking quali-\\ntatively, theydonotfindthechanceofwinning averylargefortune in\\nplaceofreceiving alargefortune outright adequate compensation for\\nevenasmallriskofbeingleftinthestatusquo.Manyofthesame\\npeoplepreferGamble 4toGamble 3;because, speaking qualitatively,\\nthechance ofwinning isnearlythesameinbothgambles, sotheone\\nwiththemuchlargerprizeseemspreferable. Buttheintuitively ac-\\nceptable pairofpreferences, Gamble 1preferred toGamble 2andGam-\\nble4toGamble 3,is.notcompatible withtheutilityconcept or,equiva-\\nlently,thesure-thing principle. Indeedthatpairofpreferences implies\\nthefollowing inequalities foranyhypothetical utilityfunction.\\nU($500,000)>0.1U($2,500,000) +0.89U($500,000)+0.1U($0),\\n(3)\\n0.1U($2,500,000) +0.9U($0)>0.11U($500,000) +0.89U($0);\\nandtheseareobviously incompatible.\\nExamples +liketheoneciteddo have astrongintuitive appeal;even\\nifyoudonotpersonally feelatendency topreferGamble 1toGamble 2\\nandsimultaneously Gamble 4toGamble 3,Ithinkthatafewtrials\\nwithotherprizesandprobabilities willprovideyouwithanexample\\nappropriate toyourself.\\nIf,afterthorough deliberation, anyone maintains apairofdistinct\\npreferences thatareinconflict withthesure-thing principle, hemust\\nabandon, ormodify, theprinciple; forthatkindofdiscrepancy seems\\nintolerable inanormative theory. Analogous circumstances forced\\nD.Bernoulli toabandon thetheory ofmathematical expectation for\\nthatofutility[B10].Ingeneral, a personwhohastentatively accepted\\nanormative theorymustconscientiously studysituations inwhichthe\\ntheoryseemstoleadhimastray;hemustdecideforeachbyreflection\\n—deduction willtypically beoflittlerelevance—whether toretainhis\\ninitialimpression ofthesituation ortoaccepttheimplications ofthe\\ntheory forit.\\nToillustrate, letmerecordmyownreactions totheexample with\\n}Allaishasannounced (butnotyetpublished) anempirical investigation ofthe\\nresponses ofprudent, educated peopletosuchexamples [A2].\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a798508-8807-4d77-9501-b2a606afba9e', embedding=None, metadata={'page_label': '121', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6]HISTORICAL ANDCRITICAL COMMENTS ONUTILITY 103\\nwhichthisheading wasintroduced. Whenthetwosituations were\\nfirstpresented, Iimmediately expressed preference forGamble 1as\\nopposed toGamble 2andforGamble 4asopposed toGamble 3,andI\\nstillfeelanintuitive attraction tothosepreferences. ButIhavesince\\naccepted thefollowing wayoflooking atthetwosituations, which\\namounts torepeated useofthesure-thing principle.\\nOnewayinwhichGambles 1-4could berealized isbyalotterywith\\nahundred numbered ticketsandwithprizesaccording totheschedule\\nshowninTable1.\\nTABLE 1.Prizes INUNITSOF$100,000 INALOTTERY REALIZING\\nGAMBLES 1-4\\n TicketNumber\\n12-1112-100\\n...fGamble1 5 5\\nSituation 1|Gamble210 25 5\\n. . Gamble 3 5 5 0\\nSituation2le4|0 25 0\\nNow,ifoneoftheticketsnumbered from12through 100isdrawn, it\\nwillnotmatter, ineithersituation, whichgamble Ichoose. Itherefore\\nfocusonthepossibility thatoneoftheticketsnumbered from1through\\n11willbedrawn,inwhichcaseSituations 1and2areexactly parallel.\\nThesubsidiary decision depends inbothsituations onwhether Iwould\\nsellanoutright giftof$500,000 fora10-to-1chancetowin$2,500,000—\\naconclusion thatIthinkhasaclaimtouniversality, orobjectivity.\\nFinally, consulting mypurelypersonal taste,IfindthatIwouldprefer\\nthegiftof$500,000 and,accordingly, thatIpreferGamble 1toGamble\\n2and(contrary tomyinitial reaction) Gamble 3toGamble 4.\\nItseemstomethatinreversing mypreference between Gambles 3\\nand4Ihavecorrected anerror.Thereis,ofcourse,animportant sense\\ninwhichpreferences, beingentirely subjective, cannotbein error; but\\ninadifferent, moresubtlesensetheycanbe.Letmeillustrate bya\\nsimpleexample containing noreference touncertainty. Amanbuying\\nacarfor$2,134.56 istempted toorderitwitharadioinstalled, which\\nwillbringthetotalpriceto$2,228.41, feeling thatthedifference is\\ntrifling. But,whenhereflects that,ifhealreadyhadthecar,hecer-\\ntainlywouldnotspend$93.85foraradioforit,herealizes thathehas\\nmadeanerror.\\nOnethingthatshouldbementioned beforethischapter isclosed is\\nthatthelawofdiminishing marginal utilityplaysnofundamental role\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9df6baaa-58da-4aac-ad3c-efb409f54e34', embedding=None, metadata={'page_label': '122', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='104 UTILITY [5.6\\ninthevonNeumann-Morgenstern theory ofutility,viewed eitherem-\\npirically ornormatively. Therefore thepossibility isleftopenthat\\nutilityasafunction ofwealthmaynotbeconcave, atleastinsomein-\\ntervals ofwealth. Someeconomic-theoretical consequences ofrecog-\\nnitionofthepossibility ofnon-concave segments oftheutilityfunction\\nhavebeenworkedoutbyFriedman andmyself[F12],andbyFriedman\\nalone[F111].TheworkofFriedman andmyselfonthispointiscriti-\\ncizedbyMarkowitz [M1].+\\n+SeealsoArchibald (1959)andHakansson (1970).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9d1df39d-6d95-403e-8559-1e1d5c2dfc49', embedding=None, metadata={'page_label': '123', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 6\\nObservation\\n1Introduction\\nWiththeconstruction ofutility, thetheory ofdecision intheface\\nofuncertainty is,inasense,complete. Ihavenofurther postulates\\ntopropose, andthoseIhaveproposed havebeenshowntobeequiva-\\nlenttotheassumption thatthepersonalways decides infavorofan\\nacttheexpected utilityofwhich isaslargeaspossible, supposing for\\nsimplicity thatonlyafinitenumber ofactsareopentohim.Atthe\\nlevelofgenerality thathasledtothisconclusion thereseemstobe\\nlittleornothing lefttosay.Togofurthernowmeanstogointomore\\ndetail,toinvestigate specialtypesofdecision problems. Onetypeof\\ndecision problem ofcentralimportance isthatinwhichtheperson is\\ncalledupontomakeanobservation andthentochoosesomeactinthe\\nlightoftheoutcome oftheobservation.\\nTheconsideration ofsuchobservational decision problems isastep\\ntowardthoseproblems ofgreatinterest forstatistics inwhichtheper-\\nsonmustdecidewhatobservation tomake,thatis,ofcourse,whatto\\nlookat,notwhattosee.Theyaretheproblems ofdesigning experi-\\nmentsandotherobservational programs.\\nSomeremarks onobservation weremadeinChapter 3,butonlynow\\nthatthetheoryofutility isestablished isitpossible togivearelatively\\ncomplete analysis oftheconcept.\\nObservation isaconcept essential tothestudyofstatistics proper,\\nmostofwhathasbeensaidthusfarbeingpreliminary to,butnotreally\\npartof,statistics; evenafterthischapter andthenextone,onobser-\\nvation, therewillstillremain amajortransition. Oneimportant fea-\\ntureofmuchofwhatisordinarily calledstatistics is,according to\\nmyanalysis, concerned withthebehavior notofanisolated person,but\\nofagroupofpersons acting, forexample, inconcert. Inlaterchapters\\nIwilldeal,sofarasIamable,withtheproblem ofgroupaction,but\\npreliminary considerations bearing onitwillbemadeandpointed out\\nfromtimetotimeinthischapterandthenext.\\nThough thedetailsofthesetwochaptersmayseemmathematically\\nforbidding, drasticsimplifying assumptions aremadeinthemtokeep\\n105\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9d720e7e-f610-48d7-bfc3-27757b5b1302', embedding=None, metadata={'page_label': '124', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='106 OBSERVATION (6.2\\nextraneous difficulties toaminimum. Thesetypically taketheform |\\nofassuming thatcertain setsofacts,events,andvaluesofrandom varia-\\nblesarefinite.Eveninelementary applications ofthetheory, these\\nsimplifying assumptions seldom actually hold.Insomecontexts,itis\\nquiteelementary torelaxthemsufficiently; inothers, seriousmathe-\\nmatical efforthasbeenrequired; andsomearestillatthefrontier of\\nresearch. Relaxations oftheassumptions willbetouched onfromtime\\ntotime,sometimes explicitly butsometimes onlyimplicitly inthechoice\\nofsuggestive notation andnomenclature.\\nBeyond thisintroduction, thepresent chapter isdivided intofour\\nsections: §2analyzes informally andthenformally thenotionofacost-\\nfreeobservation; §§3and4discuss certainobvious butimportant con-\\nditionsunderwhichoneobservation, andsimilarly onesetofacts,is\\nmorevaluable thananother; §5abstractly discusses problems ofde-\\nsigningexperiments or,perhapsmoregenerally, observational programs.\\n2Whatanobservation is\\nTobeginwithaninformal surveyofobservation, consider adecision\\nproblem, thatis,apersonfacedwithadecision among several acts.\\nCalling itthebasicdecision problem andtheactsassociated withit\\nthebasicacts,anewdecision problem wouldarise, ifthepersonwere\\ninformed beforehemadehisdecision thataparticular event,sayB,\\nobtained. Thenewdecision problem isrelated tothebasicdecision\\nproblem inasimpleway;fortheactsassociated withitarealsothe\\nbasicacts,andthedecision istobemadebycomputing theexpected\\nutilitygivenBofthebasicactsanddeciding ononethatmaximizes\\ntheconditional expected utility.Thebasicproblemmaybemodified\\ninstillanother, though closelyrelated,way.Letthepersonsayinad-\\nvance, foreachpossible B;,whichofthebasicactshewilldecideon\\nwhenheisinformed, asheistobe,whichelement B;ofagivenparti-\\ntionobtains. Thiswillbecalledthederived decision problem arising\\nfromthebasicdecision problem andtheobservation of7,anditsacts\\nwillbecalledderived acts.Technically speaking, thederived actsare\\ndetermined byarbitrarily assigning onebasicacttoeachelement of\\nthepartition. Foranystates,theconsequence ofaderived actisthe\\nconsequence forsofthebasicactassociated withtheparticular B;in\\nwhich slies.Thetermsinformally introduced inthisparagraph are\\ndefined formally laterinthesection.\\nAderived decision problem isnotnecessarily different inkindfrom\\nthebasicproblem; indeeditisquitepossible thatthebasicproblem can\\nitselfbeviewed asderived fromsomeotherbasicproblem andobser-\\nvation.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a80aef49-13eb-498b-a469-a732be5a6873', embedding=None, metadata={'page_label': '125', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"6.2] WHATANOBSERVATION IS 107\\nFormidable thoughthedescription ofaderivedproblemmayseem\\natfirstreading, itssolution is,inasense,easyandhasalreadyalmost\\nbeengiven;foritisclearthat,ifP(B;)>0,theperson willdecideto\\nassociate withB;abasicacttheexpected utilityofwhichgivenB;is\\nashighaspossible, and,ifP(B;)=0,itisimmaterial totheperson\\nwhichbasicact1sassociated withB;.\\nItisalmostobvious thatthevalueofaderived problem cannotbe\\nless,andtypically isgreater, thanthevalueofthebasicproblem from\\nwhich it isderived. Afterall,anybasicactisamongthederivedacts,\\nsothatanyexpected utilitythatcanbeattained bydeciding onabasic\\nactcanbeattained bydeciding onthesamebasicactconsidered asa\\nderived act.Inshort,theperson isfreetoignoretheobservation.\\nThatobvious factisthetheory’s expression ofthecommonplace that\\nknowledge isnotdisadvantageous.\\nItsometimes happens thatarealpersonavoidsfindingsomething\\noutorthathisfriends feeldutyboundtokeepsomething fromhim,'\\nsayingthatwhathedoesn’tknowcan’thurthim;thejealous spouse\\nandthehypochondriac arefamiliar tragicexamples. Suchapparent\\nexceptions totheprinciple thatforewarned isforearmed callforanal-\\nysis.Atfirstsight,onemightbeinclined tosaythatthepersonwho\\nrefuses freelyproffered information isbehaving irrationally andinvio-\\nlationofthepostulates. Butperhapsitisbettertoadmitthatinforma-\\ntionthatseemsfreemayproveexpensive bydoingpsychological harm\\ntoitsrecipient. Consider, forexample, asickpersonwhoiscertain\\nthathehasthebestofmedical careandisinaposition tofindout\\nwhetherhissickness ismortal.Hemaydecidethathisownpersonality\\nissuchthat,thoughhecancontinue withsomecheertoliveinthe\\nfearthathemaypossibly diesoon,whatisleftofhislifewouldbe\\nagony,ifheknewthatdeathwereimminent. Undersuchcircumstances,\\nfarfromcallinghimirrational, wemightextoltheperson’s rationality,\\nifheabstaimed fromtheinformation. Ontheotherhand,suchanin-\\nterpretation mayseemforced. (Cf.Criticism (f)of§5.6.)\\nExamples ofdecisions basedonobservation areonevery hand, but\\nitwillbeworthwhiletoexamineoneinsomedetailbeforeundertaking\\nanabstract mathematical analysis ofsuchdecisions. Anyexample\\nwouldhavetobehighlyidealized forsimplicity, because thecomplexity\\nofanyrealdecision problem defiescomplete explicit description, but\\nparticular simplicity isinorderhere.\\nThepersonintheexample isconsidering whether tobuysomeofthe\\ngrapesheseesinagrocery storeand,ifso,inwhatquantity. Tohis\\ntaste,thegrapesmaybeofanyofthreequalities, poor,fair,andexcel-\\nlent.CallthequalitiesQgenerically and1,2,and3individually. From\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a07fcadf-a217-4ee2-b928-9a5e4539189d', embedding=None, metadata={'page_label': '126', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='108 OBSERVATION (6.2\\nwhatthepersonknowsatthemoment, including ofcoursetheappear-\\nanceofthegrapes,hecannot becertain oftheirquality,butheattaches\\npersonal probability toeachofthethreepossibilities according to\\nTable1.\\nTaBLe 1.P(Q)\\nQ(uality) |I23 \\nP(robability) |1/41/2 1/4\\nThepersoncandecidetobuy0,1,2,or3pounds ofgrapes; these\\narethebasicactsoftheexample. Takingoneconsideration withan-\\nother,hefindstheconsequences ofeachact,measured inutiles,in\\neachofthethreepossible eventstobethosegiveninthebodyofTable\\n2.Theexpected utilities intherightmargin ofTable2follow, of\\ncourse,fromTable1andthebodyofTable2.\\nTaBLE 2.Utiurty f(Q)FoREACHfANDEACHQ\\n \\n  Q\\nf 1 23E(f)\\n0 00 O 0\\n1—1 18 1\\n23 051/2\\n3-6—-26—1\\nTheentriesinTable2havenotbeenchosenhaphazardly, butwith\\nanattempt atverisimilitude. Thusitissupposed thatiftheperson\\nbuysgrapesofpoorquality hisdissatisfaction withthebargain will\\naccelerate rapidly withtheamount bought, whichseemsreasonable,\\nespecially ifthekeeping quality ofpoorgrapes islow.He1s,ofcourse,\\nunaffected bythequality ifhebuysnone.Again,buyingafewfair\\ngrapesmaybemildlydesirable, butoverbuying isnot.Finally, excel-\\nlentgrapesareworthbuying, eveninlargequantities, buttheutility\\nofthepurchaseincreases lessthanproportionally totheamount bought.\\nThecorrectsolution ofthebasicdecisionproblem istobuy1pound\\nofgrapes; forthatacthas,according totherightmargin ofTable2,\\nanexpected utilityof1,which isthelargestthatcanbeattained.\\nNow,suppose theperson isfreetomakeanobservation, thatis,a\\nnewobservation inaddition tothosethatmayhavecontributed tothe\\ndetermination oftheprobabilities inthebasicproblem. Itmaybe,for\\nexample, thatthegrocerinviteshimtoeatafewofthegrapesorthat\\ntheperson isgoingtoaskthewomanbesidehimhowthey looktoher.\\nLettherebefivepossible outcomes ofhisobservation; callthemz\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='389ed2d7-29b5-412c-9e78-1926c908ec4d', embedding=None, metadata={'page_label': '127', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.2] WHATANOBSERVATION IS 109\\ngenerically and1,2,3,4,and5individually. Iassume, though this\\nfeature isratherincidental totheexample, thatlowvalues ofxtend\\ntobesuggestive oflowquality. Thejointdistribution ofxandQ,that\\nis,theprobability thatxandQsimultaneously haveanygivenpairof\\nvalues, isofcentral technical importance. Thoseprobabilities, each\\nmultiplied by128forsimplicity ofpresentation, aregiveninthebody\\nofTable3.Theright-hand andbottom margins ofthetablegive,\\n \\n TABLE 3.128P(x 1)Q)\\nQ\\nx 1 2 3128P(2)\\n1 15 oD 1 21\\n2 10 15 2 27\\n3 4 24 4 32\\n4 2 15 10 27\\noD 1 oD 15 21\\n32 64 32 128\\n128P(Q)   \\nalsomultiplied by128,theprobability ofeachvalueofxandofeach\\nvalueofQ.Themarginal entries are,ofcourse,obtained byadding\\nrowsandcolumns. Asindicated inthelowerright-hand cornerofthe\\ntable,theprobabilities assumed doindeedaddupto1,andthebottom\\nmargin recapitulates Table 1.\\nConditional probabilities caneasilybereadfromTable3.Thus,for\\nexample, theconditional probability thatxis2,giventhatQis3,is\\n2/32,andtheconditional probability thatQis2,giventhatxis4,is\\n15/27. Itwillbeseeninlatersections thatthedistribution ofx given\\nQis,in@sense,evenmorefundamental thanthejointdistribution of\\nxandQ.\\nThereare4°=1,024derived acts,sinceoneofthefourbasicacts\\ncanbeassigned arbitrarily toeachofthefivepossible outcomes ofthe\\nobservation. Itisaneasyexercise, usingTables 2and3,toverify\\nTable4,whichshowstheconditional expectation oftheutilityofeach\\nTaBLE4.E(f|2x) \\n x\\nf 1 2 3 4 5\\n00/21 0/27 0/320/270/21\\n1—7/21 11/27 32/8243/2749/21\\n2—40/21 —20/27 8/3244/2772/21\\n3—94/21 —78/21 -—48/32 18/2774/21  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cc23b29b-08b6-4eb5-9537-3d4b9dfa546b', embedding=None, metadata={'page_label': '128', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='110 OBSERVATION (6.2\\nbasicactgiveneachpossible outcome oftheobservation. Foreachz,\\nthehighestexpected utility,giventhatvalueofz,hasbeenitalicized.\\nThus,forexample, onlyifxis1willtheperson refrainfrombuying\\ngrapesaltogether, andonlyifxis5willheriskbuying3pounds. In\\nfull,thebestderived act,callitg,istobuy0,1,1,2,or3pounds,ifx\\nis 1,2, 3,4,or5,respectively. Thevalueofthederivedproblem isthe\\nexpected valueofg,which iscomputed thus:\\n(1) E(g)=>>E(g|2)P(2)\\n=(0+11+32+44+74)/128\\n161/128~1.26utiles.\\nSincethevalueofthebasicproblem is1utile,theenvisaged observa-\\ntionisworth0.26utile;thatis,thepersonwould ifnecessary payup\\nto0.26utilefortheobservation.\\nExercise\\n1.Suppose thatthepersoncoulddirectly observe thequality ofthe\\ngrapes.Showthathisbestderived actwouldthenyield2utiles,and\\nshowthatitcouldnotpossibly leadhimtobuy2poundsofthegrapes.\\nThenotionofadecision problem basedonanobservation willnowbe\\nformally described, withspecialreference tomathematical notation and\\nothertechnical details.\\n1.There isasetofbasicacts,Fwithelementsf,f’,etc.\\nIntheexample ofthegrapesFconsisted ofthefourenvisaged acts\\nofbuying 0,1,2,or3pounds ofgrapes.\\nTheconvention laiddownattheendof§5.4,requiring thatthecon-\\nsequences ofactsbemeasured inutiles,willbeadhered to,anditwill\\nbesupposed thatv(F)isfinite.\\n2.Theobservation isa(notnecessarily real)random variable x\\nassociating with each statesanobserved valuex(s)insomesetXof\\npossible observed values xz,x’,etc.\\nIntheexample ofthegrapes, thestatess(ofwhichthepostulates\\nrequire thattherebeaninfinitenumber) wereneverfullydescribed,\\nandconsequently therandom variable xwasnotfullydescribed either.\\nInthesamesenseitmaybesaidthatthebasicacts,whicharealso\\nreallyrandom variables, werenotfullydescribed either. Allthatis\\nreallyimportant, however, istoknowthesimultaneous distribution of\\ntheconsequences oftheactsinFandofthe values ofx.Intheexample\\nofthegrapesthatinformation wasimplicit inTables2and3.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9183bdd3-b1b5-4dfe-90b9-3112ce37c0f6', embedding=None, metadata={'page_label': '129', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3] MULTIPLE OBSERVATIONS, ANDEXTENSIONS 111\\nFormathematical simplicity intheformalworktofollow, itwill\\ngenerally beassumed thatXhasonlyafinitenumber ofelements,\\nthoughtheassumption canandmustberelaxed inmanypractical\\nsituations. WhenXisassumed finite,therandom variable xis,for\\nallpurposes ofthepresent context, simplyapartition ofS,namely,\\nthepartition intothesetsonwhichxisconstant. Indeed, earlierin\\nthissection, thenotionofobservation wasdescribed intermsofapar-\\ntition,butthedescription intermsofarandom variable ismorefamiliar\\ninstatistics andmayhavetechnical advantages, especially whenthe\\nrestriction thatXbefiniteisrelaxed.\\n3.Thesetofstrategy functions isthesetofallfunctions associating\\nanelement ofFwitheachelementxofX.Letthevaluesofthegeneric\\nstrategy function bedenotedbyf(x)andthefunction itselfbyf(x).\\nThenotion ofstrategy function wasnotintroduced intheinformal\\ndescription ofobservation, norintheexample ofthegrapes,because\\nitisbutamathematical intermediary tothedefinition ofderived acts\\nanddidnotseemtocallforexplicit expression inthelessformalcon-\\ntexts.\\n4.Toeachstrategy function f(x)corresponds aderived actg,inthe\\nsetofallderived actsF(x),definedby\\n(2) g(s)=f(s;x(s)) forallscS.\\nItwasexplained thatintheexample ofthegrapesthereare4°de-\\nrivedacts.Inthesameway,itcanbeseeningeneral thatifXhasé\\nandFhas¢elementsthereare¢éderived acts.\\n5.ThevalueofFgivenz,\\n(3) v(F|2)=rsupE(f|x).\\nThisisthefunction ofxindicated, fortheexample ofthegrapes,\\nbyitalicsinTable4.\\n3Multiple observations, andextensions ofobservations andofsets\\nofacts\\nIfseveralrandom variables x),---,Xn,associating elements ofS\\nwithelements ofsetsX,,---,Xn,aresimultaneously underdiscussion,\\nit isnatural toformthenewrandom variable, denoted x={x,,---,\\nXn},thatassociates witheachelement ofSanordered n-tuple ofele-\\nmentsofX1,--+,Xn,respectively. Ifthecontext issuchthatx,---,\\nX,arethought ofasobservations, thenxcanalsobethought ofasan\\nobservation andwillsometimes becalledamultiple observation—to\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16a6ae8b-bbad-458f-ab06-1ceed65d4177', embedding=None, metadata={'page_label': '130', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='112 OBSERVATION [6.3\\nemphasize themanner ofitsformation. Toillustrate, anyitem such\\nas profession orbodytemperature thatmightbeenteredonapatient’s\\nhistorycanbethought ofasanobservation; butthewholehistory, or\\nafilingcabinet ofhistories, canalsobethought ofasanobservation,\\nthehistory beingamultiple observation ofitems,andthecabinet a\\nmultiple observation ofhistories.\\nConsider twoobservations xandy.Itisaninteresting possibility\\nthatxandyaresorelated toeachotherthatknowledge ofthevalue\\nofxwould(almost certainly) imply(almost certain) knowledge ofy.\\nInthatcase,observation ofximplies essentially theobservation ofy\\nandgenerally something besides, whichsuggests thefollowing three\\ndefinitions.\\nIfandonlyifxandyareobservations suchthat,forallsands’in\\nsomeBofprobability one,z(s)=x(s’)implies y(s)=y(s’);thenxisan\\nextension ofy,andyisacontraction ofx.Ifxisanextension ofy,\\nandyisanextension ofx,thenxandyareequivalent.\\nStrictly speaking, oneshouldsaynotthatxandyareequivalent,\\nbutratherthattheyareequivalent regarded asobservations, forthis\\nwouldnotbeagoodconcept ofequivalence toapplytorandom varia-\\nblesregarded assuch.Forexample, apairofequivalent observations\\ncanobviously beapairofrealrandom variables withdifferent expected\\nvalues. Someproperties oftherelations ofextension, contraction, and\\nequivalence between observations aregivenbythefollowing easybut\\nimportant exercises. Throughout thissetofexercises itisunnecessary\\ntosuppose theobservations confined toafinitesetofvalues; inthecase\\nofExercise 3b,itisimpossible todoso.\\nExercises\\n1.xandyareequivalent, ifandonlyifxisbothanextension anda\\ncontraction ofy.\\n2a.IfP{x(s)=y(s)}=1,xandyareequivalent.\\n2b.Anyobservation xisequivalenttoitself.\\n3a.IfthereisavalueyosuchthatP{y(s)=yo}=1,thenevery\\nXisanextension ofy,andany two suchobservations areequivalent.\\nSuchanobservation, ofcourse,amounts toobserving nothing atall\\nandwilltherefore becalledanullobservation.\\n3b.Ifx(s)=sforalmost alls¢S,thenxextends everyy.\\n4.Ifxisanextension ofy,andyisanextension ofz,thenxisan\\nextension ofz.Stateandverifytheanalogous factaboutequivalence.\\n5a.Ify’isafunction associating anelement ofYwitheachelement\\nofX,andxisanobservation, thentheobservation ysuchthaty=\\ny’(x)isacontraction ofx.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0223e127-b084-42b9-8f8d-714e9fb6134a', embedding=None, metadata={'page_label': '131', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3] MULTIPLE OBSERVATIONS, ANDEXTENSIONS 113\\n5b.Ifyisacontraction ofx,thenthereisafunction y’suchthat\\nP{y(s)=y’(a(s))}=1.Whatfreedom isthereinthechoiceofthe\\nfunction y’?\\n5c.Whataretheimplications ofExercises 5aand5bforequivalence\\nbetween observations?\\n6.Ifxandyareobservations andz={x,y}isthecorresponding\\ndoubleobservation, thenzisanextension ofxandofy.(Thisexercise\\nseemstocallforaconverse sayingthateveryextension canberegarded\\nasadoubleobservation, butnoreallyneatonesuggests itselftome.\\nNonetheless,inthinking aboutextensions andcontractions, thesort\\nbrought outbytheexercise isatypicalandstimulating example.)\\n7.{x,y}isequivalent tox,ifandonlyifxextendsy.\\nTherelations ofextension, contraction, andequivalence haveparal-\\nlelsforsetsofacts,defined thus:\\nIfFandGare(non-vacuous) setsofactssuchthat,forsomeBof\\nprobability one,thereisforeachg¢Ganf¢Fwithf(s)=g(s)forall\\ns¢B;thenFisanextension ofG,andGisacontraction ofF.IfFis\\nanextension ofG,andGisanextension ofF,thenFandGareequiv-\\nalent.\\nMoreexercises\\n8.IfFisanextension of(equivalent to)G,thenv(F)>(=)v(G).\\n9.Discuss theanalogues ofExercises 1,2b,and4forsetsof\\nacts.\\n10.IfF>G,thenFextends G.\\n11.IfF(x)isderivedfromFonobservation ofx,thenF(x)extends\\nF.\\n12.Hyp.\\nF(x)isderivedfromFonobservation ofx;\\nF(y)isderivedfromFonobservation ofy;\\nF(x,y)isderivedfromFonobservation of{x,y};\\nF(x;y)isderivedfromF(x)onobservation ofy.\\nCONCL.\\n1.F(x,y)1sequivalent toF(x;y).\\n2.F(x,y)extends F(x)andF(y).\\n3.Ifxisequivalent toy,thenF(x)isequivalent to F(y).\\n4.Ifyextends x;thenF(x,y)isequivalent toF(y),F(y)isequiva-\\nlenttoF(x;y),andF(y)extends F(x).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a54333f-88e3-4163-8cc1-a1dfe15119f0', embedding=None, metadata={'page_label': '132', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='114 OBSERVATION [64\\n18a.Underthehypothesis of12,theequivalences andrelations of\\nextension amongthesetsofactsarisingoutoftwoobservations can,\\nwithevident conventions, bediagrammed thus: \\nyY%Yy;x\\n{ {\\nx——_>0<—_y   \\n13b.Ifyextends x,thediagram becomes \\nXYxyy;xylox-—d.\\n   \\n13c.Ifxandyareequivalent, thediagram becomes \\nx,yx,yy,x\\nx y—0.\\n   \\n14.IfF(x)andG(x)arederivedfromFandG,respectively, andif\\nFextends G,thenF(x)extends G(x).\\n15.o(F(x))=E(F|x]=f»(F|x(s))dP(s)>o(F).\\n4Dominance andadmissibility\\nAccording toExercise 3.14,ifonesetofacts,regarded asbasic,ex-\\ntendsanother, thefirstisatleastasvaluable asthesecond inthelight\\nofanyobservation whatever. Thissection explores arelation, domi-\\nnance,whichhasthesameproperty butisnotsostrictasextension.\\nDominance isofsomeimportance forthetheoryofpersonal probability\\nasithasbeendeveloped thusfar.Butits‘importance willbeeven\\ngreater inthestudyofstatistics proper,whereinterpersonal agreement\\nisofparticular interest; for,asthedefinition shortly tobegivenwill\\nmakeclear,twopeoplehaving different personal probabilities willagree\\nastowhether oneoftwosetsofactsdominates another, ifonlythey\\nagreewhicheventshaveprobability zero—a condition generally met\\ninpractice, andonethatcouldifdesiredbedispensed withbyaslight\\nchange inthedefinition ofdominance.\\nItwillbeseenthatdominance andnotionsrelated toitareintimately\\nassociated withthesure-thing principle. Indeed, probability being\\ntakenforgranted, thebasicfactsaboutdominance seemtogiveacom-\\npleteexpression ofthesure-thing principle. Dominance andrelated\\nconcepts weremuchstressed byWald,in[W3]forexample.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a09a8f95-7413-4102-89f0-f818ed475946', embedding=None, metadata={'page_label': '133', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4] DOMINANCE ANDADMISSIBILITY 115\\nTwoorthreenotions, thelogicalconnections amongthem,andthose\\nbetween themandextension, aretobetreated. Thelogicalconnec-\\ntionsbeingmanybutsimple, Ithinkthatthematerial lendsitselfbet-\\ntertoformalthantoexpository treatment, forinsuchacontext the\\nreaderwholooksforthemotivating ideasseesthemhimselfmoreeasily\\nthanhecomprehends someoneelse’s verbalization ofthem.Thissec-\\ntionwilltherefore consistprimarily ofagroupofformaldefinitions and\\nseveral exercises.\\nIfandonlyifP(f(s)>g(s))=1,fdominates g.Ifandonlyifsome\\n(every)element ofFdominates (isdominated by)g,Fdominates (is\\ndominated by)g.IfandonlyifFdominates everyelement ofG,\\nFdominates G.Ifandonlyiffdominates g,butgdoesnotdominate\\nf,fstrictlydominates g.Ifandonlyiff<¢F,andfisnotstrictlydomi-\\nnatedbyanyelement ofF,fisadmissible (withrespect toF).\\nInvolving astheydoactsaswellassetsofacts,thedefinitions,\\nstrictly speaking, introduce fourdifferent kindsofdominance. How-\\never,thiscomplexity canbealleviated, withaslightlapseoflogic,by\\nidentifying eachactfwiththesetofactsofwhichfistheonlyelement,\\nforitiseasilyseen that thisidentification isinsuchharmony withthe\\ndefinition that,onceit ismade,thefourkindsofdominance collapse\\nintoone.\\nExercises\\nla.Consider analogues ofExercises 3.2band3.4.\\nlb.Whencantwoactsdominate eachother?\\n2a.IfFextends G,thenFdominates G.Discuss theconverse.\\n2b.F(x)dominatesF.\\n2c.IfFDG,thenFdominates G.\\n3a.IfFCG,andFdominates G,theneachadmissible element ofG\\ndominates andisdominated byanelement ofF.\\n3b.Afteranyfinitenumber ofnon-admissible elements isdeleted\\nfromF,whatremains ofanysubsetofFthatdominated Fcontinuesto\\ndominate F.\\n3c.Though thesetofadmissible elements ofFmayinsomeinstances\\ndominate F,nopropersubsetofthesetofadmissible elements canever\\ndoso;but,ifanyothersubsetdominates F,somepropersubsetofit\\nalsodoesso.\\n3d.IfFisfinite,thesetofadmissible elements ofFdominates F.\\n3e.Discuss theroleof“finite” in3band3d.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea9a685d-6db6-40f7-b316-92c5eb0d4857', embedding=None, metadata={'page_label': '134', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='116 OBSERVATION [6.5\\n4a. Ifthe set ofadmissible elements ofFdominates G,andGdomi-\\nnates F,then the setofadmissible elements ofFisequivalent tothe\\nsetofadmissible elements ofG.\\n4b. IfFandGdominate each other, and either isfinite, then the\\nsets ofadmissible elements ofFand G,respectively, are equivalent to\\neach other, and each dominates both Fand G.\\n5.IfFdominates G,then v(F) >v(G).\\n6.IfFdominates G,then, forany observation x,F(x) dominates\\nG(x).\\n5Outline ofthe design ofexperiments\\nOften, especially instatistics, adecision problem can beseen asthe\\nproblem ofdeciding which ofseveral experiments—or which ofseveral\\nobservational programs, ifthat isreally amore general term—to under-\\ntake.\\nInthis section the notion ofthe decision problem derived from a\\nbasic decision problem and anobservation must beelaborated little,\\nbecause, asderived acts have been treated thus far, they correspond to\\nthe possibility ofmaking anobservation free ofcharge. Though obser-\\nvations are sometimes free, there istypically acost associated with\\nmaking them; information must typically bebought either from other\\npeople or,more often from nature, so to speak. The cost ofinforma-\\ntion may bemoney, trouble, one’s own life, that ofanother, orany of\\ninnumerable possibilities, but allcan inprinciple bemeasured interms\\nofutility. The cost ofanobservation inutility may benegative as\\nwell aszero orpositive; witness the cook that tastes the broth.\\nInprinciple, ifanumber ofexperiments are available to aperson, he\\nhas but tochoose one whose set ofderived acts has the greatest value\\ntohim, due account being taken ofthe cost ofobservation. That simple\\nformulation, likesome others inthis book, is,inasense, oversimple; it\\nabstracts from theenormous variety ofconsiderations that enter into\\nthe careful design ofany experiment. The possibility ofsoabstracting\\nfrom variety does not remove the ultimate necessity ofstudying some\\naspects ofthat variety indetail. R.A. Fisher’s The Design ofExperi-\\nments [F4], forexample, isconcerned almost exclusively with experiments\\nbased onaspecial technique called the analysis ofvariance, andit is\\nbut anintroduction toeven that important facet ofstatistics. Again,\\nthere isagrowing literature (inwhich thework ofA.Wald isoutstand-\\ning) onsequential analysis, which isconcerned in principle with allex-\\nperiments inwhich later parts ofthe experiment areconducted inthe\\nlight ofwhat happens in earlier parts; but this literature has, byneces-\\nsity, been confined toarelatively tiny part ofthat domain.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='42348144-4a11-4d48-9977-2637f605670d', embedding=None, metadata={'page_label': '135', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.5] OUTLINE OFTHE DESIGN OFEXPERIMENTS 117\\nBefore turning toamore formal recapitulation ofthe outline ofthe\\ndesign ofexperiments, thismay beagood place forafew speculative\\nwords about the difference, ifany, between experiment and observation.\\nSome sciences arecommonly called experimental asopposed toothers\\nthat are called observational. Aerodynamics, the psychology ofrote\\nlearning, and the genetics offruit flies would typically becalled experi-\\nmental sciences; and, totake parallel examples, meteorology, the psy-\\nchology ofdreams, and human genetics would becalled observational.\\nBut itiswidely agreed, and the most casual consideration makes it\\nclear, that any basic difference that may really bepresent resides not\\ninthe sciences themselves but inthemethods typical ofeach. Toillus-\\ntrate the role of observation insciences ordinarily considered experi-\\nmental and vice versa, observations ofwild populations offruit flies\\nhave been useful inthestudy ofthe genetics offruit flies; the effects of\\nfatigue, forexample, ondream content may well bethe subject ofan\\nexperiment; and, except fortheatom, notopic inscience ismore popu-\\nlartoday than experimental rain making. The illustrations could be\\nextended indefinitely, and there isalso aless direct sort exemplified by\\nthe discipline called experimental medicine, which typically studies ex-\\nperiments onanimals with thehope, often justified, that the findings\\nthus obtained can beextrapolated tohumans.\\nThe problem, then, istodistinguish anexperiment from anobserva-\\ntion. Except forbrevity, itmight bebetter tosaymere observation,\\nfor, ingeneral usage, anexperiment would beconsidered aspecial sort\\nofobservation.\\nThefirst apparent contrast that comes tomind isthat experimenta-\\ntion isgenerally thought ofasactive and observation aspassive. But,\\nupon examination, itisseen that observation isalso active, for obser-\\nvations are typically made bygoing somewhere toobserve, orwaiting\\nattentively tillsomething happens. Often itisnot only the observer\\nhimself who must betransported and put inreadiness tomake anob-\\nservation, but also aconsiderable body ofapparatus. What demands\\nmore activity than themodern observation ofasolar eclipse?\\nAnother apparent contrast isthat theexperimenter acts onthething\\nheobserves, whereas the observer acts only onhimself and oninstru-\\nments ofobservation that may beregarded asextensions ofhisown\\nsense organs. Ifthis criterion were accepted altogether naively, there\\nwould benosuch thing asaphysiological experiment onone’s self;\\neven sophisticated interpretations might find itdifficult toembrace\\npsychological experiments onone’s self.\\nFinally, experiments asopposed toobservations arecommonly sup-\\nposed tobecharacterized byreproducibility and repeatability. But\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be9ece16-fbb6-477c-9379-f6cddeb28e29', embedding=None, metadata={'page_label': '136', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='118 OBSERVATION (6.5\\ntheobservation oftheangle between twostars iseasily repeatable and\\nwith highly reproducible results indouble contrast toanexperiment to\\ndetermine the effect ofexploding anatomic bomb near abattleship.\\nAll inall,however useful the distinction between observation and ex-\\nperiment may beinordinary practice, Idonot yet seethat itadmits\\nofany solid analysis. Atany rate, noformal use ofthe distinction will\\nbeattempted inthis book.\\nReturn now tothe notion ofobservation subject tocost. Itmay be\\nthat the value oftherandom variable xisobservable but only ata\\ncost c,areal-valued random variable measured inutiles. If,ashereto-\\nfore, F(x) denotes the set ofacts derived from Foncost-free observa-\\ntion ofx,letF(x) —cdenote the setofderived acts subject tothe ran-\\ndom cost c.This notation isinterpreted tomeanthat, iffisthegeneric\\nelement ofF(x), then f—c(which, being autility-valued function of\\ns,isanact) isthe generic act ofthe setF(x) —c.Very often the cost\\nofanobservation isindependent of s,but not, forexample, forhim that\\ntests the sharpness ofathorn with his finger. Since observations are\\ntypically paid for before, orsimultaneously with, making the observa-\\ntion, the cost istypically observed along with the observation proper.\\nPut differently, the cost cistypically acontraction oftheobservation\\nx.Thus, ifinsome special context any advantage were tobegained\\nbysodoing, itwould not bedrastic toassume the cost ofobserving x\\ntobeafunction oftheform c’(x); but, asamatter offact, nosuch ad-\\nvantage has come tomy attention. Itisnot difficult tothink ofex-\\nperiments towhich the assumption does not apply. For example, in\\nthe present state ofuncertainty about the long-term effects ofx-rays,\\nanyone conducting ashort-term experiment inwhich young human be-\\nings were subjected tolarge doses ofx-radiation would risk costs that\\nmight not overtly manifest themselves for half acentury, oreven for\\ngenerations.\\nMuchthat would ordinarily becalled observation cannot bedescribed\\nbysaying that therandom cost issimply tobesubtracted from each de-\\nrived act ofthe corresponding observation thought ofasfree ofcost.\\nAllowing that itmay belegendary, theform oftrial byordeal inwhich\\nthe guilty floated safely tobehanged and theinnocent drowned tobe\\nexonerated epitomizes such asituation; except inpoint ofabsurdity,\\nordinary industrial destructive testing ofelectric fuses and other prod-\\nucts ismuch thesame. Strictly speaking, discrepancy occurs even in\\nthe ordinary context inwhich the cost ofobservation isafixed sum of\\nmoney; forthe utility ofmoney isnot strictly linear, sothe cost ofob-\\nservation typically affects different derived acts somewhat differently.\\nThis sort ofsituation isindeed socommon astointroduce at least a\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4205032-6647-4e08-9063-0e926731a194', embedding=None, metadata={'page_label': '137', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.5] OUTLINE OFTHE DESIGN OFEXPERIMENTS 119\\nslight error into almost every application ofthenotion ofcost asasub-\\ntractive term. Itwould therefore bedesirable toextend considerably\\nthe notion ofcost ofobservation, but, thus far, Iseenoway todoso\\nthat does not destroy themathematical advantage ofsingling problems\\nofobservation out ofthe class ofdecision problems generally.\\nItisconvenient now toanalyze the appropriateness ofregarding the\\nnumber v(F) asameasure ofthevalue ofF.Asmust already beclear\\ntothe reader, ifaperson istomakeapreliminary decision limiting his\\nnext decision toone oranother of several sets ofacts, say, F,G,andH,\\nthen hispreliminary decision will select asetthat has the highest value\\nofv,and thepreliminary and secondary decisions, regarded asasingle\\ngrand decision, amount tothe problem ofdeciding onanact from\\nFUG UH. Sofar asthis use ofvisconcerned, any increasing mono-\\ntonic function ofvsuch asv®or3”would beequally satisfactory, but v\\nhas anadvantage inarithmetic simplicity when costs ofobservation\\nare involved. Consider, forexample, theproblem ofwhether tomake\\naparticular observation attherandom cost cortomake noobservation\\nat all. The two sets ofacts involved may then besymbolized by\\n(F(x) —c)and F,respectively. The peculiar simplicity ofvasameas-\\nureofthevalueofasetofacts, in this context, isexhibited bythealmost\\nobvious fact that v(F(x) —c)=v(F(x)) —E(c). Itmay beremarked\\ninpassing that visaparticularly good measure inany problem where\\nF,G,orHis,sotospeak, made available bylot, apossibility realized\\nin(7.3.2), forexample.\\nFinally, ifoneamong several observations istobechosen, each with\\nitsown random cost (possibly including the null observation), the per-\\nson will choose anobservation forwhich v(F(x)) —E(c) isaslarge as\\npossible. Ifthenumber ofobservations among which decision isto\\nbemadeis infinite, that function may not attain amaximum value,\\nbut thevalue ofthe situation totheperson can reasonably beregarded\\nasthesupremum ofthe function; there are, ofcourse, observations\\namong those available forwhich the supremum isarbitrarily nearly\\nattained.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da8d53a0-f576-4478-a8d1-6ef45aad87ab', embedding=None, metadata={'page_label': '138', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 7\\nPartition Problems\\n1Introduction\\nIntheintroduction ofthepreceding chapter itwasexplained that\\nthetreatment ofdecision problems ingeneralhadbeencarried toa\\nlogicalconclusion, andthattostudydecision problems further ithad\\nbecome necessary tospecialize. Thenotionofobservation wasaccord-\\ninglychosenasthesubject ofspecialization. Thesituation nowre-\\npeatsitselfatanewlevel,forIhavenowcovered themainpointsthat\\noccurtomeaboutobservation ingeneral, though Iseeconsiderably\\nmoretosayaboutacertaintypeofobservation.\\nThetypeofobservation problem towhichthepresent chapter isde-\\nvoted,though relatively special, isstillverygeneral. Indeed, itsgen-\\nerality issuggested bythefactthatnoothertypeofproblem issyste-\\nmatically treated inmodernstatistics. Inobjectivistic terms, itwould\\nbedescribed asthetypeofdecision problem inwhichtheconsequence\\nofeachbasieactdepends onlyonwhichofseveral (possibly infinitely\\nmany)probability distributions doesinfactapplytotherandom vari-\\nabletobeobserved.\\nModern statistics hasnonameforthistypeofproblem, because it\\nrecognizes noother type; andnoparticularly suggestive nameoccurs\\ntome.Iamtherefore tentatively adopting thenoncommital name\\n“nartition problem.” Suchmotivation asthereisforthatnamewill\\nbeapparent whentheconcept isdefined.\\nInnon-objectivistic terms,apartition problem hasthefollowing\\nstructure. Thereare,ofcourse, basicactsFandanobservation 2.\\nThepeculiar feature isarandom variable b,whichistypically notsub-\\njecttoobservation, withtheproperty thateveryfinFisconstant\\ngiventhatbhasanyparticular valueb.\\nInmanypractical problems btakesonaninfinity, evenanon-de-\\nnumerable infinity, ofvalues, butsystematic consideration ofsuch\\nproblems wouldinvolve thoseadvanced mathematical techniques that\\nareexplicitly beingavoided inthisbook.Glossing oversuchquestions\\noftechnique forthemoment, thestateoftheworld,which isitselfa\\n120\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3bb0d64a-52b3-4322-8259-9c4aba0cf63c', embedding=None, metadata={'page_label': '139', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.2]STRUCTURE OF(TWOFOLD) PARTITION PROBLEMS 121\\nrandom variable, mightplaytheroleofb;withrespecttothisb,any\\nobservational decision problem wouldpresumably beapartition prob-\\nlem.Itmay,therefore, beinaccurate tocallpartition problemsspecial,\\nbuttheyarespecialwhenever bisnotequivalent tothestateofthe\\nworld.\\nAshasjustbeenmentioned, thegeneral policyofthisbookwithre-\\nspecttomathematical technique restricts formaltreatment ofpartition\\nproblems heretothoseinwhichbassumesonlyafinitenumberofdif-\\nferentvalues,thatistosay,thoseinwhichistoallintentsandpur-\\nposesapartition B;,whence thename“partition problem.”’ Forthe\\nreaderwhoisnotfamiliar withtheelements ofthegeometry ofn-dimen-\\nsionalconvex bodies, therewillbeadistinct expository advantage in\\nconfining theformaltreatment stillfurther totwofold partitions. At\\nthesametime,byexplicit statements andbytheuseofsuggestive no-\\ntation, allreaders willbegivenatleastsomeideaoftheextension of\\nthetheoryto n-fold partitions; indeed, areaderfamiliar, forexample,\\nwithSections 16.1-2of[V4],orwith[B20]willfindtheextension as\\nplainasifithadbeenmadeexplicitly. Thustherestriction totwofold\\nasopposed ton-foldpartitions willbetotheadvantage ofsomeandto\\nthedisadvantage ofnone.\\nPartition problems areevencloserthanareobservational problems\\ngenerally tothesubjectmatter ofstatistics proper. Inparticular, in\\nthecourseofthischapter, multipersonal considerations willfromtime\\ntotimebepointed outinconnection withpartition problems.\\n2Structure of(twofold) partition problems\\nAcentralfeature ofatwofold partition problem is,ofcourse,atwo-\\nfoldpartition, ordichotomy, B;,7=1,2.Bywayofabbreviation let\\nB(t)=P(B;,),and8={8(1),B(2)}.The8(2)’scanbeanytwonumbers\\nsuchthat6(¢)>0andZ6(z)=B(1)+B(2)=1.SinceB(2)=1—\\nB(1),itmightseemsuperfluous tohaveaspecialnotation forB(2);but\\nthisredundancy morethanpaysforitselfinsymmetry, especially in\\ntheextension ofthetheoryton-foldpartitions. Thepossibility that\\noneofthe8(z)’svanishes hasbeenruledout,foritisneither typicalnor\\ninteresting, anditsretention wouldmartheexposition ofthetheory.\\nEachbasicactf¢Fischaracterized byapairofnumbers f;suchthat\\n(1) P(f(s)=f;|B)=1\\nforeach7.Thetechnical assumption willbemadethatasfranges\\noverFthenumbers f;arebounded fromaboveforeach2,whichisa\\nlittlemorestringent thanthenowfamiliar assumption thatv(F)<o.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be09e176-315f-4aff-b5ad-ffc47ba6668e', embedding=None, metadata={'page_label': '140', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='122 PARTITION PROBLEMS (7.2\\nTheassumption expressed by(1)ismadefordefiniteness andsim-\\nplicity,though itsfullforcewillseldombeused.Thepossibility ofre-\\nlaxing (1)incertain contexts willbementioned fromtimetotime,es-\\npecially sincethispossibility isofsomeinterestevenintheexploitation\\nof(1)itself.Inparticular, forseveralpagesnowitwillscarcely ever\\nbenecessary toassume anything aboutthestructure ofFrelative to\\nB,,exceptthatE(f |B;)isbounded fromaboveforeach7;formaking\\ntheabbreviation f;=E(f |B;),almosteverything fromherethrough\\nExercise 1appliesverbatim.\\nTheexpected utilityofanyf¢Fcanbecomputed inseveralforms\\nthus:\\n(2) E(f)=E(£|B,)P(B,)+E(f|Bz)P(Be)\\n=fiB(1)+feB(2)\\n=f(t)\\n=fo+(fi—fe)B(1).\\nThefirstoftheseformsexpresses theexpected valueingeneral terms;\\nthesecond utilizes abbreviations; thethirdisanobvious mathematical\\ntranscription ofthesecond, particularly suggestive ofextension tothe\\nn-foldsituation; the fourth sacrifices thesymmetry exhibited bythe\\npreceding threeinordertotakeadvantage oftherelation between\\nB(1)and6(2).Fromthefourthformof(2),it isclearthat,forfixedf,\\nEf)isalinearfunction of8(1).Henceforth thatfact,forexample,\\nwouldbeexpressed insymmetric formbysayingthatE(f)islinearin\\n8,andthedependence ofE(f)on8mightbeexplicitly indicated by\\nwriting E(f |GB).\\nSinceinanyonedecision problem 8isconstant, itmightseempoint-\\nlesstoemphasize thatK(f |8)islinearin8.Butthereare,infact,two\\ndifferent reasons forbeinginterested invariation of8.Inthefirstplace,\\noncetheobservation xhasbeenobserved tohavethevalue z,thebasic,\\norapriori,decision problem isreplaced byanaposteriori problem in\\nwhichP(B; |2)playstheroleoriginally playedbyP(B;)=B(t).Sec-\\nond,interest incomparing different people isbecoming increasingly\\nmoreexplicit asthebookproceeds. Inparticular, itisofinterest to\\ncompare peoplewhohaveavailable thesamesetofbasicactsandwho,\\natleastsofarasthedistribution ofxandtheactsinFareconcerned,\\nhavethesameconditional personal probability givenB,;,butwhoat-\\ntachdifferent probabilities 6(z)totheelements ofthepartition.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8293815a-ab9e-4772-bd39-d48eba3e4a6f', embedding=None, metadata={'page_label': '141', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.2]STRUCTURE OF(TWOFOLD) PARTITION PROBLEMS 123\\nToemphasize itsdependence on£,v(F)willsometimes bewritten\\nv(F|8);itscomputation inthefollowing fashion isfundamental to\\nthetheory ofpartition problems.\\n(3) v(F|8)=supE(t|8)\\nsup[fi8(1)+feB(2)]\\n=k(8),\\nwherek(8)isdefinedbytheequation inwhich itoccurs. According to\\nExercise 4ofAppendix 2,thefunction kisconvex in8,thatis,kis\\nconvexwhenrecognized asafunction of8(1)alone.Interpreted asa\\npairofaprioriprobabilities, 8isconfined totheopeninterval defined\\nby26(j)=1,B(z)>0,butitisvaluable torecognize thatkisdefined,\\nconvex,andcontinuous ontheclosedinterval 2@(j)=1,B(t)>0.\\nManytypical features oftherelationship between FandB;areillus-\\ntratedgraphically byFigure 1.Theabscissa ofthatgraphrepresents\\n \\n  a\\nbc a\\nd\\n—6)—— an B(2) >\\ne  \\nFigure 1\\nboth6(1)and8(2),asindicated, andtheordinate ismeasured inutiles.\\nThestraight lines,theleftendsofwhicharemarked a,b,c,d,ande,\\ngraphasfunctions of6theexpected valuesofthefivebasicactsofthe\\nparticular problem represented. Theordinates attheirrightandleft\\nends,respectively, arethecorresponding values ofthef,’sandfo’s.\\nThegraphofkismarkedbyheavylinesegments. Itisseenthatthe\\nlinesa,c,ande,andtheyalone,touchthegraphofk,fortheyrepre-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6fdfac0-32a8-4c99-8330-b6ade320e40c', embedding=None, metadata={'page_label': '142', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='124 PARTITION PROBLEMS (7.2\\nsenttheonlyactsthatareoptimal forsomevalueof8.Theactrepre-\\nsentedbydisinadmissible (if(1)istakenliterally), beinginfactstrictly\\ndominated byeveryotheractexcept e,anditistherefore superfluous\\ntotheperson,nomatterwhatthevalueof8;bisobviously equally\\nsuperfluous, butforadifferent reason.\\nInmanytypicalproblems inwhichFhasaninfinity of elements, k\\nis,unlikethekinFigure 1,strictly convex; thatis,itsonlyintervals\\noflinearity arepointintervals.\\nExercise\\n1.Compute andgraphkforthesetFofdichotomous actsofthe\\nform\\nfig)=1-(1+9)’;\\nfel)=1-—(1—9);\\nAnswer. k(8)=[8(1)—6(2)]?=[28(1)—1)’.\\nTurnnowtotherelations between anobservation xandthedichotomy\\nB;.Asbefore, itwillbeassumed formathematical simplicity thatthe\\nvaluesofxareconfined toafinitesetX.Theprobability thatxat-\\ntainsthevaluexgivenB;,writtenP(x |B;),isfundamental inconnec-\\ntionwithpartition problems. Foronething,ashasalreadybeenindi-\\ncated,thereisinterest inconsidering peoplewho,though differing with\\nrespect to8,agreewithrespect toP(x |B;).Theprobability P(z,B,)\\nthatxattainsthevaluezandthatB;simultaneously obtains, theproba-\\nbilityP(x)thatxattains thevalue xz,andtheprobability B(7|x)ofB;\\ngiventhatx(s)=xarederivedfromP(x |B;)andBbymeansofBayes’\\nrule(3.5.4)andthepartition rule(3.5.3)thus:—2<5¢5 +2\\n(4) P(x,B;)=P(x|ByB(A).\\n(5) P(x)=2XP(e,Bi).\\n(6) B(i|x)=P(x,B;)/P(a),\\nifP(x)¥0;andBi|x)ismeaningless otherwise. Itmustberemem-\\nberedthatP(x,B;),P(x),andB(7 |x)dependonthevalueof8andthat\\nareallycomplete notation wouldshowthatdependence. Ontheother\\nhand,the condition thatP(x)¥0isindependent ofthevalueof8.\\nWhenasecondobservation yistobediscussed, B(i|y)is,indefiance\\nofstrictlogic,tobeunderstood astheanalogue ofB(c|x);thatis,as\\ntheconditional probability ofB;giventhaty(s)=y,notasthesame\\nfunction asB(c|x)withysubstituted forx.Corresponding conven-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb56e554-d2ad-44b3-b8ce-49ac2612833c', embedding=None, metadata={'page_label': '143', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3] THEVALUEOFOBSERVATION 125\\ntionsapplytoP(y),P(y|B;),andP(y,B,).Finally, freeusewillbe\\nmadeofsuchcontractions asB(x)for{B(1 |x),B(2 |x)}.\\nEquation (1)implies that\\n(7) Ef|By,x)=EfBy)\\nforallf¢FandforallxsuchthatP(x |B;)>0.Equation (7)isthe\\nmathematical essence oftheconcept ofapartition problem, andvir-\\ntuallyallthatistobesaidaboutpartition problems appliesverbatim,\\nif(7),evenwithout (1),applies tosuchobservations asmaybeunder\\ndiscussion.\\nInviewof(7),\\n(8) Ef|B,2)=2)E(f|Bi,x)P(B;|2)\\nifP(x)>0.\\n3Thevalueofobservation\\nIftheobservation xismade,anditisfoundthatx(s)=x,thenthe\\naposteriori valueofthesetofbasicacts,writtenv(F |x),ormorefully\\nv(F|8,x),willtypically bedifferent fromtheapriorivaluev(F|B).\\nIndeed, inviewof(2.8),\\n(1) v(F|8,2)=supE(£|8,2)\\n=o(F|8(2))\\nk(8(z)).\\nThisisthefirstillustration ofthetechnical convenience ofthefunction k.\\nItisknownongeneral principles thatv(F(x))>v(F),butthereis\\nsomeinterest inreverifying theinequality inthepresent context; in\\nparticular, itispossible heretosayininteresting termsjustwhenequal-\\nitycanobtain.\\n(2) v(F(x) |8)=E(o(F |B(x) |8)\\n=E(k(6(x)) |8)\\n>k(E(8(x) |8),\\nwheretheterminal inequality isanapplication ofTheorem 1ofAppen-\\ndix2.Toappreciate theinequality (2),it isnecessary tocalculate\\nE(B(t |x))explicitly. Thiscalculation, typical ofmanythereadermust\\nhenceforth beexpected tomakeforhimself, runsasfollows, whereitis\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de83bf74-f86c-4306-b949-684f6b97305b', embedding=None, metadata={'page_label': '144', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='126 PARTITION PROBLEMS (7.3\\ntobeunderstood thatthesummation withrespect toxapplies only\\ntothosetermsforwhichP(z)isdifferent from0.\\n(3) E(p(i|x)| 8)=©B(|2)Pz)\\n \\n=P(B;)=B().\\nSubstituting (3)into(2)leadstotheanticipated conclusion that\\n(4) v(F(x) |8)>(8)=o(F8).\\nAccording toTheorem 1ofAppendix 2,v(F(x) |8)isdefinitely greater\\nthano(F |6)unlessB(x)isconfined withprobability onetosomeinter-\\nvaloflinearity ofk,inwhichcasetheobservation xmayfairlybe\\ncalledirrelevant tothebasicdecision problem athand. Ifxisirrelev-\\nant,theinterval oflinearity towhich8(x)isconfined must,inviewof\\n(3),contain 6.Intheparticularly interesting case—and theonlypos-\\nsibleone,ifk(8)isstrictlyconvex—in which@(x)iswithprobability\\noneequaltoaconstant value,thatvaluemusttherefore be8.Anob-\\nservation forwhich8(x)iswithprobability oneequalto8mayfairly\\nbecalledutterly irrelevant, because it isirrelevant nomatterwhatset\\nFofbasicactsisassociated withthedichotomy.\\nTosaythatxisutterly irrelevant istosaythat,withprobability\\none,\\nP(z|B;)B(2)(5) BG|2)=“5a\\n=B(i).\\nSince(7)>0,(5)isequivalent tothecondition that\\n(6) P(x|B;)=P(e),\\natleastwhenP(x)>0.Furthermore, itisobviousfrom(2.5),again\\nnotingthat6(7)>0,that,ifP(x)=0,thenP(x |B;)=0.Therefore\\nxisutterly irrelevant, ifandonlyif(6)holdsforallxand7;thatis,if\\nandonlyifthedistribution ofxgivenB;isindependent of7.Thisform\\nofthecondition isintuitively evokedbythewords“‘utterly irrelevant’’\\nandhastheadvantage ofnotinvolving 8.\\nItisnoteworthy thatwhether anobservation isutterly irrelevant\\ndepends neitherontheparticular setofbasicacts,noronthevalueof\\nB,sopecplewillagreeonwhatisutterlyirrelevant independent oftheir\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c1a463e9-5bd1-4a99-a20b-a6bb492d9960', embedding=None, metadata={'page_label': '145', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"7.3] THEVALUEOFOBSERVATION 127\\npersonal aprioriprobabilities andtheactsamongwhichtheyarefree\\ntochoose.\\nThegreatest lowerboundinxofv(F'(x) |8),namelyv(F |B),andthe\\ncircumstances underwhichthisbound isattained havingbeenestab-\\nlished, it isnatural toturntoaparallel investigation oftheleastupper\\nbound.Afoothold forthatinvestigation isfoundintheremark that\\nthechordjoiningtheendsofthegraphofkneverliesbelowthegraph.\\nAnalytically,\\nwhere[(8)isdefinedbythecontext. Unlessoneofthe8(7)’svanishes,\\nequality holdsin(7),ifandonlyif&(@)isalinearfunction. Inviewof\\n(7)and(3),\\n(8)v(F(x) |8)=E(k(B(x)) |8)<E(x)|8)=16).\\nTheinequality (8)givesanupperbound forv(F(x)). Ingraphical\\ntermsitsaysthat,forany8,noobservation canaddmoretothevalue\\nk(@)ofFthanthevertical distance at6between thegraphofkand\\nthegraphofthechordjoiningtheendsofk.\\nEquality obtains in(8),ifkislinear,inwhichcasetheupperand\\nlowerbounds areequaltoeachotherirrespective ofthevalueof8and\\nthenatureoftheobservation. If.Fisdominated bysingle f,thatis,\\nifthereisasinglefoptimal givenB;forbothvaluesof7,thenkislinear.\\nItcaneasilybeverified that,providedisfiniteand’(1)actually ob-\\ntains,thisisindeedtheonlycircumstance underwhichislinear,and,\\neveniftheseprovisions arenotsatisfied, thepossibilities arenotmuch\\nmoreinteresting.\\nSuppose, then,thatkisnotlinear;equality canholdin(8),ifand\\nonlyif8(x)iswithprobability confined totheendsoftheinterval, a\\ncondition thatdoesnotdepend atallonF.Bysimpleconsiderations,\\nwhichhavebynowbeenrendered familiar, thiscondition onxisequiv-\\nalenttothecondition that\\n(9) P(x|B,)P(«| Be.)=0,\\nforallxAnobservation satisfying (9)mayfairlybecalleddefinitive,\\nbecause, if(1)obtains, suchanobservation removes alluncertainty\\nabouttheoutcome ofeachf¢F,nomatterwhat6maybe.\\nPerhapsmanyoftheobservations madeineverydaylifearedefini-\\ntive,orpractically so.OnceOldMotherHubbard lookedinthecup-\\nboard,herdoubtswerereduced tothevanishing point.Nonetheless,\\ndefinitive observations donotplayanimportant partinstatistical\\ntheory, precisely because statistics ismainly concerned withuncer-\\ntainty,andthereisnouncertainty onceanobservation definitive for\\nthecontext athandhasbeenmade.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b3dd2b6-01fc-4c9d-a243-7b5c3272fefd', embedding=None, metadata={'page_label': '146', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='128 PARTITION PROBLEMS [7.4\\n4Extension ofobservations, andsufficient statistics\\nItwasshownin§6.4thatastatistic, or contraction, yofanobser-\\nvationxisneverworthmorethanxandistypically worthless.The\\npurpose ofthepresent section istoexploretherelationbetween anob-\\nservation andacontraction ofitselfinthecaseofapartition problem,\\nespecially toexplorethespecialconditions inthatcaseunderwhichthe\\nstatistic isasvaluable astheobservation itself.\\nLetxandybetwoobservations suchthatyisastatistic ofx,that\\nis,suchthat,forsomefunction y’,y(s)=y’(#(s)) withprobability one.\\nThevaluesofF(x)andF(y)canbecompared bythefollowing calcula-\\ntion,whichinthelightofthepreceding section willneedbutlittleex-\\nplanation.\\n(1) v(F(x))=E(&(6(x)) |8)\\n=>>E(k(6(x)) |8,y)P(y).\\n(2) E(k(8(x) |6,y)>k(E(B(x)) |B,y)),\\nifP(y)>0.\\n(3) E(e(i|x)|6,y)=2)BG|x)P|y)\\n_ybELOPY)\\nPP(y)\\nifP(y)>0.\\nBecause ofthespecialrelationship between xandy,P(az,y)=0un-\\nlessy’(x)=y,inwhichcaseP(x,y)=P(x).Understanding thatthe\\nsummation indicated by2’in(4)belowextends only over thosevalues\\nofxforwhichy’(x)=y,thecalculation iscontinued thus:\\n_P(a,Bi)P(x)\\nP(x)P(y)\\n>)P(x,B;)\\nPly)\\n_Py,Bi)\\nPly)\\n=B(i|y).  (4) E(@(i|x)|6,y)=\\n \\nTherefore,\\n(5) »(F(x) |8)>>>k(B(y))P(y) =»(F(y)|8).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36d8a59c-b500-4f93-9a84-278650551556', embedding=None, metadata={'page_label': '147', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.4] SUFFICIENT STATISTICS 129\\nAfterthepreceding section, itseemsalmostsuperfluous toexplain\\nthatthepointofthecalculation aboveisnottoobtaintheinequality\\n(5),whichhasalready beenderived withlesslaborandgreater gener-\\nalityinExercises 6.3.8and6.3.13b,buttobeabletodiscusswhenequal-\\nityholdsin(5).Thecalculation makes itclearthatequality holdsin\\n(5),ifandonlyifequality holdsin(2)foreveryyofpositive probability.\\nThisinturnisequivalent tothecondition that,giveny,8(x)isconfined\\nwithprobability onetoaninterval oflinearity ofk.Asufficient con-\\nditionforthatisthat,giveny,B(x)beconfined withprobability oneto\\nasinglevalue,whichcannotbeotherthan@(y);ifkisstrictlyconvex,\\nthealmostcertainconfinement ofB(x)toB(y)isalsonecessary. Now,\\nif,foreveryyofpositive probability, P(@(x(s)) =B(y) |y)=1,then\\nitistruethatB(2)=8(y)withunconditional probability one,thatis,\\n(6) P(B(x(s))=B(y(s)))=1.\\nThecondition (6)clearlydoesnotdependonF,andthefollowing\\ncalculation soexpresses itastomakeclearthatitdoesnotdependon8\\neither. Equation (6)issatisfied, ifandonlyif\\nP(z|BBG)—P(y’(x) |BBO)\\nPit)=P(x)\\nwhenP(x)>0;or,ifandonlyif\\nP(«|B;)—P(2)\\nPYy|B)Ply)\\nwhenP(x |B,)>0;or,again,ifandonlyif\\n(9) P(x|By,y)=P(«|y),\\nwhenP(y |B;)>0;orfinally ifandonlyifP(x |B;,y)1sindependent\\nof<forthosevaluesof2forwhich itisdefined. Inthisform,andyet\\nanother tobederived inconnection with(10),thecondition iswidely\\nstudied inmodern statistical theoryandastatistic satisfying thecon-\\nditionistherecalledasufficient statistic. Thenameiswelljustified;\\nfor,ashasjustbeenshown,itissufficient, foranypurpose towhichx\\nmightbeput,toknowy,ifandonlyifyisasufficient statistic forx.\\nAdifferent, andperhapsmorecongenial, approach tosufficient sta-\\ntistics isthefollowing. Ifthepersonobserves theparticular valuey\\nofy,hisoriginal basicdecision problem isreplaced byanewonewith\\nthesamebasicacts,butwith6replaced byB(y).Strictly speaking,\\nthiswillfailtobeapartition problem, incaseB(y)is(0,1)or(1,0),or,\\nforbrevity, ifB(y)isextreme.Toseewhether v(F(x) |8)isreallygreater (7)\\n (8)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='51e685a6-a9f8-4240-9bf9-c01c0575acdd', embedding=None, metadata={'page_label': '148', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"130 PARTITION PROBLEMS [7.4\\nthanv(F(y) |8),itisenough toinvestigate whether, forsomeyofposi-\\ntiveprobability forwhich6(y)isnotextreme, xisrelevant tothepar-\\ntitionproblem basedon6(y),forifB(y)isextreme therecan benovalue\\ninfollowing theobservation thatyhasoccurred bytheobservation of\\nx.Therefore, xwillbeaworthless addition toy,if,foreveryyfor\\nwhichB(y)isnotextreme, xisutterlyirrelevant, thatis,ifyissufficient\\nforx.Ifkisstrictly convex, thecondition isalsonecessary.\\nTherecognition ofsufficient statistics inexplicitproblems isoften\\nfacilitated bythefollowing factorability criterion. Astatistic yissuff-\\ncientforxifandonlyifthereexistsatleastonepairoffunctionsRand\\nSsuchthat\\n(10) P(x|By)=R(y'(z); 1)S(z).\\nThenecessity ofthecondition followsfromtheexhibition ofaparticu-\\nlarRandSforasufficient statistic thus:\\n(11) P(x |By)=DYP(e|Bi,y)P(y |Bi)\\n=>)P(x|y)Pty|Bi)\\n=P(y'(z) |B)P(z| y’(2)).\\nOntheotherhand, ifP(x |B;)canbeexpressed intheform(10),y\\ncanbeseentobesufficient forxthus:IfP(x |B;,y)ismeaningful, it\\nisgivenby\\n(19) P(e|B,y)=y|Bi)\\nPty |Bi)\\n=0, ify'(x)#y,\\n_P(a|Bi)\\n~P(y|Bd)\\n_8)SSSe)\\ny(x’)ay\\nwhich isindependent of7.Thereadermaybeinterested inasking\\nhimself, asanexercise, whatfreedom thereisinchoosingRandSwhen\\natleastonesuchpairoffactors exists.\\nInterest insufficient statistics isnotconfined, ofcourse, totwofold,\\norevenfinite,partitions. Withthatinmind,thevarious criteria for\\nsufficient statistics havebeengiveninsuchtermsastobevalidforany\\nfinitepartition andtheusualinfiniteones.Theyrequiresomemodifica-ify’(z)=y,\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3990036a-be64-43d3-8158-98fa09e4c305', embedding=None, metadata={'page_label': '149', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"7.4] SUFFICIENT STATISTICS 131\\ntioniftheobservations arenotconfined toafinite,oratanyratede-\\nnumerable, setofvalues,butformaldetailsofthatimportant extension\\nwillnotbegivenhere.Elementary treatments aregiveninmosttext-\\nbooksofmathematical statistics; moreadvanced andgeneral treat-\\nmentsaregivenin[B2],[L6],and[H3].\\nThereareseveral examples ofsufficient statistics intheexercises\\nbelow,othersaregiveninalmostanyfairlyadvanced textbook onsta-\\ntistics(inparticular, in[C9]),andoneothergeneralexample ofextraor-\\ndinaryimportance istreated inthenextsection.\\nExercises\\nIntheseexercises, letxdenoteamultiple observation x={x,,---,\\nX,},where,givenB,,thex,’sareindependent andidentically distributed.\\nTherewillbenorealadvantage hereinthinking ofthepartition as\\ntwofold, or evenfinite,andforsomeoftheexercises itwillbeimprac-\\nticaltodoso.\\n1.LetP(z,|B)=p, if2,=1,\\n=q,  ifz,=0,\\n=0,otherwise,\\nwherep;+q;=1;andlety’(z)=>>a,.\\nShowthat:\\n(a)P(«|Bi)=piat;\\n(b)yissufficient forx,usingthefactorability criterion;\\nn n\\n(c)Piy|B;=()ptar™, where,asalways,()=n!/yl(n—y)!;\\ny yn\\\\72\\n(a)P(e|y'@2))=(})|\\ny'(x)\\n2.Foreachpositive integer7,let\\nP(a,|B) =i,ifz,<i,\\n=Q, otherwise,\\nwherethevalues ofx,areconfined tothepositive integers; andlet\\ny’(x)=max2z,.Showthat:\\n(a)P(x | B,)1a”, ify<1,\\n=0, otherwise;\\n(b)yissufficient forx.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7dc9a89d-ac3d-474d-9955-752a9b6f8967', embedding=None, metadata={'page_label': '150', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='132 PARTITION PROBLEMS (7.4\\n3.Inthetwoexercises above ithasbeenpossible tochoosethefac-\\ntorSidentically equalto1.Toexhibitamoretypicalexample,let7,\\nxy,andybeconfined tothepositive integers withy’(x)=max7,,as\\ninthepreceding exercise, andlet\\n22, ; ;P(x,|B)=———_—*—iiifa, <i,\\na(2+1)\\n=0, otherwise.\\nShowthat:\\n2 n\\nP(z|By-(—~) »,ify<4, (a)P(x|Bi)4DITs ify <i\\n=0, otherwise.\\n(b)yissufficient forx.\\n4.Putnorestriction ontheconditional distributions P(x, |B;),ex-\\nceptthatx,beconfined withprobability onetosomefixed finiteset.\\nSay,forthemoment, thattwovaluesxandx’ofxareteammaies, if\\nonearisesfromtheotherbypermutation ofthecomponent observa-\\ntions.Thisdivides thepossible valuesofxintoteams,and,academic\\nthough itmayseem,theteamtowhichxbelongs canbetaken asy’(z).\\nShowthattheprobability ofxgiveny’(x)andB;isindependent of7\\n(ifitisdefinedatall),sothatthestatistic y’(x)issufficient forx.\\nIfthevaluesofthex,’shappen toberealnumbers, thenforanyx\\nit1spossible topermute thecomponent observations toobtainanon-\\ndecreasing sequence ofn(notnecessarily distinct) numbers, andonly\\nonesuchnon-decreasing sequence canbesoobtained fromeach7.\\nThesequence thusattached through xtoeachsiscalledinstatistical\\nusagethesequence oforderstatistics corresponding tox.Sinceteam\\nmates,andonlyteammates,havethesameorderstatistics, thesetof\\norderstatistics regarded asasinglestatistic isequivalent totheteam\\nstatistic y’(x)definedmoregenerally intheparagraph above andis\\ntherefore sufficient.\\n5.Letx,givenB;besubject tothenormal probability density with\\nmeany,,andvariance c,’,thatis,\\n(13) (x, |B;)=(2x07) exp{—(a,—wi)?/20,7}.\\nThissituation, thoughelementary, doesnotfallwithinthetechnical\\nscopeofthisbook,because x,isnotconfined toafinitesetofvalues.\\nThereaderfamiliar withprobability densities willsee,however, that\\nthedensity ofxis\\nza,\" Milly My\\n14 1°°? B)=(2 2)\"ex{= —™nN()o(x yon |i) (Ne)p20,2+a2 20,2  \\n  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1702e99f-e909-41cf-b498-4ee0cedc3063', embedding=None, metadata={'page_label': '151', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"7.4] SUFFICIENT STATISTICS 133\\nwhichsuggests thaty,definedby\\n(15) y’(2)={Za,”, Ltr},\\nmayfairlybecalledasufficient statistic forx.\\nShowinthesameheuristic waythat,ifo;isindependent of7,then\\ny'(x)=Ya,definesasufficient statistic; andthat,ifuw;isindependent\\nofi,theny’(x)=nZax,?—(Za,)?doesso.\\n6.Ifwandzareobservations independent ofeachothergivenB,,\\nunderwhatconditions canwbesufficient for{w,z}?\\n7.Tobreakawayfromindependent observations, suppose that,in\\ntheeventB;,ncardsaredealtfromathoroughly shuffled deckofn+1\\ncardseachbearing adifferent serialnumber from1throughn+2.\\nLetw,bethenumber ontherthcarddealtandw={w,,---,Wn}.\\nShowthatmaxw,definesasufficient statistic forwandthatthew,’s\\nr\\narenotindependent.\\n8.Ifzextends w,andwissufficient fory,thenzisalsosufficient for\\n9.Ifzissufficient forw,andyisindependent ofbothzandw,then\\n{z,y}issufficient for{w,y}.\\n10.Everydefinitive statistic issufficient.\\nInvirtually allstatistics textsitwouldbesaidthattheydefinedby\\n(15)constitutes notonestatistic, buttwo;similarly, thesetoforder\\nstatistics wouldordinarily bereferred toasnstatistics ratherthanas\\none.Therearecontexts inwhich itisappropriate totrytocountsta-\\ntisticsinthatfashion, but,sofarasthetheory ofsufficient statistics\\nisconcerned, itoftenseemsfruitless, ifnotpositively detrimental, to\\ndoso.\\nTheconcept ofsufficient statistics hasproved ofgreatvalueinsta-\\ntisticaltheoryandpractice. Thereasonforthisdoesnotseemtome\\naltogether easytoanalyze, but, as theexercises aboveillustrate, the\\nfamilies ofdistributions mostfrequently studied instatistics aregen-\\nerallyrichinsufficient statistics. Itishardtoseparate causefrom\\neffecthere;forthedistributions thataremoststudied tendtobethose\\nhavingthegreatest mathematical simplicity, andthepresence ofstrik-\\ningsufficient statistics, suchasthoseexhibited byExercises 1,2, 3,5,\\nand7,areamongthesources ofmathematical simplicity mostoften\\nmetinthestudyofparticular families ofdistributions.\\nItmustbeemphasized thatsufficient statistics oftenprovideasignifi-\\ncantsaving inthemechanical laborofstoringandpresenting data.\\nThus,inanyexperiment faithfully represented byExercise 1,itis\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8484b266-4bb1-43ba-9b96-f4b75a6c6d57', embedding=None, metadata={'page_label': '152', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='134 PARTITION PROBLEMS [7.5\\nsufficient, inboththetechnical andordinary sensesoftheword,to\\nrecordasingleintegeryinplaceofthelistofz,’s,whichmightwellbe\\nverylong.Several oftheotherexercises wouldinprinciple alsolead\\ntogreatsavings ofthissort,butExercise 5istheonlyotherthatarises\\nfrequently inpractice.\\nTheconcept ofsufficient statistics wasintroduced, together with\\nmuchofthetheoryassociated withit,byR.A.Fisher (cf.index,[F6)).\\nThesubjecthasbeenoneofcontinuing interestandhasbeenexplored\\ninseveral directions; keyreferences are[B2],[E1],[L6],[H3],[K15],\\nand[M5],and(LeCam 1964).\\n5Likelihood ratios\\nTherandom variable 6(x)hasplayedsoimportant aroleinpreced-\\ningsections that the readerwillprobably notbesurprised tofindthat\\nB(x)isasufficient statistic forx,aconclusion that,inthelightofthe\\nfactorability criterion (4.10),canbeseenthus:\\nP(B; |x)P\\n (1) P(z|B)=aH (2)\\nB(i|2)= P(2).aa”\\nIfastatistic issufficient, itissufficient irrespective ofthevalueof8;\\nmoreover, anymultiple ofitbyanon-zero constant isalsosufficient.\\nTherefore, (1)impliesthatforanynumbers a(z),suchthata(z)>0,\\nthemultiple observation r(a)definedby\\nP(x|B;)\\n‘Za(j)P(x |B;) r(x;a)=p\\n(2)\\nT(z;a)=Dfiri(2,a),ro(x,a)}\\nisasufficient statistic forx.Since\\n(3) Ya(Ari(ar;a)=1\\nthereissomeredundancy inretaining bothcomponents, butthisre-\\ndundancy ismorethancompensated bytheadvantage ofretaining\\nsymmetry, especially whenn-foldpartitions arecontemplated.\\nFormally, ther(a)’sareaninfinitefamily ofsufficient statistics, one\\nforeacha;buttoallintentsandpurposes theyrepresent butonesuffi-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4907f9e4-7439-4f4b-b6f5-a113e47176f4', embedding=None, metadata={'page_label': '153', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.5] LIKELIHOOD RATIOS 135\\ncientstatistic, foranyr(a)isequivalent toanyother,sayr(a’),ascan\\nbedemonstrated thus:\\nP(x |B;)/Za\\'(k)P(x| By)\\nZa(j){P(x |B,)/Za’(k)P(« |Bx)} (4) ri(X,a)=\\n_\"s(x,a’)\\nZa(j)r;(x, a’)\\nHaving suchamultiplicity offormsforwhatisessentially oneim-\\nportant statistic 1sratherembarrassing, sothereissomeincentive to\\npickastandard form.Settingeacha(j)=1recommends itselfascon-\\nvenientandleadstotheparticular statistic r={r,,ro},where\\nP(x|B;)\\nThisformisindeedconvenient fortwofold and,moregenerally, forn-\\nfoldpartitions, but,whereinfinite partitions aretobedealtwith,its\\napparent naturalness ismisleading, forthesuminthedenominator of\\n(5)isthentypically divergent. inthecaseoftwofold partitions, a\\nconvenient formforthestatistic isthatofalikelihood ratio,inthe\\nsenseintroduced in§3.6,foritiseasytoseethat,infinitenumbers\\nbeingadmitted, P(x |B,)/P(x |Be)isequivalent tor.Henceforth, any\\nstatistic equivalent torwillbecalledalikelihood ratioofxwithre-\\nspecttothepartition B;—adefinition thatdoesnotseriously conflict\\nwithordinary statistical usageoftheterm.\\nFigure 1illustrates ageometric interpretation oflikelihood ratios\\nthatissometimes valuable. Thefigurecanbestbedescribed bytelling\\nhowtodrawit.Firstdrawapairofcartesian coordinate axesforvaria-\\nblesu;andue.Nextdrawthetwolinesegments represented byu;+\\nug=1and(u;/a(1))+(ue/a(2)) =1withtheu,’snon-negative. The\\nleftendsofthesesegments areindicated inFigure 1byaand3,re-\\nspectively, theparticular valuea={1/3,2/3}beingusedforillustra-\\ntion.Nowplotthepoint{P(x |B,),P(x |Bz)}. Ifxhaspositive\\nprobability (forany,andthereforeforall,6);thispointwillbedifferent\\nfromtheoriginO,soitwillbepossible todrawthe(dashed) linecon-\\nnectingtheoriginwiththe point {P(x |By),P(x|Be)}.Thisline(or\\nraythrough theorigin,asitisoftencalled)mustnecessarily pierce\\nthelinesegments aandb.Theimportant geometrical fact,whichthe\\nreader willhavenodifficulty inverifying, isthattheseintersections\\noccuratthepoints{7;(x),re(x)}and{ry(z,a),re(z,a)},respectively.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='19c8e1c4-386c-4973-a290-abe0d2387c94', embedding=None, metadata={'page_label': '154', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='136 PARTITION PROBLEMS [7.5\\n  \\n   ”77\\n_-°[P(#|By), P(x|Bo)}\\n   \\na“\\n“try(2,a),lo(x,a)}\\n \\nFigure 1\\nItisalsoobvious thattheratioP(x |B,)/P(a |Bz)isthereciprocal of\\ntheslopeoftheray.\\nSince,toeachxthatoccurswithpositive probability, therecorre-\\nspondsaraythrough theorigin,theraycanbetakenasastatistic;\\naccording tothegeometrical construction ofthepreceding paragraph,\\nthisstatistic isequivalent torandistherefore alikelihood ratioofx\\nwithrespecttothepartition B;.\\nTherayconnecting theoriginwithapoint{u,,ue}canconveniently\\nberepresented bythesuggestive notation u,:U2,though, ofcourse,dif-\\nferentpairsofnumbers canrepresent thesameray.Moreexplicitly,\\nifXisanynumberdifferent from0,Au,:Aue represents thesameray\\nASUy:U2. Inanalytical projective geometry anypairofnumbers rep-\\nresenting arayinthisfashioniscalledasetofhomogeneous coordinates\\noftheray.Theredundancyofthenotation w;:w2mayberemoved by,\\nforexample, characterizing theraybythereciprocal ofitsslopeu1/wz.\\nSuchnon-homogeneous coordinatization entailsasacrifice insymmetry\\nandthenecessity ofadmitting infinity asameaningful valueofthe\\nquotient; bothlossesarequitetroublesome inextension ofthesegeo-\\nmetricconcepts tocartesian spaceofndimensions, which isnecessary\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ec54a0b-a059-4b28-a7c4-148883ca88f4', embedding=None, metadata={'page_label': '155', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.5] LIKELIHOOD RATIOS 137\\ninconnection withn-foldpartitions. Inhomogeneous coordinates the\\nlikelihood ratiocanconveniently berepresented byanyoftheequally\\ngoodsetsofhomogeneous coordinates, P(x |B,):P(z |Boa),71(2)i1re(2),\\nandr(x,@):ro(x, a).Finally, itmayberemarked thatP(x |B,)/\\nP(x |Bz)isanon-homogeneous coordinate. Thusthemanyequivalent\\nformsinwhichthelikelihood ratiostatistics canbenaturally expressed\\ncorresponds tothemanydifferent notations bywhicharaythrough the\\norigincanbenaturally designated.\\nThemostremarkable factaboutthelikelihood ratioconsidered asa\\nstatistic isthatitisnecessary, sotospeak,aswellassufficient. Bythat\\nImeanthattohavetheadvantages ofknowing xit isnecessary as\\nwellassufficient toknowthelikelihood ratio.Thepointcanbeput\\nformally thus:\\nTHEOREM |Ifyissufficient forx,thenyisanextension ofr.\\nProor. Thetheorem isvirtually obvious intermsofthefactora-\\nbilitycriterion forsufficient statistics, forinthenotation of(4.10)\\nR(y(x), 2)\\n(6) r(x)=———->ZR(y(x), j)\\nwithprobability one,exhibiting r;asafunction ofy.@\\nCoROLLARY 1Ifzissufficient forx,andifeveryysufficient forx\\nisanextension ofz,thenzisequivalent tor.\\nByordinary analytic standards, thelikelihood ratioseemstobea\\nrathercomplicated statistic, atleastinthecaseofn-foldpartitions,\\nwhere 7nisatalllarge;for,toonewhotakesseriously theideathata\\nmultiple statistic shouldnotalsoberegarded asasinglestatistic, the\\nlikelihood ratioseemsatfirstsighttoben,orperhaps (n—1),statis-\\ntics.YetTheorem 1anditscorollary showthatthelikelihood ratiois,\\ninafundamental sense,themostcompact sufficient statistic thata\\npartition problem admits.\\nAsanexplicitexample ofalikelihood ratio,consider thetwofold par-\\ntitionproblem arisingfromExercise 4.1onconfining attention totwo\\ndifferent values ofp,sayp;andpo.Thelikelihood ratioriseasily\\ncomputed thus:\\n (7) P(z|By)=p#U1—py”\\n.\\\\ut) \\\\Y\\n=(1—pi(a)=qi\"3)\\nl1—p; qiSO ho\\nNn./q.\\\\v(8) r(x) qi(p;/qi)\\nZq;\"(p./q;)”\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0450f276-96f4-4838-928d-f5f834a58dd8', embedding=None, metadata={'page_label': '156', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='138 PARTITION PROBLEMS (7.5\\nTheorem 1isthereby verified inthepresent instance; for(8)exhibits\\nrexplicitly asacontraction ofy,andiseasilyexhibited asacontrac-\\ntionofrthus:ae(2)|log “fl\\nr2(x)1\\nP1492log——\\nP2q1\\nInthisexample, yis,inviewof(8)and(9),equivalent tothelikelihood\\nratio. \\n (9) y(z)=\\nExercises\\n1.Express k(@(x))andv(F(x)) intermsofthelikelihood ratiothus:\\n(10) Bi;r)=perB(H)/QLiB);\\n(11) k(B(x))=k(B(r(z))).\\n(12) (F(a)|6)=Dea)|DP|Bye|:\\n2.Thisextended exercise develops thepersonalistic andbehavioral-\\nistictheory ofwhat,following theobjectivistic andverbalistic tradi-\\ntionsofstatistics, iscalledthetesting ofasimpledichotomy, atypeof\\ndecision problem that,thoughseldomveryrealistic, isapopularand\\ninstructive example withimportant implications formorerealistic prob-\\nlems.Verbalistically suchaproblem isdescribed asthatofmaking the\\nbestguessonthebasisofanobservation as towhetheritisB,orBz\\nthatobtains. Behavioralistically, thisisgenerally interpreted asthe\\nproblem ofdeciding, onthebasisofobservation, between twoprimary\\nactsoneofwhich ispreferable totheotherifB,;obtainsandviceversa\\nifBydoes.Hereisonetopicinwhichtheassumption that7isconfined\\ntotwovalues israthermorethansimplyapedagogical simplification;\\nareaderinterested inrelaxing theassumption willfindpages127-130\\nof[W3]stimulating.\\nSuppose thatFcontains onlytwoactsf;andf.andisdominated by\\nneither. Letoij=DfEc; |B;).\\n(a)There isnolossofgenerality insupposing\\n22—$12 $11—$21 3 7%2=pi—_5-__ >9\\nwhichwillhenceforth bedone.Thatis,itwillbesupposed thatf,is\\nappropriate onlytoB,andviceversa.(13) 61=pf\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b8ca64a8-2589-41f2-a96b-ee1f4a0161ac', embedding=None, metadata={'page_label': '157', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.5] LIKELIHOOD RATIOS 139\\n(b)Showthat\\n(14)(8)=2d1j8(3) forB(1)>81/(51+2)=Bo(1)\\n=2)¢2;6(j) forB(2)>62/(1+82)=Bo(2)\\n=Ebr+$01)8(1)+3(dr2+$22)8(2)+\\n|816(2)—628(1) |\\n= 25¢8(j)+|618(2)—628(1) |,\\nJ\\nwhere8»andthee,’saredefinedbythecontext.\\n(c)E(f; |8B)=k(8),ifandonlyifB(7)>Bo(i).Thiscondition ob-\\ntainsforboth7’ssimultaneously, ifandonlyifB=Bo.\\n(d)Showthat\\n(15)(6(r))=p>€t8(J)+|8yr28(2)—82r18(1)}/Er3(3)\\n=2857) forr;>ri*(B,Bo),\\nJj\\n where\\nBo(7)/B(2)\\n16 i*)=nDue) 7B;Po)DE6o(H)/8@\\nandthat\\n(17)(F(x) |8)=228G)+DoOP(r|Be)8(2)—b2P(r |Bia(1)|\\n=fe+d[1—2P(r1<r1*(8,Bo)|Bx)\\n—P(r=r*(B,Bo)|Bi)]}8(1)\\n+{e+6[1—2P(re<r2*(8,Bo)|Be)\\n—P(r=r*(B,Bo)|Be)]}8(2).\\n(e)Anyderived actf(x)determines afunction iassigning an7to\\neachz,ibeingimplicitly defined thus:f(x)=fiz).Conversely anyi\\ndetermines aderived act.ShowthatE(f(x) |8)=v(F(x) |8),ifand\\nonlyifry¢2)(x)>itz)*(B,Bo)foreveryx.Suchafunction 7(zx)iscalled\\nalikelihood-ratio testassociated withr*.Showthatatleastonelikeli-\\nhood-ratio testisassociated witheveryvalueofr*,andthatifP(r=r*)\\n=0(which istypically thecase)thereisonlyone.\\n(f)Iff(x)isdetermined byafunction ofi,theprobability ofdeciding\\nontheinappropriate valueofiincaseB;obtains isgenerally called\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34ce856e-eb9b-4490-9919-01b6d3155d2b', embedding=None, metadata={'page_label': '158', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='140 PARTITION PROBLEMS [7.6\\ntheprobability ofanerrorofthej-thkind.Analytically theprobabili-\\ntiesoferrorofthefirstandsecondkindare,respectively,\\n(18) ey=peP(i(z)=2|By),2=vsP(i(x)=1|Ba).\\nIfi*isalikelihood-ratio testassociated withr*,showthatitserrors\\nofthefirstandsecondkindaresubjecttothebounds\\n(19) P(r,<r1*|Bi)<e:*<P(r,<171*|By)\\n(20) P(r,>ry* |Bo)<eg*<P(r>11* |Bp).\\nWhataboutthetypical casethatP(r=r*)=0?\\n(¢)Showthat,if1isatleastasgoodasi*inthesensethate;<e,*\\nforboth2z’s,theniisalikelihood-ratio testandiisvirtually i*inthat\\ne;=e;*forboth7’s.Hint:Consider anFanda8forwhichr*(@,Bo)\\n=r*,showing thattheseexist,andnotethat,forthisdecision problem,\\nE(E,s |B)={e,—82(1—2e;*)}8(1)+{eg—51(1—2eg*)}8(2)\\nv(F(x) |8)\\nE(f, |8)={¢1—82(1—2¢:)}8(1)+{e2—51(1—2e9)}8(2)\\n>v(F(x)|8),\\nwithequality ifandonlyifiisalikelihood-ratio test.\\nThisimportant conclusion aboutlikelihood-ratio testshasbeenmuch\\nemphasized, especially bytheNeyman-Pearson school.(21)\\nTheconcept oflikelihood ratio,sometimes simply calledlikelihood,\\nisnowoneofthemostpervasive concepts ofstatistical theory. It\\nseemstohavebeenintroduced in1922byR.A.Fisher (ef.indexof\\n[F3]),whoemphasized itinconnection withtheimportant method of\\nestimation namedbyhim‘‘themethod ofmaximum likelihood.” Its\\nuseintestinghypotheses wasapparently firstemphasized byJ.Ney-\\nmanandE.8.Pearson (seeVol.II,p.303of[K2]).Inconnection with\\nlikelihood ratiosasnecessary andsufficient statistics, mathematically\\nadvanced readers willbeinterested inSection 6of[L6],[B2],and\\n[M5].Oneoftheearliestcontributions inthisdirection wasmadeby\\nC.A.B.Smith[S14].\\n6Repeated observations\\nIfx(n)={x,,---,Xn},where,givenB;,thex,’sareindependent\\nidentically distributed random variables, thenv(F(x(n))) isanon-de-\\ncreasing function ofn,forthe(n+1)-tuple isanextension ofthen-\\ntuple. If(8)isstrictlyconvex—a condition thatyounowrecognize\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97c37a23-5ca6-4065-8c9e-01c0aec7e028', embedding=None, metadata={'page_label': '159', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.6] REPEATED OBSERVATIONS 141\\nasinteresting—v(F(x(n))) iseasilyseentobestrictly increasing inn,\\nunlesstheindividual x,’sareeitherutterly irrelevant ordefinitive.\\nItistobeexpected, especially inthelightoftheapproach tocertainty\\ndiscussed in§3.6,that,asnbecomes verylarge,x(n)willbecome prac-\\nticallydefinitive. Indeed, §3.6makes itpossible tostateandprovea\\nformaltheorem tothateffect.\\nTHEOREM 1]\\nHyp. 1.x(n)={x},---,Xn},where,givenB,,thex,’sareinde-\\npendent andidentically distributed random variables.\\n2.Thex,’sarenotutterly irrelevant toB;.\\n3.o(F|6)=k(6).\\nConcn. lim_»(F(x(n))| 8)=18)=neB(1)K(A,0)+6(2)K(0, 1)\\nuniformly inB..\\nProor. Writingxasshortforx(n),\\n(1) v(F(x) |8)=Elk(6(x))).\\nFor anarbitrary ¢>0,lettheclosedinterval Jonwhichkisdefined\\nbepartitioned intotwosubsetsJandK,whereJisthesetofthose\\n6’ssuchthat\\n(2) k(g)=(8)—«,\\nandKisthecomplement ofJrelative toI.\\nItfollowsfromthecontinuity ofthefunctions oneachsideof(2)\\nthatBeJ,ifeithercomponentof8issufficiently large.\\nThecomputation initiated in(1)cannowbecarriedforward thus:\\n(3)Elk(8(x))]=E[k(6(x)) |B(x(s))¢JIP(B(x(s)) eZ)\\n+E{k(6(x)) |B(x(s))¢KIP(6(2(s)) ¢K)\\n>ETL((x)) |B(2(s))¢JIP(B(2(s)) ¢J)\\n+mink(6’)-P((2(s)) ¢K)—«\\n=E{l(6(x))]—{E[l(8(x)) |B(x(s)) ¢K]\\n—mink(6)}P(B(2(s)) ¢K)—¢\\n=1(8)—max|k(6’)|-P@(@(s)) ¢K)—«\\nNow,inviewoftheparagraph inwhich(3.6.15) occursandthefact\\nthat,ifeithercomponent of8iscloseto1,8¢J;P(8(x(s)) ¢K)becomes\\narbitrarily smallforsufficiently largen.@\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27a1fc09-464c-4497-8245-cdf8321d7f74', embedding=None, metadata={'page_label': '160', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='142 PARTITION PROBLEMS (7.7\\n7Sequential probability ratioprocedures\\nThepresent section digresses todiscussaninteresting application of\\ntheideaspresented inthischapter towhatiscalledsequential analysis.\\nSequential analysis refersinprinciple tothetheoryofobservational pro-\\ngramsinwhichtheselection ofwhatobservations tomakein later\\nphases oftheprogram depends onwhathasbeenobserved inearlier\\nphases. Suchbehavior iscommonplace ineveryday life;forexample,\\nyoulookforsomething untilyoufindit,butnotlonger. Statistics it-\\nselfhasalwaysusedsequential procedures. Forexample, itisnotrare\\ntoconduct apreliminary experiment todetermine howamainexperi-\\nmentshouldbecarried out.Thus, ifonewererequired toestimate\\nwitharoughly preassigned precision themeanofanormaldistribution\\nofunknown meanandunknown variance, onemightreasonably begin\\nbytakingtenortwenty observations, whichwouldgivesomeideaof\\nthevariance andwouldtherefore determine abouthowmanyobserva-\\ntionsarenecessary forachieving therequisite precision.\\nCommonplace thoughproblems withsequential features are,A.Wald\\nwasthefirsttodevelop (1943)asystematic theory ofaconsiderable\\nbodyofproblems ofthissort.Forearlyhistory seetheIntroduction\\nof[W2]andtheForeword ofSection Iof[S17].\\nSomelaterideasonsequential analysis, duemainly toWaldand\\nWolfowitz, arethesubject ofthissection. Itwillnotbepractical to\\nproceed withfullrigor,primarily becauserandom variables capable of\\nassuming aninfinitenumber ofvaluesarenecessarily involved. Full\\ndetailsaregivenin[W3]andmorecompactly in[A7],butnotinWald’s\\nbookonsequential analysis [W2].\\nLetx={x(1),---,x(v),---},wherethex(v)’sareconditionally an\\ninfinitesequence ofindependent, relevant, identically distributed ran-\\ndomvariables. Rather informally, asequential observational program\\nwithrespect toxisaruletellingwhether toobserve x(1)orwhether to\\nmakenoobservation atall;iftheparticular valuex(1)isobserved,\\nwhether toobserve x(2)ortodiscontinue observation; ifthevalues\\nz(1)andx(2)areobserved whether toobserve x(8)ortodiscontinue\\nobservation, etc.\\nMoreformally, letNbeafunction oftheinfinitesequence ofvalues\\nx={x(1),---,z(v),---}suchthat,ifthesequence x’agreeswithxin\\neverycomponent from thefirstthrough theN(z)th,thenN(2’)=N(a).\\nSuchafunctionNdetermines asequential observational program,\\nwhich isacontraction ofx,callity(x;N),defined thus:\\n(1) y(x;N)=Df{x(1),a)x(N(x))}.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbe3209b-1fb9-4753-96a7-7f9ad5db55f2', embedding=None, metadata={'page_label': '161', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.7] SEQUENTIAL PROBABILITY RATIOPROCEDURES 143\\nItistobeunderstood that,ifN(x)iszeroforsome2,itisidentically\\nzero,andthaty(x;0)isanullobservation.\\nItwillbeassumed thattherandom costassociated withasequential\\nobservational program isproportional tothenumber ofrandom varia-\\nblesobserved, thatis,c=N(x)y,y>0.Nocategorical defense of\\nthisassumption issuggested, butclearlythereareinteresting problems\\ninwhich itismetatleastapproximately. Thedomain ofapplicability\\nofthetheorycanactually beconsiderably extended bymodifying the\\nassumption toincludeafixedoverhead costthatappliesexcept incase\\nNisidentically zero;thisdoesnotgreatly complicate theanalysis, as\\ntheinterested readerwillbeabletoseeforhimself. Thetheorywould\\nevenremain virtually unchanged, ifcwereonlyassumed tobeofthe\\nform\\nN(2)\\n(2) c=h+ >>civ), ifN>0,\\nv=1\\n=Q, ifN=0,\\nwhereh,c(1), c(2), ---areindependent withfiniteexpected values\\nE(h)>0,E(c(r))>0,andthec(v)’sareidentically distributed.\\nForanyFtherearesomevaluesof8forwhich itwouldbeunwiseto\\nadoptanysequential observational program otherthanthenullobser-\\nvation. Suppose, forexample, that8Bissoclosetoanextreme value\\nthat1(6)—k(8)<y;underthiscircumstance themostthatcouldbe\\ngainedbyobserving evenxitselfwouldbelessthany,butthecostof\\nmaking somuchasoneobservation isatleasty.Letthesetofvalues\\nof6forwhich itisnotJustified tomakeanybutthenullobservation be\\ndenoted forawhilebyJ(F;y),orsimplyJ,forshort.\\nNow,if8ed,theperson’s utilitycan,bythedefinition ofJ,bemaxi-\\nmizedbyrefraining fromanyobservation butthenullobservation and\\naccepting theutilityk(8);otherwise therewillbesomeadvantage to\\nhiminobserving x(1).Ifthepersondoesobserve theparticular value\\n2(1)ofx(1),hefindshimselfwithaposteriori probabilities B(7(1)) in\\nplaceoftheapriori8,hehaspaid(oratanyrateentailed) acosty,\\nandhemustnowdecidewhether tomakeanyfurther observations.\\nHisnewproblem issimplytheproblem hewouldhavefacedattheout-\\nsethadhisaprioriprobabilities been8(x(1)) instead of8,exceptthat\\nallutilities arenowreduced byy.HeJustifiably accepts theutility\\nk(B(a(1))) —vy,ifB(a(1)) eJ;otherwise hewillobserve x(2).Continu-\\ningthislineofargument stepafterstep,itfollowsthatoptimal action\\nconsists inobserving successive x(v)’suntilanaposteriori probability\\ninJoccurs,andthenadopting abasicactconsistent withtheaposteriori\\nprobability.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e889e38-8a15-47f0-b6b2-4854fdf65881', embedding=None, metadata={'page_label': '162', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='144 PARTITION PROBLEMS (7.7\\nInactualpractice, itisfarfromeasytodetermine whether aparticu-\\nlarvalueof6belongs toJ(F;y),because inprinciple thewholeenormous\\nvariety ofsequential observational programs hastobeexplored tode-\\ntermine whether anyoneofthemhasaderived valuegreaterthank(§).\\nThepractical advantage achieved inthepreceding paragraph isthat\\nofgreatly restricting theclassofprograms thatmeritconsideration.\\nThustheproblem ofdetermining whether 8¢J(F;y)doesnotrequire\\nasurvey ofallobservational programs, butonlyofthosedefined in\\ntermsofsomesetJ’according totherulethatN(z)isthefirstinteger\\nforwhichB(x(1), ---,x(n))eJ’.\\nIfprograms corresponding toallsetsJ’hadtobeexamined, the\\nprocesswould stillbemathematically impractical; indeed, inallbut\\nspecial cases,practical solutions haveyettobefound. But,ifany\\nspecial conditions thatJmustnecessarily satisfyarediscovered, only\\nsetsJ’satisfying thoseconditions needbeexamined. Someverygen-\\neralconditions arethese:/contains theextreme pointsof J;Jistopo-\\nlogically closed,thatis,ifavalue@pisnotinJ,thenthenearneighbors\\nofBoarealsonotinJ.Thefirstoftheseconditions requires nocom-\\nment,andthesecondfollows easilyfromthecontinuity asafunction of\\nBof\\n(3) E{k(B8(y(x; N)))—yN|6]—&(8).\\nTheseconditions alonedonotgofartoward narrowing topractical\\nlimitsthevariety ofsetstobeexplored. Thusfarinthedevelopment\\nofthesubject, reallypowerful conditions havebeenobtained onlyat\\ntheexpense ofconsiderable restrictions onthestructure ofFor,equiv-\\nalently, ofk.\\nSuppose, then,thatFisdominated byafinitenumberofactsor,\\nwhatamounts toalittleless,thatthegraphofkispolygonal, asitis\\nforthekgraphed inFigure 2.1.Technically, thisrestriction onkmay\\nbeexpressed bysayingthattheinterval Jistheunionofafinitenum-\\nberofintervals oflinearity ofk.Undertherestriction, relatively much\\ncanbeconcluded aboutthestructure ofJ(F;y),foritistrueingeneral,\\naswillbeshowninthenextparagraph, thattheintersection ofJwith\\nanyinterval oflinearity ofkisaclosedinterval.\\nSuppose, indeed, that8;andB2belongto/andtoacommoninterval\\noflinearity ofk,butthatBoontheinterval between 8;and8,doesnot\\nbelongtoJ.Acontradiction follows according tothefollowing com-\\nputation, inwhichhisanyactderivedfromasequential observational\\nprogram, costincluded, thatisadvantageous atBo.\\n(4) xE(h|Bj)Bo(j)>k(Bo),\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='387dabea-85f4-4e48-a923-83b4d2d5c3ce', embedding=None, metadata={'page_label': '163', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.7] SEQUENTIAL PROBABILITY RATIOPROCEDURES 145\\nforhissupposed tobeadvantageous atBo;and\\nfornoderived actissupposed tobeadvantageous atBm,sinceB»¢J.\\nSince8isaweighted average, sayL¥mBm, oftheB»’s,andsincek()is\\nlinearintheintervalbetween 6,andBo,itfollowsfrom(4)and(5)that\\n(6) dX,E(h |Bi)Bo(t)<k(Bo),\\ncontradicting (4).Thesupposition thatBp¢~Jhasthusbeenre-\\nducedtoabsurdity.\\nThedemonstration Justgivenextends directly ton-foldproblems.\\nThegeneral conclusion isthattheintersection ofJwithanydomain\\noflinearity ofkisconvex, sothat,ifkispolyhedral, Jistheunionofa\\nfinitenumber ofclosedconvex sets,eachlyingwholly inadomain of\\nlinearity ofk.Thepractical implications oftheconclusion areenor-\\nmously greater fortwofold thanforhigher-fold problems, because\\ntwofold problems leadtoone-dimensional bounded, closed,convex\\nsets,whichpresentnogreatvariety, allofthembeingclosedbounded\\nintervals. Butthreefold problems, forexample, leadtoclosedbounded\\ntwo-dimensional convex sets,arestriction thatleavesgreatroomfor\\nvariety.\\nIfkispolygonal, thevariety of setsJ’tobesurveyed isenormously\\nreduced, forJ’mustbetheunionofaknownnumberofintervals, each\\nofwhich isconfined toaknown interval. Suppose thatthisnumberis\\nm;theclassofsequential observational programs tobesurveyed can\\nbecharacterized bythetwoendpointsofeachofthemintervals, ex-\\nceptthatthepossibility thatsomeoftheintervals arevacuous mustbe\\nborneinmind.Sincetheextremes ofJareneeessarily inJ,andthere-\\nforenecessarily appear asendpointsofintervals inJ,theexploration\\nhasbeenreduced toa2(m—1)parameter familyofpossibilities.\\nThepossibility thatm=1,whichalmostmeansthatFisdominated\\nbyasingleelement ofitself, istrivial;forthenall@’sareinJ,andob-\\nservation isnevercalledfor.Thiscanbeseenmmanyways.Inpar-\\nticular, itfollows asanillustration ofthemachinery thathasjustbeen\\ndeveloped, thus:Theendpoints, orextremes, ofJarebothinJ,asal-\\nways,and,sincem=1,theyarebothinthesameinterval oflinearity\\nofJ;therefore theinterval between them,namely everyvalueof8,\\nliesinJ.\\nThepossibility thatm=2—inordinary statistical usage,these-\\nquential testing ofasimpledichotomy—is ofparticular importance.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1bb32fe2-f780-4785-9f61-e7e39e43bb2a', embedding=None, metadata={'page_label': '164', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='146 PARTITION PROBLEMS (7.7\\nItoccurstypically whenFisdominated bytwoacts,neither ofwhich\\ndominates theother,asinExercise 5.2.Oneofthetwoactsisapprop-\\nriatetoone“hypothesis” B,,andtheotherisappropriate toBy.In\\ncasem=2,itiseasilyseen,bymethods thathavenowbeenindicated\\nmorethanonce,thateachofthetwoclosedintervals thatconstitute J\\nhasasoneendpointoneoftheextremes ofJ.Neither ofthetwointer-\\nvalscanbevacuous, norcaneitherconsistonlyofasinglepoint. Itis\\nrelatively easytofind,atleastapproximately, thetwovaluesof6that\\ndetermine J(F;y),andthetheoryofthissituation hascorrespondingly\\nbeenbrought toarelatively highdegree ofperfection; fordetails, see\\n[S17],[W2],[W3],and[A7].\\nFollowing (oratleastparaphrasing) Wald[W2],asequential obser-\\nvational program characterized bymaking successive observations un-\\ntiltheaposteriori probabilities fallintosomesetJ,followed byadopt-\\ningabasicactappropriate totheaposteriori probability, iscalleda\\nsequential probability ratioprocedure. Thereason forthisnomencla-\\ntureisthattoobserve untiltheaposteriori probabilities fallintoJis\\ntoobserve untilthenumbers\\nB()P(@(1), «++,2(0)|Bd)\\nDBG)P(@(1), +++,(0)|By) (7) B(i|2(1),---,x(@))=\\nlieinacertain set,or,whatamounts tothesamething, satisfycertain\\nconditions. But,theparticular valueof8 having been assigned, this\\nistantamount torequiring theratiosofprobabilities\\nP(z(1), «++,#(N) |By)\\nP(x(1),+++,2(N) |Bs) (8)\\ntosatisfycertain conditions.\\nSince(7)and(8)arewaysofexpressing thelikelihood ratio,theob-\\nservational program together withtheactderived fromitmightalso\\nbereferred to asasequential likelihood-ratio procedure. Indeed, but\\nfortheprecedent established byWald,thatwouldseemthebetter\\nname.\\nAsanactualexample ofasequential probability ratioprocedure,\\nsuppose thatthedistribution of x(v)givenB;attaches theprobabilities\\np;andg;=1—p,;tothevalues 1and0,respectively. Theexpression\\n(8)caninanycasebewritten inthefactored form\\nN[Pet |2),9 ee\\n”) P(zx(v) |By) v=]\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5a8377a-65bd-4fdc-965a-827f0b5631a0', embedding=None, metadata={'page_label': '165', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.7) SEQUENTIAL PROBABILITY RATIOPROCEDURES 147\\nandinthepresentexample thistakesthespecialform\\n(10) ay(ay _(“yee\\nP2 q2 q2P241\\nwhere\\nN\\n(11) y(N)=dD)xv).\\nv=]\\nItisnoteworthy, inconnection withsufficient statistics, thatthecon-\\nditionthattheaposteriori probability beinJisinthiscaseexpressible,\\naccording to(10),asacondition ony(N)andN.Specializing theex-\\namplefurther, suppose thatJisofthesortappropriate totestinga\\nsimpledichotomy. Thecondition thattheaposteriori probability be\\nin~Jisthenexpressed byeachofthefollowing equivalent pairsof\\ninequalities, wherea(1)anda(2)arepositive numbers suchthata(1)\\n+a(2)<1.\\nB(1|2(1),---,2(N))<1—a(1),\\n(12)\\nB(2|x(1),«++,2(N))<1—a(2).\\nB(1)Q\\nsca+a@~~*(\\n0) (2)Ba+6a)~~%)\\nwhereQforthemoment denotes thelikelihood ratio(10).\\nB(2)(1—a(1))_\\n(1)a(1)\\npQ)a2)_o,\\nB(I)(1=@(2))\\nwhereQ*,Qxaredefinedbythecontext. Since,according to(13),the\\nstructure of~Jissuperficially determined bythreeparameters, sayby\\nB(1),e(1),and(2),itisworthyofsomenotethatthecorresponding con-\\ndition isultimately expressed intermsofonlytwospecialparameters,\\nQ*andQs;thisisonlynatural, considering that~Jisanopeninterval\\ndetermined byitstwoendpoints.Theactthatwouldbeappropriate\\ntoB,iscalledforbyvaluesofQ>Q*,andtheoneappropriate toBz\\niscalledforbyvaluesofQ<Qx. Q< Q*,\\n(14) Q>\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7faaff6d-0357-4a3c-b807-d3acf3b26d9d', embedding=None, metadata={'page_label': '166', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='148 PARTITION PROBLEMS [7.8\\nThusfar,theparticular form(10) of thelikelihood ratiohasnot\\nreallybeenexploited inthecalculation, so(14)applies to thetestingof\\nsimpledichotomies generally. Takingaccount of(10),(14)canbyele-\\nmentary manipulation beputinthefollowing form.\\ny(N)<{logQ*+ Nlog(q2/q:)}/log(p192/P291),\\ny(N)>{logQx+Nlog(q2/q1)}/log (p192/p2q1),(15)\\nwhere, fordefiniteness, itissupposed thatp,;>po.Thus,theregion\\ninthe(N,y)planedetermined by~/,theregioninwhichfurther ob-\\nservations arecalledfor,isabandbounded bytwoparallel linesof\\npositive slope.\\n8Standard form,andabsolute comparison between observations\\nIfxandyaresuchthat,foreveryFand8,v(F(x) |B)>oF) |B);\\nthenximitates, sotospeak,anextension ofy,anditmayappropriately\\nbesaidthatxisavirtualextension ofy.Correspondingly, ifxisavir-\\ntualextension ofy,andyisavirtualextension ofx,itmaybesaidthat\\nxandyarevirtually equivalent.\\nNomatterwhataprioriprobabilities apersonmayhave,orwhat\\nbasicactsare available tohim,hewillhavenopreference between a\\npairofvirtually equivalent observations, sovirtually equivalent obser-\\nvations areindeedequivalent formanypractical purposes. Wherecom-\\nbinations ofobservations areunderconsideration, however, therela-\\ntionofvirtual equivalence doesnotresemble trueequivalence. For\\nexample, ifxandyareequivalent, theneachisequivalent tothemul-\\ntipleobservation {x,y},butifxandyareonlyvirtually equivalent,\\ntheymaywellbeindependent, inwhichcaseneither willtypically be\\nequivalent to{x,y}.\\nThissection explores thenotions ofvirtual extension andvirtual\\nequivalence. Inparticular, aninteresting standard representative of\\ntheclassofobservations virtually equivalent toagivenobservation x\\nisdefinedanddiscussed. Thismaterial isscarcely referred tolaterin\\nthebook,anditmaywithoutmuchlossbeskipped orglossed over.It\\nwillbecouched frankly inthelanguage ofn-foldasopposed totwofold\\npartitions, butreaders withtherestofthechapter behindthemwill\\neasilybeabletoconcentrate onthetwofold situation, iftheyfindit\\nmoreunderstandable.\\nMostoftheideastobepresented in thissectionwereoriginated by\\nH.F.Bohnenblust, L.8.Shapley, and8S.Sherman inaprivatememo-\\nrandum datedAugust 1949,whichIwasprivileged toseeatthattime.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a617b03e-48ae-40c1-b150-77e3dd8bc200', embedding=None, metadata={'page_label': '167', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.8] STANDARD FORM 149\\nThisworkwasextended andbrought totheattention ofthepublicby\\nDavidBlackwell in[B16].\\nItisobvious that,ifyisasufficient statistic forx,thenxandyare\\nvirtually equivalent. Inparticular thelikelihood ratiorderivedfrom\\nXisvirtually equivalent tox.Moreover, thereadermayanticipate, and\\nitwillbeformally showninthecourseofthissection, thatifandonly\\nifobservations arevirtually equivalent dotheirlikelihood ratioshave\\nthesamedistribution foreveryvalueof8,or,whatcomestothesame\\nthing,giveneachB;,1=1,---,n.Thusthenconditional distribu-\\ntionsofthelikelihood ratiogiveneachB;couldbetakentocharacterize\\ntheobservations virtually equivalent toagivenone,sayx.Actually,\\naswillbeshown,theclassofobservations virtually equivalent toxcan\\nberepresented bythedistribution ofthelikelihood ratioforanysingle\\nnon-extreme valueof8.Fordefiniteness, theparticular value6*=\\n{1/n,--+,1/n}willbeused,buttheinterested readerwillfindita\\nsimpleexercise toextend alltheconsiderations basedon8*toany\\nothernon-extreme 8,aswouldbenecessary inanyextension ofthetheory\\ntoinfinite partitions.\\nLetm(r)betheprobability thatthelikelihood ratiointhestandard\\nform(5.5)attainstheparticular valuerwhen6=6*.Withself-evi-\\ndentabbreviations,\\n(1) m(r)=P(r|6*)\\n=DP(r|Bi)(1/n)\\n1=-))DdP(x|B)).\\nN3r(x)=r\\nThesecond lineof(1)exhibits m(r)expressed intermsofthendistri-\\nbutionsP(r |B;).Itisrathermoreinteresting toseethatthosendis-\\ntributions canthemselves allbeexpressed intermsofthesingledis-\\ntribution m,asfollowsfromthedefinition (5.5)ofrandthethirdline\\nof(1)thus:\\n(2) P(r|B)=xP(x |B,)\\n=2r;(2)DXP(z|B;)\\nr(z)=r j\\n=nrym(r).\\nSimilarly,\\n(3) P(r|8)=n1xra}m(r).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8763ee5f-3387-4a60-b79d-a08d841ae430', embedding=None, metadata={'page_label': '168', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='150 PARTITION PROBLEMS (7.8\\nRegarded asaprobability measure onthesetofalln-tuples ofnum-\\nbersr,mhasthefollowing threeimportant properties.\\nP(r;>0|m)=1;\\n(4) P(Dy= tlm)=1,\\nE(x;|m)=n—,\\nOfthese,thefirsttwoareobviousfromthedefinition ofr,andthethird\\nfollowsbycalculation from(2)thus:\\n(5) 1=PCr|B)=nDram) |\\n=nE(r; |m).\\nConversely, suppose thatmisanymathematical probability defined\\nonthesetofn-tuples rofnumbers, subjecttotheconditions (4),then,\\nascaneasilybeverified, »mathematical probabilities areformally\\ndefinedbytheequation P(r|B,)=nrym/(r). Mathematically, rdis-\\ntributed thuscanberegarded asanobservation. Thefollowing calcu-\\nlationdemonstrates theexpected conclusion thatthelikelihood ratio\\nofthisobservation istheobservation itselfandthatitsdistribution\\ngiven6*ism.\\nP(r |B;)_nrym(r) _\\nDXP(r |B;)onxrjm(r)7\\nP(r|B*)=DXnrgm(r)(1/n) =m(r). \\n(6)\\nItisinteresting andfruitfultocompute v(F(x) |8)intermsofm.\\n(7)o(F(x) |8)=ERG) |8)\\n=Elk({r8()/ 22148(9)}) |6)\\n=nF[k({r8(i)/2,198(9)})Lo148(A) |mi}.\\nTemporarily adopttheconvention that,ifaisanyn-tuple ofpositive\\nnumbers andhanyfunction ofr(notnecessarily convex), T(a)hisa\\nfunction ofrdefinedthus:\\n(8) T(a)h(r)=psh({rea(t)/2)r30(9)})Zrja(J).\\nThen(7)takestheabbreviated form\\n(9) E(k(6(x)) |8)=nE(T(B)k(x)|m).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='237b753f-dfc5-466a-a495-a0d3ffa02026', embedding=None, metadata={'page_label': '169', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.8] STANDARD FORM 151\\nToseetheimplications of(9),itisnecessary toknowsomething about\\nwhattheoperation 7\\'(8)doestothefunction k,inparticular toknow\\nthat7(6)kisconvex inr.Thederivation ofthesenecessary factsis\\nstraightforward andislefttothereaderasasequence ofexercises.\\nExercises\\nla.T(a)T(6)h =T({a(1)B(1), ---,a(n)8(n)})h =T(B)T(a)h.\\nlb.h=T({a(1)7}, «++,a(n)})T(a)h.\\n1\\n2.T(g*)h=—h.\\nnN\\n3.Ifh(r)>g(r)forrbetween 7’andr”;thenT(a)h(r)>T(a)g(r)\\nforrbetween r;aO/X rj/a(7)andr\\'al/X r;’a(j).\\nIfhislinear,thensois7\\'(a)h.\\n.Ifhisconvex (strictly convex), thensoisT\\'(a)h.\\nFxercise 5isobvious inthelightofExercises 3and4,butsomemay\\npreferthedemonstration suggested bythefollowing calculation, where\\n\\\\+w=153A,w=>0;andobvious abbreviations areused.\\n(10)T(a)hAr+pr’)\\nNarr r par’ r=(—“\"_--ea) a:(Ar+pr’)\\na:(Ar+ur’)ara(Ar+pr’)arr’\\nr r’<A(~«)ar+ph(——)ar’\\nar ar\\n=AT(a)h(r)+wT(a)h(r’).\\nItisamusing toestablish oncemorethatobservation generally pays,\\nthistimebymeansof(10),(4),andExercises 5and2.\\n(11) nE(T(6)k(r) |m)>nT(6)k(E(e |m))\\n=nT(B)k(6*)\\n=k(6).\\nIfxandx’areobservations andmandm’arethecorresponding dis-\\ntributions, itisnoweasytosayintermsofmandm’whenxisutterly\\nirrelevant, whenitisdefinitive, andwhenxisvirtually anextension ofx’.\\nMoreexercises\\n6.Theobservation xisutterly irrelevant ifandonlyifP(r=6*|m)\\n=1.\\n7.Theobservation xisdefinitive; ifandonlyifP(r;=1|m)=I/n,\\nor,equivalently, ifandonlyifP(r;=0|m)=(n—1)/n.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f9d3091-7980-4855-87bb-eef81335c71f', embedding=None, metadata={'page_label': '170', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='152 PARTITION PROBLEMS (7.8\\n8a.Theobservation xisavirtualextension ofx’,ifandonlyif,for\\neveryconvexfunction hdefined forr,\\n(12) E(h(r) |m)>E(h(r) |m’).\\n8b.Thetwoobservations arevirtually equivalent, ifandonlyif,for\\neveryconvexfunction’ h,\\n(13) E(h(r) |m)=E(A(z) |m’).\\nTheconclusion reached inExercise 8bcanbemuchimproved. In-\\ndeed,itwillbeshownthatthetwoobservations arevirtually equiva-\\nlent,ifandonlyifmandm’arethesameprobability measures. This\\nwillbeachieved if,forexample, itisshownthatmandm’havethe\\nsamemoments, forit1swellknownthattwodifferent countably addi-\\ntiveprobability measures confined toaboundedsetofn-tuples ofnum-\\nberscannothavethesamemoments.t Themoments inquestion are\\nexpected valuesofmonomials oftheform\\n(14) g(r)=ry87g?++ry,\\nwherethee,’sarenon-negative integers. Ingeneral, gwillnotbe\\nconvex, soitcannotbeconcluded immediately thatghasthesame\\nexpected valuewithrespecttomandm’.If,however, ahighlyconvex\\nfunction isaddedtog,thenthesumwillbeconvex anditsexpected\\nvaluewillbethesamewithrespect tomandm’.Since,byhypothesis,\\nthisisalsotrueoftheconvextermofthesum,itmustalsobetrueof\\nthenotnecessarily convex term. Specifically, let\\n(15) h(r)=g(r)+Dor’,\\nwhereAisapositivenumber tobedetermined later.Totesthforcon-\\nvexity, letsbeforthemoment anarbitrary n-tuple ofnumbers andco\\narealvariable, andcompute thesecondderivate ofh(r+os)withre-\\nspecttocato=0.\\nd?h(r+os)>079(r)\\n2do o=0 ty,jON;OT;\\n (16) $8;+X>»s;”.\\nj\\nConsidering thateachr;isbetween 0and1,theabsolute valuesofthe\\nderivatives ofgthatappearin(16)haveacommon upperbound,say\\ntSee,forexample, Corollary 1.1,p.11,of[S13].\\nUnderourusualsimplifying assumption thatxisconfined toafinitenumber of\\nvalues,miscertainly countably additive. Actually, thewholetheorycanbede-\\nveloped mutatis mutandis assuming onlythatthedistribution ofxiscountably\\nadditive onsomesuitable Borelfield.\\n+MorseandSacksteder (1966)show,ineffect,thatthetestcanbeconfined\\ntotheveryspecialconvexfunctions maxp;,r;,wherethep;arearbitrary posi-\\ntivenumbers.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f88ed394-045e-4590-83ce-9d8e1bbc3f7a', embedding=None, metadata={'page_label': '171', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.8] STANDARD FORM 153\\nu;80,if\\\\>yn”,hisconvexintheregionwhereeachr;liesbetween0\\nand1andisafortioriconvex intheintersection ofthatregionwith\\nthehyperplane 2r;=1.\\nNowthatithasbeenestablished thatmandm’represent virtually\\nequivalent observations, ifandonlyifmandm’areidentical, it isap-\\nparentthatm—or,moreexactly, thesetofconditional distributions\\nP(r|B;)=nrym(r)—is auniquestandard formforallobservations\\nvirtually equivalent tox.\\nIfxvirtually extendsy,itistobeexpected that,nomatterwhatrea-\\nsonable definition of“informative”? maybesuggested, xwillbeatleast\\nasinformative asy.Inparticular, itistobeexpected thattheinfor-\\nmation ofB;withrespecttoB;(asdefined in§3.6)willbeatleastas\\nlargeforxasfory,whichthefollowing calculation verifies, supposing\\nforsimplicity that,forbothobservations, infinite information isim-\\npossible. Thepointinquestion depends ontheconvexity ofthefunc-\\ntionhdefinedby\\n(17) h(r)=r;(logr;—logr;),\\nbecause\\n(18) l,j=E(log |iologrj|B;)\\nnE[r;(og r;—logr;)|m].\\nTherequired convexity can bedemonstrated muchasitwasin(15)t\\nforadifferent function alsomomentarily calledh:\\nq? 07h(r) d7A(r) d7A(r)\\n19)—~A(r+ as= s;°72———- s,s;——s;\\n(do (°o==Q) ar,” +Or;Or;*i+dr; ’2\\n 8,7288; 1385\"~~ 2ry rj rj\\n  \\n=rary(758:—138;)\">0.\\nItwouldbeinteresting toknowwhether everyvirtualextension is\\nrealized byanactualextension, thatis,whether whenever xisavir-\\ntualextension ofythereexistrandom variables x’andy’suchthatx\\nandx’arevirtually equivalent, yandy’arevirtually equivalent, and\\nx’extends y’.Tothebestofmyknowledge thatconclusion hasthus\\nfarbeenestablished onlyinthecaseoftwofold problems, thedemon-\\nstration forthatcasebeinggivenbyBlackwell in[B16].\\n+Actually, thiscalculation depends onlyontheconvexity of(logr;—\\nlogr;)inrj/Ty\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='192f5cd2-6e37-4834-bfdf-1ba734d8283c', embedding=None, metadata={'page_label': '172', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 8\\nStatistics Proper\\n1Introduction\\nIthinkanyprofessional statistician, whether ornothefoundhimself\\ninsympathy withthepreceding chapters, wouldfeelthat,evenallow-\\ningfortheabstractness expected inabookonfoundations, thosechap-\\ntersdonotreallydiscuss hisprofession. Hewouldnot,Ihope,findthe\\nsameshortcoming inthisandthesucceeding chapters, fortheyarecon-\\ncernedwithwhatseemstometobestatistics proper. Thepurpose of\\nthepresent shortchapter istoexplain thistransition andtoserveasa\\ngeneral introduction toitssuccessors.\\n2Whatisstatistics proper?\\nSofarasIcansee,thefeaturepeculiar tomodernstatistical activity\\nisitsefforttocombattwoinadequacies ofthetheoryofdecision, asI\\nhavethusfardiscussed it.Inthefirstplace,therearethevagueness\\ndifficulties associated withwhatin§4.2werecalled‘‘unsure probabili-\\nties.”Second, therearethespecialproblems thatarisefrommorethan\\noneperson’s participating inadecision.\\nFromthepersonalistic pointofview,statistics propercanperhaps be\\ndefined astheartofdealingwithvagueness andwithinterpersonal\\ndifference indecision situations. Whether thisverytentative defini-\\ntionisJustified, latersections andchapters willpermitthestatistical\\nreadertojudge.Atanyrate,vagueness andinterpersonal difference\\naretheconcepts that,directly orindirectly, dominate therestofthis\\nbook.\\nIwillnottrytodiscussvagueness inthischapter, butsomething\\nmayprofitably besaidhereaboutinterpersonal differences.\\n3Multipersonal problems\\nAsIhavealready frequently said,itseemstomethatmultipersonal\\nconsiderations constitute muchoftheessence ofwhatisordinarily\\ncalledstatistics, andthatitislargelythrough suchconsiderations that\\ntheachievements oftheBritish-American Schoolcanbeinterpreted in\\n154\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='522d56aa-1c3d-4c7b-bbb9-1134d47d435e', embedding=None, metadata={'page_label': '173', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.3] MULTIPERSONAL PROBLEMS 155\\ntermsofpersonal probability. Thisisaviewthatcanbestbedefended\\nbyillustration, andtherequisite illustrations willbescattered through-\\noutlaterchapters; butsomesupport islenttoitbythosecriticsof\\npersonal probability whosaythatpersonal probability isinadequate\\nbecause itappliesonlytoindividual people,whereas themethods of\\nscience are,moreorlessbydefinition, thosemethods thatareaccepta-\\nbletoallrational people.\\nThesortofmultipersonal problems Imeantocallattention toare\\nthosearisingoutofdifferences oftasteandjudgment, asopposed to\\nthose,sofamiliar ineconomics, arisingoutofconflicting interests. Asa\\nmatter offact,thelattertypeofmultipersonal situation can,ifone\\nchooses, beregarded asamongtheformer; itmay,forexample, be\\nsaidthatyouandIhavedifferent tastesfortheprocess oftakingadol-\\nlarfrommeandgiving ittoyou.\\nThoughmodern statisticians donotatalldenytheexistence ofdif-\\nferenttastesindifferent people, onlyoccasionally dotheytakethat\\ndifference explicitly intoaccount. Inparticular, thetheory ofutility\\nhasscarcely everentered explicitly intotheworksofstatisticians. Our\\nintellectual ancestors whobelieved intheprinciples ofmathematical\\nexpectation werelesstolerant thanmodern statisticians insofaras\\ntheydeniedrationality inthosewhosetastesdeparted fromthatprin-\\nciple,andsomeoftheirbigotry isoccasionally metwithtoday.\\nIndealingwithmultipersonal situations, itisclearlyvaluable to\\nrecognize thoseinwhichthepeopleinvolved mayallreasonably be\\nexpected tohavethesametastes,thatis,utilities, withrespect tothe\\nalternatives involved inthesituation. Explicit attempts todiscover\\ngeneral circumstances underwhichpeople’s tasteswillbeidentical are\\nrare.Themostimportant andfruitful attempt ofthissortisrepre-\\nsentedbyD.Bernoulli’s ideathatutilityfunctions willtypically be\\napproximately linearwithin sufficiently confined ranges ofimcome.\\nConsciously orunconsciously, thatprinciple isrepeatedly appealed to\\nthroughout statistics; itwas,forexample, brought outin§6.5thatthe\\nveryideaofanobservation dependsforitspractical valueonBernoulli’s\\nprinciple ofapproximate linearity.\\nRelatively inexplicit exploitations ofsimilarity oftastearesometimes\\nmadeinstatistics. Theideaisoftenexpressed, forexample, thatthe\\npenalty formakinganestimate discrepant fromthenumber tobeesti-\\nmatedwill,foreveryone concerned, beproportional (withinareason-\\nablerange)tothesquareofthediscrepancy; anargumentforthisprin-\\ncipleasaruleofthumbappropriate tomanycontexts willbegivenin\\n§15.5.Again,therearesituations inwhichitisagreedthatthepen-\\naltywilldepend onlyonthediscrepancy andnotonthetruevalueof\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7939359-f553-4cfb-8d5d-60616f55f0e2', embedding=None, metadata={'page_label': '174', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='156 STATISTICS PROPER [8.4\\nthenumber tobeestimated. Ofcourse,thereareproblems inwhich\\nbothrulesareinvoked simultaneously, thepenalty beingsupposed to\\nbeproportional tothesquare ofthediscrepancy andindependent of\\nthevaluetobeestimated.\\nTurnnowtodifferences injudgment, thatis,todifferences inthe\\npersonal probability, fordifferent people, ofthesameevent.Though\\nmodern objectivistic statisticians mayrecognize theexistence ofdif-\\nferences ofjudgment, theyargueintheoretical discussions thatstatis-\\nticsmustbepursued without reference totheexistence ofthosediffer-\\nences,indeedwithout reference tojudgmentatall,inorderthatcon-\\nclusions shallhavescientific, orgeneral, validity. Toputthesame\\nideainpersonalistic terms,Iwouldsaythatstatistics islargelydevoted\\ntoexploiting similarities inthejudgments ofcertain classes ofpeople\\nandinseeking devices, notably relevant observation, thattendtomin-\\nimizetheirdifferences.\\nThetendency ofobservation tobringaboutagreement hasbeenil-\\nlustrated in§3.6.Someoftheothergeneralcircumstances inwhich\\ndifferent peoplemaybeexpected toagree,oratleastnearlyagree,in\\nsomeoftheirjudgments havealsobeenmentioned. Forexample, it\\nmaywellhappen thatdifferent peoplearefacedwithpartition prob-\\nlemsthatarethesameinthatthesamevariable istobeobserved by\\neachperson,butdifferinthateachpersonhashisownaprioriproba-\\nbilities8andhisownsetofavailable actsF.If,however, thecondi-\\ntionaldistribution ofxgivenB;isthesameforeachperson, thenthe\\npeople will,forexample, agreeastowhether acontraction yofxis\\nsufficient, which isoftenofgreatpractical value. Again,therearecir-\\ncumstances underwhicheachofthesesamepeoplewillagreethatcer-\\ntainderived actsarenearlyoptimal.\\n4Theminimax theory\\nInrecentyearstherehasbeendeveloped atheoryofdecision, here\\nwithdueprecedent tobecalledtheminimax theory,thatembraces so\\nmuchofcurrent statistical theory thattheremaining chapters can\\nlargelybebuiltaround it.Theminimax theorywasoriginated and\\nmuchdeveloped byA.Wald,whoseworkonitisalmostcompletely\\nsummarized in hisbook[W3].Wald’sminimax theory, ofcourse,de-\\nrivesfrom,andreflectsthebodyofstatistical theorythathadbeen\\ndeveloped byothers,particularly theideasassociated withthenamesof\\nJ.Neyman andE.8.Pearson. Itseemslikely that,inthedevelopment\\noftheminimax theory,WaldowedmuchtovonNeumann’s treatment\\nofwhatvonNeumanncallszero-sum two-person games,whichthough\\nconceptually remotefromstatistics, ismathematically allbutidentical\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06c8c695-b525-41a0-80e5-a7a36163d275', embedding=None, metadata={'page_label': '175', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4] THEMINIMAX THEORY 157\\nvithstudyoftheminimaxrule,thecharacteristic feature ofthemini-\\nnaxtheory.\\nWaldinhispublications, andeveninconversation, heldhimself\\nilooffromextramathematical questions ofthefoundations ofstatistics;\\nindtherefore manyoftheopinions expressed inlaterchapters onsuch\\nyoIntsinconnection withtheminimax theorywereneithersupported\\n10ropposed byhim.Itmayfairly besaid,however, thathewasan\\nobjectivist andthathisworkwasstrongly motivated byobjectivistic\\nideas.\\nMypolicy hereofholding difficulties ofmathematical technique toa\\nminimum bymakingstringent simplifying assumptions willbeadhered\\ntoinconnection withtheminimax theory.AlargepartofWald’sbook\\n[W3]isconcerned withovercoming thedifficulties intechnique thatare\\nhereavoided bysimplifying assumptions, butthatmustbefacedin\\nmanypractical problems. Despite Wald’s ableeffort,important prob-\\nlemsofanalytic technique stillremain inconnection withtheminimax\\ntheory. Itshouldalsobeappreciated thattheindividual mathematical\\nproblems raisedbyapplications oftheminimax theoryareoftenvery\\nawkward, evenwhenstringent simplifying assumptions arecomplied\\nwith;consequently muchworkonspecific applications ofthetheory is\\nstillinprogress.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6044553c-4294-4ef6-a21b-9ad26fcd2964', embedding=None, metadata={'page_label': '176', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 9\\nIntroduction to\\ntheMinimax Theory\\n1Introduction\\nThischapter explains whattheminimax theory is,almostwithout\\nreference tothetheoryofpersonal probability. Thiscourseseemsbest,\\nbecause thetheorywasoriginated fromanobjectivistic pointofview\\nandasthesolution ofanobjectivistic problem. Moreover, aphilo-\\nsophically moreneutralpresentation seemstoresult, iftheideasofper-\\nsonalprobability areherekeptoutoftheforeground.\\nTheminimax theorybeginswithsomeoftheideaswithwhichthe\\ntheoryofpersonal probability, asdeveloped inthisbook,alsobegins.\\nInparticular, thenotions ofperson,world,statesoftheworld,events,\\nconsequences, acts,anddecisions presented in§§2.2-5applyaswell\\ntotheminimax theory—from whichtheywereinfactderived—as to\\nthetheoryofpersonal probability.\\nThepointatwhichthetwotheories departfromeachotheris§2.6,\\nwhichpostulates thattheperson’s preferences establish asimpleorder\\namong allacts.Thatassumption isnecessarily rejected byobjectivists,\\nforit,together withthesure-thing principle (whichtheypresumably\\naccept), impliestheexistence ofpersonal probability. Forobjectivists,\\nofcourse,conditional probability doesnotapplytoallordered pairsof\\nevents. Morespecifically, itseemstobeatacitassumption ofobjecti-\\nvisticstatistics thattheworldenvisaged inanyoneproblem isparti-\\ntionedintoeventswithrespect toeachofwhichtheconditional proba-\\nbilitiesofallevents(ignoring themathematical technicality ofmeasura-\\nbilityconsiderations) aredefined, butthatconditional probability with\\nrespect tosetsotherthanunions ofelements ofthepartition arenot\\ndefined. That,incidentally, iswhypartition problems dominate objec-\\ntivistic statistics. Thepartition inquestion isingeneral infinite, but,\\nformathematical simplicity, itwillherebeassumedtobeafinitepar-\\ntitionB;.\\nTheobjectivistic position isnotinprinciple opposed totheconcept\\nofutility. Inparticular, theminimax theory ispredicated ontheidea\\n158\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3c53eb8f-6759-497f-ab07-f05d133a137f', embedding=None, metadata={'page_label': '177', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2] THEBEHAVIORALISTIC OUTLOOK 159\\nthattheconsequences ofthoseactswithwhichitdealsaremeasured\\nnumerically byaquantity theexpected valueofwhichtheperson\\nwishestohaveaslargeaspossible, whenever (fromtheobjectivistic\\npointofview)theconcept ofexpected valueapplies. Itwilltherefore\\nbedoingtheminimax theory littleornoinjustice topostulate here,as\\nelsewhere, thattheconsequences ofactsaremeasured inutility.\\nThesepreliminaries disposed of,thegeneral objectivistic decision\\nproblem istodecideonanactfinsomegivenF,bycriteriadepending\\nonlyontheconditional expectations E(f|B;),andtherefore without\\nreference tothe‘‘meaningless’”’ P(B,).\\nTakinganypersonalistic ornecessary pointofviewliterally, it is\\nnonsensical toposeanobjectivistic decision problem, thatis,toask\\nwhichfofFisbestfortheperson,without reference totheP(B;).On\\ntheotherhand,many,ifnotall,holdersofobjectivistic views,likeWald,\\nfindthemselves logically compelled bytwowidelyheldtenetstocon-\\nsidersuchproblems meaningful. First,forreasons Ihavealluded toin\\nChapter 2andwillsoonexpandupon,manytheoretical statisticians\\ntodayagree,atleasttacitly,thattheobject,oratanyrateoneobject,\\nofstatistics istorecommend wiseactioninthefaceofuncertainty—a\\npointofviewthatWaldwasparticularly activeinbringing tothefore.\\nSecond,statisticians oftheBritish-American School, ofwhichWaldis\\ntobeconsidered amember,areobjectivists andaretherefore committed\\ntotheviewthattheprobabilities P(B;)aremeaningless, or,atany\\nrate,thattheycannotbelegitimately usedinsolutions ofstatistical\\nproblems.\\nSofarasIknow,Waldistheonlyonewhohasproposed anysolution\\ntothegeneral objectivistic decision problem, barring minorvariations.\\nHisproposal, whichisherecalledtheminimax theory, israthercompli-\\ncatedtostate.Inviewofitscomplexity andtheimportance ofthis\\ntheoryfortherestofthisbook,andforstatistical theorygenerally, I\\nhopethereaderwillhaveparticular patience withthepresent chapter.\\n2Thebehavioralistic outlook\\nPriortoWald’sformulation ofwhatisherecalledtheobjectivistic\\ndecision problem, theproblems ofstatistics werealmostalwaysthought\\nofasproblems ofdeciding whattosayratherthanwhattodo,though\\ntherehadalreadybeensomeinterest inreplacing theverbalistic bythe\\nbehavioralistic outlook. Thefirstemphasis ofthebehavioralistic out-\\nlookinstatistics wasapparently madebyJ.Neyman in1938in[N3],\\nwherehecoinedtheterm“inductive behavior” inopposition to“‘in-\\nductive inference.”’ Intheverbalistic outlook, which stilldominates\\nmosteveryday statistical thought, thebasicactsaresupposed tobe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7a918950-81fd-46d0-accc-3c9c87ca415f', embedding=None, metadata={'page_label': '178', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='160 INTRODUCTION TOTHEMINIMAX THEORY [9.2\\nassertions; andschemes basedonobservation aresoughtthatseldom\\nleadtofalse,oratanyrategrosslyinaccurate, assertions.\\nTheverbalistic outlook instatistics seemstohaveitsorigininthe\\nverbalistic outlook inprobability criticized in§2.1,whichinturnis\\ntraceable to theancienttradition inepistomology thatdeductive andin-\\nductive inference arecloselyanalogous processes.\\nI,andIbelieveotherssympathetic withWald’swork,wouldanalyze\\ntheverbalistic outlook instatistics thus:Whatever anassertion may\\nbe,itisanact;anddeciding whattoassertisaninstance ofdeciding\\nhowtoact.Therefore decision problems formulated intermsofacts\\narenolessgeneralthanthoseformulated intermsofassertions.\\nIf,ontheotherhand,asufficiently broadinterpretation isputonthe\\nnotionofassertion, perhaps everydecision toadoptanactcanbere-\\ngarded asanassertion totheeffectthatthatactisthebestavailable,\\ninwhichcasethedifference between theverbalistic andthebehavioral-\\nisticoutlooks isonlyterminological; butIdothinkthat,evenunder\\nsuchaninterpretation, thebehavioralistic outlook withitstendency\\ntoemphasize consequences offersthebetterterminology.\\nFallacious attempts toanalyzeawaythedifference between thever-\\nbalisticandbehavioralistic viewpoints arealsosometimes putforward,\\nespecially ininformal discussion. Forexample, itissometimes said\\nthatoneshouldactasthough hisbestestimate ofaquantity werein\\nfactthequantity itself.Butonthatbasisfewofuswouldbuylife\\ninsurance fornextyear,forwedonottypically estimate theyearof\\nourdeathtobesoclose.Otherexamples arediscussed byCarnap in\\nSection 50of[Cl].\\nIfassertions are,indeed, tobeinterpreted asaspecialclassofacts\\nofparticular importance tostatistics, Ihavenoclearideawhatthat\\nclassmaybe;butitwouldpresumably exclude certain acts,suchasthe\\ndesignofanexperiment, thatsurelyareofimportance tostatistics.\\nActually theverbalistic outlook hasledtomuchconfusion inthefoun-\\ndations ofstatistics, because thenotion ofassertion hasbeenusedin\\nseveral different, butalways ill-defined, senses,andbecause emphasis\\nonassertion distracts fromtheindispensable concept ofconsequences.\\nIconclude thatthebehavioralistic outlook isclearer, fuller,andbetter\\nunifiedthantheverbalistic; andthatsuchvalueasanyverbalistic con-\\nceptmayhaveitowestothepossibility ofoneormorebehavioralistic\\ninterpretations.\\nThisanalysis isreallytoobriefandmustbesupplemented bycertain\\nremarks. Tobeginwith,thereadermaywonderwhether theverbalistic\\noutlookhasadherents whodefend itagainstthebehavioralistic, andif\\nsowhattheirarguments maybe.Actually, thestatistical publicseems\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d43596cd-2262-400c-898a-af530935ea3c', embedding=None, metadata={'page_label': '179', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"9.2] THEBEHAVIORALISTIC OUTLOOK 161\\ntogreetthebehavioralistic outlook asarelatively newidea—howold\\nitmayactually beisbesidethepointhere—which assuchmustbere-\\ngardedwithsomeskepticism. Tothebestofmyknowledge, however,\\nonlyoneobjection against thebehavioralistic outlook hasbeenpre-\\nsented. Itmustbediscussed next.\\nIthasbeenseenasanobjection tothebehavioralistic outlook that\\ntheconsequences ofsomeassertions, particularly thoseofpurescience,\\nareextremely subtleanddifficult toappraise. Asafunction ofthetrue\\nbutunknown velocity oflight,what,forexample, willbetheconse-\\nquences ofasserting thatthevelocity oflightisbetween 2.99x10?°\\nand3.01X10'°centimeters persecond? But,ifsomeactsdohave\\nsubtleconsequences, thatdifficulty cannotproperly bemetbydenying\\nthattheyareactsorbyignoring theirconsequences. Certain practical\\nsolutions ofthedifficulty areknown. Forexample, considerations of\\nsymmetry orcontinuity may,asisillustrated inChapters 14and15,\\nmakeawisedecision possible eveninsomecaseswheretheexplicit\\nconsequences oftheavailable actsarebeyondhumanreckoning. Again,\\nanalysis sketched inthenexttwoparagraphs tendstoshowthatasser-\\ntionswithextremely subtleconsequences playasmaller roleinscience\\nandotheraffairsthanmightatfirstbethought.\\nNoworkerwouldactually publish—indeed nojournalwouldaccept\\n—asresearch thehypothetical assertion aboutthe.velocity oflightmen-\\ntionedintheparagraph above.Theconsequences mightbesubtle, if\\nhedid;buttheywouldnotbeveryimportant, fornoonewouldtake\\nhimseriously. Anactualworkerwoulddoasmuchaswaspractical\\ntosaywhatobservations relevant tothevelocity oflighthe,andper-\\nhapsothers,hadperformed andwhathadbeenobserved. Tobesure,\\nhisstatement ofthe observations wouldtypically bemuchcondensed;\\nhewouldresorttosufficient statistics orotherdevices toputhisreader\\nrapidly inposition toactasthough thereaderhimselfhadmadethe\\nobservations. Assertions aboutthevelocity oflight,andcountless\\nothersofthatsort,areofcoursepublished intextbooks andhandbooks.\\nTheseassertions doindeedhavecomplicated consequences, sojudgment\\niscalledforinthecompilation ofsuchbooks;buttheseriousness ofthe\\nconsequences oftheirassertions islimitedbecause ofthepossibility of\\nreferring tooriginal research publications, apossibility serious text-\\nbooksandhandbooks facilitate bytheinclusion ofbibliographies.\\nOntheotherhand,itisobvious thatmanyproblems described ac-\\ncordingtotheverbalistic outlook ascallingfordecisions between asser-\\ntionsreallycallonlyfordecisions betweenmuchmoredown-to-earth\\nacts,suchaswhethertoissuesingle-ordouble-edged razorstoanarmy,\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1cc252e6-6549-4898-afab-f59089604ed5', embedding=None, metadata={'page_label': '180', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='162 INTRODUCTION TOTHEMINIMAX THEORY [9.3\\nhowmuchpostage toputonaparcel,orwhether tohaveawatchre-\\nadjusted.\\nItistimenowtoturnbacktoobjectivistic decision problems.\\n3Mixed acts\\nSpeaking withpedantic strictness, itmightbesaidthatWalddoes\\nnotpropose asolution forthegeneral objectivistic decision problem,\\nbecause, beforeundertaking asolution, heinsiststhatFbesubject to\\nacertain condition. Ontheotherhand,hearguesthatthecondition\\nistypically metinpractice; hemightfairlyhaveinsisted thatitisthe\\nveryheartofmuchactual statistical practice. Beforediscussing the\\nissueindetail,letmegiveasmallbuttypicalillustration ofit.\\nSuppose thatinarentallibrary Iamconfronted withthechoicebe-\\ntweentwodetective stories,eachofwhichlooksmorehorrifying than\\ntheother.Atfirstsightitwouldseemthatonlytwoactsareopento\\nme,namely, torentonebookortheother,butWaldpointsoutthat\\nthereareotherpossibilities, notordinarily thought ofassuch.Inpar-\\nticular, Icaneliminate oneofthebooksbyflipping acoin.Moreaccu-\\nratelyandmoregenerally, Icanletmychoicedependontheoutcome\\nofarandom variable thatisutterlyirrelevant tothefundamental par-\\ntition—in thisexample, arandom variable theoutcome ofwhichisin-\\ndependent oftherelative meritsofthetwobooks.Therandom varia-\\nblemayaswellbeconfined attheoutsettotwovaluescorresponding to\\ntherentalofoneortheotherofthebooks,andrandom variables as-\\nsigning thesameprobabilities tothebooksareequivalent forthepur-\\nposeathand.Inpractice, especially serious statistical practice, such\\nrandom variables are,takingreasonable precautions, readilyprovided\\nbycoins,cards,dice,tablesofrandomnumbers, andotherdevices.\\nIntermsofthegeneral objectivistic decision problem, Wald’s point\\ncan(except formathematical technicalities) beformulated thus:Iff,\\nrepresents afinitenumber ofelements of F,and¢(r)isacorresponding\\nsetofnon-negative numbers suchthat2¢(r)=1,thenthepersoncan\\nmakethemixedact\\n(1) f=D7)o(rf,\\navailable tohimselfbyobserving atnoappreciable costarandom varia-\\nbletakingthevaluesrwithcorresponding probabilities ¢(7)irrespec-\\ntiveofwhichB;obtains, soFmaybeassumed toinclude f.Techni-\\ncally,thesumin(1)should, forfullgenerality, bereplaced byaninte-\\ngralwithrespect toaprobability measure. Butsuchintegrals become\\nsuperfluous underthesimplifying asssumption, whichisherewith made,\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='37760b75-86da-4431-bc5b-91f15da54820', embedding=None, metadata={'page_label': '181', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.4] INCOME ANDLOSS 163\\nthatthereareinFafinitesetofactsf,,tobecalledprimary acts,with\\nrespecttowhicheveryactinFcanberepresented intheform(1).In\\ntherental-library example, thetwoactscorresponding tothetwobooks\\ncanberegarded asprimary.\\nSincemixedactsarealsoavailable fromthepersonalistic pointof\\nview,itmaywellbeaskedwhetheritisadvantageous toconsiderthem\\ninconnection withthatpointofview,and,ifnot,howtheycanbeof\\nadvantage fromonepointofviewbutnottheother.Theanswer to\\nthefirstpartofthequestion iseasy.Indeed,iffisdefinedby(1)then\\nitispersonalistically impossible thatfshouldbedefinitely preferred to\\neveryf,,thatis,that\\n(2) E(f)=2)d(r)E(é-)>maxE(f,),\\nforaweighted meancannotbegreaterthanallitsterms. Technical\\nexplanation oftheefficacy ofmixedactsfromtheobjectivistic pointof\\nviewcanbestbepresented afterthewholestatement oftheminimax\\nrule,butthoseatallfamiliar withmodern statistical practice willde-\\nrivesomeinsightfromtheremark thattheusualpreference ofstatis-\\nticlansforrandom samples represents apreference forcertainmixed\\nacts.\\n4Incomeandloss\\nItissometimes suggestive, andinconformity withsomestatistical\\n(though notquitewitheconomic) usage,torefertoE¢|B;)asthe\\nincome offwhenB;obtains, and,correspondingly, tousethenotation\\nI(f;2).Animportant concept associated withtheincomeisthatwhich\\nIshallrefertoastheloss(symbolized byL(f;7))incurred bytheactf\\nwhenB;obtains. BythatImeanthedifference between theincome\\nthepersoncouldattainifhewereabletoactwiththecertainknowledge\\nthatB;obtained andthatwhichhewillattain ifhedecidesonfwhen\\nB;doesinfactobtain. Formally,\\n(1) L(f;1)=p¢maxI(f’;7)—If;2).’\\nIfthepersondecidesonfwhenB;obtains, L(f;7)measures intermsof\\nincometheerrorhehasmade. Ifhewerehimselfinformed ofB;after\\nfhadbeenchosen,whichisnottypically thecase,L(f;7)would,soto\\nspeak,measure hiscauseforregret.Onthataccount, somehavepro-\\nposedtocallloss“regret,’’ butthattermseemstomecharged with\\nemotion andliabletoleadtosuchmisinterpretation asthattheloss\\nnecessarily becomes knowntotheperson.Ontheotherhand,the\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='88c4326a-f74b-4c77-aa37-09049201095c', embedding=None, metadata={'page_label': '182', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='164 INTRODUCTION TOTHEMINIMAX THEORY [9.5\\nterm“loss”hasbeenusedbyWaldinthesenseofnegative income,\\nbutincontexts wherelossasdefined hereis,ofthetwosenses,theonly\\ndefensible one,aswillbeexplained in§8.Ihopethesenseproposed\\nherewillnotcauseseriousconfusion.\\nExercises\\n1.Foreach7,thereisatleastoneprimary actf,suchthat\\n(2) I(f,;2)=maxI(f;2).\\nSuchaprimary actmayfairlybecalledcorrectfor1.\\n2.L(f;2)=Zd(r)LE,; 2)>0,equality holding ifandonlyiffisa\\nmixture ofactscorrectfor7.\\n3.L(f;7)=max[(f,-;4)—I(£;72).\\n4.L(f;2)=—I(f;7),ifandonlyif\\n(3) maxI(f,;7)=0.\\nr\\n5Theminimax rule,andtheprinciple ofadmissibility\\nThemostcharacteristic feature oftheminimax theory isacertain\\nruleofbehavior, orrecommendation totheperson. Thisrule,tobe\\ncalledtheminimax rule,cannowbeformulated thus:Decideonan\\nactf’,suchthat\\n(1) maxL(f’;7)=minmaxL(f;2),\\n‘ f ¢\\nwherefandf’are,ofcourse,confined toF.\\nInwords,theminimax rulerecommends thechoiceofsuchanact\\nthatthegreatest lossthatcanpossibly accruetoitshallbeassmallas\\npossible. Anfsatisfying therecommendation oftheminimax rulewill\\nbecalledaminimax act,andthegreatest lossthatcanaccruetoamini-\\nmaxactwillbecalledtheminimax valueofthe(objectivistic) decision\\nproblem andwritten L*.Underthesimplifying assumptions thathave\\nbeenmade, it isnottechnically difficult toshowthatatleastonemini-\\nmaxactexists.Thestatement oftherulecanbereasonably extended\\ntomathematically moregeneral situations, butadigression aboutthis\\npossibility isnotappropriate here.Thenameoftheruleispresumably\\nderivedfromtheabbreviation “minmax”in(1)orfromtheLatin\\nphrase“minimum maximorum”’ thusabbreviated.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c9939d94-5dce-4f39-8c52-77ed0af66445', embedding=None, metadata={'page_label': '183', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.6] ILLUSTRATIONS OFTHEMINIMAX RULE 165\\nItmaywellhappen thatFcontains morethanoneactthatismini-\\nmaxfortheproblem, inwhichcasetheminimax rulerecommends, not\\naparticular act,butonlythatthechoicebenarrowed tothesetof\\nminimax acts.Someothercriterion mustthenbeinvoked tonarrow\\nthechoicefurther. Inparticular, itcanbeshownthatatleastoneof\\ntheminimax actsisadmissible, inthesenseof§ 6.4.AsWaldindicates,\\nitwould,therefore, beaninexcusable violation ofthesure-thing prin-\\nciplenottonarrow thechoicetoadmissible acts.Thisapplication of\\nthesure-thing principle willbecalledtheprinciple ofadmissibility.\\nTheminimax ruleandtheprinciple ofadmissibility constitute thesub-\\njectmatter of,andthereby define,theminimax theory.\\n6Illustrations oftheminimax rule\\nItwouldbehardtoimagine anobjectivistic decision problem simpler\\nthanthatofwhether tomakeaneven-money (ormoreaccurately, even-\\nutility)betinfavorofacertaineventortorefrainfrombetting. That\\nproblem, therefore, provides aconvenient firstexample oftheminimax\\nruleandtheconcepts associated withit.Supposing, asonemaywith-\\noutlossofgenerality, thatthebetisforoneutile,theobjectivistic de-\\ncisionproblem iscompletely described byTable1,whichgivesthein-\\nTaBLE 1.THEINCOMEOFANEVEN-MONEY BET,I(f,;2) \\n \\n Event\\nAct\\nBy By\\nBet, f;|1—1\\nDon’tbet,fe0 0\\n   \\ncomeofeachofthetwoprimary actsforeachofthetwoelements of\\nthepartition corresponding totheeventinquestion anditscom-\\nplement.\\nInviewofExercises 4.2and4.3thecorresponding lossfunction is\\ndescribed byTable2.Therefore,\\n(1) maxL(f;7)=max2¢(r)L(f,; 7)\\n=max¢(i)>3,\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='31d863f5-f6e7-4739-a59e-c48c30d5b55f', embedding=None, metadata={'page_label': '184', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='166 INTRODUCTION TOTHEMINIMAX THEORY [9.6\\nequality obtaining ifandonlyif¢(1)=¢(2)=4.Therefore, L*=3,\\nandtheonlyminimax actisf=$f,+3fo.\\nTaBLE 2.THELOSSOFANEVEN-MONEY BET,L/(f,;2) \\n \\n Event\\nAct\\nBy, By\\nf;0. 1\\nfy 1 0\\n   \\nInthisproblem, therefore, theminimax rulerecommends thatthe\\npersondecide,ineffect,byflipping afaircoin.Iftheoddsinthebet\\nhadnotbeeneven,theminimax rulewouldhaverecommended the\\nuseofacoinwithacertain bias;thismoregeneral example willbe\\nworkedoutin detail in§12.4.Itisnoteworthy inconnection withthe\\npresentproblem—for ithappens inmanyothers—that, fortheminimax\\nactf,L(f;7)=L*foreveryvalue of7.\\nThefollowing moreelaborate example, illustrating themechanism of\\nobservation, isparaphrased fromaslightly incorrect example in[S2].\\nOfthreenumbered coins,twoarepenniesandoneisadime,orelseone\\nisapennyand two aredimes.Thisgivesrisetoasixfoldpartition B,,\\nbecauseanyofthethreecoinsmaybethesingularone,andintwoways.\\nTheavailable primary actsaredescribed intwostagesthus:First,the\\npersonmayselectoneofthecoinsbynumberforobservation, orhe\\nmayrefrainfromsodoing;second,hemustguessatthedenomination\\nofthesingular coin.Hisincome inutilesisdefinedbythefollowing\\nconditions:\\n1.Ifthesingular coinisapenny,hemustpayataxof10;ifitisa\\ndime,hereceives abonusof20.\\n2.Ifhechooses toobserve acoin,hemustpayaninspection feeof\\n1,regardless oftheparticular coinselected forobservation.\\n3.Ifhisguessisincorrect hepaysapenalty of8.\\nItiseasytoseethatthefirstofthethreetermsintheperson’s in-\\ncomeisirrelevant tohisloss,sincehisdecision doesnotaffectthemag-\\nnitudeofthatterm.Hislossistherefore thesumoftwoterms.The\\nfirstoftheseis1or0depending onwhether hedecidestomakeanob-\\nservation; thesecondis0or8,depending onwhetherhisguessiscorrect.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d33ca31-acdc-4e46-b538-ebd36826965d', embedding=None, metadata={'page_label': '185', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.6] ILLUSTRATIONS OFTHEMINIMAX RULE 167\\nIfthepersonchoosesnottopaytheinspection fee,itisclearfromthe\\npreceding example that,nomatterwhathedoes,hislossmaybeas\\nhighas4,andthatit 1scertaintobethatsmallifandonlyifhegoverns\\nhisguess(essentially) bytheflipofafaircoin.\\nSuppose nextthatthepersondecides tomakeanobservation. If\\nheselectsanyparticular coinforobservation, heisasbadlyoffashe\\nwasbeforetheobservation, andhehasinaddition incurred theinspec-\\ntionfee.Thus,evenifthepersonknowsthatthefirstcoinisapenny,\\nthereisnothinghecandotobesurethathistotallosswillnotbemore\\nthan5,and,asbefore,hecanguarantee thatsmallalossonlybygovern-\\ninghisguesswiththeflipofafaircoin.\\nIthinkeverypracticing statistician wouldsaythat,ifanobservation\\nistobemadeatall,oneofthethreecoinsshould beselected atrandom\\n(i.e.,theprobability 1/3shouldbeattached toobserving eachofthem)\\nandaftertheobservation thepersonshouldguessthatthesingular\\ncoinisopposite indenomination totheoneobserved. Itwillbeshown\\ninthenextparagraph thatthiscommon-sense actisminimax.\\nInthefirstplace,thelossL(fp;7)fortheactfoinquestion is,foreach\\nt,equalto1+4X8=33,which islessthan4;fortheinspection fee\\nis1andtheprobability ofmakingawrongguess,whichwouldresult\\ninthelossof8,is1/38.Toshowthatfpisminimax,itwillbeenoughto\\nshowthateveryactcanresultinalossofatleast32.Onepossibility\\nfordoingthis(whichin§12.3willbeshowntobeanatural onetotry)\\nistoshowthat,foracertain setofweights, theweighted average of\\nL(f;<)withrespectto7isatleast32forallf.Infact,it issufficient,\\ninviewofExercise 4.2,toestablish suchaninequality fortheprimary\\nacts.Inthepresentexample, ithappens thattheweightscanbecho-\\nsentobeequal.Whatistobeshown,then,isthatthefollowing in-\\nequality obtains foreveryprimary f.\\n(1) Lf)=pr§2Lf;4)=33.\\nNow,iftheprimary actfdoesnotinvolve observation, L(f)=4;be-\\ncausethreeofthesixtermstobeaveraged arethen8,andtheother\\nthreeare0.Suppose next,fordefiniteness, thatfinvolves theobser-\\nvationofthefirstcoin;therearethenthreepossibilities toconsider.\\nFirst,theguessismadewithout regardforthedenomination observed,\\ninwhichcasetheobservation is,sotospeak,thrownaway,making\\nL(f)=5.Second, thedenomination guessedmaybethesameasthe\\ndenomination observed, inwhichcasetheguesswillbewrongforfour\\nofthesixvaluesof7,making L(f)=63.Finally, thedenomination\\nguessedmaybetheopposite oftheoneobserved, inwhichcasetheguess\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4b6c955-d1fb-4be2-aca2-defcba6f433c', embedding=None, metadata={'page_label': '186', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='168 INTRODUCTION TOTHE MINIMAX THEORY [9.7\\nwillbewrongfortwoofthesixvaluesof7,makingL(f)=33.This\\nargument showsthatL*>32;and,sinceL(fp;1)=3%foreveryi,fy\\nisaminimax actandL*=32.Itwouldnotbedifficult toshowthat\\nfpistheonlyminimaxactforthisproblem.\\n7Objectivistic motivation oftheminimax rule\\nTheminimax rulerecommends anactforthepersontochoose;more\\nstrictly, 1trecommends asharpnarrowing ofhischoice. Buthowcan\\nthisparticular recommendation bemotivated? Tothebestofmy\\nknowledge noobjectivistic motivation oftheminimax rulehasever\\nbeenpublished. Inparticular, Waldinhisworksalwaysfrankly put\\ntheruleforward without anymotivation, sayingsimplythatitmight\\nappealtosome. Though myheart isnolongerintheobjectivistic point\\nofview,Iwillinthenextfewparagraphs suggest arelatively objecti-\\nvisticmotivation oftherule.\\nIevolved thisfarfromsatisfactory argument atatimewhenItook\\ntheobjectivistic viewforgranted. Now,asapersonalist, itstillseems\\ninteresting tomeinthatitshows,oratleastsuggests, howstatistical\\ndevicescombat vagueness, atopicIfindverydifficult todiscuss di-\\nrectly.Onadifferent level,theargument mayshedlightontheper-\\nsonalistic viewbysuggesting howpersonalistic ideasentered themind\\nofatleastoneobjectivist.\\nAcategorical defense oftheminimax ruleseemsdefinitely outofthe\\nquestion. Suppose, forexample, that the person isofferedaneven-\\nmoneybetforfivedollars—or, tobeultra-rigorous, forfiveutiles—\\nthatinternal combustion engines inAmerican automobiles willbeobso-\\nleteby1970.Ifthereisanyeventtowhichanobjectivist wouldrefuse\\ntoattachprobability, thatcorresponding totheobsolescence inques-\\ntionisone.Astheexample centering aroundTables6.1-2makesclear,\\ntheminimax rulerecommends thatthebetbetakenorrejected accord-\\ningasafaircoinfallsheadsortails.Yet,IthinkImaysaywithout\\npresumption thatyouwouldregardthebetagainst obsolescence asa\\nverysoundinvestment, agreeing thatprovision foradequate interest\\nandcompensation forchanges inthevalueofmoney isimplicit inmeas-\\nurement ofincome inutiles.\\nOntheotherhand,therearepractical circumstances inwhichone\\nmightwellbewillingtoaccepttherule—even onewho,likemyself,\\nholdsapersonalistic viewofprobability. Itishardtostatethecir-\\ncumstances precisely, indeedtheyseemvaguealmost ofnecessity.\\nBut,roughly, theruletendstoseemacceptable when L* isquitesmall\\ncompared withthevaluesofL(f;7)forsomeactsfthatmeritserious\\nconsideration andsomevaluesofzthatdonotincommonsenseseem\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06a7d507-ea04-42c6-ac40-6a26c196defe', embedding=None, metadata={'page_label': '187', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.8] LOSSASOPPOSED TONEGATIVE INCOME 169\\nnearlyincredible. Suppose, forexample, thatIwerefacedwithsuch\\nadecision problem, inwhichitmaybeassumedforsimplicity thatthere\\nisonlyoneminimax actf,andconsiderhowImightdefendthechoice\\nofthatacttosomeone whoproposed another tome.Hemight,for\\nexample, tellmethatheknowsfromlongexperience, orbyatipfrom\\nhisbroker,thatsomeactgispreferable tof.‘‘Well,”’ Imightsay,‘I\\nhavealltherespect intheworldforyouandyoursources ofinforma-\\ntion,butyoucanseeforyourself—for itisobjectively so—that the\\nmostIcanloseifIadoptfisL*.”’Hewillnotbeabletosaythesame\\nforg,andinmanyactualsituations thegreatest possible lossunderg\\nmaybemanytimesasgreatasL*andofsuchamagnitude astomake\\naseriousdifference tomeshould itoccur,whichmaywellendtheargu-\\nmentsofarasIamconcerned.\\nItisofinterest, however, toimagine thatmychallenger pressesme\\nmoreclosely, reminding methatIamabeliever inpersonal probability,\\nandthatinfactImyselfattachanexpected lossLtogthatisseveral\\ntimessmallerthanL*.Eventhen,depending onthecircumstances, I\\nmightanswerfranklythatinpractice thetheoryofpersonal probability\\nissupposed tobeanidealization of one’sownstandards ofbehavior;\\nthattheidealization isoftenimperfect insuchawaythatanauraof\\nvagueness isattached tomanyjudgments ofpersonal probability; that\\nindeedinthepresent situation IdonotfeelIknowmyownmindwell\\nenough toactdefinitely ontheideathattheexpected lossforgreally\\nisL;butthatIdo,ofcourse, feelperfectly confident thatfcannot re-\\nsultinalossgreaterthanL*,aprospect thatinthecaseathanddoes\\nnotdistressmemuch.\\nItseemstomethatanymotivation oftheminimax principle, ob-\\njectivistic orpersonalistic, depends ontheideathatdecision problems\\nwithrelatively smallvaluesof£*oftenoccurinpractice. Themecha-\\nnismresponsible forthis1sthepossibility ofobservation. Thecostof\\naparticular observation typically doesnotdependatallontheusesto\\nwhichitistobeput,sowhenlargeissuesareatstakeanactincorporat-\\ningarelatively cheapobservation maysometimes havearelatively\\nsmallmaximum loss.Inparticular, theincome, so tospeak,froman\\nimportant scientific observation mayaccruecopiously toallmankind\\ngeneration aftergeneration.\\n8Lossasopposed tonegative incomeintheminimax rule\\nAsavarianttotheminimaxruleasIhavestated(orperhaps Ishould\\nsayinterpreted) it,onemightconsider thepossibility oflettingthe\\nnegative ofincomeplaytheroleofthelossin(5.1).Indeed, strictly\\nspeaking, Waldhimself alwaysproposed theminimax ruleinthat\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1975de50-9f22-41bf-8ef4-b9373f79adef', embedding=None, metadata={'page_label': '188', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='170 INTRODUCTION TOTHEMINIMAX THEORY [9.8\\nform.Ibelievehenevermadewritten allusion totheruleformulated\\nintermsofloss(as“loss”isdefined here);orallyhetooktheposition\\nthatlossandtheformoftheminimaxrulebasedonitwereinventions\\nofmine,towardwhichhewastentatively sympathetic. Thereisvir-\\ntuallynomathematical difference between thetworules,anditwas\\ncharacteristic ofWald’sapproach tothefoundations ofstatistics tobe\\nreluctant tocommit himselfwithrespect toanyotherdifferences.\\nThough theminimax rulefounded onthenegative ofincomeseems\\naltogether untenable, aswillsoonbeexplained, andthoughnoonebut\\nmyselfseemstoquestion thatIoriginated thevariant ofthetheory\\nbasedonloss,littleornooriginality isattributable tomeinthisre-\\nspect.Waldmorethanforeshadowed theidea,for,thoughhebased\\nhisminimax ruleonthenegative ofincome,hemadeitclearinpublica-\\ntions,including [W3],thatheregarded astypicalproblems inwhich\\ntheincome has,forevery 7,theproperty specified inExercise 4.4.\\nTherefore, inthesituations Waldregarded astypical, thedistinction\\nbetween thetwoformsoftherulevanishes, so,untilhearing hisex-\\nplicitdisavowal, Iconsidered theideaoflossasopposed tonegative\\nincome his.\\nToseethattheminimax rulefounded onthenegative ofincomeis\\nutterlyuntenable forstatistics, consider, forexample, atwofold parti-\\ntionproblem withtwoprimary actsinwhichtheincomeisasinTable1.\\nTaste 1.I(f,;1) \\n  Event\\nAct\\nB, Bo\\nf;—l—1\\nfo—10 1\\n   \\nNow,ifthepersonwereinterested inminimizing themaximum ofthe\\nnegative income, hewouldhavenorecourse buttodecideonf;,inwhich\\ncase(butinnoother)hecouldbesurethatthenegative incomewould\\nbeatmost1,whichever B;obtained. Thismaynotinitselfseemob-\\njectionable, butsupposenowthatthepersonhasavailable freeofcost\\nanobservation, howeverrelevant toB;.Then,nomatterwhatderived\\nacthechooses, ifB,obtains, hisnegative income willbeatleast1\\nutile;and,tobesurethatitisnotmore,heagainhasnorecourse but\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e6a913e-bc2d-4aa4-9b6a-1f2614a1ca55', embedding=None, metadata={'page_label': '189', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.8] LOSSASOPPOSED TONEGATIVE INCOME 171\\ntodecideonf;.Inshort,fortheproblem athand,theperson’s behavior\\nwouldnotbeinfluenced byanyobservation, however relevant. This\\nseemstomeabsurd onthefaceofit,butperhaps theabsurdity canbe\\nbrought outbyalessabstract situation paralleling theexample just\\ngiven.Apersonhasaladder,and,justasheisabouttouseit,itoc-\\ncurstohimthatthe ladder maypossibly bedangerously defective.\\nHeenvisages twobasicprimary acts:f;,tothrowtheladderawayand\\nbuyanewone,whichwillcost1utileineitherevent;andfe,tousethe\\nladder,whichwill,iftheladder isdefective, resultinhisinjurytothe\\nextentof10utiles,andwill,iftheladder issound,accomplish hisob-\\nject,which isworth 1utile.Now,ifthepersonactsontheprinciple of\\nminimizing themaximum ofnegative income,hewillthrowtheladder\\naway,nomatterwhatteststendtoshowthatit issound.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c47c60b6-5576-465e-a26c-dc95a8c92ac9', embedding=None, metadata={'page_label': '190', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 10\\nAPersonalistic Reinterpretation\\noftheMinimax Theory\\n1Introduction\\nInthischapter areinterpretation oftheminimax theory,basedon\\nthetheoryofpersonal probability andtheideathatstatistical problems\\naretypically multipersonal, istentatively putforward. Thereinter-\\npretation isbasedonamodelorschemethatcaptures, Ibelieve,much\\noftheessence ofactualstatistical situations, but1tmaybepossible to\\neffectthatendwithotherequallysimpleandevenmorerealistic models;\\nfortheonetobepresented hereleavesmuchtobedesired. Instruc-\\nture,thischapter iskeptroughly parallelwithChapter 9,toenablethe\\nreadertoexamine ascloselyashemaywishtheparallelism between the\\nobjectivistic interpretation giventhereandthepersonalistic onegiven\\nhere.Inparticular, theliberty istakenofgivingoldsymbolsnewmean-\\ningsinordertobringouttheparallelism between thetwointerpreta-\\ntions.\\n2Amodelofgroupdecision\\nConsider agroupofpeople,indexedbynumbers 7.Thesepeopleare\\nsupposed tohavethesameutility function, atleastfortheconsequences\\ntobeconsidered inthepresent context, buttheirpersonal probabilities\\narenotnecessarily thesame.Thegroupofpeople isplacedinasitua-\\ntioninwhich itmust,actinginconcert, chooseanactffromafinite\\nsetofavailable actsF,theconsequences oftheactsbeingmeasured in\\ntermsofthecommonutility ofthemembers ofthe group.\\nThesituation justdescribed willbecalledagroupdecision problem.\\nItisepitomized byajury.Themembers ofthejury,inlegaltheory,\\naresupposed tohavecommon valuejudgments inconnection withthe\\nlegalmatters athand;fortheseareincorporated inthelawasstated\\nintheinstructions ofthecourt.Butitispartoftheveryconcept ofa\\njurythatitsmembers maybeofdifferent opinions; thattheirjudgments\\n172\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='01fa035d-0b9f-436d-a08b-7d1c55052ea3', embedding=None, metadata={'page_label': '191', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.3] THEGROUPMINIMAX THEORY 173\\nastoquestions offactmaydiffer;that,toputittechnically, theymay\\nhavedifferent systems ofpersonal probability. Stillothersituations\\nresembling thegroupdecision problem arewidespread inscienceand\\nindustry, thoughthegroupdecision problem doesbynomeansrepre-\\nsenttheonlysortofsocialinteraction tending tomakethetheoryof\\npersonal probability, confined toasingleperson, inadequate. When-\\neverahospital orafactory modifies itsprocedures, wheneveradoctrine\\nisadopted withlittlereservation byvirtually alltheworkers ina\\nscience, orwhenever apanelofexperts draftsareport,something like\\ngroupdecision istaking place.\\nSincethemembers ofthegroupinagroupdecision problem, though\\nrequired toactinconcert, typically differfromoneanother intheir\\nprobability judgments, itistoomuchtoexpectthatanyrulecanbe\\nformulated thatwillbeacceptable to,orinanysoundsenseproperfor,\\nallgroupsunderallcircumstances. Ontheotherhand,theremaybe\\noneormorerulesofthumbthatwillleadthegrouptoanacceptable\\ncompromise inmanypractical circumstances. Twosuchsuggestions,\\nthegroupminimax ruleandthegroupprinciple ofadmissibility, will\\nbemadeandexplored inthenextsection.\\n3Thegroupminimax rule,andthegroupprinciple ofadmissibility\\nInthefirstplace,thepossibility ofusingmixedactsistobepointed\\nout.If,forexample, youandI,walking together, disagree aboutwhich\\nbranchofaforkintheroadleadshome,wecan,andinfactmay,de-\\ncidewhichtotrybyflippingacoin.\\nIngeneral, mixedactsareavailable inagroupdecision problem for\\nreasons analogous tctheiravailability inobjectivistic decision prob-\\nlems,for,though themembers ofagroupmaygenerally differinthe\\nprobabilities theypersonally assigntosomeevents,thereisinpractice\\nanabundance ofeventsassociated withcoins,cards,randomnumbers,\\nandthelikethatmakeitpossible forthegrouptomixtheprimary acts\\ninanyproportion, allmembers ofthegroupbeinginagreement about\\nwhattheproportions are.Theexample oftheforkintheroadillus-\\ntrateshowtheuseofmixedactscaneffectsuchacompromise as to\\nmakedecision possible inwhatmightotherwise beanimpasse. Asin\\ntheaccount oftheobjectivistic decision problems, itwilltherefore be\\ntakenforgrantedfromnowonthatFcontains allmixtures ofitsele-\\nments,andoncemore,formathematical simplicity, itwillbeassumed\\nthatthereareafinitenumber ofprimary actsf,inF,ofwhich all\\nothers are mixtures.\\nTheithperson inthegroupattaches acertainexpected utility, or\\n(personal) income, totheactf;callit7(f;7).Inthejudgment ofthe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36f8f794-f2f9-4ced-8ebd-1f89e1585313', embedding=None, metadata={'page_label': '192', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='174 APERSONALISTIC MINIMAX THEORY [10.4\\nithperson,adoption oftheactfwouldrepresent a(personal) loss,\\n(1) L(f;7)=maxIf’;7)—I(f;2).\\n(possibly zero)ascompared withtheincome orexpected utilitythat\\ninhisopinionwouldresultfromanactheconsiders mostpromising.\\nThegroupminimax ruleisthesuggestion thatanactbeadopted\\nsuchthatthelargestlossfacedbyanymemberofthegroupwillbeas\\nsmallaspossible. Toputitformally, thesuggestion isthatanf’be\\nadopted suchthat\\n(2) maxL(f’;7)=L*=p-minmaxL(f;7).\\ni fi\\nTheparallelism between thegroupminimax ruleandtheminimaxrule\\nstatedin§9.5isgreat.Inparticular, (2)isidentical inappearance\\nwith(9.5.1). Thisisreallyonlyapun,thoughafruitful one,because\\nL,zt,andevenfhavealtogether different meanings inthetwocontexts.\\nAsindicated attheoutset, itcannotbeexpected thatthegroupmini-\\nmaxrulewill,orreasonably should,beaccepted byeverygroupfaced\\nwitheveryproblem. But,muchasinthecorresponding objectivistic\\ndecision problems, itmayhappen that,ifL*issmall,inarathervague\\nsense,thegroupwillacceptthegroupminimax rule.Indeed, ifL*is\\nsmall,thegroupminimax rulerequires nomemberofthegrouptoface\\nalargeloss,sonomemberwillfeelthatthesuggestion isaseriousmis-\\ntake.Inanyevent,nomemberofthegroupcansuggestanalternative\\nthatwillnotmakesomemember’s lossasgreatasL*,forthereisnone.\\nMoreover, inmanyproblems thegroupminimaxrulewillleadtothe\\nsamelossL*foreverymemberofthegroup(asisexplained in§12.3),\\nacircumstance which,whenitoccurs,mayaddtotheacceptability of\\nthesuggestion bymaking itseemfair.\\nOfcourse itispossible that,asintheobjectivistic interpretation,\\nmorethanoneactfulfilling theminimax principle exists. Here,apara-\\nphrase oftheprinciple ofadmissibility willfurthernarrow thechoice,\\nforif\\n(3) L(g;1)<L(t;2)\\nforevery 71,withinequality obtaining forsomei,thegroupcannotseri-\\nouslyconsiderf.\\n4Critique ofthegroupminimax rule\\nSomeofthecriticisms thathavebeen,ormaybe,raisedagainstthe\\nminimax rulecanaswellbediscussed inconnection withoneinterpre-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4c70fd13-ea4a-40bd-9666-cf7f5866f103', embedding=None, metadata={'page_label': '193', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.4] CRITIQUE OFTHEGROUPMINIMAX RULE 17%\\ntationaswiththeother,andChapter 13willbedevoted tosuchcriti-\\ncisms.Butsomethatbearspecifically onthemultipersonal interpre-\\ntationinthischapter shouldbediscussed here.\\nInthefirstplace,thegroupminimaxrule isflagrantly undemocratic.\\nInparticular, theinfluence ofanopinion, underthegroupminimaxrule,\\nisaltogether independent ofhowmanypeople inthegroupholdthat\\nopinion. Ingeneral, itisdifficult togiveaformalanalysis oftheconcept\\nofdemocratic decision, apointdiscussed atlengthbyArrow[A5],Hil-\\ndreth[H4a],andothers. Perhaps, considering thatthepeopleinthe\\ngrouparepostulated tohaveacommonutility function, asatisfactory\\nanalysis ofdemocratic decisions couldbegiveninthecaseofagroup\\ndecision problem bysomesuchprocedure asminimizing theaverage\\nwithrespecttozofL(f;2). But,inmanysituations inwhichIenvisage\\napplication ofthegroupminimaxprinciple, thegroupwillinfactbea\\nrathernebulous bodyofpeople, forexample thegroup ofallspecialists\\ninsomefield.Theprinciple wouldinsuchacasebeadministered bya\\nsinglemember ofthegroupsomewhat inthefollowing fashion. In\\nplanning aninvestigation, theresultsofwhichheintends topublish,\\nhewillendeavor totakeaccount ofallopinions, sofarashecanknow\\norguessthem,thatareconsidered atallreasonable inhisfieldof\\ninvestigation. Andwhenhepublishes hisresults hewillsay,in\\neffect,‘““Whatever reasonable opinions haveheretofore beenheldby\\nmembers ofthisspecialty, inthelightofmyinvestigation andthemin-\\nimaxrule,itisnowproperforthemembersofthespecialty, insofar\\nastheyarecalledupontoactinconcert, toagreetosuchandsuchan\\naction.” Toputitalittledifferently, insuchanapplication thegroup\\nisratherfictitious, andtheindividual investigator isadmitting asrea-\\nsonable aratherlargeclassofopinions, butexcluding manythathe\\nissurehisconfreres willagreeareutterlyabsurd. Hewill,forexample,\\nfeelquitefreetoexclude thoseopinions thatalmost alleducated people\\nregardassuperstitious.\\nThegroupminimax ruleisalsoobjectionable insomecontexts, be-\\ncause,ifoneweretotrytoapplyitinarealsituation, themembers of\\nthegroupmightwelllieabouttheirtrueprobability judgments, in\\nordertoinfluence thedecision generated bytheminimax ruleinthe\\ndirection eachconsiders correct. Thisobjection is,however, scarcely\\nserious inthefictitious sortofapplication suggested above.\\nItisappropriate, interminating thissection, todiscussacertain dis-\\ntinction, neglect ofwhichcan,aswaspointed outtomeorallybyBruno\\ndeFinetti, leadtoseriousmisunderstanding ofthegroupminimaxrule.\\nVoluminous observation typically tendstomakeanyonepersonalmost\\ncertainofthetruth,andalso,whenagroupofpeople isinvolved, it\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d59883d4-490d-450f-aea8-b39dd2beea40', embedding=None, metadata={'page_label': '194', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='176 APERSONALISTIC MINIMAX THEORY [10.4\\ntypically tends tomake L*small. These two tendencies, though re-\\nlated, areseparate phenomena, asanillustration will bring out.\\nSuppose that Peter and Paul are required tobet 1utile inconcert\\neither that the majority ofalarge electorate has voted for, orthat it\\nhas voted against, acertain issue; but that before betting they areto\\nbeallowed toexamine arandom sample of 1,001 ballots.\\nIfspecific opinions about the division ofthe electorate areassigned\\ntoPeter and Paul, the situation can beregarded asagroup decision\\nproblem. To start with an interesting extreme possibility, suppose\\nthat it1sPeter’s unequivocal opinion that 55% ofthe electorate isfor\\nand 45% isagainst the issue and Paul’s that the division is45% for\\nand 55% against; that is,Peter, forexample, issupposed toact as\\nthough heknows that the division 1s55%—-45%.\\nIf,finally, itisunderstood that the group decision problem consists\\ninthe two people, Peter and Paul, deciding, before the sample isac-\\ntually observed, how their bet istobedetermined bythecomposition\\nofthesample; then theunique minimax act istobet that the electorate\\nmajority iswhatever the sample majority happens tobe. Granting\\nthis easily established solution oftheminimax problem, itisobvious\\nthat thetwo people both face theminimax loss L*. Peter, tobespecific,\\nregards L*astheprobability that through random fluctuation thesam-\\nple will accidentally fail tocorroborate his‘‘knowledge” that thema-\\njority isforthe issue. Numerically, L* isabout 0.0008.\\nPeter and Paul, recognizing that the possibility ofobserving the\\nsample reduces theminimax loss toabout 0.0008 ascompared with the\\n0.5that itwould beifnosample were available, may well find themin-\\nimax act asatisfactory compromise; atany rate, itishard tosee in\\nthis situation how they could arrive atany other.\\nThough the incorporation ofthesample into theproblem hasgreatly\\nreduced L*, observation ofthe sample does not affect the opinion of\\neither person inthe slightest, forunequivocal opinions such asthey\\nhold arenot subject tomodification inthe light ofevidence. Atleast\\none ofthetwo people isimmovably wrong, and the observation ofno\\nsample, however large, can bring them both close tothe truth. This\\nbrings out acontrast between thereduction ofL*and theapproach to\\ncertainty ofthe truth, both ofwhich typically occur with theaccumu-\\nlation ofevidence.\\nThe same contrast isexpressed byremarking that, though thetwo\\npeople may readily adopt theminimax act, each feeling that atthe ex-\\npense ofasmall risk he isdiverting the obstinacy ofhis colleague to\\ntheir common good; after the observation ofthe sample, one orthe\\nother ofthem isbound tofeel that the prize has been lost byasad\\nand improbable accident.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c5bee0c5-d8fa-40dc-8952-1ca4bf46e34a', embedding=None, metadata={'page_label': '195', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.4] CRITIQUE OFTHE GROUP MINIMAX RULE 177\\nThe wary will ask, ‘‘Who will feel how, when the actual majority is\\nlisclosed and settlement made? What ifPeter’s unequivocal opinion\\nturns out tobefalse?”’ Such questions suggest that paradox lurks in\\nanexample inwhich different people unequivocally hold mutually in-\\n30nsistent opinions, sothere issome interest inconsidering amodifica-\\ntion ofthe example, free ofthat objectionable feature.\\nSuppose then that Peter and Paul, though strongly opinionated about,\\nthe division ofthe electorate, are not absolutely unequivocal intheir\\nypinions. Tobequite definite, suppose that Peter attaches probability\\n\\\\-10—!° tothedivision 55%-45% and probability 107!° tothe divi-\\nsion 45%-55%, and that Paul attaches the same probabilities but in\\ntheopposite order tothetwo divisions. Here, asintheexample ofthe\\nunequivocal opinions, theunique minimax actis toletthebetbechosen\\ninaccordance with thesample majority; L* isatrifle lower than before.\\nObservation ofthesample does now generally affect the opinions ofthe\\ntwo people, but, though itradically reduces the minimax loss, itdoes\\nnot typically bring the two people into close agreement. If,for ex-\\nample, the division isinfact 45%-55%, Paul’s strong apriori belief\\nthat that isthe actual division isalmost sure tobestrengthened bythe\\nsample, and Peter’s equally strong but false belief isalmost sure tobe\\nweakened. Still, the probability isonly about 1/2 that Peter will be\\nledbythe sample toattach anaposteriori probability even asgreat\\nas0.05 tothe actual division. Thus, speaking loosely but practically, the\\napproach tocertainty ofthe truth ishere not typically nearly sofar\\nadvanced byobservation asisthereduction oftheminimax loss.*\\nItmay not besuperfluous topoint out that the preceding paragraph\\nalludes not only tothe two different personal probability systems of\\nPeter and ofPaul, but also tocertain conditional probabilities that\\nyou and Ihave accepted hypothetically insetting upthe example.\\nWhichever division does actually obtain, itisrather probable that,\\nonce the sample isobserved, either Peter orPaul will wish hecould\\nbreak his contract. This seems tometo reflect aserious objection to\\nthegroup minimax principle, especially inthose applications inwhich\\nthemembers of thegroup arenot literally consulted, forpeople cannot\\nbeexpected toabide bydisappointing contracts they might have made\\nbut didn’t.\\nFor other approaches tothe group decision problem see deFinetti\\n[D6], [D7a], deFinetti (1954), Staél von Holstein (1970, p.65andff.).\\nand Winkler (1968).\\n+AsdeFinetti hasremarked, theseparation between thetwo phenomena is\\nmore clearly brought out ifPeter and Paul decide which bet tomake onthe\\nbasis ofatennis match between themselves. For, ifeach thinks himself much\\nthe superior player, L* will bedepressed, though the opinions ofPeter and\\nPaul about the election remain completely unaffected bytheoutcome ofthe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9d8a2199-735e-4e2e-875d-9143e993d76e', embedding=None, metadata={'page_label': '196', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 11\\nThe Parallelism between\\ntheMinimax Theory and\\ntheTheory ofTwo-Person Games\\n1Introduction\\nJohn von Neumann, in1928 [V3], developed atheory ofgames in\\nwhich two people play each other formoney.t This theory ismathe-\\nmatically soclosely akin tothat oftheminimax rule and hashad such\\ninfluence onitsdevelopment that itwould beartificial togive anexpo-\\nsition oftheminimax rule without saying something ofthe theory of\\nwhat von Neumann calls zero-sum two-person games, though the ac-\\ncount given here must necessarily behighly compressed. The most\\nconvenient references inEnglish tothe theory ofzero-sum two-person\\ngames, should the reader beinterested inafuller account, are [B18],\\n[M3], and Chapters ITand IIIof[V4]; though those who read German\\nmay find itbest tostart with the expository sections ofthepaper [V3]\\ninwhich von Neumann first discussed the subject.\\nThe sort ofsystematic punning bywhich the formal parallelism be-\\ntween the objectivistic and personalistic minimax theories was empha-\\nsized inChapter 10will beused once more, tobring out the formal\\nparallelism between those theories and that ofzero-sum two-person\\ngames. Logic will bestill further sacrificed toclarity and convenience\\nbycalling thetwo people who play thegame “‘you”’ and “I.”\\n2Standard games\\nAcertain sort ofgame, here called astandard game, isdefined thus:\\nYou secretly choose anumber rfrom finite setofpossibilities, and I\\nsecretly choose anumber 7, also from afinite set ofpossibilities. The\\nnumbers rand 7having been chosen, you pay me thesum ofmoney\\n(possibly negative) L(r; 7),where Lisanarbitrary function ofrand7,\\nknown toboth ofus. Itisassumed that, forthesums involved, each\\nofusfinds money proportional toutility.\\n+Inthis completely independent development hewas tosome extent anticipated\\nbyEmil Borel. Consult [F9], [F10], and [B21] fordetails and further references.\\n178\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2db87e53-a602-4bb4-9a2e-e2eacefe332e', embedding=None, metadata={'page_label': '197', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.2] STANDARD GAMES 179\\nAtfirst sight, standard games look very dull, though it is immediately\\nrecognized that some such games are played. Atiny but typical ex-\\nample isthe gameof ‘‘Button, button, who’s got the button?”’; “Stone,\\npaper, scissors” isalmost asfamiliar anexample; and others could be\\nmentioned. But, and this seems remarkable atfirst, any game, except\\npossibly those dependent onphysical skill, canbeviewed asastandard\\ngame. The great generality ofstandard games isdemonstrated inde-\\ntail inChapter IIof[V4], but informal discussion ofasingle example\\nwill render the idea intuitively clear. Suppose then that you and Iare\\ntoplay agame ofpoker (ofaspecified variety). At first sight poker\\ndoes not seem tobeastandard game, because itinvolves several ran-\\ndom events, and several decisions onthepart ofeach ofus,some tobe\\nmade inthe light ofothers. But, itcan beargued, there are only a\\nfinite number ofdifferent situations that can arise inthe course ofa\\ngame ofpoker. You could, therefore, inprinciple write into anotebook\\nexactly which choice you would makein each ofthe possible situations\\nwith which you might befaced inplaying poker with me. The number\\nofpossible ways ofcompiling such notebooks, orpolicies ofplay, is\\nfinite; so, except forlimitations oftime and patience, you will beat\\nnodisadvantage inplaying one game with me, ifyou simply chose\\nonce andforall that one ofthemany possible policies ofplay that seems\\nbest toyou. Similarly, from my point ofview, thegame consists, in\\nprinciple, inchoosing one policy ofplay. Once you have chosen one\\nofthe policies possible foryou, say the rth, and Ihave chosen one of\\nthe policies possible forme, say the 7th, theamount you will have to\\npaymeatthetermination ofthegame isarandom variable. Since it\\nisagreed that thepayments are effectively inutiles forboth ofus,your\\npayment tomeis effectively theexpected value ofthisrandom variable,\\nwhich may becalled L(r; 7)and which isinprinciple known toboth\\nofusasafunction ofrand 7.The elaborate game oftwo-person poker\\nisthus exhibited, atsome expense torealism, asa standard game.\\nRegarding the choice ofanrbyyou oran7bymeas aprimary act,\\nboth ofusare atliberty tousemixed acts. Indeed, explicit attention\\napparently was first called tothe possibility ofusing mixed acts by\\nBorel (see [B21]), injust this context.\\nLet fand grepresent mixed acts assigning probabilities ¢(r) and y(z)\\ntothevalues rand 7,respectively. The standard gameisnow replaced\\nbyasomewhat different game inwhich you choose anf;Ichoose ag;\\nand you pay metheamount L(f; g),where\\n(1) L(f;8)=pt L(r;or) v1).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60cb6611-9905-4efd-b471-6ebcb0605836', embedding=None, metadata={'page_label': '198', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='180 MINIMAX THEORY ANDTHEORY OFGAMES [11.4\\n3Minimax play\\nVonNeumann adduces anargument, thestatement ofwhichwillbe\\nbrieflypostponed, that,ifyouhaverespect formyintelligence, youwill\\nseetoitthatthemostIcanpossibly takefromyoushallbeassmall\\naspossible, thatis,youwillchooseanf’forwhich\\n(1) maxL(f’;g)=L*=p;minmaxL(f;g).\\ng fg\\nSymmetrically, according tohisargument, Ishould choose ag’such\\nthat\\n(2) minL(f;g’)=Lx=psmaxminL(f;g).\\nf gf\\nSince,making therecommended choice,youare sure thatyouwill\\nnotpaymemorethanL*,andIamcorrespondingly surethatyouwill\\nnotpaymelessthanLx;itfollowsthatL+<L*.Thisinequality\\nwould,ofcourse,haveobtained evenifmixedactswerenotpermitted.\\nItisaremarkable mathematical fact(nottobeproved inthisbook)\\nthat,permitting mixedacts,equality alwaysobtains; sothespecial\\nsymbolL+issuperfluous here.\\nTheargument fortherecommended choices restsontheequality of\\nL*andLx.YourealizethatIcantakeatleastL*fromyouandthat,\\nifyouarenotcareful, Imaytakemore.Ontheotherhand,Irealize\\nthatyoucanpreventmytakingmorethanL*fromyouandthat,if\\nIamnotcareful, Imaygetless.Thissuggests tomanythatapairof\\nintelligent players, eachrespecting theintelligence oftheother,will\\neachadoptoneoftherecommended acts.\\n4Parallelism andcontrast withtheminimax theories\\nSomeformal parallelism between theminimax theories ofdecision\\nandthetheory ofzero-sum two-person gamesisevident, buttheparal-\\nlelismismuchmorecomplete thanmayappearatfirstsight.Themix-\\nturesgarewithout counterpart inthetwominimax theories ofdeci-\\nsion,andtheappearance ofgin(3.1)attheplacewhere7appears in\\n(9.5.1)mayseemtomartheparallelism between thesetwoequations.\\nBut,letting\\n(1) Lif;1)=pD2L(r;i)o(r),\\ninthegametheory (incloseparallelism withthedecision theories),\\n(2) Lif;g)=2,Lif;i)7(2)<maxLf;é),\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ffc305cb-103d-4a29-b1ca-5e5e23d5ba12', embedding=None, metadata={'page_label': '199', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.4] GAMESANDTHEMINIMAX THEORIES 181\\nand\\n(3) maxL(f;g)=maxL(f;2).\\nTherefore (3.1)isequivalent to\\n(4) maxL(f’;2)=minmaxL(f;27)=L*.\\nThusfromthepointofviewoftheminimax theories ofdecision the\\ng’srepresent nomaterial innovation andareatworstuselessbaggage.\\nActually, though oflittleifanyrelevance intheinterpretation ofthe\\nminimax theories, theg’sconstitute ausefulmathematical device.\\nTheirusefulness hasinfactbeenillustrated inworking outthesecond\\nexample in§9.6andwillbesystematically demonstrated inthenext\\nchapter, alongwiththeusefulness oftheapparently irrelevant ‘‘maxi-\\nmin”problem posedby(8.2)andofthefactthatLx=L*.\\nSomeremarks onthepossibility ofinterpreting theg’sintheminimax\\ntheories arepostponed totheendofthissection.\\nInthegametheory,Lmaybeanyfunction whatsoever ofitsargu-\\nmentsrand7,but,inthedecision theories,Lissubjecttothecondition\\nthat,forevery 1,\\n(5) minL(r;7)=0,\\nwhereL(r;2)isofcoursetobeinterpreted asL(f,;7).Hereistheonly\\nmathematical difference between thegametheoryandthedecision\\ntheories, theformerbeingmathematically slightlymoregeneral than\\nthelatter.\\nThough themathematical differences arenegligible, theintellectual\\ndifference between thesituations leading tothegametheoryonthe\\nonehandandtothedecision theories ontheotherisgreat.Serious\\nmisunderstandings ofthe(objectivistic) minimax theoryhaveoftenre-\\nsultedfromidentifying itwiththegametheory.Among otherthings,\\nlossisthenconfounded withnegative income, andthemisconception\\nthatthe(objectivistic) minimax ruleisultrapessimistic iscreated. I\\nhaveevenhearditstatedonthisaccount thattheminimax ruleamounts\\ntotheassumption thatnature ismalevolently opposed totheinterests\\nofthedeciding person.\\nThough mathematical convenience seemstobethebasicreason for\\nintroducing theg’sintheminimaxtheories, itistempting toaskwhether\\ntheg’shavealsosomenatural interpretation inthosetheories. Atthe\\nmoment, Idonotseeaconvincing interpretation ineithertheory,but\\ncompleteness demands anaccount ofaninterpretation suggested by\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='17677ba0-7020-4c0f-aa45-9233aad4bf5a', embedding=None, metadata={'page_label': '200', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"182 MINIMAX THEORY ANDTHEORY OFGAMES [11.4\\nWaldforhisversion oftheobjectivistic theory, especially sincethis\\ninterpretation influenced someofWald’smostwidelyusedterminology.\\nTheobjectivistic problem ofdeciding on anactinignorance ofwhich\\npartition element B;obtains, theP(B;)beingregarded asmeaningless,\\nsuggests anewproblem thatmayperhaps alsobecalledobjectivistic.\\nThenewproblem arisesonpostulating thatP(B,) 1smeaningful but\\nutterlyunknown, thatis,P(B;)=y(¢),wherethey(z)’sarethecom-\\nponents ofaghereinterpreted astheaprioridistribution unknown to\\nthedeciding person.\\nSinceforWald‘“‘loss’’wassynonymous with“negative expected in-\\ncome,” he naturally calculated thelossofthenewproblem thus:\\n(6) L(f;g)—E(£|g)\\n>—E(f |B)P(B,)\\n=DLe;0,\\narriving thusattheveryfunction suggested bythegametheory. In\\nWald’sversion ofthetheory, thenewproblem therefore amounts to\\ntheformalintroduction oftheg’sinconnection withtheoldone,which\\nneatly fulfillsthereasonable expectation thatthereshouldbenoma-\\nterialdifference between regarding P(B;)asmeaningless andregarding\\nitasmeaningful bututterlyunknown.\\nThesuggested interpretation ofagasanunknown—or, tomirror\\nWaldmorefaithfully, fictitious—a prioridistribution doesnotwork,\\nhowever, ifthelossfunction ofthenewproblem isdefinedby(9.4.1),\\nforthenewfunction L(f;g)isnotthengenerally thesameasthefunc-\\ntionL(f;g)suggested bythegametheory;thus\\n(7) Lif;g)maxE(f’—f|g)\\nmax2)Ef’—£|Bi)r(@)\\nmax2,(Lif;4)—Lt’;)}v@\\nL(f;g)—minL(f';g)\\n<Lif;g),\\nequality holding foratypicalg(i.e.,agsuchthaty(z)>Oforevery 27)\\nonlyinthealtogether trivialsituation thatFisdominated byoneof\\nitselements.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6875e103-4098-492c-b3c8-61961b437a75', embedding=None, metadata={'page_label': '201', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.4] GAMES AND THE MINIMAX THEORIES 183\\nDoes thismean that, contrary toexpectation, there isamaterial dif-\\nference between thenew problem with lossLand theoldone? Thefol-\\nlowing exercises show that itdoes not.\\nExercises\\n1.max L(f; g)=max L(f; 2).\\ng $\\n2.min max L(f; g)=L*.\\nf &\\n3.max D(f; g)=L*, ifand only ifmax L(f;7) =L*.\\n& i\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='757b2e55-be3c-438b-8d38-3ff424731d4b', embedding=None, metadata={'page_label': '202', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 12\\nTheMathematics\\nofMinimax Problems\\n1Introduction\\nSincethetwodifferent minimax decision theories andthetheory of\\nzero-sum two-person gameshaveacommon mathematical core,itwill\\nbeworthwhiletodigress forachapter evenattheexpense ofsome\\nrepetition, todiscuss thiscommon coremathematically, thatis,vir-\\ntuallywithout reference toitsvarious possible interpretations. The\\ndiscussion willhavetobedrastically confined relative tothelargebody\\nofrelevant literature, butthereaderwhowishestopursuethesubject\\nmuchfurther willfind[B18],[V4],[W3],and[M3]tobekeyreferences.\\n2Abstract games\\nTobeginwithaverygeneral situation, whichwilllaterbespecialized\\ntotheoneofmaininterest, letfandgdenotegenericelements ofany\\ntwoabstract sets,andletL(f;g)bethevalueofanessentially arbitrary\\nreal-valued function. Itwill,however, beassumed forsimplicity that\\nforeveryf’andg’thequantities\\nmaxL/(f’;g),minL(f;g’)\\ng f\\n(1)*=prminmaxLe;g), Lx=p¢MaxminLf;g)\\nf6g ef\\nexist.Tosaythatamaximum, forexample, existsisnotonlytosay\\nthatthefunction inquestion isbounded fromabove,butalsothatthe\\nmaximum valueisactually attained foratleastonevalueoftheargu-\\nment.Forwantofamoreneutral term,callthefunction L(f;g)an\\nabstract game.\\nAnf’iscalledminimax, ifandonlyif\\n(2) maxL(f’;g)=L*;\\ng\\nandag’iscalledmaximin, ifandonlyif\\n(3) minL(f;g’)=Lx.\\nf\\n184\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe20ca11-1209-4e33-a146-89fc6ad28275', embedding=None, metadata={'page_label': '203', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.2] ABSTRACT GAMES 185\\nThe existence ofminimax and maximin values ofthe variables isim-\\nplicit in(1). Itisaneasy exercise toshow that f’isminimax, ifand\\nonly if\\n(4) Lf’; g)<L*\\nforevery g.\\nThe corresponding characterization ofmaximin g’’s asthose such\\nthat\\n(5) L(f;g’)>Lx\\nforevery fcould similarly beshown. But thesymmetry ofthe situa-\\ntion issuch that itwould besuperfluous toderive this characterization\\nofamaximin explicitly. Indeed, every theorem, orgeneral conclusion,\\nabout L(f; g)obviously has adual, which arises onapplying the theo-\\nrem tothenew abstract game L(g; f)with L(g; f)=—L(f; g). This\\nistypical ofwhat isknown inmathematics asaduality principle. Hence-\\nforth the duals ofdemonstrated conclusions, even when not explicitly\\nstated, will beasfreely used asthe demonstrated conclusions them-\\nselves. Some conclusions areofcourse self dual. Incidentally, another\\nexample ofaduality principle was used in§5.4,and avery important\\nonewas pointed out inconnection with Boolean algebra in§2.4.\\nAnargument showing that Ls<L*was given inconnection with\\nthetheory ofgames. More formally, if f’and g’are, respectively, mini-\\nmax and maximin, then from (4)and (5)\\n(6) L*>L(f’;g’)>Lx.\\nItispossible, indeed typical, that Lx<L*. Suppose, forexample,\\nthat fand garevariables that take only two values and that L(f; g)\\nisdescribed byTable 1.Here, asthe reader should verify, both f’s\\nTaBLE 1. L(f; g)\\n&\\n12\\n1;0 1\\nf\\n2,1 0\\nand both g’s are minimax and maximin, respectively, and L*=1,\\nIn=0.\\nThe following theorem isfrequently applicable tothe identification\\nofminimax and maximin values offand g,and ofL*and Lx.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b34a85a4-c252-49c0-ae66-21dfb1029a2e', embedding=None, metadata={'page_label': '204', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='186 THEMATHEMATICS OFMINIMAX PROBLEMS [12.3\\nTHEOREM 1Iff’,g’,andthenumberCaresuchthatL(f’;g)<C\\n<L(f;g’)foreveryfandg;thenL*=Lx=C=L(f’;g’),f’ismini-\\nmax,andg’ismaximin.\\nProor. First,C>L*,because\\n(7) C>maxL(f’;g)>minmaxL(f;g)=L*;\\ng fg\\nand,dually,C<Lx.ButLe<L*;soC<Lx<L*<C,thatis,\\nL*=Lx=C.Now(4)and(5)apply.@\\nCoROLLARY 1Iff’andg’aresuchthatL(f’;g)<L(f;g’)forevery\\nfandg;thenf’andg’are,respectively, minimax andmaximin, andL*\\n=Ty=L(f’;g’).\\n3Bilinear games\\nIfonestumbles somehow onto apairf’,g’satisfying thehypothesis\\nofCorollary 2.1,thenhehasdiscovered aminimax, amaximin, and\\nthevalues (inthiscaseequaltoeachother)ofL*andLx.Butthat\\npossibility ofdiscovery doesnotexistunlessL*=Lx,whichatthe\\nlevelofgenerality ofthelastsection isunusual. Almost allrealinter-\\nest,however, centersonaveryspecialclassofabstract games,hereto\\nbecalledbilinear games, forwhich itisdemonstrable thatL*isin-\\nvariably equaltoLx.\\nThedefinition ofbilineargamesinvolves severalsteps.First,con-\\nsideranabstract game,L(r;27),basedonapairofvariables, rand7.\\nThetwovariables arehereassumed forsimplicity tohaveonlyafinite\\nnumber ofpossible values,anassumption thatcan,andforstatistics\\nmust,beconsiderably relaxed. Next,letfandgbenon-negative func-\\ntionsofrand7,respectively, arbitrary exceptfortheconstraint that\\n(1) Life)=De®=1,\\ninshort,probability measures onther’sand2’s,respectively. Finally,\\nthebilineargameL(f;g)isdefined thus.\\n(2) Lf;8)=peDL;Df).\\nItisimportant torecognize thattheduality principle continues to\\nhold,thatis,ifL(f;g)isabilinear game,thenL(g;f)=—L/(f;g)is\\nalsoone.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='beb352d1-4ec6-49c4-9adf-91037f1df7fb', embedding=None, metadata={'page_label': '205', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3) BILINEAR GAMES 187\\nIn terms oftheauxiliary functions\\nLf; t)=veDoL(r;f(r),\\n(3) ‘1L(r;8)=p2,L(r; t)9(0),\\nthe following equalities and inequalities can easily beverified bythe\\nreader.\\nmax L(f; g)=max L(f; 2),\\ng ;\\n(4) , ,min L(f; g)=min L(r; g).\\nf r\\n(5) minmax L(r; 7)>minmax L(f; 2)=L*>Lx\\ni f ¢r\\n=max min L(r; g)>max min L(r; 1).\\ng r 3 r\\nBut more canbesaid inconnection with (5), for ithasbeen shown by\\nvonNeumann [V3] that forthe special class offunctions now under\\ndiscussion L* isactually equal toZx. This important equality cannot\\nconveniently beproved here, but the interested reader can refer tothe\\nrelatively simple proof given byvon Neumann and Morgenstern in\\nSection 17.6 of[V4] (reading first, ifnecessary, the introduction tothe\\nmathematics ofconvex sets that constitutes Chapter 16ofthat book)\\nortothe version ofitpresented in[B18].\\nInthe light ofthe equality ofL*and Lx, (5)becomes\\n(6) minmax L(r;7) >minmax L(f; 7)=L*\\nr 1 f t\\n=max min L(r; g)>max min L(r; 2).\\ng r $ r\\nInview of(4)and (6),Theorem 2.1can bemuch improved upon for\\nbilinear games:\\nTHEOREM | For bilinear games, the following three conditions on\\nf’,g’,andCareequivalent:\\n1.ffminimax, g’maximin, andL*=C.\\n2.L(f’;g) <C<Lif; g’) forevery fand g.\\n3.Lf’;7) <C<L(r; g’) forevery 7andr.\\nProor. Condition 2implies 1,byTheorem 2.1; 1implies 3by(6);\\nand 3implies 2by (4).@\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1d5cf385-79b3-40b1-8da1-08412f6a5d8f', embedding=None, metadata={'page_label': '206', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='188 THEMATHEMATICS OFMINIMAX PROBLEMS [12.3\\nCoROLLARY 1Anecessary andsufficient condition thatfbemini-\\nmaxisthat,forsomeg,L(f;7)<L(r;g)foreveryrand7.Under\\nthatcondition L*=L(f;g),andgismaximin.\\nCorollary 1seemsanespecially appropriate expression ofTheorem 1\\ninconnection withtheminimax decision theories, wheretheg’s are, after\\nall,notreallyofinterest inthemselves. Theorem 1,andequivalently\\nCorollary 1,areofgreatpractical value.Tobesure,therearealgo-\\nrithms, orrules(givenbyShapley andSnowin[S812]),bywhichL*\\nandallminimax valuesoffcaninprinciple becomputed, buttheseal-\\ngorithms aresoawkward toapplythatinpractice onegenerally guesses\\noneormoreminimax f’s,andalsoamaximin g,onthebasisofsome\\nclues,verifying theguessandevaluating L*byCorollary 1.Tofinish\\nthejob,onethenfinds, ifonecan,anargument toshowthatthemini-\\nmaxf’sthusdiscovered areallthereare.Thisratherimperfect pro-\\ncedure isespecially important, sinceitcanrelatively easilybeextended\\ntomanysituations inwhichrand7arenotconfined tofiniteranges,as\\ndoesnotseemtobetrueofthealgorithms.\\nAswasmentioned in§10.3andastheexamples thathavebeengiven\\nillustrate, iffismmimax, thenL(f;7) isinpractice oftenactually equal\\ntoL*forall,or atleastmany,valuesof7.Insight intothatphenome-\\nnonisgivenbythefollowing theorem.\\nTHEOREM 2Ifzissuchthatthereexistsamaximin gforwhich\\ng(t)>0,thenL(f;+)=L*foreveryminimaxf.\\nProor. L(f;7)<L*,because fisminimax. Therefore L(f;g),be-\\ningaweighted average oftheL(f;7z)’s,isatmostL*;anditisactually\\nless,ifanytermwithpositive weight isnotequaltoL*.ButL(f;g)\\n>L*,because gisMaximin.@\\nItcanhappen, andinstatistical practice itoftendoeshappen, that\\nevery7satisfies thehypothesis ofTheorem 2,inwhichcaseL/(f;7)=\\nL*forevery7andeveryminimaxf.\\nTheorem 2oftenprovides abasisforguessing aminimax f,amaximin\\ng,andthevalueofL*,whichcanthenbechecked byapplication of\\nCorollary 1.Totakeasimpleexample, suppose thattherearenvalues\\nofr,andnoft.Theremaybesomereasontoconjecture thateach7\\nisusedbysomemaximing,thatis,thateach7satisfies thehypothesis\\nofTheorem 2.Iftheconjectureisinfacttrue,thenf(r)andL*satisfy\\nthesystem ofequations>I(r)+OL*=1\\n”) >, Lr;f(r)—1L*=0.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='286c7624-db78-461f-b242-7c127dbeeb45', embedding=None, metadata={'page_label': '207', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.4] ANEXAMPLE OFABILINEAR GAME 189\\nTypically, (7)asasystem ofn+1linear equations inn+1variables\\nwill have exactly one solution (f(r), £*). This solution, ifthe conjec-\\nture isvalid, will actually consist ofthecomponents ofaminimax f\\n(inthis case the only one) and the value ofL*. But the conjecture is\\nnot yet confirmed. Inparticular, ifany f(r) mnthe solution of(7) 1s\\nnegative, itiscontradicted; ifnot, the investigation can proceed. The\\ncandidates formaximin values of garenow, bythe dual ofTheorem 2,\\namong the solutions ofthe system.\\nDIg(é)+OL*=1\\n(8)>L(r;tg) —1L* 0,\\n4\\nwherer isconfined tothe values forwhich f(r)>0.Toconsider only\\nthe simplest and most typical case, suppose f(r)>0forevery r.Re-\\ngarding L*asknown, (8)consists of»+1equations fornvariables,\\nwhich atfirst sight might beexpected generaliy tohave nosolution.\\nToput the matter differently, ifone forgets forthemoment that L*\\nhas been determined by (7), itmight seem possible that (8)could lead\\ntoadifferent value, say L*’. But, using the latter part of(8)and then\\nthe first part of(7), itisseen that\\n(9) LLMOO =DIOL” =L*,\\nand dually the double sum equals L*; sodiscrepancy between L*and\\nL*’ isnotamong the real snags inthe tentative program—irrespective\\nofthenumber of r’sparticipating in(8). Finally, if(8)leads toeven\\none set ofpositive g(2)’s, itfollows from Corollary 1that the fand L*\\nderived from (7)aretheunique minimax and the true value ofL*, re-\\nspectively.\\nThe converse ofTheorem 2has been proved byBohnenblust, Karlin,\\nand Shapley in[B19], though their proof cannot be reproduced here.\\nAsis pointed outbythese authors, the converse does not extend atall\\nreadily tosituations involving infinite ranges ofrand 7.Theorem 2\\nand itsconverse can besummarized thus:\\nTHEOREM 3 There exists amaximin gforwhich g(z)>0,ifand\\nonly ifL(f; 7)=L*forevery minimax f.\\n4Anexample of abilinear game\\nItisnow convenient todiscuss acertain example, orrather aclass of\\nexamples, ofbilinear games, namely those inwhich 7takes only two\\nvalues, say 1and 2.Two preliminary remarks will help toorient the\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d7d3f805-37f1-45c1-b7a7-d7ead518bff5', embedding=None, metadata={'page_label': '208', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='190 THEMATHEMATICS OFMINIMAX PROBLEMS [12.4\\ndiscussion. First,bilmear gamesinwhich7takesonlyonevalueare\\ndevoid ofinterest, fortheminimax problem inthatcaseissimplya\\nproblem offindinganordinary minimum. Second,thediscussion ofbi-\\nlineargamesinwhich7takesonlytwovaluesincludes, ineffect,be-\\ncauseoftheduality principle, thediscussion ofthoseinwhichrtakes\\nonlytwovalues.\\nIf«takesonlythetwovalues 1and2,thevaluesg={g(1),g(2)}\\ncanberepresented graphically bypointsonaninterval, asillustrated\\natthefootofFigure 1.Forevery r,L(r;g)islinearasafunction of\\nL\\n      A— g(l) ~ g(2) =\\nFigure 1\\ng,asisL(f;g)forevery f.Itis,ofcourse, justbecause theL(f;g)ofa\\nbilinear gameislinearinthissenseanditsdualthatIusetheterm‘‘bi-\\nlinear.”’” InFigure 1thefiveslanting solidlinesrepresent thefivelinear\\nfunctions L(r;g)ofabilineargameinwhichr(forillustration) takes\\nfivevaluesand7takestwo.Thedashedlinesrepresent twovaluesoff,\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7cb2c0b6-0335-4042-88cb-aa5e0b23b8ba', embedding=None, metadata={'page_label': '209', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.4] ANEXAMPLE OFABILINEAR GAME 191\\neach ofwhich hasfor simplicity been sochosen astouse, ormix, only\\ntwo valuesof r.\\nAsmay beverified byinspection, the particular bilinear game rep-\\nresented byFigure 1has the special property that min L(r; 7)=0for\\neach 7,which isthe distinguishing property ofthose bilinear games that\\narise inconnection with theminimax decision theories described in\\nChapters 9and 10.\\nFigure 1bears amore than accidental resemblance toFigure 7.2.1.\\nInparticular, the concave function\\n(1) min L(r; g)\\nmarked byheavy line segments inFigure 1isclosely analogous tothe\\nconvex function somarked inFigure 7.2.1. The particular gempha-\\nsized byFigure 1isthat forwhich thefunction (1)attains itsmaximum\\nvalue, which according to(38.6) is L*. This gistherefore the unique\\nmaximin. Ithasbeen shown quite generally in[B19] that bilinear games\\nwith more than one minimax ormaximin are, inasense, unusual;\\nFigure 1makes itgraphically clear that the special bilinear games now\\nunder consideration dousually have aunique maximin, because there\\nismore than oneMaximin only incase (1)happens tohaveahorizontal\\nsegment.\\nWhatare theminimax f’s forthebilinear game represented byFigure\\n1?According tothedual ofTheorem 3.2, anrcannot beused inthe\\nformation ofaminimax funless L(r; g)=L* for the (in this case\\nunique) maximin g.That consideration eliminates allbut two ofthe\\nr’sfrom consideration, and itisgraphically clear that this will usually\\nbethecase forbilinear games inwhich 7takes only two values. Theo-\\nrem 3.2 itself, applied tothe particular game under discussion, shows\\nthat thegraph ofL(f; g)asafunction ofgmust behorizontal forany\\nminimax f.The two preceding conditions together eliminate all values\\noffexcept theone corresponding tothehorizontal dashed line inFig-\\nure 1;and that fisindeed minimax, because L(f; 7)=L*forboth\\nvalues of7.\\nTospecialize still further, suppose that 7aswell as7takes only two\\nvalues. Such agame can, ofcourse, berepresented graphically inthe\\nspirit ofFigure 1.Several qualitatively different situations can occur,\\nwhich might, forexample, beclassified bythe retation ofthetwo linear\\nfunctions L(r, g)toeach other. The reader should graph and consider\\nmany orallofthese possibilities for himself. The only one treated\\nhere will bethat inwhich thetwo functions cross each other atanin-\\nterior g,with onefunction sloping upand theother down. Itisgraphi-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a4611c0-a9c4-4286-b4c4-d03ba2f93790', embedding=None, metadata={'page_label': '210', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='192 THEMATHEMATICS OFMINIMAX PROBLEMS [12.4\\ncallyclearthattherewillthenbeauniqueminimax andauniquemaxi-\\nmin,aswillnowbeshownanalytically.\\nThecondition postulated canbeexpressed without lossofgenerality\\nthus:\\n2) L(1;2)>00;1),  L(2;1)>L(2;2),\\nL(2;1)>£(1;1),L(1;2)>L£(2;2).\\nOr,moremnemonically,\\n(3) L(1;2),L(2;1)>L(1;1),L(2;2).\\nItisconjectured, inthiscaseongraphical grounds, thattheprogram\\noutlined inconnection with(3.7—-8)applies,andthe reader canindeed\\nverifythatthatprogram leadstotheconclusion\\n(4) L*={L(1;2)L(2;1)—L(1;1)L(2;2)}/4,\\nwhere\\n(5) A=L(1;2)+£(2;1)—£01;1)—LQ;2);\\nandthattheuniqueminimax fandmaximin gare\\n6) io=[L(2;1)—L(2;2)]/A\\nf(2)=(LG;2)—£0;DI/A,\\nam \"=[L(1;2)—L(2;2)]/A\\ng(2)=[L(2;1)—L(1;1)]/A.\\nIfthegamearisesfromanapplication oftheminimaxdecision theory,\\n(3)almostalways applies. Moreprecisely, inthiscase,exceptpossibly\\nfortheorderofnumbering,\\n(8)L(1;1)=L(2;2)=0 and L(1;2),L(2;1)>0;\\nso,ifonlytheinequalities in(8)arebothstrict, (3)applies. Then\\n(4-7)specialize to\\n(9) L*=L(1;2)L(2;1)/4,\\nwhere\\n(10) A=L(1;2)+L(2;1);\\n(11) fl)=£2;1)/4, f(2)=LC;2)/A,\\n(12) g(1)=L(1;2)/A, g(2)=L(2;1)/A.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1c62e7be-143f-42a2-a552-7f655354e892', embedding=None, metadata={'page_label': '211', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.5] BILINEAR GAMES EXHIBITING SYMMETRY 193\\n5Bilineargamesexhibiting symmetry\\nMathematically thesolution ofabilinear gameisoftensimplified by\\nconsiderations ofsymmetry. Forstatistical applications, theimplica-\\ntionsofsymmetry forbilinear gamesareoffundamental importance\\ninsofarastheyrepresent acounterpart intheminimax theoryofthe\\ndisreputable butirrepressible principle ofinsufficient reason. Thissec-\\ntiondiscusses theseimplications inanelementary, butformal, way.\\nItcanbeskimmed overorskipped outright withoutmuchdetriment\\ntotheunderstanding oflatersections.\\nAnydiscussion ofsymmetry involves, atleastimplicitly, thebranch\\nofmathematics known asthetheory ofgroups. Though whatisto\\nbesaidhereaboutgamesexhibiting symmetry isintended tobeclear\\nwithout priorknowledge ofthetheoryofgroups, itmaybementioned\\nthatintroductions tothatsubject aretobefoundinmanyplaces, for\\nexample in[B14].\\nItcan,andinpractice oftendoes,happen thatabilineargamehas\\nsomesymmetry.{ Thismeansthattherearepermutations, heresym-\\nbolizedbyT,7”,etc.,ofthevaluesoframongthemselves andthe values\\nof¢amongthemselves suchthat\\n(1) L(Tr;Tt)=L(r;1)\\nforeveryrand7,where,ofcourse, 7’rand71arethe values intowhich\\nTcarriesrand7respectively. Permutations satisfying (1)aresaidto\\nleavethegameinvariant, ortobelongtothegroup(ofsymmetries) ofthe\\ngame.Thepermutation Uthatleaveseveryrandevery7fixedmust\\nbecountedamongthepermutations inthegroupofthegame,butthe\\ngamehasnosymmetry (worthy ofthename)unlessthereareother\\npermutations besidesUinitsgroup.\\nAnexample ofagamewithhighsymmetry isthegameimplicit in\\nthesecondexample of§9.6,for,toanypermutation whatsoever ofthe\\nsix7’sinthatgameamongthemselves, there isacorresponding permu-\\ntationofther’ssuchthatthetwopermutations takentogether leave\\nthegameinvariant. Itwas,ofcourse, theexploitation ofsymmetry\\nthatmadethetreatment ofthatexample relatively simple.\\nReturning tobilineargamesingeneral, if7and7”areinthegroup\\nofthegame,thentheproduct 77”definedbythecondition that\\n(2) (TT\\')r=ptT(T’r), (TT\")t=p:T(T\\'2)\\nisobviously alsoapermutation inthegroupofthegame.Thismulti-\\n+Thisconceptmustnotbeconfused withthatof‘symmetrical games,’’ whichare\\nsymmetrical inthesensethattheequation L(r;7)=—L(1;r) ismeaningful andtrue\\nforeveryrand.1.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9da10792-7066-4f4e-8508-b79890af562e', embedding=None, metadata={'page_label': '212', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"194 THEMATHEMATICS OFMINIMAX PROBLEMS [12.5\\nplication ofpermutations somewhat resembles theordinary multipli-\\ncationofnumbers. Inparticular, (77’)T” isevidently thesameas\\nT(T’T’’), though itisnotnecessarily truethat77’=T’T.\\nRelative tothismultiplication thepermutation Uplaystheroleof\\ntheunit,ornumber 1,inarithmetic, foritisobvious thatTU=UT\\n=Tforanypermutation T7.\\nForeverypermutation 7,thereisevidently apermutation 7’,and\\noneonly,thatundoes 7’,thatis,onesuchthatT-!7=U.Itiseasy\\ntoseealsothat77!=Uandthat,ifTisinthegroupofthegame,\\nT—'istoo.Thenotation7isofcoursemotivated bytheconsidera-\\ntionthat,relative tothemultiplication ofpermutations, 7’!playsthe\\nroleofthereciprocal of7.\\nItwillbeadopted asadefinition thatTfandTgarethefunctions\\nsuchthatTf(r)=f(T'r)andTg(t)=g(T~12)foreverypermutation\\nofTandforeveryrand7.Theintervention of7!inthisdefinition\\nmayatfirstseemarbitrary, butitismotivated bythefollowing con-\\nsiderations. First,iffis,forexample, thefunction suchthatf(ro)=1\\nandf(r)=0forr¥ro,thenTfshouldbesuchthat7f(7'ro)=1and\\nTf(r)=0forr#Tro.Second, S(Tf)shouldbe(S7')fratherthan\\n(TS)f.Thedefinition havingbeenadopted, L(Tf;Tg)canbecalcu-\\nlatedthus:\\n(3) L(T£;Tg)=Ler;f(T)g(T,)\\n=})L(Tr;Tif(TTr)g(TT1)\\nr,4\\n=DLP;THF),\\nwherethebasicfactisexploited that,if7,7runsoncethroughallpairs\\nofvalues,then7'r,T7alsodoesso.Itfollowsfrom(1)and(8)that,if\\nTisinthegroupofthegame,then\\n(4) L(T£;Tg)=Lf;g).\\nAnf(g)iscalledinvariant underthegroupofthegame,ifandonlyif\\nTf=f(Tg=g)foreveryTinthegroup. There isanaturalwayto\\nconstruct fromanyfanfinvariant underthegroup,anddually forg.\\nNamely,let\\n1f=pr—>,Tf,\\nnT\\n(5)\\n1\\n&=p:—>Tg,a\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='85a512c1-ea68-4643-b0e4-0bdde8db81b9', embedding=None, metadata={'page_label': '213', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"12.5] BILINEAR GAMES EXHIBITING SYMMETRY 195\\nwhere(hereandthroughout thissection)nisthenumber ofelements\\ninthegroupandthesummation isoverallelements ofthegroup.The\\ndefinition (5)accomplishes itsobjective, because\\n. 1\\n(6) Din=-Dvr\\nr nT -\\n1[Dingo\\nand\\n(7) T’f(r)=f(T’)\\n==Sr7nT\\n1 -=-2)T'TS(r)=fr)nT\\nforeveryrandforevery7”inthegroup.In(7)useismadeofthe\\neasilyestablished factsthatT7~!7’—'=(T’T)~' andthatasTruns\\noncethrough thegroupsodoes7”7'.Thejustification of&is,ofcourse,\\ndualtothatoff.Itisnoteworthy thatf=f,ifandonlyiffisinvariant\\nunderthegroupofthegame.\\nSuppose Ff(J)isasetofther’s(2’s).Then,bydefinition, reTR\\n(¢ eTI),ifandonlyifT—'reR(T—1i¢J);andthesetR(J)isinvariant\\nunderthegroupofthegame,ifandonlyifTR=R(TI=I)forevery\\nTinthegroup.\\nExercises\\nla.IfRisinvariant, sois~f.\\nlb.IfRandPR’areinvariant, soareRf)R’andRkUR’.\\nlc.Thevacuous setandthesetofallr’sareinvariant.\\n2.ForeveryR,letR=pUrTR,whereTisofcourseconfined to\\nthegroup;and,forevery r,definethetrajectory ofras|r],where [r]is,\\nasiscustomary, thesetwhoseonlyelementisr.\\n(a)Risthesmallest invariant setcontaining R.\\n(b)Ristheintersection ofallinvariant setscontaining R.\\n(c)R=Ufr.\\nreR\\n(d)[r]isthesmallest invariant setofwhichrisanelement.\\n3a.IfRisinvariant, andRN[r]¥0,thenR>[r].\\n3b.IfRisinvariant, andreR,then&D[r].\\n3c.If[r]N[r’]0,then[r]=[r’].\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58e94b06-0026-470c-aba2-0d4adbab6698', embedding=None, metadata={'page_label': '214', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='196 THE MATHEMATICS OFMINIMAX PROBLEMS (12.5\\n4a.The following conditions areequivalent:\\na.Risinvariant.\\n6B.R=R.\\ny.For every r¢R, [r]CR.\\n6.Rispartitioned into sets each ofwhich isatrajectory.\\n4b.The following conditions areequivalent:\\na.fisinvariant.\\n8.The set ofr’sforwhich ftakes any given value isinvariant.\\ny.fisconstant onevery trajectory.\\n5a. IfT’r=1,then (TT’T!)Tr =Tr.\\n5b. If{r}denotes thenumber ofelements ofthe group that leave r\\nfixed, then {r}={Tr}.\\n5c. If ||r||denotes thenumber of elements in [r],then n={r}|| r||.\\n5d.Both {r}and ||r||aredivisors ofn.\\n5e.The value offeverywhere onthetrajectory ofris\\n(8) v7Df).\\n|r |r&[r]\\n6.Note thedual ofeach ofthepreceding exercises.\\nInthe establishment of allthese preliminaries, the theory ofbilinear\\ngames has been almost lost sight of,but it is now possible tosaymuch\\nabout the significance ofinvariant functions and sets forbilinear games.\\nIbegin with atheorem valued forsome ofitscorollaries rather than\\nforany charm ofitsown.\\nTHEOREM 1 IfL(f’; Tg)<L(f’’; Tg) forevery T,then L(f\\'; g)<\\nL(t”; &). Ifinaddition L(’; g)<L(t’; g),then L(@’; g)<L(t’; 8).\\nProor.\\n(9) L(T~\"f\\'; g)=L(f’; Tg)<Lf”; Tg).\\nTherefore\\n(10) L@38) =—DLT)\\n1<->) Lit”; Tg)=L(t”; 2).nT\\nIfLf’; g)<L(f’”’; g),then (9) isstrict for7=U,and therefore (10)\\nisalso strict. @\\nCoroLuaryY 1 IfL(f’; Tg)=L(f\"’; Tg) forevery 7’,then L(f’; g)=\\nL(f\"; ).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b25d1dc-655c-4a1b-93a4-b06284dcfb02', embedding=None, metadata={'page_label': '215', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.5] BILINEAR GAMES EXHIBITING SYMMETRY 197\\nCorottary 2 IfL(f’; g)=L(f’”’; g)forevery g,then L(f’; g)=\\nL(f\"’; &)forevery g.\\nCorotLtary 3 ~=—L:(f; g)=L/(f; &)=L/(f; &)forevery fand g.\\nCOROLLARY 4 Iffisinvariant under thegroup ofthegame, L(f; g)\\n=L(f; &)forevery g.\\nParaphrasing some ofthenomenclature of§ 6.4, ifL(f’; g)<L(f”’; g)\\nforevery g,say that f’dominates f’’; iff’dominates f’’,but f’’does not\\ndominate f’,say that f’strictly dominates f\\'’; iff’dominates f’’,and f’’\\ndominates f’, say that f’and f’’areequivalent; iff’isnot strictly domi-\\nnated byany f,say that f’isadmissible.\\nCoROLLARY 5 Iff’dominates, strictly dominates, orisequivalent\\ntof’’,then f’dominates, strictly dominates, orisequivalent tof’’,re-\\nspectively.\\nCorotuary 6 IfLif; Tg)<L(f; Tg) forevery T,then L/(f; g)=\\nLif; g).\\nCoroLttaRy 7 IfL(f; 1)<L(f; 7)forevery 7¢I,where Iisinvari-\\nantunder thegroup ofthegame, then L(f; 7)=L(f; 7)for? ¢J.\\nCoroLLaRy 8 Itisimpossible that fstrictly dominates f.\\nTHEOREM 2 max L(f;g) <max L(f;g), equality holding, ifand only\\n& &\\niftheright-hand maximum is attained foraginvariant under thegroup\\nofthegame.\\nPRoor.\\n(11) max L(f; g)=max L(f; &)\\ng g\\n<max L(f; g).\\n&\\nThe inequality in(11) follows from the fact that every &isag;equality\\nholds, ifand only ifthe final maximum isattained forsome ,thatis,\\nforsome invariant g.@\\nCoroLLaRy 9 Iffisminimax, so isf.\\nCoroLuary 10 There exists aminimax finvariant under thegroup\\nofthegame.\\nIfagame has more than one minimax f, itistempting tosuppose\\nthat instatistical, ifnot inall, applications ofthe theory aninvariant,\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='29f8afc7-43a4-4627-94b7-5bb6a6ded8ec', embedding=None, metadata={'page_label': '216', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='198 THE MATHEMATICS OFMINIMAX PROBLEMS [12.5\\norsymmetrical, minimax fwould recommend itself atleast ashighly\\nasany other minimax f.This supposition, being vague, cannot be\\nreally proved, but certain facts tend tosupport it. Inparticular, the\\nfollowing theorem isareassuring improvement ofCorollary 10.\\nTHEOREM 3 There isatleast one admissible, invariant, minimax f.\\nProor. Itisadirect consequence ofatheorem (Theorem 2.22, p.54,\\nof[W3]) ofWald’s, too technical forstatement orproof here, that at\\nleast one invariant minimax is strictly dominated bynoinvariant f’.\\nIfthat fwere strictly dominated byany f”(invariant ornot), itwould\\nalso, according toCorollary 5,bedominated byf’’,which isimpossible.\\nTherefore fisadmissible. @\\nIfthe bilinear game has high symmetry or,more explicitly, ifthe\\nnumber oftrajectories into which the r’sorthe 2’s, orboth, are parti-\\ntioned issmall; the search for invariant minimax f’sand invariant\\nmaximin g’s isrelatively simple. An invariant minimax ischaracter-\\nized asaninvariant f’such that\\n(12) max L(f’; g)=minmax L(f; g)=L*\\ng f g\\nBut, since atleast one invariant minimax exists, the criterion (12) is\\nnotchanged iftheminimization onitsright side isconfined toinvari-\\nant f’s; with fsoconfined, the criterion remains unchanged, ifboth\\nmaximizations are confined toinvariant g’s (as Corollary 3shows).\\nThus the search for invariant minimax f’sand invariant maximin g’s\\namounts tothe solution ofanabstract game that arises from the origi-\\nnal bilinear game byruling out certain values offand g,namely the\\nun-invariant ones.\\nThis new and smaller abstract game can beexhibited asabilinear\\ngame thus: Let itbeunderstood forthemoment that r’ranges over\\nsuch aset ofthe r’sthat there isexactly one r’inevery trajectory [r];\\ndually for 2’.For invariant fand g,\\n(13) Lit}g) =DULG; M9)\\n=LLL LLG Oa@\\nrat rée(r] ie[i]\\n=2LXfra) bedLLr; 4)\\nre([r} ie{i}\\n=DLLME Ow,\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ed96f6b-e864-4d16-99f1-e70b54ba2d2a', embedding=None, metadata={'page_label': '217', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"12.5] BILINEAR GAMES EXHIBITING SYMMETRY 199\\nwhere\\n(14) L'(r'; 0’)=e DaLYLr;4),\\n[|rTWhPr ll44AG\\nand\\n(15) f(r’) =ne|| 7F793 9'@) =del| 7[lo@.\\nFinally, it is easily verified that, except for the conditions f'(r’) >0,\\ng'(a)>0,and Zf’(r’) =Zg’(e’) =1,the coefficients f’(r’) and g’(7’) are\\narbitrary. The new gameis therefore toallintents and purposes abi-\\nlinear game with only asmany r’’sand 7’’s asthere are r-trajectories.\\nand i-trajectories, respectively, inthe original game. The new game,\\nincidentally, may well have symmetry of itsown.\\nIfthere isonly one r-orone 7-trajectory, thenew gameissosimple it\\nscarcely deserves tobecalled agame. This occurs, forexample, inthe\\nsecond example of§9.6, where there isonly one 7-trajectory. Inthat\\nsituation there isonly one invariant g,and itisequal atevery 2tothe\\nreciprocal ofthe total number of7’s(which ishere the valueof ||7||\\nforevery 7). That gmust therefore beanadmissible maximin. The\\nvalue ofL* istherefore given by\\n(16) L*=min >>L(r; 2).\\nr 4 t\\nThe invariant minimax f’sare those and only those invariant f’ssuch\\nthat f(r)=0forevery rthat fails tominimize thesum in(16). More-\\nover, here the minimax f’s (invariant ornot) are allequivalent, ascan\\nbeargued thus: Any invariant minimax fissuch that\\n(17) Lf; g)=Lif; g)=\\nforevery g. Ifany minimax fwhatsoever failed tosatisfy (17), it\\nwould strictly dominate f;but according toCorollary 8that isimpos-\\nsible. Therefore inthe very special situation athand allminimax f’s\\nsatisfy (17) and areaccordingly equivalent.\\nItis,ofcourse, important toextend consideration ofsymmetry to\\nbilinear games with infinite sets ofr’sand 2’s,and infinite groups of\\nsymmetries, but the task has not yetproved straightforward. Two key\\nreferences bearing on itare [L4] and [B17].\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbfcb893-c40a-4ed1-aeb8-a4b2f53984fe', embedding=None, metadata={'page_label': '218', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 13\\nObjections to\\ntheMinimax Rules\\n1Introduction\\nIhavealready expressed andsupported myopinion thatneither the\\nobjectivistic northepersonalistic minimax rulecanbecategorically de-\\nfended (§9.7and§10.3).Ontheotherhand,certain objections have\\nbeenleveled against theobjectivistic rule(thatbeingthewell-known\\none)thatseemtometocallforreinterpretation, ifnotoutright refu-\\ntation.\\n2Aconfusion between lossandnegative income\\nSomeobjections validagainst theminimax rulebasedonnegative\\nincome areirrelevant tothatbasedonloss.Thenotions thatthemini-\\nmaxruleisultrapessimistic andthatitcanleadtotheignoring ofeven\\nextensive evidence havealreadybeendiscussed asexamples ofsuchob-\\njections.\\nAnother example Iwouldputinthesamecategory hasbeensuggested\\nbyHodgesandLehmann [H5].Inthisexample apersonwhobasob-\\nservednindependent tossesofacoinforwhichtheprobability ofheads\\nhasanunknown valuepisrequired topredict theoutcome ofthe\\n(n+1)thtoss.HodgesandLehmann hereinterpret prediction inthe\\nfollowing somewhat sophisticated, butreasonable, sense.Theperson\\nis,inthelightofhisobservation, required tochooseanumber pbe-\\ntween0and1andtopayafineof(1—p)”orp”according asthe\\n(n+1)thtossisinfactheadsortails.Thusthe(expected) mcome\\nattached totheprimary actpandeventpis\\n(1) I(p;p)=—p(1—p)?—(1—p)p’\\n=—(p—p)?—p(l—p).\\nAsHodges andLehmann show,theonlyderived act(mixed orpure)\\nthatyieldstheminimax ofthenegative incomeistosetp=3irrespec-\\ntiveoftheobservation. Butitis,incommonsense, absurdthustoig-\\n200\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='498aa159-ed30-40ac-8722-0cae14b3ff0c', embedding=None, metadata={'page_label': '219', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3] UTILITY ANDTHEMINIMAX RULE 201\\nnoretheobservation ofthefirstntosses. Inviewofthisabsurdity,\\nalmosteveryone wouldagreethatapplying theminimax ruledirectly\\ntothenegative of(1)isafoolishactforthepersontoemploy.\\nTheabsurdity ofminimizing themaximum ofnegative income in\\nthisexample isofcoursenovalidargument againstminimizing the\\nmaximum loss.Itiseasytoseethatthelosscorresponding to(1)is\\n(2) L(p;p)=(p—p)?.\\nAsHodges andLehmann happen toshowinthesamepaper[H5]\\n(though inadiffcrent context), andaswillbediscussed insomedetail\\nin§4,theuniqueminimax derived actdoesusetheobservations to\\nadvantage, resulting inalossof\\n(3)1\\n4(1+n”)?\\nirrespective ofp.Theabsurdactofsettingp=4irrespective ofthe\\nobservation resultsintheloss(p—4)*,whichinanyordinary context\\nwouldbeinferior to(8),especially forlargen.\\nIncidentally; theminimax derived from(2),though notnearlyso\\nbadassetting pidentically equalto4,isitselfopentoaserious objec-\\ntion,whichwillbeexplained in§4.\\n3Utilityandtheminimax rule\\nSomeobjections totheobjectivistic, andmutatis mutandis tothe\\ngroup,minimax ruleareineffectobjections totheconcept ofutility,\\nwhichunderlies theminimax rules.Criticisms oftheconcept ofutility\\nhavealreadybeendiscussed inChapter 5,particularly in§5.6,but\\ncertainaspects ofthediscussion needtobecontinued here.\\nItisoftensaid,andIthinkwithjustice, that,evengranting the\\nvalidity oftheutilityconcept inprinciple, apersoncanseldomwrite\\ndownhisincome function [(r;7)withmuchaccuracy. Thisideais\\nputforward sometimes withoneinterpretation andsometimes with\\nanother. Ofthese,onlythefirstisstrictlyanobjection totheutility\\nconcept.\\nThatoneisadilemma raisedbythephenomenon ofvagueness.\\nVagueness maysobluraperson’s utilityjudgments thathecannot ac-\\ncurately writedownhisincome function. Isuppose thatnoonewill\\nseriously denythis;Iwouldbeparticularly embarrassed todoso,for\\nitisalmostarecapitulation oftheveryargument thatleadsme,though\\ninprinciple apersonalist, toseesomesenseintheobjectivistic decision\\nproblem. Ontheotherhorn,ifallmeaningisdeniedtoutility(orsome\\nextension ofthatnotion) nounification ofstatistics seemspossible.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44aa9815-75f8-4ba6-9cbe-ea9dbafeb337', embedding=None, metadata={'page_label': '220', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='202 OBJECTIONS TOTHEMINIMAX RULES [13.3\\nThreespecialcircumstances areknowntomeunderwhichescapefrom\\nthedilemma ispossible. First,thereareproblems inwhichsome\\nstraightforward commodity, suchasmoney,lives,manhours,hospital\\nbeddays,orsubmarines sighted, isobviously sonearlyproportional to\\nutilityas tobesubstitutable forit.Second, thereareproblems in\\nwhichexactorapproximate minimax decisions canbecalculated on\\nthebasisofonlyrelatively little,andeasilyavailable, information about\\ntheincome function, suchassymmetry, monotoneity, orsmoothness.\\nThepossibility of cheapextensive observation, which(whenitoccurs)\\nmakestheminimax principle attractive, alsotendstomakemanyde-\\ncisionproblemsfallintobothofthetwotypesinwhichthedifficulty\\nofvaguenessisalleviated. Forexample, inamonetary decision prob-\\nlemwithcheapobservation available, itoftenhappens thattheweak\\nlawoflargenumbers, andthelike,canbeinvokedtojustifyregarding\\ncashincome asproportional toutilityincome.\\nThird,therearemanyimportant problems, notnecessarily lacking\\ninrichness ofstructure, inwhichthereareexactlytwoconsequences,\\ntypified byoverallsuccess orfailureinaventure. Insuchaproblem,\\nasIhaveheardJ.vonNeumann stress, theutilitycan,without loss\\nofgenerality, besetequalto0onthelessdesiredandequalto1onthe\\nmoredesired ofthetwoconsequences.\\nThesecond senseinwhich itmay,though notquiteproperly, be\\nsaidtobeimpossible towritedowntheincomefunction istypifiedby\\nthisexample. Amanufacturer ofsmallshort-lived objects, saypaper\\nnapkins, isfacedwiththeproblem ofdeciding onaprogram ofsam-\\nplingtocontrolthequality ofhisproduct. Hecomplains that,though\\nforthisproblem hisutility isadequately measured bymoney,hecan-\\nnotwritedownhisincomefunction becausehedoesnotknowhowthe\\npublicwillreacttovarious levelsofquality—that, inparticular, the\\nminimax ruledoesnottellhimatallhowmuchheoughttospendon\\nthesampling program, though itmaysayhowanygivenamount can\\nbestbeemployed. Themanufacturer hasarealdifficulty, thoughhe\\nexpresses itinaccurately. Heforgetsthatthelackofknowledge that\\ngivesrisetothedecision problem involves notonlythestateofhis\\nproduct, butalsothestateofthepublic;takingthestateofthepublic\\nintoaccount, thereisnorealdifficulty inwritingdowntheincomefunc-\\ntion.But,ifitisnotpractical forthemanufacturer tomakeobserva-\\ntionsbearing onthestateofthepublicaswellasthosebearingonthe\\nstateoftheproduct, theminimaxruleisnotapractical solution tohis\\nproblem; for,rigorously applied, itwouldremovehimfromthepaper-\\nnapkin business. Ibelievethatinpractice thepersonalistic method\\noftenis,andmustbe,usedtodealwiththeunknown stateofthepub-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2876f531-780e-4e0c-a721-1dc052dcd153', embedding=None, metadata={'page_label': '221', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.4] ALMOST SUB-MINIMAX ACTS 203\\nlic,while objectivistic methods, particularly theminimax principle, are\\nnow increasingly often used todeal with the state oftheproduct—a\\nsort ofdualism having some parallel inalmost allserious applications\\nofstatistics. This isnot todeny that relatively objectivistic methods\\nofmarket research cansometimes beused, nor that there arepersonal-\\nistic elements aside from those concerning the state ofthe public in\\nmuch ofeven themost advanced quality control practice.\\n4Almost sub-minimax acts\\nAnother sort ofobjection tothe objectivistic minimax rule isillus-\\ntrated bythe following example attributed toHerman Rubin and pub-\\nlished byHodges and Lehmann [H5]. An integer-valued random\\nvariable xsubject tothe binomial distribution\\nn\\n(1) P(z| p)=@ p*(1—p)\"*\\nisobserved bya person who knows nbut not p.His decision problem\\nistodecide onafunction pofxsubject tothe loss function:\\n(2) L(p; p)=E((p—p)?p)\\n=) (p(x) —p)?(\") p*(1—p)”*.\\nInother terms, hemust estimate ponthe basis ofanobservation ofx\\nand subject toaloss equal tothe square ofhis error. The traditional\\nestimate ofpisdefined byfo(x) =x/n. This estimate has manyvir-\\ntues; itisthe maximum-likelihood estimate, the only unbiased esti-\\nmate, and (as isshown in[G1]) theonly minimax estimate forasome-\\nwhat different problem from that posed by(2). But for (2)theunique\\nminimax is(as isshown in[H5]) defined by\\n1 A\\n, , 3—Po(x))\\n(3) Pi(z) =Po(z) +“V4 08\\nAsitisstraightforward toverify forevery p,\\np(l—p)\\n(4) L(Bo; p)=i\\nand\\n5 L(p1; =—————;:\\nwhich constant is,therefore, L*. The ratio ofthefirst ofthese functions\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0079a0f5-f518-4eb9-b6be-a07098da0354', embedding=None, metadata={'page_label': '222', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='204 OBJECTIONS TOTHEMINIMAX RULES [13.4\\ntothesecondis ,\\n1\\n(6) 4p(1—p)(1+=),n\\nthemaximum ofwhichoccursatp=1/2andis\\n12\\n(7) (1+).\\nThus,forlargen,themaximum lossoffpislargerthanL*byonlya\\nslightfraction. Moreover, thelossofpoislessthanL*exceptwhenp\\nliesintheinterval where\\n(8) 4p(1—p)>(1+27%)~’,\\nthat1s,where\\n(9) lp-—$1S3{1-—+04)7}4~(dn)\\nTotakea numerical example, consider n=10°(whichthepractical\\nwillnoteisratherbigforasample). Theadvantage ofp;overpoat\\np=1/2isthenonly0.64%, and,oncepdepartsbyasmuchas 0.04\\nfrom1/2ineitherdirection, theadvantage iswithpo.Itamounts,\\nforexample, to3.5%,15.5%,©%infavoroffo,whenis0.6,0.8,\\n1.0,respectively.\\nManyagreethatinsuchanexample goodjudgment will,underordi-\\nnarycircumstances, preferfo totherecommendation oftheminimax\\nrule,p;.Tomymind,thisexample constitutes avalidobjection against\\ntheminimax rule,inthesensethatitdemonstrates oncemorethat,\\nwhatever valuethatrulemayhave,itisatbestaruleofthumb.\\nTheexample isagoodillustration oftheroleofpersonal probability\\ninordinary statistical thinking, forthesourceofthedissatisfaction a\\npersonwouldordinarily feelforp;asopposed topostemsfromthefact\\nthathewouldnotordinarily attachenoughpersonal probability tothe\\nimmediate neighborhood ofp=1/2tojustifypreference forp,.It\\nfollowsfromthenumbers givenabove,forexample, that,iftheperson\\nattaches aprobability oflessthan0.84totheinterval [0.4,0.6],hewill\\npreferfptop;;thesameconclusion canbederivedfromthesupposition\\nthatthestandard deviation ofthepersonal distribution ofpisatleast\\n0.04.Ofcourse, situations canbeimagined inwhichthepersonal prob-\\nabilitieswouldbesoconcentrated about1/2astojustifypreference for\\npi;thepointoftheexample isonlythattherearesituations inwhich\\nthatwouldclearlynotbethecase.\\nInteresting material andimportant references bearing onthephe-\\nnomenonillustrated bythedecisionproblem underdiscussion aregiven\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bc69d60e-249b-4a66-9283-160b25376820', embedding=None, metadata={'page_label': '223', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.5] NO SIMPLE ORDERING 205\\nbyWolfowitz in[W17]. Itseems tobesuggested there that the diffi-\\nculty can bemet bypostulating some small amount ebywhich the\\nperson does not mind having hisincome deereased. Taken literally,\\nthis postulate implies onrepeated application that allincomes are\\nequivalent forthe person, but Wolfowitz makes itclear that hedoes\\nnotmean topropose the postulate inasense that allows repeated ap-\\nplications. The idea isreminiscent ofthose theories ofprobability\\nthat permit the neglect ofanoccasional improbable event (mentioned\\ninthe last paragraph of§4.4) and seems tomeopen toanobjection\\nsimilar totheone raised inconnection with them. Inparticular, the\\nchoice ofthe «would benotonly personal, but illdefined aswell.\\n5The minimax rule does notgenerate asimple ordering\\nFinally, anobjection made byChernoff [C7] tothe objectivistic mini-\\nmax theory must bediscussed. This will entail statement and illus-\\ntration ofthephenomenon onwhichthe objection isbased, and state-\\nment and analysis ofthe objection itself.\\nThe phenomenon pertains tothe relation between two objectivistic\\ndecision problems, tobecalled for themoment the narrow and the\\nwide problems. The narrow problem isdetermined bycertain primary\\nacts f,;and thewide one isdetermined bythose primary acts and one\\nmore, say fo. Inother words, thewide problem presents the person\\nwith one more choice than the narrow. Calling thetwo income func-\\ntions [(f; 7)and Jo(f; 7),itistobeunderstood, ofcourse, that I(f; 7)\\n=I)(f; 7)forany fthat does not use, that is,give positive weight to,\\nfp. The corresponding equation does not necessarily obtain for the\\nloss functions; indeedit clearly does so, ifand only ifthemaximum of\\nIo(f; 7)infcan beattained foreach 7without using fp. Even incase\\nnominimax ofthewide gameuses fo, it is therefore tobeexpected that\\ntheminimax f’softhe wide game will bedifferent from those ofthe\\nnarrow game. Infact, itcanhappen that nominimax of thewide game\\nuses either fyorany f,used byaminimax ofthenarrow game; this is\\nthephenomenon tobediscussed inthis section.\\nTo seehow thephenomenon can occur, suppose that Figure 12.4.1\\nrepresents the loss function ofthenarrow problem; and consider what\\nthe corresponding figure isforthewide problem, supposing that fois\\nsuch that\\nA=prz lI(fp; 2)—max I(f,; 2)>0,\\nr\\n(1)==p;max I(f,; 1)—I(fy;1)>0.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20c62747-f03b-44d0-bed6-d01a4f884a0d', embedding=None, metadata={'page_label': '224', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='206 OBJECTIONS TOTHEMINIMAX RULES [13.5\\nItisclearthatAand2canattainanypositive values,irrespective of\\nthestructure ofthenarrowproblem. Thefigureforthewideproblem\\nisconstructed thus:Thegraphcorresponding toeachf,isleftfixedat\\nitsrightendandraisedbytheamountatitsleft,andfpisrepresented\\nbyalineslopingupwithslope2fromthelowerleft-hand corner. Itis\\neasytoseethattheraisingoftheleftendsofthegraphsofthef,’scan\\nmakeanyf,withapositive slopehorizontal. If,further, suchanf,\\nminimizes L(f;g)forsomeg,itcanbemadeaminimax bychoosing 2\\nsufficiently large.Thus,speaking specifically ofFigure 12.4.1,thef,\\ncorresponding totheleftsegment oftheheavyconcave graph,whichis\\nnotusedintheminimax ofthenarrowproblem, canbecome theunique\\nminimax. Figure 12.4.1 isalittlespecial inthattheheavyconcave\\ngraphhasonlyonevertextotheleftofthemaximin ofthenarrowprob-\\nlem.Ifthereweremorethanone,thephenomenon couldalsobeex-\\nhibitedbymaking thesecondvertextothelefttheuniquemaximin,\\nwhichwouldoccurforallA’sand2’sinacertainrange.Thusthephe-\\nnomenon occursnotonlyforisolated valuesofAand2buttypically\\nforwholedomains ofvalues.\\nSuppose, totakeastriking case,thatonef,,sayf,-,istheunique\\nminimax forthenarrowproblem andadifferent one,f,-,istheunique\\nminimax forthewideproblem. Itisabsurd, asChernoff saysineffect,\\ntorecommend f,,asthebest act amongthef,’swhenonlythef,’sare\\navailable andthentorecommend f,,asthebestforanevenwider\\nclassofpossibilities. Fancysayingtothebutcher, ‘Seeing thatyou\\nhavegeese,I’lltakeaduckinstead ofachicken oraham.”’\\nItisabsurd, then,tocontend thattheobjectivistic minimax rule\\nselectsthebestavailable act.Butthatisnotsodevastating totherule\\nasmightatfirstappear, foritisnotcontended byanyoneknown to\\nmethattheruledoesselectthebest.Onthecontrary, theruleisin-\\nvokedonlyasasometimes practical ruleofthumb incontexts where\\ntheconcept of‘“‘best” isimpractical—impractical fortheobjectivist,\\nwhere itamounts totheconcept ofpersonal probability, inwhichhe\\ndoesnotbelieveatall;andforthepersonalist, wherethedifficulty of\\nvagueness becomes overwhelming. Tohaveaconsistent concept of\\n“best,” thatis,tohaveamodeofdecision thatdoesnotexhibitthe\\nphenomenon, amounts, asChernoff himselfpointsout,totheestablish-\\nmentofasimpleordering ofpreference among acts.Insofarasthat\\ncanbedoneconsistently withthesure-thing principle, personal proba-\\nbilityispractically definedthereby. Ifthesure-thing principle isvio-\\nlated,theordering isabsurdasanexpression ofpreference. Forex-\\nample,theruleofminimizing themaximum ofthenegative ofincome\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5049a04-1609-4378-be16-8d5d2b7f6c7d', embedding=None, metadata={'page_label': '225', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.5] NOSIMPLE ORDERING 207\\ndoes not exhibit thephenomenon. Itamounts toconsidering f<f’,if\\nand only if\\n(2) max I(f;7) <max I(f’; 7).\\nThis establishes asimple ordering, but one that violates thesure-thing\\nprinciple byviolating P2.\\nThe phenomenon has a particularly natural interpretation for the\\ngroup minimax rule. Itwould not be strange, for example, ifa\\nbanquet committee about toagree tobuy chicken should, onbeing in-\\nformed that goose isalso available, finally compromise onduck.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c12d5b3-e265-4820-af76-c999ae559f2b', embedding=None, metadata={'page_label': '226', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 14\\nTheMinimax Theory\\nApplied toObservations\\n1Introduction\\nInthischapter theconcept ofobservation isre-explored fromthe\\npointofviewoftheminimax rule.Inprinciple, objectivistic andgroup\\nminimax problems shouldherebetreatedonanequalfooting. But,\\nsincemathematically thetwotheories areidentical, itseemswisestto\\nfocusonone,interjecting occasional digressions abouttheother. I\\nhavechosentofocusontheobjectivistic problems. Thatchoice,being\\ninaccordance withotherliterature ontheminimaxrule,willfacilitate\\nthereader’s furtherstudyofthesubject, anditalsorendersmoreob-\\nvioustheintimate connection between theminimaxrulesandthetheory\\nofpartition problems presented inChapter 7.Thepresent chapter\\ncanindeedberegarded largely asaparaphrase ofChapter 7,so there\\nwillunavoidably bemanyreferences tothenotations andconclusions\\nofthatchapter.\\n2Recapitulation ofpartition problems\\nParalleling thetreatment ofobservation inChapters 6and7,an\\nobjectivistic observational problem willberoughly defined toconsistof\\nanobjectivistic problem, regarded asbasic;anobservation; andasec-\\nondobjectivistic problem, derivedfromthebasiconeandtheobser-\\nvation.\\nMoreexplicitly, thebasicproblemmaybeanyobjectivistic problem.\\nItwillbecharacterized bythe values ofE(f |B;),wherefrangesover\\nasetofactsFsubject totheconditions laiddownin§9.3,andB;isa\\npartition.\\nTheobservation isarandom variable x(confined, asusualin this\\nbook,toafinitesetofvalues), subject totheconditional distributions\\nP(« |B;),andsoarticulated withFthatE(f |B,,x)=Ef |B;)for\\neveryxsuchthatP(x |B;)>0.Thelastcondition is(7.2.7); asmen-\\ntionedinconnection withthatequation, thecondition willinparticu-\\n208\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e193a4b-58b0-4ec0-95b5-c889b300fbc4', embedding=None, metadata={'page_label': '227', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2] RECAPITULATION OFPARTITION PROBLEMS 209\\nlarbemet, ifevery fisconstant onevery B;,aspecialization costing\\nbut little inreal generality.\\nThe derived problem (paralleling §6.2) consists ofF(x), the set ofall\\nfunctions assigning elements fofthe basic acts Ftovalues xofthe\\nobservation x.The values ofE(f(x) |B;) forf(x) ¢F(x) arecomputable\\nfrom theE(f |B;)and theP(x |B;) thus:\\n(1) E(£(x) |B;)=E(E(£(x) |B,,x))\\n=»°E(f(x)|By,z)P(r| By\\n=)E(f(z) |B)P(x| B:)\\nItwill now beshown that the set ofderived acts F(x) satisfies the\\ntechnical conditions imposed onthe set ofbasic acts F,sothat the\\nderived problem isalso anobjectivistic decision problem. Infact, if\\nevery f<¢Fisexpressible intheform Zf(r)f, (with the usual condition\\nonthe f(r)’s), primary acts forF(x) analogous to the f,’scan bedefined\\nbyattaching toevery function r=r(x) anelement f(x; r)ofF(x),\\nwhere\\n(2) f(x; r)=pef(z).\\nThere areonly afinite number of f(x; r)’s, and allelements ofF(x) are\\nexpressible asweighted averages ofthem; the first assertion isobvious,\\nand the second poses theproblem offinding, forany system ofproba-\\nbility measures ¢(r; x)onthe r’s, atleast one probability measure on\\nthe set offunctions rwith respect towhich P(r(x) =r)=¢(r; x)for\\nevery rand x.The problem typically hasmany solutions; the simplest\\nistoletthe r(x)’s, regarded foreach xasfunctions ofr,beindependent\\nrandom variables onthe set ofr’sconsidered asaprobability space,\\nthat is,toset\\nP(t) =[I¢(r(@);2).\\nFormally, this particular solution leads tothe identity\\n(3) f(z)=2)o(r;xf,\\n=dXI o(r(x’); 2)| f(z).\\nThe identity and the fact that the coefficients inbraces arenon-nega-\\ntive and add upto1,areeasy tocheck analytically, ifitisrecognized\\nthat summation with respect tormeans multiple summation with re-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b58caef-943d-4dc0-978c-4300f67bb4a5', embedding=None, metadata={'page_label': '228', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='210THEMINIMAX THEORY APPLIED TOOBSERVATIONS [14.2\\nspecttor(1),r(2),---(thez’sbeingfordefiniteness supposed totake\\nintegral values). Equation (3)showsincidentally thatitisimmaterial\\nwhether itisbeforeoraftertheobservation thatmixedactsareintro-\\nduced.\\nTurnmomentarily totheideaofobservation ingroupdecision prob-\\nlems.HeretheE(f;B;)’sarereplaced byI(f;2)’s,theexpected income\\noffintheopinion ofthe7thperson. There isnopartition B;,except\\ninaspecial, though theoretically important, case,namely thatofthe\\nithpersonholding unequivocally thatB;obtains.\\nTheP(x |B;)’sareherereplaced byP(x;2)’s,thepersonal distribu-\\ntionofxforthe7thperson. Itispostulated that,foreachperson, the\\nconditional expectation offisunaffected byknowledgeofzx.\\nThederived actsareformally thesameasforanobjectivistic decision\\nproblem, andtheincome function ofthederived groupdecision prob-\\nlemis\\n(4) I(f(x);4)=2T(£(x);4)P(a;2).\\nReturning toobjectivistic problems, (9.4.1)definesthelossfunction\\nofthebasicobjectivistic problem and,mutatis mutandis, thatofthe\\nderivedproblem also,thus:\\n(5) L(f(x);4)=maxE(£’(x) |B;)—E(£(x) |B).\\nTherightsideof(5)admitssomesimplification, for,ifthepersonknew\\nwhichB;obtained, observation wouldbevalueless tohim.Accord-\\ningly,\\n(6) L(£(x); 7)=maxE(f’ |B;)—E(£(x) |B)).\\nAnalytically, thesimplification isjustified thus:\\n(7) maxE(f|B;)<maxE(f(x) |B,)\\nf f(x)\\nmax>E(é(x) |B)P(x |Bi)\\n<maxE(f |B,).\\nf\\nIndiscussing application oftheminimax ruletothebasicandde-\\nrivedlossfunctions, it isdoublyadvantageous tointroduce mixtures\\nofthe2’s,forthereby thetheoryofbilineargamespresented inChapter\\n12andthatofpartition problems (withsomereinterpretation) can\\nbothbebrought tobear.Letting8denoteagenericsystem ofweights\\nB(t),B(t)>0andZB(7)=1,andusingthenotation ofChapter 7,the\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1a488a7c-1469-4517-a0aa-ede6eaf01307', embedding=None, metadata={'page_label': '229', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2] RECAPITULATION OFPARTITION PROBLEMS 211\\nbilinear games associated with theprimary and derived problems are,\\nrespectively,\\n(8) Lif; 8)=(8)—E(£|8),\\n(9) L(£(x); 8)=1(8)—E(E(x) |8)\\n=1(8)—>) E(e(z) |By)P(x |BBW)\\nJj\\n=1(8)—>>E(£(z) |8,x)P(z 8).\\nIfnecessary, (9)can beinterpreted and verified bycomparison with\\n(7.3.7) and (7.2.8), inthat order.\\nInChapter 7,6(2)was generally required notonly tobenon-negative,\\nbut also strictly positive; onexamination, this slight difference from\\nthe present context will befound innocuous. Again, inChapter 7,the\\nstatement and derivation ofconclusions were, for simplicity, nominally\\nconfined totwofold partition problems. Here the extension ofthose\\nconclusions ton-fold problems will befreely used, though some readers\\nmay prefer here, asthere, tofocus ontwofold problems.\\nLetting L*denote theminimax (and maximin) value ofthe basic,\\nand L*(x) that ofthe derived problem, itisobvious, since F(x) >F,\\nthat L*(x) <L*; but there issome interest inviewing this inequality\\nasaconsequence of(7.3.4):\\n(10) L*(x) =maxminL(f(x); 6)\\n=max [1(6)—o(F() |6)]\\n<max [1(6)—o(F |8)]\\n=max main L(f; 8)=L*.\\nItisclear that themaximin §’sforthe basic and derived problems are\\nthe 6’sthat maximize the concave functions\\n(11) h(8)=ps1(8)—v(F |8)=(6)—k(6)\\nand\\n(12) (8; x)=peL(8)—v(F(x) |8)=(8)—E(k(G(x)) |8),\\nrespectively. The search forminimax f(x)’s, for example, isgreatly\\nnarrowed bythe consideration that, iff(x) isminimax, E(f(x) |8)=\\nv(F(x) |8)forsome 8,indeed forevery maximin B.According to §7.3,\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1023817-a757-4220-b8bf-4d8b6f0cbff8', embedding=None, metadata={'page_label': '230', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='212THEMINIMAX THEORY APPLIED TOOBSERVATIONS _[14.4\\nequality obtains in(10),ifandonlyifthereisamaximin Bpofthe\\nbasicproblem suchthat .\\nP(x |B;)Bo(2)\\n8) Bots)=nrLsP(2|B,)Bo(i) \\nisalsoamaximin ofthebasicproblem foreveryxsuchthat\\nZP(x |B;)Bo(j)>0.\\nThemosttypical possibility, andtheonlyonetobeexplored here,is\\nthatthebasicproblem hasauniquemaximin 8pwith89(7)>0forall\\nj.Underthisassumption, L*(x)=L*,ifandonlyifxisutterly ir-\\nrelevant, asiseasilyshown.\\nInthesamespirit, ascaneasilybeshown,L*(x)=0,ifxisdefini-\\ntive,butnottypically otherwise; and,ifxextends y,thenL*(x)<\\nL*(y)withequality if,andtypically onlyif,yissufficient forx.\\n3Sufficient statistics\\nDigressing fromtheminimaxruleforamoment, something morefun-\\ndamental canbesaidaboutasufficient statistic yofx.Namely, for\\neveryf(x)eF(x),thereexistsanf(y)eF(y) suchthatJ(f(y); 7)=\\nI(£(x); 1)forevery 7.Indeed f(y)=>>f(x)P(x |y)definessuchan\\nact.Without appeal tosoweakastep as theminimax rule,thisre-\\nmarkdemonstrates thatevenanobjectivist losesnothingbyexchang-\\ningknowledge ofanobservation forknowledge ofasufficient statistic\\nofit.Theremarkmightas wellhavebeenexpressed in§7.4,except\\nthatthereitwouldhaveinvolved somecircumlocution, mixedactsnot\\nyethavingbeenintroduced.\\n4Simpledichotomy, anexample\\nMuchofwhathasbeensaidthusfariswellillustrated bythemini-\\nmaxcounterpart ofExercise 7.5.2.Thereader isaccordingly askedto\\nreviewthatexercise andcontinue itthus:\\nExercises\\n1.Fortheproblem inquestion:\\n(a) h(8)=68(1)+6:8(2)—|616(2)—628(1)|.\\n(b)A(B;x)\\n628(1)+86(2)—D2|61728(2)—82718(1)|{xP(r|B,|\\nr j\\n=§4[2P(r)<71*(8,Bo)|Bi)+P(r=r*(8,Bo) |By)18(1)\\n+6:[2P(re<r2*(B,Bo) |Bs)+P(r=7*(B,Bo)|Bz)]8(2).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38226b43-1d3b-4405-a45c-33bed866704c', embedding=None, metadata={'page_label': '231', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.4] SIMPLE DICHOTOMY, ANEXAMPLE 213\\n2a.A8ismaximin,ifandonlyifr*(6,Bo)issuchthat\\n(1) 62P(r<171*(B,Bo)|Bi)<6:P(r2<71*(B,Bo)|Bo)\\nand\\n(2) 6oP(r1<11*(B,Bo)|Bi)>8:P(r2<11*(8,Bo)|Ba).\\n2b.There istypically onlyonemaximin, buttheremaybeaclosed\\ninterval ofthem.\\n3.Though theactsofFandF(x)asdefinedbyExercise 7.5.2donot\\nprovide formixedacts,itwillsufficetoconsider mixtures ofthef(x)’s.\\nEachofthesewillbedetermined byani,andnothing willbelostby\\nrequiring itobeoftheform7(r(x)).\\n4a.Anyminimax willbeequivalent toamixture off(x)’seachcorre-\\nsponding toalikelihood-ratio testassociated withr*(8,89) forevery\\nmaximin £.\\n4b.InviewofExercise 3,theonlylikelihood-ratio teststhatneed\\nbeconsidered foraminimax are:\\nu(r)=1,ifandonlyif7;<7;*(@,Bo).\\ni(r)=1,ifandonlyifr;<7,*(@,Bo).\\nThesearenotnecessarily different tests.\\n5a.Ifthemaximin 8isunique, theminimax actisunique(except\\npossibly forequivalent acts)andisamixture ofexactlytwof(x)’scorre-\\nsponding tothetwolikelihood-ratio testsdefined inExercise 4b.\\nThisconclusion callsforsomecomment, for,inordinary statistical\\npractice, oneortheotheroftheextremelikelihood-ratio testsisused,\\nneveramixture. Thispractice isnotinserious conflictwiththemini-\\nmaxrule,because themaximum lossassociated witheitherextreme is\\ntypically onlyslightly greaterthanL*(x). Moreover, vagueness about\\ntheexactmagnitude of6;and6gwouldusually frustrate anyattempt\\ntocalculate thecoefficients ofthemixture. Incidentally, mixture 1s\\nnotcailedforatallwhenriscontinuously distributed, forh(@,x)1s\\nthensmooth ratherthanpolygonal; thatis,ifP(r=r’|B,)=0for\\neveryr’andboth7’s,thenh(6;x)hasacontinuous firstderivative in@.\\nToshowthisandtoshowthatthederivative is62P(r;<1;*|B,)-\\n6,P(r2<re*|B,)maybetakenasanexercise onlyslightlybeyond the\\nusualmathematical levelofthisbook.\\n5b.Ifthereismorethanonemaximin 8,thenanyonethatisnot\\nextreme hasonlyonelikelihood-ratio testassociated withit,andthe\\nsameoneforall.Thef(x)corresponding tothattestisessentially the\\nonlyminimax.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='436f47e5-b4d6-481c-a146-e959835f14f7', embedding=None, metadata={'page_label': '232', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='214THEMINIMAX THEORY APPLIED TOOBSERVATIONS [14.6\\n5Theapproach tocertainty *\\nInconcluding theparaphrase of§7.1-6thathasthusfarbeenthe\\nsubject ofthepresent chapter, itshouldbementioned thattheapproach\\ntocertainty studied in§7.6obviously implies thatthecorresponding\\nL*(x(n)) approaches zerowithincreasing n.\\n6Costofobservation\\nAcostcassociated withanobjectivistic observational problem di-\\nminishes theincomebyE(c |B;)foreach7,regardless off;thatis,al-\\nlowingforthecost,J(f;1)=E(f—c|B,). Butthecost,beingun-\\navoidable, doesnotaffectthelossfunction, sotheminimax problem\\nassociated withtheobservation isindependent ofthecost.Thecosts\\ndointervene, however, inanessential wayintheproblem ofdeciding\\nwhichtochooseofseveral available observations, sayX,atcostCa;it\\nisimportant tobearinmindinconnection withthisproblem thatanull\\nobservation atzerocostistypically amongthechoices available inreal\\nlife.Thegeneric actofthiscompound problem canconveniently be\\nsymbolized by2A(a)f(x,z), orsometimes simplyby».Here,ofcourse,\\nA(a)>0,ZA(a)=1;forchoiceof\\\\means choice, foreacha,ofthe\\nprobability \\\\(a)thattheathobservation x,willbemadeandalsochoice\\nofthederived actf(x.)tobeadopted incasex,ismade. Itisintuitively\\nevident, andfollows easilyfrom(1)below,thatthemixture ofseveral\\n\\\\’sisalsoa Aasfarasincome isconcerned, somixtures of\\\\’sdonot\\nrequire explicit consideration. Theincomefunction canbewritten\\n (1) I(A;1)=ZA(a)E(£(Xa) —€a|Bi).\\nWhence\\n(2) maxI(\\\\;7)=maxE(f|B;—minE(ca |B;).\\nThelossfunction isaccordingly\\n(3) L(A;8)=D2A(a){La(£(Xa); 8)+da(8)},\\nwhere °\\n(4) da(8)=ot2X{E(ca|B;)—minE(CqB:)}8(2),\\nandL,(f(X.); 8)isthelossfunction oftheobservational problem de-\\nrivedfromtheathobservation.\\nThecompound minimax problem isintimately relatedtotheconcave\\nfunctions h(8;x,)andthelinearfunctions d,(8),asisexplained bythe\\nfollowing exercises.\\n+Somerecentreferences appropriate tothistitleareBlackwell andDubins\\n(1962),Chao(1970), Fabius (1964),andFreedman (1965).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='417f2c8f-822f-4853-9412-c179931cff38', embedding=None, metadata={'page_label': '233', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.7] SEQUENTIAL PROBABILITY RATIOPROCEDURES 215\\nExercises\\n1.Showthat\\n(5) hy\\\\(8)=oveminL(A;8)=min[h(B;Xa)+da(@)I.\\n2.If\\\\=1-f'(x,-), thenL(A;8)=hy(6);ifandonlyif:first,\\n(6) Lia(f'(Xa);8)=A(B;Xa’)\\n(inwhichcasef’(xq)willbecalledwelladapted toxqand8);and,second,\\n(7) h(B;Xa)+dar(B)=min[A(B;Xa)+da(8)]\\n(inwhichcasexX,’willbecalledwelladapted to8).\\n3a.Showthat\\n(8) Ly*=psminmaxL(A;8)=maxhy(8)\\n<minmax[A(6;Xa)+da(8)].\\naB\\n3b.Undertheimportant specialcondition thatthed,(8)areequal\\ntoconstants d,,(8)specializes to\\n(9) Ly*<min[L*(x_)+dal.\\n3c.Whencanequality holdin(8)and(9)?\\n3d.6’ismaximin,ifandonlyifh)(6’)=Ly*.\\n4.AX=DA(a)f(xa) 18minimax,ifandonlyif:\\n(x)ForeveryaforwhichA(a)>0,x,iswelladapted toeverymaxi-\\nmin£,andf(x.)iswelladapted tox,andeverymaximin 8.\\n(8)L(A;7)<Ly*forevery 7.(Ofcourse (8)isalonenecessary and\\nsufficient; thepointoftheexercise isthatthenecessary condition (a)\\nmayconveniently confine thesearchforminimax )’storelatively few\\ncandidates. )\\n5.Suppose that:(@)rand7areconfined tothevalues 1and2,and\\nL(f,;2)=|r—1 |;(8)xisconfined tothevalues 1and2,andP(1 |B,)\\n=1/2,P(1 |By)=1/4;(y)aisconfined tothevalues 1and2,andthe\\n\\\\’softhecompound problem attachweight \\\\(1)toabasicactatzero\\ncostand\\\\(2)toanactderived fromxatanon-negative constant cost\\nd.Compute andgraph: h(8),A(@;x),and(forvarious values ofd)\\nh(8).GraphL,*asafunction ofd,anddiscuss theminimax )’sfor\\nvarious valuesofd.\\n7Sequential probability ratioprocedures\\nThetypeofdecision problem thatin§7.7ledtotheconcept ofa\\nsequential probability ratioprocedure hasanintimate counterpart in\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7a110d4a-9e79-4deb-8647-a2d4cb6309f9', embedding=None, metadata={'page_label': '234', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='216 THE MINIMAX THEORY APPLIED TOOBSERVATIONS [14.8\\nanimportant type ofcompound objectivistic decision problem, for\\nwhich the concept was infact originally developed byWald [W2].\\nThe x,’s ofaproblem ofthis type range over the enormous variety of\\nsequential observational programs associated with asequence of(con-\\nditionally) identically distributed random variables x(1), x(2), ---.\\nThe technical assumption that the a’shaveafinite rangeisnot fulfilled;\\nbut, asin§7.7, Iproceed with some lapse ofrigor, referring toWald’s\\nbook [W3] or[A7] forthe full details. Exercise 6.4shows that atten-\\ntionmay beconfined toa’sthat arewell adapted toatleast one 6,and\\nthat forthose a’s itmay beconfined tof(x,)’s that are well adapted to\\nxX,and the corresponding 8.The way ispaved by §7.7, which states\\nsharply restrictive properties ofthe x,’s and f(x,)’s that aresoadapted.\\nInsome cases, recognition ofthese properties contributes greatly tothe\\npossibility ofactually computing minimax, ornearly minimax, pro-\\ncedures forsequential problems.\\n8Randomization\\nAnother important type ofcompound problem isillustrated bythe\\nsecond example of§9.6. Ageneralization ofpart ofthat example is\\npresented here toshow how theminimax rule explains, orimplies, the\\nprocess called randomization, which isone ofthemost striking features\\nofmodern statistics, and one long antedating theminimax rule. Ran-\\ndomization represents the only important use ofmixed acts that has\\nthus farfound favor with practicing statisticians, aswill bediscussed\\ninthe next section. The exact meaning ofrandomization seems alittle\\nelusive; nosharp definition isattempted here. But, roughly, random-\\nization isthe selection ofanobservation atrandom; that is,ofad\\nwith more than one A(a) actually positive, the choice ofthe A(a)’s and\\nofthe derived acts being governed largely bysymmetry. The follow-\\ningexample provides atleast afairly general illustration ofthe concept.\\nTosetthestage and provide motivation foraformal statement, the\\nexample will first bestated inlanguage that issuggestive though a\\nlittle vague. The consequences ofthe basic acts inthe example de-\\npend onthe composition ofapopulation ofnobjects, which may be\\nthought ofasnumbered from 1through n. Itmay beknown ofsome\\ncompositions that they cannot occur; but, ifacomposition isconsidered\\npossible, allpopulations having that composition (irrespective oforder-\\ning) are also considered possible. Each observation inthecompound\\nproblem consists inthe cost-free observation ofsome mofthe objects,\\nevery subset ofexactly mobjects being available forobservation.\\nFormally, the index 7ofthe partition B;runs over acertain setJof\\nn-tuples, {71, ---, t1}, ofelements considered fordefiniteness tobein-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53bf6967-a7e8-4662-9a5b-20c638c7d565', embedding=None, metadata={'page_label': '235', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.9] MIXED ACTS INSTATISTICS 217\\ntegers. If7={t,, ---, t,} ¢7,then any permutation 77of7isalso in\\nI. It1sassumed that\\n(1) E(f| B,)=Ef |Bri)\\nforevery f¢F,2¢J,and permutation T.\\nToevery subset Aofmintegers, 1<a,(A) <ao(A) <--+< Gm_,(A)\\n<dmn(A) <n, there corresponds anobservation x(A) the possible val-\\nues ofwhich are m-tuples {7(A), ---, %(A)}. The conditional dis-\\ntributions ofthe x(A)’s are defined thus: If271(A) =t4,:4), etc., then\\nP(2x;(A), ++, Lm(A) |B;)=1.\\nItisobvious that D*(x(A)) isthesame forevery A. Intypical ap-\\nplications thiscommon value islittle, ifatall, less than L*.\\nIfacompound act2A(A)f(x(A)) istobechosen, statistical common\\nsense asserts that nothing istobelost by:\\n—1\\n(a)Letting (A) beindependent ofA,and therefore equal to(”)\\nforevery A;that is,letting every sample ofsizemhave thesame prob-\\nability ofbeing chosen, orrandomizing, asitissaid.\\n(b) Letting f(71(A), ---, %m(A)) besymmetric initsmarguments\\nand independent of A.\\nItcan infact be shown, bythemethod illustrated inthe second ex-\\nample of§9.6and discussed more generally in§12.5, that there isat\\nleast oneminimax satisfying (a)and (b), and even that there isanad-\\nmissible one. Typically, ifmislarge, but small compared ton,Ly*\\nismuch smaller than thecommon value ofthe L*(x(A))’s.\\nThe importance ofrandomization inapplied statistics can scarcely\\nbeexaggerated. From the personalistic viewpoint itisone ofthemost\\nimportant ways tobring groups ofpeople into virtual unanimity; from\\nthe objectivistic viewpoint itnot only makes possible great reductions\\nInmaximum loss, but it1sseen aSaninvention bywhich the theory of\\nprobability 1sbrought tobear onsituations towhich probability on\\nfirst (objectivistic) sight would seem irrelevant.t\\n9Mixed acts instatistics\\nMany have commented that modern applied statistics makes one,\\nbut only one, important use ofmixed acts, namely indeciding, through\\nthe process ofrandomization, what toobserve. Thus, for example,\\nonce the observation has been made, the derived act isinpractice al-\\nmost always chosen, without mixing, from asetofbasic acts natural to\\nthe problem. This might seem toimply asharp conflict between the\\nminimax rule and ordinary statistical practice; but actually itreflects\\n+Iwould express myself very differently today (Savage 1962, pp. 33-34).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0683c057-609c-44ee-be4d-ad18424fe84a', embedding=None, metadata={'page_label': '236', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='218 THE MINIMAX THEORY APPLIED TOOBSERVATIONS [14.9\\nagreement, formixed acts greatly reduce theminimax loss indecision-\\nproblem interpretations oftypical practical statistical situations, when\\nand only when ordinary practice calls formixed acts ofthe samesort,\\nnamely when randomization is called for.\\nThere arecertain mechanisms that systematically tend tomake mixed\\nacts have relatively little, oreven absolutely no, advantage over un-\\nmixed acts. Inthe following discussion ofthese mechanisms, let L(r; 1)\\nbethe abstract game onwhich abilinear game L(f; g)isbased.\\nInthe first place, supposing that L(r; 7)isnon-negative forevery r\\nand 7(as isappropriate tothe context now athand), (12.3.6) can be\\ncompleted, sotospeak, thus:\\n(1) L*min (&, f)>minmax L(r; 2),\\nwhere FRand Jdenote forthemoment thenumber of values ofrand 1,\\nrespectively, and min (R, J)isofcourse theminimum ofthetwo inte-\\ngers&and J.Aninequality stronger than (1)will actually beproved.\\nConsider aminimax fforwhich the smallest possible number FR’of\\nthe f(r)’s are actually positive:\\n(2) R’L* =max R’>Lr; Of(r)\\n>max L(t’; 1)\\n>minmax L(r; 7)\\nwhere r’issochosen that R’f(r’) >1,ascan obviously bedone. Itis\\nknown [B19] that R’<min (R,J).\\nThe important lesson of(1) isthat, unless Rand Jare both large,\\nthe introduction ofmixed acts cannot reduce the minimax loss toa\\nvery small fraction ofthe value itwould otherwise have.\\nTomention adifferent mechanism, Figure 12.4.1 suggests that, if\\nthere aremany 7’s, the corners ofthe concave function emphasized in\\nthat figure may well bevery blunt, inwhich case aminimax mixed act\\nhasalmost ashigh amaximum loss asany one ofitscomponents. When\\nthenumber of 7’s isinfinite, the concave function may well bedifferen-\\ntiable, inwhich case mixed acts have absolutely noadvantage. The\\nremark appended toExercise 4.5a ispertinent here.\\nThis mechanism can berelated toacertain large class ofinfinite ab-\\nstract (i.e., not necessarily bilinear) games, discovered byKakutan!\\n(K1], forwhich L*=Lx. Bilinear games are but aspecial case of\\nthese, and numerous others seem toarise frequently inapplications.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e5edee29-938e-4d2e-9358-edccb4b6dba7', embedding=None, metadata={'page_label': '237', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.9] MIXED ACTS INSTATISTICS 219\\nIfL*=Lsforanabstract game, nothing atallcan begained byad-\\njoining mixed acts, as(12.3.5) shows.\\nFinally, itmay bementioned that inmany cases where an observa-\\ntion xmight befollowed byamixed derived act, thesame, ornearly\\nthesame, consequences can often berealized byapure act. Speaking\\nalittle loosely, this occurs whenever xhas acontinuous ornearly con-\\ntinuous contraction ythat isirrelevant, ornearly irrelevant, forthen\\nycan play the role inselecting abasic derived actthat would otherwise\\nbeassigned toatable ofrandom numbers. If,forexample, xiscon-\\ntinuous, y(x) canbetaken asthelastfew digits inthedecimal expansion\\nofxtoanextravagant number ofplaces. Again if,conditionally, x=\\n{xX}, °°°, Xn} 1sann-tuple ofcontinuously, identically, and independ-\\nently distributed real random variables, y(x) may betaken asthe per-\\nmutation that ranks the x’s inascending order, provided that n! is\\nfairly large: 10!should satisfy almost any need.\\nArecent technical reference onthe superfluousness ofmixed acts in\\nthe presence ofcontinuous observations is [D13].\\nIhave occasionally heard itconjectured that any mixed actmade\\nafter theobservation (inanobservational decision problem) iswrong in\\nprinciple. Iwould argue that theconjecture ismistaken thus: Any ob-\\nservational problem that calls forrandomization can besimulated, so\\nfarasitsloss function L(r; 7) is concerned, byabasic problem. Amixed\\nact will beasappropriate tothe basic problem asitwas tothe obser-\\nvational problem from which the basic onewas derived. Inthisway a\\ngreat variety ofsituations calling formixed acts having nothing todo\\nwith choice ofobservation can beconstructed, though they seem tobe\\natypical inpractice. Moreover, any basic problem can obviously oc-\\ncurasthe decision problem remaining after some particular value xof\\nanobservation has been observed, sothe situations just constructed\\nlead toclosely related ones calling formixed acts after observation.\\nLess abstractly, consider aperson choosing from atray ofassorted\\nFrench pastries. Even after extensive visual observation and interro-\\ngation ofthe waiter, the person might justifiably introduce considera-\\nblemixture into hischoice.\\nIthink that the conjecture that mixed acts are necessarily inap-\\npropriate after observations stems partly from themechanisms that do\\ntend tomake such acts inappropriate orunimportant inmany typical\\ncases and partly from justifiable dissatisfaction with specific mixed acts\\nthat have from time totime been suggested bystatisticians. For ex-\\nample, the suggestion that ties inrank arising innon-parametric tests\\nberemoved byranking the tied observations atrandom may inmany,\\norperhaps all, cases fairly beregarded with suspicion.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be064ad8-03ac-455e-b702-bc9ab0a1e6a0', embedding=None, metadata={'page_label': '238', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 15\\nPointEstimation\\n1Introduction\\nThischapter discusses pointestimation, andthenexttwodiscussthe\\ntesting ofhypotheses andinterval estimation, respectively. Definitions\\noftheseprocesses mustbe sought induecourse; but,forthemoment,\\nwhatever notions aboutthemyouhappen tohavewillaffordsufficient\\nbackground forcertainintroductory remarks applying equally wellto\\nbothkindsofestimation andtotesting.\\nEstimating andtestinghavebeen,andinertiaalonewouldinsure\\nthattheywilllongcontinue tobe,cornerstones ofpractical statistics.\\nTheirdevelopment hasuntilrecently beenalmost exclusively inthe\\nverbalistic tradition, oroutlook. Forexample, testingandinterval\\nestimation haveoftenbeenexpressed asproblems ofmaking assertions,\\nonthebasisofevidence, according tosystems thatlead,withhighprob-\\nability, totrueassertions, andpointestimation hasevenbeendecried\\nasill-conceived because itisnotsoexpressible.\\nWald’sminimax theoryhas,aswasexplained in§9.2,stimulated in-\\nterestintheinterpretation ofproblems ofestimation andtesting inbe-\\nhavioralistic terms;toobjectivists thishas,ofcourse,meantinterpre-\\ntationasobjectivistic decision problems. Forreasons discussed in\\n§9.2,itdoesseemtomethatanyverbalistic concept instatistics owes\\nwhatever valueitmayhavetothepossibility of oneormorebehavioral-\\nisticinterpretations.\\nThetaskofanysuchinterpretation fromoneframework ofideasto\\nanother isnecessarily delicate. Inthepresent instance, thereisapar-\\nticulartemptation toforcetheinterpretation, namely, sothatcriteria\\nproposed bytheverbalistic outlook aretranslated intoapplications of\\ntheminimax theory, thatis,oftheminimax ruleandthesure-thing\\nprinciple (asexpressed bythecriterion ofadmissibility), fortheseare\\ntheonlygeneral criteria thusfarproposed andseriously maintained\\nforthesolution ofobjectivistic decision problems. Ofcourse itisto\\nbeexpected, andIhopelatersections ofthischapterandthenextdem-\\nonstrate, thatunforced interpretations dooftentranslate verbalistic\\n220\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97f1612f-60c1-43b0-a916-a43abc038e20', embedding=None, metadata={'page_label': '239', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.3] EXAMPLES OFPROBLEMS OFPOINTESTIMATION 221\\ncriteria intoapplications ofthebehavioralistic ones.Inevaluating any\\nsuchinterpretations, itmustbe borne inmindthatananalogy ofgreat\\nmathematical valuemaybevalueless asaninterpretation; correspond-\\ningly,whatisputforward asmereanalogy shouldnotbetakentobe\\naninterpretation, muchlessbranded asaforcedone.Forexample,\\nattention hasalready beencalled(in§11.4)tothedangerofregarding\\ntheanalogy between thetheory oftwo-person gamesandthatofthe\\nminimax rulefor objectivistic decision problems asaninterpretation.\\nInfact,minimax problems areofsuchmathematical generality that\\ntheyarise,evenwithin statistics, incontexts otherthandirectapplica-\\ntionoftheminimax ruletoobjectivistic decision problems; astriking,\\nthough technical, example isTheorem 2.26ofWald’sbook[W3].\\nTheliterature ofestimation andtesting isvast;indeed ithas,I\\nthink,beenseriously contended thatstatistics treatsofnoother sub-\\njects.Thischapter andthenexttwocannot, therefore, pretend to\\npresent acomplete digestofthatliterature, evensofarasitpertains to\\nthefoundations ofstatistics. Forfurther reading certain chapters of\\nKendall’s treatise [K2]mayberecommended asakeyreference tothe\\nverbalistic tradition (Chapters 17and18forpointestimation; 19and\\n20forinterval estimation; 21, 26,and27fortesting). Manynewer\\naspects aretreated inWald’sbook[W3];andarecentreviewoftesting\\nbyLehmann [L4]isrecommended.\\n2Theverbalistic concept ofpointestimation\\nAbstractly andverygenerally, butinverbalistic language (which is\\nnecessarily vague), theproblem ofpointestimation isthis:Knowing\\nP(x |B;)forevery7andhavingobserved thevaluex,guessthevalue\\n\\\\ofaprescribed function, orparameter asitisoftencalled, A(z)with\\nvaluesinasetA.Semi-behavioralistically thisis,Ithinkuniversally,\\nunderstood tomeanthatafunction |associating avalueI(x)¢Awith\\neachx(orpossibly amixture ofsuchfunctions) istobedecided on,the\\nfunction |beingcalledanestimate (or,tobecomplete, apointesti-\\nmate)oftheparameter ».Aproblem ofpointestimation has,thus,\\nsomeofthestructure ofanobjectivistic observational problem; but,\\nsincenothing hasyetbeensaidabouttheincome, orconsequence, re-\\nsultingfromtheact/incaseB;obtains, itisatthemoment impossible\\ntoadvancecriteria forthechoiceof1.\\n3Examples ofproblems ofpointestimation\\nItwillnowbewelltopresentsomeexamples afterafewwordsof\\npreparation. Forsimplicity, Awillhenceforth generally besupposed\\ntobeaninterval (possibly unbounded) ofrealnumbers. IfA(z)=\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='95ba1988-d4e1-4223-9878-dc3b3c72b3c5', embedding=None, metadata={'page_label': '240', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE1.\\nSOME\\nCOMMON\\nESTIMATION\\nPROBLEMS  \\nName\\nConven-tionalsymbolfor\\nA(2)Conven-tionalsymbolfor\\n2\\nRange\\nof\\nxProbabilityordensityofxMaximum-likelihoodestimateofX (a)\\nBinomial\\n(of\\nsize\\nn)(b)\\nPoisson(c)\\nNormal\\nmean\\n(for\\nsample\\nofn\\nobservations\\nwith\\nvari-ance\\none)(d)\\nNormal\\nvariance\\n(for\\nsampleof\\nn\\nobservations\\nwithmean\\nzero)(e)\\nNormal\\nmean\\n(for\\nsample\\nofn\\nobservations\\nwith\\nvari-ance\\nunknown)\\n(f)Normal variance (forsample\\nofnobservations with\\nmeanunknown) (u,0”) a\\n(u,o”) a”\\n  Integers 0ton\\nNon-negative inte-\\ngers\\nn-tuples x;ofreal\\nnumbersn-tuples\\nx; of\\nrealnumbers\\nn-tuples x; of real\\nnumbersn-tuples\\nx;\\nof\\nrealnumbers\\n nyn—(*)pra°\\ney/x!\\n¢(z1—pw)++:(tn—p),\\nwhere(2)=ps(2m)~%4e—242\"ory\\n(2)\\n-a\"\\n(2—*)\\nvas\\no-\"p(4—*) vee6)(824(AZ\\n x/n1\\n 222 POINTESTIMATION [15.3\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc2d4b31-764c-4f6d-a6b1-cf0860478c24', embedding=None, metadata={'page_label': '241', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.4] CRITERIA PROPOSED FORPOINTESTIMATES 223\\nA(z’)implies 7=7’,then»ratherthan7canbeusedtoindexthepar-\\ntition;suchanestimation problem issaidtobefreeofnuisance param-\\neters.Thisusagecorresponds tothefactthatthe2’scantypically be\\nrepresented asordered couples (A,@),where)isofcourseA(z)and@is\\ncalledthenuisance parameter; if6inturnhappens toberepresented\\nasanordered n-tuple, ordinary usagecalls6ann-tuple ofnuisance\\nparameters. Itmustberecognized asatypical inestimation problems\\nfor7orAtobeconfined toafiniteset ofvalues,andoftenxisnotso\\nconfined either. Itwilltherefore benecessary toproceed heuristically\\nintodomains wherethemathematically limited theorydeveloped in\\nthisbookdoesnotrigorously apply.\\nThespecific estimation problems mostcommonly citedasexamples,\\nandmostimportant inpractice, aresummarized inTable 1,together\\nwiththeirmaximum-likelihood estimates, thatis,estimates constructed\\ninaccordance witharuletobedefined in§4.Allbutthelasttwoex-\\namplesofTable1arefreeofnuisance parameters.\\n4Criteria thathavebeenproposed forpointestimates\\nAsamatter offact,verbalistic treatments typically dogivesome\\ninkling oftheconsequence oftheact1whenB;obtains. Thus,inthe\\nexamples commonly cited,suchasthoseinTable3.1,Aisasetofreal\\nnumbers orasetofn-tuples ofrealnumbers and,therefore, asetof\\nobjectsbetween whichthenotion ofproximity. hassomemeaning.\\nWorkintheverbalistic tradition hasmadeitclearinconnection with\\nsuchexamples that,if/=A(7)fortheB;thatobtains, theguessis\\nconsidered perfectandthat,roughly speaking, itisconsidered rather\\npoorifJisfarfromA.\\nInspiteoftheapparently hopeless indefiniteness ofestimation prob-\\nlemsevenasthusformulated, various criteria, ordesiderata, foresti-\\nmateshavebeensuggested. Alistofthesecriteria, intended tobees-\\nsentially complete, isnowpresented. Eachitemisannotated andil-\\nlustrated tomakeitsmeaning clear,andsometimestocallattention\\ntorelated criteria notexplicitly listed;motivation andcriticism are,\\nhowever, deferred untillatersections, wheretheyaretreated in,connec-\\ntionwithexplicit hypotheses abouttheconsequences ofmisestimation.\\nNoattempt ismadetoincludecriteria likeintellectual simplicity or\\nfacility ofcomputation thatdependnotonlyontheestimate butalso\\nonthecapabilities ofthepeoplewhocontemplate using it.Thelist\\nisinasenselogically inhomogeneous. Forexample, noonereally con-\\nsidersitavirtueinitselfforanestimate tobeamaximum-likelihood\\nestimate (Criterion 4);rather, itisbelieved thatsuchestimates do\\ntypically haverealvirtues.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='761361d6-8f52-483c-8748-c43e48c4daff', embedding=None, metadata={'page_label': '242', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='224 POINTESTIMATION [15.4\\nIthas,tobeginthelistofcriteria, beensuggested byonepersonor\\nanotherthat:\\n1.Ifyissufficient, nothing istobelostbyrequiring theestimate1\\ntobeacontraction ofy.\\nItwillbeinstructive tobearinmindthatnecessary andsufficient\\nstatistics oftheexamples (a)-(f) inTable3.1are,respectively, 2,z,\\n#,>2”,(€,>2”),(@D>2”).\\n2.If,oftwoestimates |andI’,\\n(1) E(t—@)P|By)<EW—P|By)\\nforevery 2,withstrictinequality forsome7,then1isbetterthanI’.\\nTherearecountless variants ofthisidea.Inparticular, thesquare\\nofthedifference maybereplaced byanyotherpositive powerofthe\\nabsolute difference. Again, (1)maybeimposed atonlyonevalueof2,\\nif1andI’aresubjected tosomeothercondition, freedom frombias\\n(Criterion 6below)beingthepopular one.\\nExample (f)givesrisetoagoodillustration ofthiscriterion, which\\nisalsointeresting inalaterconnection. LettingQ=p;>.2?—n#2,”\\nitiswellknownthatE(Q| pu,0”)=(n—1)o”andthatE(Q?|u,7)\\n=(n?—1)o*.Therefore\\n(2)E({eQ—o7}?p,0”){a?(n?—1)—2a(n—1)+L}o*\\n(o-e-o)-“n+l in n+1 °\\n204=n+l\\nforallreala,withequality ifandonlyifa=(n+1)7!,omitting the\\npathological buttrivialcasethatn=1.Bythecriterion inquestion,\\nQ/(n+1)istherefore betterthananyotherestimate oftheformaQ,\\nincluding themaximum-likelihood estimate Q/nandtheunbiased es-\\ntimateQ/(n—1). \\n3.If,oftwoestimates 1andI’,\\n(3)P(-—a<U(x)-A@)<@|B)>P(-a<U(@)—dD<@|By)\\nforeverynon-negative «;ande,andforevery 7,withstrictinequality\\nforsome€1,€g,andsome7,then|isbetterthanI’.\\n+Thisexample wasgivenbyLeoA.Goodman (1953).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b521c349-4a40-4bc3-8fd5-723404aa2e40', embedding=None, metadata={'page_label': '243', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.4] CRITERIA PROPOSED FORPOINTESTIMATES 225\\nAcceptance ofthiscriterion 1sobviously implied byacceptance of\\nCriterion 2,ofwhich itmaytherefore beregarded asaskeptical] coun-\\nterpart; formaldemonstration ofamuchmoregeneral assertion willbe\\ngiveninconnection with(5.2—4). Thecriterion implies, forexample,in\\nconnection with(c)ofTable3.1that#issuperior toanyotherweighted\\naverage ofthex,’s.Amoreinteresting example willbementioned in\\nconnection withCriterion 5.\\nThatmodification ofCriterion 3inwhich itisconcluded onlythat\\nlisatleastasgoodasI’isofsometechnical interest. Incidentally, if\\nequality heldidentically in(3),therewouldpresumably benothing to\\nchoosebetween thetwoestimates byanyreasonable criterion, forthey\\nwouldthenbothhavethesamesystem ofconditional distributions.\\n4.Amaximum-likelihood estimate isoftenarathergoodestimate.\\nAmaximum-likelihood estimate isanestimate 1suchthat,forsome\\nfunction iof2,(x)=A(i(x))and\\n(4) P(«|Byzy)>P(x |By)\\nforevery2andx.Inmanynatural problems thereisonlyonemaxi-\\nmum-likelihood estimate. Taking intoaccount theanalogy between\\nprobabilities andvaluesofprobability densities, thereadershouldverify\\nthattheestimates listedinTable3.1areindeedtheuniquemaximum-\\nlikelihood estimates oftheproblems towhichtheyrefer.Whenthere\\nisauniquemaximum-likelihood estimate, it isobviously a contraction\\nofthelikelihood ratiosand,therefore, ofanysufficient statistic; which\\nfitsneatlywithCriterion 1.\\n5.Agoodestimate shouldhavethesamesymmetry astheproblem.\\nMoreprecisely, ifapermutation 7ofthe2’sandthex’sissuchthat\\n(5) P(Tx |Br;)=P(x|Bi,\\nandsuchthatA(z)=A(z’)implies A(7T7)=A(T7’);then1shouldbe\\nsuchthat,ifl(7)=A(z),(Tx)=A(T2).\\nForexample, adopting alsoCriterion 1,agoodestimate fory»in(c)\\nmaybe sought oftheforml(#).Symmetry thendictates l(+a)=\\nl(@)+aandl(—#)=—l(#);inshort,l(#)=Z.\\nThesameconclusion canbedrawnfor(e),thoughwithalittlemore\\ntrouble. Thecriterion applied to(f)leadstoestimates oftheformaQ.\\nTheconstant amightbefixedbyappealing, forexample, toCriterion\\n2,4,or6.Thesealonegivethreeslightly different determinations—\\na~'=(n+1),n,and(n—1),respectively.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c4d65018-e182-487f-aab6-15f613e3e3ad', embedding=None, metadata={'page_label': '244', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"226 POINT ESTIMATION [15.4\\nAgain, itcan beshown forExamples (c)and (e)that, among alles-\\ntimates satisfying Criterion 5,<isbest according toCriterion 3.\\n6.Itisdesirable that the estimate beunbiased.\\nAnestimate 1iscalled unbiased, ifand only if\\n(6) E(\\\\| B,)=X(2)\\nforevery 1.\\nItiseasy toverify that themaximum-likelihood estimates of(a)—(e)\\ninTable 3.1are allunbiased; that of(f),however, isnot, forE(Q/n |L,\\no”)=(1—1/n)o” instead ofo?. Again, if1isamaximum-likelihood\\nestimate of\\\\,eisamaximum-likelihood estimate ofe*. But, if1is\\nnot definitive, and 1isanunbiased estimate ofA,e'isnot anunbiased\\nestimate ofe*,asTheorem 1ofAppendix 2implies.\\n7.IfP1—a@| <|i—x) ||B)>1/2 forevery é,then 1is\\nbetter than I’.\\nAny resemblance between this criterion and Criterion 3seems tobe\\ndispelled bythefollowing example. Suppose that, forevery 7,P(l—A(2)\\n=a, l’—A(t) =5|B;) equals 2/11 ifaand bare integers such that\\n0<a<b <2,equals 5/11 ifaand bare 2and Orespectively, and\\nequals 0otherwise. According toCriterion 7,1isbetter than I’,be-\\ncause 6/11 >1/2; but, according toCriterion 3, 1’isbetter than 1,\\nbecause 5/11 >4/11 and 7/11 >6/11. The example can easily be\\nmodified tosuit any taste forsymmetry and continuity. But, if1and\\nl’are conditionally independent (which isnot anatural assumption),\\nand |isbetter than I’according toCriterion 7;then, asmay easily be\\nshown, |’cannot bebetter than |byCriterion 3.\\nThe list ofcriteria ishere interrupted byseveral paragraphs ofex-\\nplanation inpreparation fortwo concluding criteria.\\nThe approach tocertainty treated in§§3.6and 7.6has itscounter-\\npart inthe theory ofestimation. Inparticular, ifx(n) ={x,, ---, Xn}\\nisann-tuple ofconditionally independent and identically distributed\\nobservations, there will typically exist sequences ofestimates I(n) based\\nonx(n), such that\\n(7) limP(|U(a(n), n)—A(t)| <€|B,)=1\\nforevery positive «and every 7.Asequence ofestimates satisfying (7)\\nrelative toany sequence ofobservations x(n) (not necessarily n-tuples\\nofconditionally independent observations) iscalled consistent.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0be0de7f-0c62-4b44-a284-3921baada40e', embedding=None, metadata={'page_label': '245', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.4] CRITERIA PROPOSED FORPOINTESTIMATES 227\\nThecondition ofconsistency isoftenrealized inaveryspecialway,\\nnamely thattheerror[l(x(n); n)—A(z)]1s,foreveryB;andforlarge\\nn,practically normally distributed aboutzerowithvariance inversely\\nproportional ton.Moreformally, asequence ofestimates maybe\\nsuchthat\\n% ._ . a ©tmP(\"a(n)00p=ofa\\no(2) (Qn)\\nforevery7anda,whereo(2)issomepositive function of7;itisthen\\nsaidthatn”[I(x(n); n)—X(2)]isasymptotically normalaboutzerowith\\nasymptotic variance o”(i). If,inaddition, forevery 7,o°?(z)isnotless\\nthanacertainfunction, thedifferential information, tobedefined in\\n§6,thenthesequence1,iscalledefficient.\\nThere isapossible pitfallinconnection withtheideaofasymptotic\\nnormality. Though (8)implies that,forlargen,thedistribution of\\ntheerroris,inasense,almostthenormal distribution withzeromean\\nandvariance o7(7)/n, itdoesnotimplythatthemeanoftheerroris\\nclosetozero,orevenfiniteorwelldefined. Similarly, thevariance of\\ntheerrormaybemuchlarger thano7(7)/n, infinite, orilldefined; but\\nitcannot, forlargen,besmallerthano”(z)/nbyafixedfraction orless.\\nMuchliterature onestimation hasconcentrated onsequences ofes-\\ntimation problems inwhichx(n)isann-tuple consisting ofthefirstn\\nelements ofaninfinitesequence ofconditionally independent andcon-\\nditionally identically distributed random variables or,asitwillbe\\ncalledinthepresent chapter, a standard sequence; because theseare\\nthesimplest examples ofsequences ofincreasingly informative obser-\\nvations. Examples (c)—(f) inTable3.1referdirectly tostandard se-\\nquences; thebinomial distributions (a)canberegarded asthedistri-\\nbutionofthesufficient statistic >)x,ofthestandard sequence x(n)\\ninwhicheachx;takesthevalues 1and0withprobabilities pand1—p,\\nrespectively (cf.Exercise 7.4.1);again, ifeachx;isPoisson-distributed\\nwithparameter u,then>>x;issufficient forx(n)andisitselfPoisson-\\ndistributed withparameter nu.Thus,alltheexamples inTable3.1\\ngiverisemoreorlessdirectly toexamples ofstandard sequences.\\nInspeaking ofstandard, andoccasionally ofother,sequences the\\nellipsis ofreferring toasequence ofestimates simplyas‘‘anestimate’”’\\nhasbeenwidelyadopted, soonereadsrecommendations that‘“‘anes-\\ntimate’ shouldbeconsistent orefficient. Thisellipsis, though often\\nconvenient, sometimes provesdangerous. Itdistracts fromthefact\\nthataperson iscalledupontomakeanestimate, notasequence ofes-\\ntimates; sothatthequestion ofwhatconstitutes agoodsequence does\\nnotarise.Again, itmakesonefeelthatifanestimate, say113,hasbeen \\nn—\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc1bb949-0319-43c3-903c-b43cde3ae881', embedding=None, metadata={'page_label': '246', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='228 POINT ESTIMATION [15.4\\ndefined forx(13), then the definition of1,4 isthereby implied. Onefor-\\ngets, forexample, that ‘‘the’’ average ofnobservations isawhole se-\\nquence ofstatistics, asequence singled out byhuman tastes and in-\\nterests, rather than byany mathematical necessity. Inshort, the\\nellipsis establishes the atmosphere ofthe logically nonsensical (though\\nperhaps psychologically revealing) questions onintelligence tests such as:\\n‘‘What arethetwo missing terms inthesequence ____ 18281828?” f\\nThe recommendations ofconsistency and efficiency quoted above can\\nbeadded tothe numbered list ofsuggestions, inaform that avoids the\\nellipsis:\\n8.Ifeach l(n) isagood estimate for the corresponding x(n) ofa\\nstandard sequence, then thesequence I(n) isconsistent.\\nThe sequence ofmaximum-likelihood estimates ofthe sequences of\\nproblems (a), (c)-(f) are consistent; and, forthe sequence ofproblems\\nofestimating from anobservation y,Poisson-distributed with parame-\\nternu,themaximum-likelihood estimates y,/n are consistent.\\nIfthere isone consistent sequence ofestimates, for asequence of\\nproblems there is aplethora. Each term ofaconsistent sequence can,\\nforexample, bemultiplied by (1+n—”) without destroying consist-\\nency. Again, the sample medians f{are in(c)aconsistent sequence\\ndifferent from the sequence ofmaximum-likelihood estimates.\\n9.Under thehypothesis ofCriterion 8,thesequence 1(n) isefficient,\\natleast ifany efficient sequence ofestimates exists.\\nThe sixsequences ofmaximum-likelihood estimates mentioned under\\nCriterion 8are allwellknown tobeefficient, assequences ofmaximum-\\nlikelihood estimates forstandard sequences typically are. The asymp-\\ntotic variances and certain other interesting quantities associated with\\nthese sixsequences are presented inTable 1. Itisremarkable that,\\nforeach oftheexamples inTable 1,theexpected values oftheestimates\\napproach the estimated parameter; ntimes the variance ofthe esti-\\nmate, and ntimes theexpected squared error, both approach theasymp-\\ntotic variance ofn”times the error. For the first five examples the\\nrelations mentioned hold, indeed, not only inthe limit, but exactly,\\nfor alln. All sixexamples arerather special, ormagical, but the limit-\\ning relations Just mentioned may fairly beexpected tohold insome\\ngenerality, though they arenot (ashas already been mentioned) really\\nimplied bythe asymptotic normality ofthe sequence oferrors times\\nn”. To illustrate the exceptions that can occur, |z| is,in(c), the\\n{ ¢=2.7182818285 toeleven significant figures.\\n{See any statistics text for definition, ifnecessary.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7297fd4-8032-4b51-aaa8-34b84b88fe3a', embedding=None, metadata={'page_label': '247', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.5] BEHAVIORALISTIC REVIEW OFESTIMATION 229\\nmaximum-likelihood estimate of|Ls|?foru»~0;thissequence ofes-\\ntimates isefficient; andn”(|@|—!—|«|—!)isasymptotically normal\\naboutzerowithasymptotic variance u—*;buttheotherthreeentries\\nforTable1areinfinite inthisexample.\\nTABLE 1.EXAMPLES OFBEHAVIOR OFMAXIMUM-LIKELIHOOD ESTIMATES \\n Asymp-\\nnXexpected totic\\nSequence Mean nXvariance squareofvariance\\nerror of n”%xX\\nerror\\n(a) p pq pq pq\\nPoissonpn Ub Ub iv Ub\\n(c) iv 1 1 1\\n(d) o” 20% 204 204\\n(e) U o” o” o?\\n(f) (1-=)o2(1-=)o*(2-=)o*204\\nAsinthecaseofconsistency, wherethereisoneefficient sequence,\\ntherearemany,butefficiency is,ofcourse, amuchmorerestrictive\\nproperty thanconsistency. Forexample, multiplication by(1+n~”)\\ntypically destroys efficiency, thoughmultiplication by(1+n—')never\\ndoes.Again,theconsistent sequence ofmedians mentioned underCri-\\nterion8isnotefficient. Indeed, itiswellknownofthatsequence that\\nthesequence oferrorstimesn”isasymptotically normal aboutzero\\nwithasymptotic variance 7/2ratherthan1. \\n5Abehavioralistic review ofthecriteria forpointestimation\\nItistimenowtointroduce thenotion ofconsequences, or(equiva-\\nlently, Ibelieve) ofloss,thereby interpreting estimation problems as\\ndecision problems. Letitbesaidthenthatanestimation decision prob-\\nlemisanobservational decision problem withthefollowing distinguish-\\ningfeature. There isaone-to-one correspondence between thebasic\\nactsfandthevaluesattained byareal-valued function A(z),suchthat\\nLc;2)=0,iffistheactthatcorresponds withA(z).Itissimpler,\\nmoresuggestive, andharmless toletthenumber /thatcorresponds to\\nfreplace fitselfinallfurther discussion ofestimation decision problems.\\nToillustrate thenewnotation, itmaybesaidthatL(l/;7)=0,ifl=A(z).\\nIbelievethatanysituation ordinarily saidtocallfor(point)estima-\\ntioncanbeanalyzed asanestimation decision problem. Forexample,\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1733f721-faac-487a-a916-e1f7d0ced7ad', embedding=None, metadata={'page_label': '248', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='230 POINTESTIMATION [15.5\\nestimating howmuchpaintwillcoverawallmay,depending oncir-\\ncumstances, meandeciding: howmuchpainttobuy,whattobidfora\\ncontract, orwhatnumber toenterinaguessing pool.Undereachof\\nthoseinterpretations therewillbezeroloss,ifand,typically, onlyif\\ntheestimate is‘‘correct,’’ asonesays.\\nTheconsequences ofanestimate may,likethoseofmanyreallife\\ndecisions, bedifficult toappraise. Itishardtosayeveninrelatively\\nconcrete situations whatitwillcosttomisestimate thespeedoflight,\\naparticular mortality rate,orthenational income. If,toreverttoan\\nexample already discussed, theestimate istobepublished somewhere\\nfortheuseofwhoever hasauseforit,theconsequences ofpublication\\nmayseembeyondallreckoning. Nonetheless,Ireaffirm theconvic-\\ntionthattheconcept ofconsequence measured inincome orlossis\\nvaluable indealing withsuchsituations, asIhopethepresent treat-\\nmentofestimation willillustrate Incidentally, itseemsindifferent,\\nasIhavealready said,whether lossorincomeistakenasthestarting\\npoint. Itiseasilyshownthatthedecisions oftheidealized personof\\nthepersonalistic probability theory willbethesameintwoproblems\\nhaving possibly different income, butthesameloss,functions. This\\nfeature Iwouldexpect tobeacceptable eventoobjectivists, andI\\nalsothinkitappropriate totheories ofgroupdecision.\\nIknowofnothing interesting thatdistinguishes estimation decision\\nproblems asaclassfromobservational decision problems generally.\\nButactualestimation situations suggest certain relatively wideclasses\\nofestimation decision problems aboutwhichinteresting andvaluable\\nconclusions canbedrawn. Indeed,itwillbeshowninthisandthenext\\ntwosections thatsevenoftheninelistedcriteria forestimation canbe\\njustified tosomeextentasflowingfromapplication oftheprinciple of\\nadmissibility andtheminimax ruletosuchclasses ofestimation de-\\ncisionproblems.\\nBeforemakinganyrealspecialization, itmaybemostsystematic to\\nmention thatCriterion 1issimplyaninstance ofthegeneral principle,\\nwhichwehavenowstudiedfromseveralpointsofview,thatnothing\\nislostbyconfining attention tosufficient statistics, atleastifmixtures\\nareallowed.\\nItisclearinalmostanyestimation situation, eveninthoseforwhich\\nthenotionofJossisvaguest, thatiftwoerrorshavethesamesignthe\\nlargerentailsatleastasgreatalossasthesmaller. Analytically,\\n(1) L(l;1)<LW;2)\\nfor\\\\(7)<1<I’andforA\\\\@)>1>1’.Situations towhich(1)fails\\ntoapplycanreadilybeimagined. William Tell,forexample, inesti-\\n+Thisideawasexpressed byGauss(1821,Section 6).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='04212ecb-bc04-4dd8-85f0-3cc3c4466ea1', embedding=None, metadata={'page_label': '249', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.5] BEHAVIORALISTIC REVIEW OFESTIMATION 231\\nmating theanglebywhichtoelevate hiscross-bow fortheappleshot\\nmighthavepreferred adownward errorof10°tooneof1°;butsuch\\ncircumstances seemexceptional. Furthermore, itisusually justifiable\\ntoassume thatstrictinequality holdsin(1),though therearemany\\nexceptions inwhich, forexample, ‘‘amissisasgoodasa mile” orone\\nhitisasgoodasanother.\\nAsis,Ithink,intuitively evident, whenstrictinequality holdsin\\n(1),Criterion 3issimplyanapplication oftheprinciple ofadmissibility.\\nThatconclusion canbeshown incomplete generality without serious\\ndifficulty, but,incompliance withtheusualmathematical limitations\\nofthisbook, itwillherebeshownonlyundertheassumption thatx\\nisconfined toafinitenumber ofvalues.\\nWhatistobeshownisthis:If1andI’areapairofestimates satisfy-\\ningthehypothesis ofCriterion 3,andif(1)holdswithstrictinequality;\\nthenLil;7)—L(’;7)<0forevery 7,withstrictinequality forsome\\n1.Tobegintheproofcalculate thus:\\n(2)LA;2)—LW;2)=LG;d[Pd@)=1|BY)—PU(2)=1|Bd]\\nl\\n>Lil;)Q(;7)\\nl\\n~LU,)AG)+VLGa;4),\\nL<d(Z) L>X(z)\\nwherethedefinition ofQ(l;7)isclearfromthecontext, andwhereit\\nhasbeentakenintoaccount thatL(A(2); 7)=0.Itwillbeshownthat\\nbothsumsinthelastpartof(2)arenon-positive andthatforsome7at\\nleastoneofthemisnegative. Focus, fordefiniteness, onthesecond\\nsum.LetJo=A(z)and1,le,---be,inorderofincreasing magnitude,\\nthevaluesof/>A(z)forwhichQ(l;7)~0.Withtheabbreviations\\nL(k)=prLU;2),ACK)=peL(A)—L(k—1),andQ(k)=vrQh;2),\\nthesumtobeinvestigated is\\n(3) >»LQ=>)Qk)DIAk’)\\n0<k 0<k 0<k’sk\\n=2)Ak’)DDQh).\\n0<k’ kek’\\n(Thisrearrangement mayseembizarreonfirstencounter, butitis\\nwidelyusedinmathematics generally andisinfactanexactanalogue,\\nforsums,ofthemorefamiliar integration byparts,forintegrals.) It\\nfollowsfrom(1)readwithstrictinequality thatA(k)>0;anditfol-\\nlowsfromthehypothesis ofCriterion 3thatQ(k)<0,andthatsome\\nQ(k)—or ananalogous termassociated withthefirstsuminthelast\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f85fc6f0-109c-4a16-934b-20263a9d8786', embedding=None, metadata={'page_label': '250', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='232 POINTESTIMATION [15.5\\nlineof(2)—1is strictly negative forsome7.Thiscompletes thededuc-\\ntionofCriterion 3fromthestrictformof(1)andtheprinciple ofad-\\nmissibility. Essentially thesameargument leadsfrom(1)asactually\\nwritten tothemodification mentioned inthenoteunderCriterion 3.\\nAveryslightstrengthening of(1),together withtheminimax rule,\\nprovides awidelyapplicable justification ofCriterion 8(consistency),\\naswillnowbeexplained. Suppose that(1)notonlyholdsbutalsois\\nstrict, ifJ=A(z);thatis,inaddition to(1)suppose onlythatL(l’;7)\\n>0foralll’¥A(z).Inthiscontext, letx(n)beasequence ofobser-\\nvations suchthattheminimax L*(n)ofthecorresponding estimation\\nproblems approaches zerowithincreasing n;thenanysequence ofmini-\\nmaxestimates l(n)isconsistent. Indeed, ifthesequence l(n)isnot\\nconsistent, then,forsome7,andsomepositive eand6,\\n(4) P(|U(an;n)—Ma)|>€|Bi)>6\\nforsomearbitrarily largevaluesofn.Thisimplies\\n(5)L*(n)=Lil(n); 7)=6min{LA(2)+€;2),LAG)—€52)}>0,\\nwhichcontradicts thehypothesis.\\nTurnnexttoCriterion 5(symmetry). Suppose that the estimation\\ndecision problem hassymmetry inthesensedefinedunderCriterion 5.\\nThatdoesnotinitselfreallycallforestimates withthesamesymmetry.\\nBut,ifZLalsohasthesymmetry, thatis,ifD(A(2’); 7)=LACT’); T2)\\nforallappropriate 7’,thenthediscussion ofsymmetry in§12.5sug-\\ngeststhattypically there is,atanyrate,asymmetrical, admissible,\\nminimax estimate. Whether LZhastherequisite symmetry isaques-\\ntionthatcanoftenbeanswered without detailed knowledge ofL.\\nItisoftenjustifiable tosuppose thatthefunction L(l;7)issmooth\\nenough tobedifferentiated twicewithrespect toJ,atleastwhen |is\\nnearA(z).Thiscondition, thoughveryoftenmet,isnotquitesode-\\nvoidofcontent asitmayseemtoareaderbrought upinthetradition\\nthatitmakesnopractical difference whether afunction hasafewsharp\\ncorners because theycanalwaysberounded offwithalmostnochange\\ninthefunction. If,forexample, Z(J;7)isforallpracticable purposes\\nequalto|7—|;thenLcannot be regarded asdifferentiable even\\noncewhen /=4,andthetheorytobedeveloped herefortwicedifferen-\\ntiableL(l;2)’sinthepresence ofextensive observation doesnotapply.\\nItwilltherefore beusefultodigress totheconsideration ofanexample,\\nillustrating howcornerscanariseandthephenomena thattendtoround\\nthemoff.\\nSuppose thatapersonmustestimate theamount \\\\ofshelving for\\nbooks,pricedat$1.00perfoot,tobeordered forsomepurpose. Itis\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e932cf3f-c39a-43a3-96d7-bd8a9bfb87aa', embedding=None, metadata={'page_label': '251', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.5] BEHAVIORALISTIC REVIEW OFESTIMATION 233\\npossible thatthefollowing economic analysis ofthesituation wouldbe\\nsufficiently realistic. Thepersonholdseveryfootofshelving lessthan\\nthenumberoffeet,A,ufbookstobeshelved tobeworth$a,a>1,\\nbutsuperfluous shelving heholdstobeworthless. Formally,\\n(6) Ll;dA)=(a-DA-D) forl<A\\n=(1—X) for!>X.\\nThere isthenacorner,or kink, at 1=);sodifferentiation, evenonce,is\\nimpossible.\\nButthefollowing analysis ismuchmorelikely tobesufficiently real-\\nistic.Theurgency oftheshelving ofthebooksisvariable. Somewould\\nbeworthshelving, evenifthecostofshelving wereveryhigh;atthe\\notherextreme, therearesomethatwouldnotbeworthshelving unless\\nthecostwereverylow.Morefully,thevalueof/feetofshelving isa\\nfunction 7(l)thatpresumably hasthefollowing features. Itismono-\\ntonically increasing, strictly concave, andtwicedifferentiable in1;\\n1(0)=0;(0)<0;7’(0)>1.Theincome attached toordering L\\nfeetofshelving, attheprice$1.00perfoot,isclearly\\n(7) I(l;4)=i(l)—1.\\nItismaximized attheoneandonlyvalue\\\\forwhichdi(A)/dA =1,so\\nthat\\n(8) Lil;2)=[(A)—A]—fe)—JO,\\nwhich isofcoursetwicedifferentiable in1.\\nThemoralofthesetwopossible economic analyses ofoneexampleis\\nofwideapplicability, asiswellknownamong economists. Wherea\\nsuperficial analysis suggests akink, or evenadiscontinuity, inanin-\\ncomefunction, deeper analysis willoftenshowthatthefunction is\\nsmoothed outbyvarious economic phenomena suchastheinhomo-\\ngeneityandthemutual substitutability ofcommodities.\\nToreturnfromthedigression, ifListwicedifferentiable in/(at\\nleastwhen/isclosetoA),LcanbeexpandedinaTaylorseriesthus:\\n@)Lis)=L0;)+0-N=G9\\n l=)(7)\\n2$idSLGa) — +o-»)2az?\" l=)(12)\\n where,following standard usage,o((J—)”)isafunction ofJand7,not\\nnecessarily thesamefromonecontext toanother, suchthato((J—A)”)+\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='efb9c13b-7db8-405d-9ca2-19ed5af4b22c', embedding=None, metadata={'page_label': '252', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='234 POINTESTIMATION [15.6\\n(1—\\\\)?approaches zeroas1approaches A(z)forfixed7.Thefirstterm\\nontherightsideof(9)vanishes bythedefinition ofestimation; the\\nsecondmustvanish also,forotherwise Lcouldbenegative. Therefore,\\n2\\n(10) Ll;7)=,(1—n)?=L(I;1)+o((1—d)?)2 al l=)\\n=(1—X(2))*a(z)+o((l—d)?),\\nwherea(z)isdefinedbythecontext.\\nInviewof(10),it isplausible thatLmay,inmanyproblems where\\nestimates ofgreataccuracy arepossible, besupposed tobepractically\\noftheform\\n(11) L(l;i)=(1—A(2))’a(2),\\nwherea(z)>0forevery 7.Thisdoesnotexactlymeanthatareason-\\nableLZcanbecloselyapproximated byfunctions oftheform(11)for\\nall7.Inparticular, theabsurdassumption thatLisunbounded (which\\nsuchapproximation wouldtypically imply) isnottobemade. Itmeans,\\nrather, thatunderfavorable circumstances (11)mayleadtoareason-\\nablygoodevaluation ofL(1;7).Insofarastheform(11)canbesup-\\nposedadequately torepresent L,Criterion 2isobviously anapplica-\\ntionoftheprinciple ofadmissibility. Aninteresting discussion and\\napplication of(11)isgivenbyYates[Y2].\\n6Abehavioralistic review, continued\\nThusfar,Criteria 1,2,3,5,and8havebeendiscussed inbehavioral-\\nisticterms. Infact,undersuitable hypotheses, eachhasbeenfoundto\\nhaveconsiderable behavioralistic justification. Criteria 4and9also\\nhavesuchjustification, butmydiscussion ofthemissobulky ithad\\nbetterbeisolated inaspecialsection. AsforCriteria 6and7,theonly\\nonesremaining, theydonotseemtometohaveanyserious justifica-\\ntionatall,aswillbediscussed instillanother section.\\nCriterion 4,therecommendation ofmaximum-likelihood estimates, is\\nofextraordinary interest, for,ofallthecriteria oftheverbalistic tradi-\\ntion,itisessentially theonlyonethatselectsauniqueestimate inal-\\nmosteveryestimation situation ofpractical importance. Thepresent\\nsection demonstrates that,inthepresence ofextensive observation,\\nmaximum-likelihood estimates areoftenalmostminimax estimates; it\\nalsogivessomeanalysis ofCriterion 9,whichreferstoefficiency. The\\nwaytothesegoalsisroundabout; itbeginswithastudyofinformation\\ninthetechnical sensementioned in§3.6.Inthissection itwillbeas-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='46da8b33-24b8-4733-b523-d8d9ee836b80', embedding=None, metadata={'page_label': '253', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6] BEHAVIORALISTIC REVIEW OFESTIMATION 235\\nsumedformathematical simplicity thateachobservation underdiscus-\\nsionisconfined toafinitenumber ofvalues,eachhaving positive prob-\\nabilityforeveryelement ofwhatever partition isunderdiscussion.\\nIfB;andB;areelements ofapartition, notnecessarily finite,andx\\nisanobservation, say,inthespiritof(3.6.11), thattheinformation of\\n7relative to1fortheobservation xis\\nrjB;)=—Elog—| B;}-\\nr;\\nTheexpression ofJintermsoflikelihood ratiosisimportant, especially\\nfortheextension ofthediscussion tomoregeneral observations than\\nthosecontemplated here.Thereadershould, therefore, trytobearin\\nmindthatthewholediscussion couldbecarriedonintermsoflikeli-\\nhoodratios;Irefrainfromsodoingonlyformomentary reasons ofno-\\ntational convenience. Thetheory ofJcanconveniently bepresented\\ninaseriesofexercises.P(x |B;)\\nP(x |Bi)\\n  (1)J(t,j;x) =pe-B(tog\\nExercises\\nla. Ifyisacontraction ofx,thenJ(7,7;x)>J(t,j;y). Withequality\\nwhen? Hint:\\n P(xB; PyB;\\n(2) -B(togP(e! Bi)i)>—logPu!ByP(x |Bi) Pty |Bi)\\nlb.J(t,7;x)>0.Withequality when?\\n2a.IfxX},---,X,areconditionally independent, then\\n(3) T(t,5;1,+++,Xn)=LEI(6,5;Xe)\\n2b.Ifinaddition thex,’sareconditionally identically distributed,\\nthen\\n(4) J(i,J;Xi,°°\";Xn)=nJ(1,J;X}).\\nItisinteresting toevaluate theinformation J(\\\\,\\\\+Ad;x)whereA\\nand\\\\+Adaretwocloselyneighboring valuesoftheparameter ofan\\nestimation problem, supposed, forsimplicity, tobefreeofnuisance\\nparameters. IfP(x |A)iscontinuous inX,itisalmostobvious that\\nJ(A,\\\\+AA;X)approaches zeroasAdapproaches zero.IfP(x |A)18\\ndifferentiable in,itiseasytoshowfurther (considering thatJisnon-\\nnegative) thatevenJ(A,4+Ad;x)/AXapproaches zeroasAAap-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8243a6e7-8ff6-44f5-bcbb-6b7b4e660f1a', embedding=None, metadata={'page_label': '254', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='236 POINTESTIMATION [15.6\\nproaches zero.Butinthiscasemuchmorecanandwillbeshown,\\nnamely, (5) iJ(A,A+AA;xX)1HO\\ni =— >xMoo An? 2\\n10logP(x\\\\)\\\\?\\n2 On\\n ThefunctionHisgenerally, following Fisher, calledinformation, but\\nherewehadbetter callitdifferential information. Chronologically, as\\nexplained attheendof§3.6,theconcept ofdifferential information is\\nolderthanthatherecalledsimplyinformation andofwhichitis,ac-\\ncording to(5),alimiting case.\\nThedemonstration of(5)beginswiththeconsideration that\\n    (6) log(1+)=¢—$@+o(#?).\\nTherefore,\\nP(z|’+ Ar)||P(x|+AA)—P(x|»|\\n(7)logPald) =log;1+ Paty\\n-[Peed+Ad)—P(x |»|\\n7 P(x|a)\\n1PEINa) —Fein} 9_=| Paly +o(And*).\\nSince the expected valuegiven\\\\oftheterminthesecond lineof\\n(7)iseasilyseentobeexactly zero,itwillbetactfultoleavethatterm\\nalone;butthesecondmaybeapproximated thus: Pein+o)PEIN\" [Rerel i\\n°)|P(x|») ~|\\nP(w|a)arF(AN)\\n2\\n=ayo210FEIN) +0(AX?).\\nTherefore,\\n(9) J(A,\\\\+AA;x)=GH(A;x)Ad+0(Ad’),\\nwhichestablishes (5).\\nMoreexercises\\n3.Ifthekthderivative (k>0)withrespectto\\\\ofP(x|A)exists\\nforeveryx,then\\n(10)E( =P(e|2)|)=(EPel»)=o.P(x|-n)ar*\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13da8624-aaab-4bb6-b257-2f913f767900', embedding=None, metadata={'page_label': '255', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6] BEHAVIORALISTIC REVIEW OFESTIMATION 237\\n4.Iftherequisite secondderivative exists,then\\nQ”\\n(11) H(\\\\;x)=-2(5 logP(x |d)|\\\\).\\n5.Ifyisacontraction ofx(andH(A;x)iswelldefined), thenH(A;y)\\n<H(A;x).\\nRemark: Theinequality isobvious inthelightofExercise laandthe\\nfirstpartof(5).Butitcanalsobederivedfromthefollowing applica-\\ntionofTheorem 1ofAppendix 2,which isusefulinthenextexercise.\\n2119) 1PIN)\" 2( 1PEIN|»)\\nPiy|d) an P(x|d) aa\\nwr),(1aP(z|a<EP(z|r) aa\\n0withequality foreveryyandX,ifandonlyifanlogP(x |\\\\)canbeex-  \\n \\n \\npressedasafunction ofyandalone.\\n6a.Ifyisacontraction ofx,H(A;x)=H();y)forevery );ifand\\nonlyifyissufficient forx.\\n6b.H(A;x)=0foreveryX,ifandonlyifxisutterly irrelevant.\\n7a.Ifx1,--+,X,areindependent givenA,then\\n(13) H(A;X1,+++,Xn)=2,H(A;%).\\n7b.If,inaddition, thex,’sareidentically distributed given\\\\,then\\n(14) H(a;Xr, °°°;Xn)=n(n; X}).\\n8.If1isareal-valued contraction ofx,andH();x)iswelldefined,\\nthen\\n (a)d@ _ d logPOY|)\\n(15) >E(1|\\\\)=B(uw oD r\\n(b)\\n—]?7|\\\\)AA;) >|<a| (16) E({lx?|HA;=>\\nwithequality ifandonlyif\\né\\n(17) >108P(l|xX)=@—Nk\\nforsomeconstant k.Hint:UseExercise 3andapplytheSchwartz in-\\nequality to(15).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='95088640-1391-4f08-ab20-662dea09d398', embedding=None, metadata={'page_label': '256', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='238 POINTESTIMATION [15.6\\n(c)IfH(A;x)>0,then\\n2\\n(18) B(0—»P|a)>|E(i||/H(d;x).\\nExercise 8cisanimportant, andnowfamous, inequality. It,together\\nwithitsn-dimensional generalization, hasbeencalledtheCramér-Rao\\ninequality because ofitsindependent publication byRaoandCramér\\nin1945and1946respectively (see[H6]).Butthenameisnotatall\\nwelljustified historically. Fréchet presented theinequality in1948\\n[F8],andDarmois extended Fréchet’s inequality to»dimensions, at\\nleastforunbiased estimates, inapublication [D1]notlaterthanRao’s.\\nTheinequality hasalso,though Ithinkerroneously, beenattributed to\\nanearlypaperbyAitkenandSilverstone [A1].andtoonebyDoob\\n[D10].Mypointis,ofcourse,nottogiveadefinitive history ofthein-\\nequality, butmerely tosuggest thatforthetimebeinganimpersonal\\nnamewouldbebetter. Itentatively propose calling ittheznformaton\\ninequality. Somerecentreferences pertinent totheinformation in-\\nequality andothertopicstreated thusfarinthissection are[W15],\\n[M5],[C6],and[H6].Thetechniques usedintheremainder ofthis\\nsection, whichrevolve around theinformation inequality, werepub-\\nlishedposthumously byWald[W5].\\nTheinformation inequality hasanimportant bearingonapplication of\\ntheminimax ruletoestimation, ofwhichthefollowing theorem may,\\ninviewof(5.11)betaken asafirstillustration.\\nTHEOREM 1\\nHyp. 1.Forevery)inaclosedinterval oflength 6,H(\\\\;x)<H,\\nwhereHisaconstant.\\n2.1isareal-valued contraction ofx.\\n%,2\\\\”?CONCL. ForsomeJintheinterval, E((1—)? |A)=(x‘nn;] ,\\nProor. Suppose thatthetheorem isfalse.Thenaccording toEx-\\nercise8c,\\n(19) 1>w*(H*+2)>|<za»)\\n5dd\\nforevery\\\\intheinterval. Therefore,\\n2\\n(20)°[\\\\—\\nE|»)]>1-4”(1+‘\\\\-TF\\ndn 5(5H+2)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9e9082d6-a519-4bcb-9293-d7ab2fa7cd9d', embedding=None, metadata={'page_label': '257', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6] BEHAVIORALISTIC REVIEW OFESTIMATION 239\\nforevery\\\\intheinterval. Therefore, atoneendoftheinterval or\\ntheother,\\n2 5 \\\\—}\\n21 \\\\—E(1| \\\\)|>—————_- --=(H”-) (21) | (1A)|GHA4D2(+;\\nThisleadstoacontradiction through thewell-known inequality\\n(22) E(t.—a}?|)>{FA—a]a)}?=|a—FA]a)|,\\nwhichcanbederived asadirectapplication ofTheorem 1ofAppendix\\n2,oroftheSchwartz inequality, oroftheusefulidentity\\n(23) E((l—a}?|)=Vda)+(Fd-A]d)}2.@\\nIntheremaining portion ofthissection, letitbeunderstood that:\\n1.Thex,’sareaninfinitesequence ofobservations thatare,givenX,\\nidentically distributed andindependent.\\n2.x(n)={x,,---,Xn}forn=1,2,---.\\n3.l(n)isareal-valued contraction ofx(n).\\nThecontraction I(n)istobethought ofasanestimate of\\\\basedon\\nobservation ofx(n).Inthespiritoftheminimax theory itisreally\\nmixed, ratherthanordinary, estimates thatshouldbetreated here.\\nButthisentailsnoessential change inthefollowing discussion onceit\\nisrecognized thatamixedestimate is,ineffect,anordinary estimate\\nbasedonobservation ofy(n)=ps(I(m),x(n)),wherex(n)issufficient\\nfory(n),sothatH(A;y(m))=H(A;x(n))forallA.\\n4.eand6arepositive numbers.\\n5.Aoisaclosedinterval oflength 6contained intherangeof\\\\and\\nincluding agivenvalue Apo.\\nThenexttheorem showsthat,ifL(/;A)isoftheform(5.11),L(l(n);\\n)cannot ordinarily bekeptmuchsmaller thana(Ao)/nH (Ao;x1)for\\nlargen,eveninasmallinterval aboutAo.\\nTHEOREM 2IfH(A;xX)iscontinuous andpositive atAo,andif\\na(\\\\)isa non-negative function continuous atAo,then,forsufficiently\\nlargen,E((I(n)—d)?a(d) |4)>(1—e)a(An)/nAH (Ap;X1)forsome\\nrN&Ao.\\nProor. There isnolossofgenerality insupposing that«<1and\\nAosuchthat,forAAo,a(A)>a(Ao)(1—62)”andH(A;x,)%<\\nH(do;¥1)%[1+(1—€)~*]/2. UsingExercise 7b,\\nns\\n(24)H(\\\\;x(n))*=nH(5m1)\"S—H(do;m1)ll+(1—4\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5191d2c8-497a-4256-bb48-b8be9d837233', embedding=None, metadata={'page_label': '258', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='240 POINTESTIMATION [15.6\\nforXeAo.ByTheorem 1,ifn>16/6?H (Xo;x;)[(1—6)~*—1],\\nthen% )—2\\n(25)E(I(n)—»)2]a)>“SH(do;x){1+(1—4]+“|\\n(1—6)”\\n~NH(Xo;X1)\\nforsomeA¢Ap.@\\nThenexttheorem extendsTheorem 2topractically anylossfunction\\nthatistwicedifferentiable in/for1andXclosetoApo.\\nTHEOREM 3\\nHyp. 1.H(A;X,)ispositive andcontinuousatXo.\\n2\\n2.a(A)=pr-—Lil;v iscontinuous atApo. Q)=vessahd)| 0\\n3.Inequality (5.1)holdsfor\\\\inAo.\\nCoNCL. Forsufficiently largen,L(I(n);r) >(1—€)a(Ap)/nH (Ao;X1)\\nforsomeAé€Ag.\\nProor. Itmaybesupposed without lossofgenerality that«<1;\\nandthat,for1,\\\\¢Ao,L(l;A)>(1—€)“a(a)(I—A)?.\\nItmayalsobesupposed thatI(x;n)¢Ap.Thisisso,because itwould\\nsufficetoprovethetheorem foranewestimate I’(n),where l’(x;n)is\\ndefined tobethenumber inAgclosesttol(x;»),whichinturnfollows\\nfromthefactthatL(’(n); A)<L((n); d)forA€Ao.\\nThesesuppositions havingbeenmade,thetheorem isadirectcon-\\nsequence ofTheorem 2.@\\nCoROLLARY |IfL(l;\\\\)satisfies (5.1)andhastwoderivatives with\\nrespect to/continuous inAforevery\\\\andforevery /sufficiently close\\ntoA,andifH(A;x,)iscontinuous andpositive, then,forsufficiently\\nlargen,\\n(26) L*(n)>(1—€)supa(A)/nH(; x,),\\nwhereL*(n) istheminimax valueoftheestimation decision problem\\nderived fromL(l;\\\\)andx(n),unlessthesupremum inquestion isin-\\nfinite,inwhichcasenL*(n)approaches infinity.\\nOfcourse, itwouldbeenough to assumeonlythatL(J;\\\\)andH(A;x;)\\narewellbehaved atsomesequence ofvaluesof\\\\onwhichthesupremum\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='843b1c20-3cc8-4987-9387-745ddfefaac0', embedding=None, metadata={'page_label': '259', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6] BEHAVIORALISTIC REVIEW OFESTIMATION 241\\ninquestion isapproached. Inparticular, ifthesupremum isactually\\nattained atsome\\\\,theyneedonlybewellbehaved there.\\nNow,turning tothesequence ofmaximum-likelihood estimates, let\\nthembedenoted forthemoment byI(n).Itisknown thatunder\\nrathergeneralhypotheses n”*(i(n)—d)isasymptotically normalabout\\nzerowithasymptotic variance 1/H(A; x,).{Thissuggests, andex-\\namplestendtoconfirm, that,undersomesupplementary conditions,\\n 27 limnE((i(n)—)?)= (27) im((I(n) d)*)Hacx,)\\nIndeed, onesetofconditions implying (27)isstatedin[W5],butone\\nthatseemsdifficult toapply. Itcanbeshownthat(27),together with\\ntheusualasymptotic behavior of1(n),implies\\n. : a(n)(28) limnL(1(n); A)=————__n— H(;x1)\\nprovided, forexample, thatL(l;\\\\)isbounded foreachAandthatthe\\nsecondderivative ofL(l;\\\\)withrespect to/existswhen /=dX.Easily\\napplied rigorous theorems implying (28)muchless(27)donotseemto\\nhavebeenformulated yet;butexamples suggest that,underconditions\\ngeneral enough formanyapplications, (28)actually doesholduni-\\nformly, inthesensethat,fornsufficiently large,\\n1— r - 1+ »\\n(29) C=920)<iin);)<SLEnH(v; X1) nH; X;)\\nforallXsimultaneously. If(29)holds,then,inviewofCorollary 1,\\n1(n)isnearlyminimax forlargen,inthesensethat\\n(30) L*(n)>(1—€)supL(I(n);a).\\nGoodexamples canbebasedon(a)ofTables 3.1and4.1,letting\\nLil;p)beanylossfunction havingtwocontinuous derivatives in|\\nthroughout 0<1,p<1.Inparticular, theexample discussed in\\n§13.4arises,ifL(l;p)=(1—p)*®.Itcanbearguedthatthephenome-\\nnondiscussed inconnection withthatexample isprobably notrare;\\n+Somekeyreferences fortheasymptotic behavior ofT(n)are[K2],[C9],[L3],\\n[W16], [N4].Theliterature onthissubject isextraordinarily complicated. There\\nareacknowledged mathematical mistakes insomeofitsmostsophisticated publica-\\ntions;othersprovemuchlessthananybutthemostattentive readerwouldbeled\\ntosuppose; fewgiveanadequate statement oftheirrelations totheirpredecessors;\\nandthosethatmakeseriouspretentions torigorinvolve complicated hypotheses.\\nFordocumentation ofthislament see[N4],[W4],and[L3].\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c0e7608f-a2ef-4617-ac60-04ed45607cdb', embedding=None, metadata={'page_label': '260', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='242 POINTESTIMATION [15.6\\nbecause, forminimax I(n),L(1(n); A)is,Judging fromexamples, often\\nconstant and,therefore, nearlyequaltosupa(A)/nH(A; x1),butL(1;A)\\nr\\ncloselyfollowstheriseandfallofa(A)/nH(\\\\; x;).\\nTurnnowtoCriterion 9,efficiency. Itseemsdifficult todefendthe\\ncriterion asithasbeendefined inconnection with(4.8);forwhatvir-\\ntueisthereintheasymptotic normality required by(4.8)?Itisper-\\nhapsnoteworthy thatthesequence ofminimax estimates, p(n),aris-\\ninginconnection with§13.4doesnotsatisfy (4.8).Indeed, (13.4.3)\\nimplies thatn”(pi(n) —p)isasymptotically normal notaboutzero,\\nbutabout(5—p).\\nItismyimpression thattheessence oftheefficiency concept resides\\nnotinasymptotic normality, butintheoverallbehavior ofthemean\\nsquareerrorofasequence ofestimates. Itherefore propose tentatively\\ntomodify thedefinition andtocallasequence ofestimates 1(n)effi-\\ncient, ifandonlyifitsmeansquare errorbehaves atleastaswellas\\ncantypically beexpected forasequence ofmaximum-likelihood esti-\\nmates.\\nFormally, Ipropose tocall1(n)efficient, ifandonlyif,fornsuffi-\\nciently large,\\n(31) (Mn)—xP)<S12nH(A;X1)\\nforevery\\\\simultaneously.\\nIthinkthemainobjection thatislikelytoberaisedtothisproposed\\ndefinition isassociated withthepossibility thatinsomeproblems of\\ntheoretical, andperhaps alsoofpractical, importance (31)isnotsatis-\\nfiedbyanysequence ofestimates whatsoever, though themaximum-\\nlikelihood sequenceisefficient inthe‘‘official’’ sense.Insuchaprob-\\nlem,arethemaximum-likelihood estimates notasgoodforallpractical\\npurposes forsufficiently largenasthough theirvariances wereactually\\nequaltothoseofthenormal distributions towhichthey approximate?\\nItisnatural tothinksobyanalogy withothercontexts inthetheory\\nofprobability, butapproximate normality isactually nosubstitute for\\n(31)inthepresent context. Thenextparagraph isdevoted toanex-\\nampleillustrating theinadequacy ofasymptotic variance asameasure\\nofasymptotic loss.Itcanbeskipped without lossbyanyone notin-\\nterested insuchtechnicalities.\\nThebestexample Ihavebeenabletoconstruct isderivedfromase-\\nquence ofobservations thatisnotastandard sequence. Whether the\\ninteresting features thatitexhibits canactually berealizedbystandard\\nsequences, Idonotknow;buttheexample willdotoillustrate theis-\\nsue.Lety(n)beanyrealrandom variable subject tothedensity\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='87d3f077-4725-4955-984a-e06d3c8c03b7', embedding=None, metadata={'page_label': '261', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6] BEHAVIORALISTIC REVIEW OFESTIMATION 243\\nno((y—\\\\)n”;n),defined thus:¢(z;n)isthestandard normaldensity\\ninsidetheinterval [—6(n), 5()],6(m)beingsuchthatthestandard\\nnormalprobability ofthisinterval is(1—n~!);¢(z;n)=2776(2n)/4\\nfor6(2n)<|z|<n”;$(z;7)issodefinedelsewhere astobeasym-\\nmetricpositive probability density withthefirsttwomoments finite,\\nwithabounded derivative approaching zerolikez~*withincreasing z,\\nandwithuniqueabsolutemaximum atz=0.Itisevidentthatn”\\n(y(n)—d)isasymptotically normal aboutzerowithunitvariance.\\nTheinformation H(A;y(n))iswelldefined (evenaccording tothestrict\\nconditions imposed byCramér,Lemma 1,Section 32.2of[C9]).The\\nmaximum-likelihood estimates of\\\\arey(n),andthesearealso(accord-\\ningtoTheorem 3.3of[G1])minimax forthesimplequadratic loss\\nfunction (J—\\\\)?.But\\n(32) E(ly(n)—A?|)=E(y(n)? |0)\\n1\\n>2n7fy*o(yn™; n)dy\\n5(2n)n—4\\n=in]_§(2n)n—”*| 6(2n),\\nwhichdoesnotsatisfy (31).Evenforthebounded, andtherefore more\\nrealistic, lossfunction,\\n(33) L(l;x)=min{1,[2—}*},\\nitfollows easilyfromTheorem 3.3of[G1]thateveryestimate must\\nsomewhere incuralossatleastasgreatasthelowerboundestablished\\nby(32).Tosummarize, therearenoestimates efficient inthesense\\nof(81),noreveninthesensethatwouldarisefrom(31)onreplacing\\nthesimplequadratic lossfunction byabounded lossfunction; these-\\nquence ofestimates y(n)isefficient intheofficial sense,so tospeak,\\nbutdoesnot,ofcourse, resultinlossesoftheorderofn7!.\\nWhatcanbesaidinpositive Justification ofthecriterion ofefficiency\\nasdefinedbv(81)orthelike?Roughly, theelements ofsuchase-\\nquencenearlydominate everyestimate foreverysmooth lossfunction.\\nAlittlemoreprecisely, forlargen,thelossassociated withanelement\\nofasequence efficient inthesenseof(31)isatmostlargerbyasmall\\nfraction thanthatofanyotherestimate, exceptpossibly insomeshort\\nintervals.t| Themaximum lossofsuchanelement isatmostlargerby\\nasmallfraction thantheminimax loss,sotheelements ofthesequence\\naretypically nearlyminimax. Moreover, theytypically haveconsid-\\n+Ithasactually beendemonstrated thatthetotallength oftheseexceptional\\nintervals (withinanyfixedinterval) issmall[L3].\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1e71f4aa-2f23-4637-b44e-3f53c0cb1015', embedding=None, metadata={'page_label': '262', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='244 POINTESTIMATION [15.7\\nerablysmaller lossesthananyminimax estimate, exceptinshortinter-\\nvalsthataretypically veryimprobable aprioriinthepersonal sense.\\nThustheprinciple ofadmissibility, theminimaxrule,andthepersonal-\\nisticconcept ofprobability combine tosuggest thatefficiency asde-\\nfinedby(31)isapromising guideinthesearchforgoodestimates.\\nAnextensive critique oftheconcept ofefficiency, including much\\nmaterial onitshistory, hasbeengivenbyLeCam in[L3],whichunfor-\\ntunately wasnotavailable tomeinitsentirety asIwrotethissection.\\nR.A.Fisher’s nameisthemostprominent inthehistory ofmaximum-\\nlikelihood estimation andefficiency. Somehistorical detailsaregiven\\nin[N4]andonp.45ofVol.IIof[K2].\\n7Abehavioralistic review, concluded\\nCriteria 6(unbiasedness) and7arenowtheonlyonesinthelistfor\\nwhichIhavenotsuggested somejustification intermsofthetheoryof\\ndecision problems, and,indeed, Icannot. Unbiased estimates fascinate\\nmanytheoretical statisticians, including myself,andthestudyofthem\\nundoubtedly hascertainvaluable by-products. Yetitisnowwidely\\nagreedthataseriousreasontopreferunbiased estimates seemsnever\\ntohavebeenproposed.\\nThreeweakdefenses aresometimes heard. First,unbiasedness isas-\\nsertedtohaveanintuitive appeal; whether itdoesornotdepends, of\\ncourse,ontheexperience oftheintuiter. Second, averages ofincreas-\\ninglymanyunbiased estimates aretypically consistent. Ifthisisa\\nvirtue, itisalimitedoneandpertains totheunbiased estimate notas\\nanestimate, butasastepinthedefinition ofotherestimates. Third,\\nanallusion ismadetoequity. If,forexample, ithasbeenagreedthat\\nonepartywillbuyasackofsugarfromanother atsomuchperpound,\\nitseemsfairthatthenominal weight ofthesackbedetermined byun-\\nbiasedestimate. Thisethicalconclusion couldperhaps begivensome\\njustification intermsofapproximately linearutilityfunctions oralong-\\nrunargument, though thereisdanger offallingintosuchpitfalls asthe\\nconclusion thataccuracy isunimportant forequity;anditmightfind\\nsomeapplication inthetheory ofbarter;butitseems,atbest,tangen-\\ntialtoestimation inthesenseofthepresent chapter.\\nForaproperappraisal ofthecriterion ofunbiasedness itshouldbe\\nrealized that,evenif\\\\)admitsanunbiased estimate, manynot-at-all\\npathological functions of\\\\(whichcaninturnberegarded asparame-\\nters),mayfailtodosoandthatsuchunbiased estimates as\\\\doesadmit\\nmaybepreposterous. ‘Thesephenomena arebothillustrated bythe\\nfollowing simpleexample. Letxbeconfined totwovalues,say1and\\n2;letPU |1)=1-—PQ |\\\\)=A;andlet\\\\beconfined totheinterval\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d242b2c8-14e4-43b5-b5ca-26a1c27e6202', embedding=None, metadata={'page_label': '263', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.7] BEHAVIORALISTIC REVIEW OFESTIMATION 245\\n[1/3,2/3].Then,bydefinition, 1isanunbiased estimate of¢(A),if\\nandonlyif1(1)A+1(2)(1—A)=1(2)+(11)—U(2))A=o(A)—a con-\\nditionthatcanbemet,ifandonlyif¢islinear. Suppose, forexample,\\n#(A)=JforeveryA,then/(1)=1,1(2)=0definestheonlyunbiased\\nestimate of¢(A).Thisestimate isworse,according toanemphatic\\nvariant ofCriterion 3,thanthe biased estimate I’suchthatl’(1)=2/3\\nand1'(2)=1/3;forIl’(wheniterrsatall)errsinthesamedirection as\\n1,butnevernearly asfar.\\nAsforCriterion 7,itisonfirstencounter appealing topostulate that,\\nif1isusually closerto\\\\thanI’is,then1isbetterthan1’.But,speaking\\natleastformyself, theinitialappeal ofCriterion 7seemstohavebeen\\nboundupwiththeconjecture thatCriterion 7isinsomesenseofthe\\nsamesortasCriterion 3.Theexample givenunderCriterion 7almost\\nentirely evaporates theconjecture, andwithittheappeal.\\nInthepaper[P5]inwhich thecriterion isputforward forconsidera-\\ntionandexploration, Pitman mentions thatthecriterion seemsac-\\nceptable incontexts where‘‘thedeviltakesthehindmost.”’ Thisallu-\\nsiontothedevilseemstooffernojustification forthecriterion asacri-\\nterionofestimation, forIunderstand theallusion toreferonlytothe\\nfollowing kindofdecision problem, which isquiteremotefromestima-\\ntion asordinarily understood andishardlyeverencountered: Aperson\\nmustchoosebetween |and1’,winning aprizeiftheestimate ofhis\\nchoicefallscloserto\\\\thandoestheotherone.\\nAccording toPitman, therelationship of“better than,” or‘closer\\nthan”’ashecalls it,definedbyCriterion 7,isnotnecessarily transitive.\\nHeargues,IthinkwithsomeJustice, thatthisbreakdown oftransitivity\\ndoesnotinitselfinvalidate thecriterion whenthecriterion isapplied\\ntoselectthe‘‘best’’fromsomeprescribed classofestimates; but‘‘best”’\\ncannotherebetakenliterally.\\nCriterion 7isunusual inthatitdepends onthejointconditional dis-\\ntributions ofpairsofestimates ratherthanonthedistributions ofeach\\nestimate considered separately. Onanyordinary interpretation ofes-\\ntimation knowntome,itcanbeargued (asitwasunderCriterion 3)\\nthatnocriterion needdependonmorethantheseparate distributions.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4a98bda-1ebc-43e5-980c-9f6241e625c6', embedding=None, metadata={'page_label': '264', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 16\\nTesting\\n1Introduction\\nInprinciple, this chapter onthe statistical process oftesting (often\\nreferred tomore fully asmaking tests ofhypotheses orsignificance\\ntests) might have been organized onthe pattern ofthe preceding chap-\\nteronpoint estimation: astatement ofverbalistic ideas, followed by\\nmotivation and criticism interms ofbehavioralistic ideas. But Iam\\ndissuaded from repeating that pattern byseveral considerations. It\\nwould, inthe first place, beneedlessly repetitious. Thus, inthe pres-\\nence ofthe preceding chapter Ineed mention only inpassing that suffi-\\ncient statistics and symmetry play the samerole intesting asinother\\nobservational decision problems, and that acertain scheme oftesting,\\nclosely related tomaximum-likelihood estimation, has asymptotic, or\\nlarge sample, virtues. Again, the pattern ofthe preceding chapter is\\nless attractive here, because the criteria for tests developed inthe ver-\\nbalistic tradition donot onthewhole seem tohave such satisfying be-\\nhavioralistic motivation asdotheir counterparts inthe theory ofpoint\\nestimation. Finally, it is Inappropriate toattempt anything like a\\ncomplete list ofverbalistic criteria for tests here, especially inview of\\nthe availability oftwo excellent and mutually complementary keyref-\\nerences (Chapters 21, 26,and 27of[K2]; and [L4]).\\nThe organization actually adopted isthis: First, testing andcriteria\\nfor tests are discussed from afrankly behavioralistic viewpoint. In\\nthis discussion ideas stemming from the verbalistic tradition are used\\nfreely, and somecriteria ofthe verbalistic tradition are criticized. Sec-\\nond, anattempt ismade toanalyze some ofthe important statistical\\nsituations towhich the theory oftesting isordinarily applied. Itis\\nbecoming increasingly recognized that many ofthese applications are\\nvery crude, and that their replacement bysounder procedures consti-\\ntutes some ofthemost important and provocative statistical problems\\noftoday.\\nTerms introduced inboldface in this chapter areamong the most\\nfrequent inordinary statistical usage. The definitions given are in-\\n246\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09c98196-b7bb-4411-89df-01e76cad8810', embedding=None, metadata={'page_label': '265', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.2] ATHEORY OFTESTING 247\\ntendedtobeinreasonable accordwiththatusage,butsomesmallcon-\\ncessions aremadetotheparticular forminwhichthetheoryoftesting\\nisexpressed here.\\n2Atheoryoftesting\\nVerbalistically, theproblem oftestingmeanstoguess,onthebasis\\nofobservation, whichoftwodisjointandmutually exhaustive hypoth-\\nesesobtains. Behavioralistically, thiswouldgenerally beagreedto\\npointtothedefinition: Aé<stingproblem isanobservational decision\\nproblem derivedfromexactlytwobasicactsfoandf;.Thesetwobasic\\nactsarecalled(forareasonthatwillsoon be clear)accepting andre-\\njectingthenullhypothesis, respectively.\\nConsidered abstractly asbilinear games, testingproblems may,so\\nfarasIknow,havenospecial featurebeyond theuninteresting one\\nthatoneoftwof’sisappropriate toeach7.But,considered as obser-\\nvational problems, testingproblems dopresent someinteresting special\\nfeatures. Inthefirstplace,sinceatleastoneofthetwobasicactsis\\nappropriate toeach7,thesetJofall2’scanbepartitioned intothree\\nsets,Ho, Hi,andN,definedthus:\\nL(fo;t)=OandL(fi;7)>0forzeHp,\\n(1) L(fp37)>OandL(fi;7)=OforieAy,\\nL(fo;7)=0andL(f;;7)=0forieN.\\nWhenitisrecalled thatthe2’scorrespond toapartition B;ofS,the\\nsetsHo,Hi,andNmay,withslight clashoflogicalgears,beregarded\\nasthreeeventspartitioning S.Thetraditional names ofHoandA,\\narethenullandthealternative hypothesis, respectively; NV,beingquite\\nunimportant andofteneitherignored ormadevacuous bysometrick\\nofdefinition, hasnosuchname. Rejecting thenullhypothesis whenit\\ndoesinfactobtainandaccepting itwhenitdoesnotobtainarecalled\\nerrors,morespecifically errorsofthefirstandsecond kind,respec-\\ntively.\\nAtestisaderived actofatestingproblem. Atestmayconveniently\\nbeidentified withthereal-valued contraction zoftheobservation x,\\nsuchthatz(x)istheprobability prescribed bythetestforrejection of\\nthenullhypothesis incasexisobserved. Anunmixed test(whichwas\\nuntilrecently theonlykindcontemplated) corresponds toazconfined\\ntothetwovalues0and1,whichrespectively implyoutright acceptance\\nandrejection ofthenullhypothesis.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aaeb81db-7602-4a2e-98a9-35e9b9d301ef', embedding=None, metadata={'page_label': '266', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='248 TESTING [16.2\\nTheloss associated with the test zwhen 7obtains is clearly\\n(2) L(z; 4)=Lo;)E(1 —z|4)+Li;EC|4)\\n=Li{f,; i)E(z |7) fori eHo\\n=L(fo;i)[1— E(7z|a)] forte Hy\\n=0 forzeN.\\nThe functions E(z |7)and [1—E(z |i)]are, respectively, the proba-\\nbility ofrejecting and accepting the null hypothesis with the test z\\nwhen 7obtains. There isobviously nothing tochoose between them\\ninimportance orconvenience, each being equivalent tothe other.\\nThey arecommonly called thepower function, and operating charac-\\nteristic, respectively.\\nInview of(2), one test zdominates another z’,ifand only if\\nE(z|\\\\i) <E(z\\'|1) fori eHp\\n(3) . ie .E(z|1) >E(z |2) for 7¢Hy;\\nor,again, ifand only ifthe probability oferror with z’isatleast as\\ngreat aswith zforevery 7.Thus, dominance, admissibility, and equiv-\\nalence depend onthe basic loss function, L(f,; 7),only insofarasthat\\nfunction determines Hpand H,. This isnot only remarkable but also\\nuseful; forHpand H,;may well beclearly defined incontexts where\\nthebasic loss isvague, orotherwise illdetermined.\\nIfzisadmissible inthe spirit of(3)relative toapair ofsetsHoand\\nHy, then (if~isforthemoment admitted asapossible value foraloss)\\nthere exists abasic loss function leading toHoand Hyand having z\\nasitsessentially unique minimax. Indeed, let\\nL(fp;}2) =(1—E(z|a)}\"! fori eHy\\n=0 elsewhere;\\n(4) . I ;L(f\\\\; 2)=E(z| 7) forieHo\\n=0 elsewhere.\\nWith this loss and reckoning 0-« =0(as isappropriate here), L(z |1)\\n=1or0,according asthere isorisnot positive probability ofmaking\\nanerror at2with z.Inview of(2)and (4),any minimax z’not equiva-\\nlent toZwould strictly dominate z,contrary totheassumption that z\\nisadmissible. The moral ofthat conclusion can beput thus: Without\\nspecial assumptions about the basic loss, the principle ofadmissibility\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b2c9cf0c-2fea-4eed-b88f-c08f70b405da', embedding=None, metadata={'page_label': '267', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16,2] ATHEORY OFTESTING 249\\nandtheminimaxruleleadtonocriteria expressible solelyintermsof\\nHo,H,,andtheconditional distributions oftheobservation xother\\nthanthatofadmissibility itself.Whether someother objectivistic prin-\\nciplecouldjustifysuchcriteriamaybeconsidered anopenquestion,\\nbut,asIhavealready said(in§15.1),noothergeneral objectivistic\\nprinciples havebeenseriously maintained.\\nItisnatural, forexample, todemand thatzhavethesamesymmetry\\nasP(x |?)andHpandHi;butthatcriterion cansurelynotbejustified\\natall,unlessthebasiclossisalsoassumed tohavethesamesymmetry,\\nthejustifiability ofwhichinturndepends onthecase.\\nTotakeanother important example,itisoftenproposed thatasatis-\\nfactory testmustbeunbiased,{ thatis,itspowerfunction mustnever\\nbehigherinHpthaninH,.Moreformally, thetestzisunbiased, if\\nandonlyif\\n(5) E(z|tp)<E(z|i)\\nforevery%)¢Hpandevery 72;¢Hj.\\nAssuming thatL(fp;7)andL(f,;7)areconstant inH,andHp,re-\\nspectively, itwillbeshownthatanymmimax mustbeunbiased. Asa\\nsteptoward thatdemonstration, consider atestingproblem asamini-\\nmaxproblem, without anyspecialassumption aboutthebasicloss.\\nItispossible thatL*=0,inwhichcasetheminimaxtests areallequiv-\\nalentandallunbiased. Putting thatpossibility aside,Iassert,andwill\\nshow,that(undertheusualmathematical simplifications)\\n(6) maxL(z;7)=maxL(z;7)=L*\\nteHo 1éHy\\nforanyminimax z.Itisobvious thatneithermaximum exceeds L”*,\\nandalsothatoneortheothermustequalL*.Butsuppose, forexam-\\nple,thatthesecondmaximum wereactually lessthanL*,andconsider\\nz’=azwith0<a< 1.According to(2),ifz’issubstituted forz,\\nthefirstmaximum in(6)willbedepressed, and,forasufficiently close\\nto1,thesecondwouldremain actually lessthanL*,whichcontradicts\\ntheassumption thatzisminimax, establishing (6).\\nNowmakethespecialassumption that\\n7) L(fo;1)=Afor7¢Hy\\nL(f1;7)=Bfor7¢Ho,\\nandsuppose thatzcouldbeminimax butbiased. Therewouldthen\\n}Adefinition unifying thevarious concepts ofunbiasedness instatistics isput\\nforward in[L5].\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09f881ba-21c1-4424-82d3-68316016bcbe', embedding=None, metadata={'page_label': '268', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='250 TESTING [16.2\\nexist79¢eHpand7;¢H;suchthat\\n(8)L*=L(z;%))=BE(z|%) =A—AE(z|71)=L(z;14),\\nandsuchthatH(z;t)>H(z;7,;).Butconsideration ofthetestthat\\nsimply assigns toevery+thenumber 8midway between E(z;79)and\\nE(z;11)showsthatzcouldnotbeminimax.\\nThecondition (7)isareasonable assumption insometesting problems,\\nand,where (7)issatisfied, thecriterion ofunbiasedness hassuchsup-\\nportastheminimaxrulecangive.Inmanyother typical testingprob-\\nlems,however, thereareborderline errorsthathardlymatteratallbut\\ncanscarcely beprevented, andserious errorsthatcanlargelybepre-\\nvented. Thefollowing example, whichcanbevariedtosuitdiverse\\ntastes,showsthatitcanbefollytoinsistonunbiasedness insuch\\nproblems.\\nLet7takethethreevalues0,1,2,andletxtakethevalues0and1\\nwithconditional probabilities defined thus:\\n(9)P(0|0)=99/100, P(O|1)=0, P(O|2)=1.\\nLetthebasiclossbedefinedbythecondition that7¢Hpor7¢Hy,ac-\\ncording as7=0ornot,andby\\n(10) L(f,;0)=1,L(fo;1)=1,L(fo;2)=1/101.\\nThen\\nL(z;0)=[992(0)+2(1)]/100\\n(11) L(z;1)=1—2(1)\\nL(z;2)=[1—2(0)]/101.\\nItiseasilyverified thattheonlyminimax z*isdefinedbyz*(0)=0,\\n2*(1)=100/101, andthatL(z*;7)=L*=1/101forevery 7.Butit\\nisalsoeasilyverified thattheonlyunbiased testsareabsurd inthat\\ntheyignoretheobservation x;theyareinfactjustthoseforwhich\\n2(0)=a(1).\\nIthasuntilquiterecently beensaidbymanythatattention should\\nbeconfined totestssuchthatthereisafixedprobability a(calledthe\\nsizeofthetest)ofmaking anerrorofthefirstkindforevery7¢Ho.\\nIndeed, thecriterion ofsizehasoftenbeentakensoseriously astobe\\nincorporated intotheverydefinition ofatest.Thoughmanyimpor-\\ntanttestshappen tohaveasize,othersequally important donot;so\\nitnowseemstoberecognized [L4]thatthepossession ofasizecannot\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e634aeee-fc6e-439a-8d10-c31f65ba7f95', embedding=None, metadata={'page_label': '269', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.2] ATHEORYOFTESTING 251\\nbetakenseriously asacriterion.t Totakeaneveryday example, con-\\nsiderthebinomial distributions\\n101\\n(12) P(z|p)=(;)wa—p=,\\nwheretheparameter pconfined to[0,1]plays the roleof7andx=0,\\n--+,101;andsuppose thatHpisthehypothesis thatp<1/2.Atest\\nofsizeaisatestforwhich\\n100\\n(13) D2(2)(.rae—p==a\\nforallp<1/2.Thisobviously implies\\n(14) Dete)-al(\"\"(2)=0x1—p\\nforallp<1/2,whence z2(x)=aforeveryx.Soonlyabsurd tests\\nhavesize,inthisexample, though thereareclearly testsherethatare\\nquitesatisfactory formanyapplications, forexample, letz(x)equal 0\\nor1according asx<50orx>50.\\nInviewofthecriticism justmade,thereisatendency toredefine\\nsizesothatanytesthasaszzea,namely,\\n(15) a=pzmaxE(z|2).\\nt&€Ho\\nIntermsofthisdefinition ofsize,aconcept oftestingsomewhat differ-\\nentfromthatproposed inthissectionhasbeendefinedanddefended\\n(Wald, p.21of[W3],andLehmann, pp.17-18of[L4];namely, itis\\npostulated thatatestistobe chosen notfromamongallpossible tests,\\nbutonlyfromamongthosehavingsizea(inthesenseof(15))given\\naspartofthetestingproblem.{ Thisconcept oftesting isnotdefended\\ntotheexclusion oftheoneproposed here,butitisasserted bythe\\nauthors citedtobemorerealistic forsomeproblems. Theargumentsof\\nbothauthors onthispointaresimilarand,Ithink,quiteweakintwo\\ncrucial places, fortheadvantage issupposed toflowinsomeunspeci-\\nfiedwayfromtheundemonstrated impossibility ofcomparing prefer-\\nencesforconsequences ofqualitatively different kinds. Itseems,ifI\\nmaybeallowedsuchaconjecture, thattheconcept oftestingundera\\n|Statisticians interested intheBehrens-Fisher problemmaybeinterested inpp.\\n35.173a—-b of[F6],whichhingeonthequestion ofsizeasacriterion.\\ntTheconstraint actually imposed, especially byLehmann [L4],isthatthesize\\nbeatmosta.But,asLehmann explains, thisdifference ismoreapparent thanreal.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8967989f-a818-40f0-adb4-4029637f799d', embedding=None, metadata={'page_label': '270', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='252 TESTING [16.3\\nconstraint ofsizerepresents aProcrustean attempt tofitthe(older)\\nNeyman-Pearson theory oftesting hypotheses toocloselywiththe\\n(newer)minimax theory. Itisnottobedenied, ofcourse, thatthere\\nmaysometimes beamathematical advantage instudying andcompar-\\ningtestsofgivensize.\\nItshouldbementioned, beforeconcluding thesubject, thatanythe-\\norytaking sizeseriously introduces anasymmetry ofthetheorywith\\nrespect toHpandA,anasymmetry thatissurelynotalwaysappropri-\\nate.\\nSignificance level,orlevelofsignificance, isasynonym (neglecting\\naslightdistinction madein[L4])ofsize,probably morewidelyused\\nthansizeitself.\\n3Testing inpractice\\nThetheoryoftestingadmitssomefairlyrealistic applications, but\\nthepresent stateofstatistics issuchthatthetheoryoftesting isin-\\nvokedmoreoftenthannotinproblems onwhich itdoesnotbear\\nsquarely. Thissection discusses typical applications ofthetheory,\\npointing outtheshortcomings Iamawareof.\\nThedevelopment ofthetheory oftestinghasbeenmuchinfluenced\\nbythespecialproblem ofsimpledichotomy, thatis,testingproblems\\ninwhichHyandHihaveexactlyoneelement each.Simpledichotomy\\nissusceptible ofneatandfullanalysis (asinExercise 7.5.2andin\\n§14.4),likelihood-ratio testsherebeingtheonlyadmissible tests;and\\nsimpledichotomy oftengivesinsight intomorecomplicated problems,\\nthough thepointisnotexplicitly illustrated inthisbook.\\nCoinandballexamples ofsimpledichotomy areeasytoconstruct,\\nbutinstances seemrareinreallife.Theastronomical observations\\nmadetodistinguish between theNewtonian andEinsteinian hypotheses\\nareagood,butnotperfect, example, andIsuppose thatresearch inMen-\\ndeliangenetics sometimes leadstoothers. There is,however, atradi-\\ntionofapplying theconcept ofsimpledichotomy tosomesituations to\\nwhich itis,tosaythebest,onlycrudely adapted. Consider, forex-\\nample,thedecision problem ofapersonwhomustbuy,fo,orrefuseto\\nbuy,f,,alotofmanufactured articlesonthebasisofanobservation x.\\nSuppose that7isthedifference between thevalueofthelottotheper-\\nsonandthepriceatwhich thelotisoffered forsale,andthatP(x |1)18\\nknown totheperson. Clearly, Ho, H,, andNaresetscharacterized\\nrespectively by1>0,7<0,72=0.Thisanalysis ofthis,andsimilar,\\nproblems hasrecently beenexplored intermsoftheminimax rule,for\\nexample bySprowls [S16]andalittlemorefullybyRudy[R4],andby\\nAllen[A3].Itseemstomenaturalandpromising formanyfields of\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='596ae91e-e29f-4472-8833-90e3b06b3727', embedding=None, metadata={'page_label': '271', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.3] TESTING INPRACTICE 253\\napplication, butitisnotatraditional analysis. Onthecontrary, much\\nliterature recommends, ineffect,that the personpretend thatonlytwo\\nvaluesof7,i9>0and7,<0,arepossible andthatthepersonthen\\nchooseatestfortheresulting simpledichotomy. Theselection ofthe\\ntwovalues zpand7;islefttotheperson,thoughtheyaresometimes\\nsupposed tocorrespond totheperson’s judgment ofwhatconstitutes\\ngoodqualityandpoorquality—terms reallyquitewithout definition.\\nTheemphasis onsimpledichotomy istempered insomeacceptance-\\nsampling literature, whereitisrecommended thatthepersonchoose\\namongavailable testsbysomelargelyunspecified overallconsideration\\nofoperating characteristics andcosts,andthathefacilitate hissurvey\\noftheavailable testsbyfocusing onapairofpointsthathappen toin-\\nteresthimandconsidering thetestwhoseoperating characteristic\\npasses(economically, inthecaseofsequential testing) through the\\npairofpoints. Thesetraditional analyses arecertainly inferior in the\\ntheoretical framework ofthepresent discussion, andIthinktheywill\\nbefoundinferior inpractice.\\nTomakeasmalldigression, thereisacomplication inconnection with\\ntestingwhether tobuythatisnotordinarily envisaged bystatistical\\ntheory;namely, theeconomic reaction between thebuyerandthesup-\\nplier. If,forexample, thesupplier knowsthetestthebuyer isgoing\\ntoapply,thatknowledge willinfluence thequality ofthelotsupplied.\\nThereseemstobelittle, ifany,successful workohtheeconomic prob-\\nlemthusraisedaboutthegame-like behavior ofthetwopeopleinvolved\\n(cf.pp.331,340,and346of[W6)}).\\nTheproblem whether tobuyalotobviously hasmanyformalcoun-\\nterparts inotherdomains. Insomeofthemitisparticularly clearthat\\npurelyobjectivistic methods donotsuffice.Toillustrate, imagine two\\nexperiments: onedesigned todetermine whether itisadvantageous to\\naddacertainsmallamount ofsodium fluoride tothedrinking waterof\\nchildren, theothertodetermine whether thesameamount ofoilof\\npeppermint isadvantageous. Granting thateachofthetwoadditions\\ncanbemadeatthesamecashcostforlaborandmaterial andthatthe\\ndesigns ofthetwohypothetical experiments differonlyintheinter-\\nchange oftherolesofsodium fluorideandoilofpeppermint, thecorre-\\nsponding testingproblemsareobjectivistically completely parallel, that\\nis,thesamewithregardtolossfunction andconditional probability of\\ntheobservations. Butitmustbeacknowledged, I:think,thatthepeople\\nactually charged withthedecision ineitherofthesetwocaseswould\\nandshouldtakeintoaccount opinions theyhadbeforetheobservation.\\nForexample, theymightoriginally haveconsidered itnearlyimpossible\\nthattheoilofpeppermint couldresultinanyhygienic advantage large\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8db0825e-a86d-4059-ad72-e0e39fa39f73', embedding=None, metadata={'page_label': '272', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='254 TESTING [16.3\\nenough tocompensate foreventhesmallcostofitsadministration, but,\\ninviewofrecentdentalresearches onthesubject, theymightnothave\\nconsidered itatallunlikely thatthesodium fluoride shouldhavean\\noveralladvantage. Inthatcase,parallel observations inthetwoex-\\nperiments wouldnotalwaysleadtoparallel decisions. Objectivists\\ntypically admitsuchapossibility butgoontosaythatitisunreasonable\\ntoisolatetheexperiment andthatitisthetotality ofinformation bear-\\ningonthesubject thatshouldbetreated objectivistically. Ifobjectiv-\\nistscouldgiveamoredetailed discussion ofhowtodealwithsucha\\ntotality ofinformation, itmightdomuchtoclarifytheirposition.\\nIturnnowtoadifferent and,atleastforme,delicate topicinconnec-\\ntionwithapplications ofthetheoryoftesting. Muchattention isgiven\\nintheliterature ofstatistics towhatpurport tobetestsofhypotheses,\\ninwhichthenullhypothesis issuchthatitwouldnotreallybeaccepted\\nbyanyone. Thefollowing threepropositions, though playful incon-\\ntent,aretypical informoftheseextreme nullhypotheses, asIshallcall\\nthemforthemoment.\\nAThemeannoiseoutput ofthecerealKrakl isalinearfunction of\\ntheatmospheric pressure, intherangefrom900to1,100millibars.\\nBThebasalmetabolic consumption ofspermwhales isnormally\\ndistributed [W11].\\nCNewYorktaxidrivers ofIrish,Jewish,andScandinavian extrac-\\ntionareequally proficient inabusive language.\\nLiterally totestsuchhypotheses astheseispreposterous. If,forex-\\nample,thelossassociated withf,iszero,except incaseHypothesis A\\nisexactly satisfied, whatpossible experience withKraklcoulddissuade\\nyoufromadopting f,?\\nTheunacceptability ofextreme nullhypotheses isperfectly well\\nknown; itiscloselyrelated totheoftenheardmaximthatscience dis-\\nproves,butneverproves, hypotheses. Theroleofextreme hypotheses\\ninscienceandotherstatistical activities seemstobeimportant butob-\\nscure.Inparticular, though I,likeeveryone whopractices statistics,\\nhaveoften‘‘tested’’ extreme hypotheses, Icannotgiveaverysatisfac-\\ntoryanalysis oftheprocess, norsayclearlyhowitisrelatedtotesting\\nasdefined inthischapter andothertheoretical discussions. Nonethe\\nless,itseemsworthwhiletoexplore thesubject tentatively; Iwilldo\\nsolargely intermsoftwoexamples.\\nConsiderfirsttheproblem ofacerealdynamicist whomustestimate\\nthenoiseoutput ofKraklateachoftenatmospheric pressures between\\n900and1,100millibars. Itmaywellbethathecanproperly regardthe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6763d1a-9381-46ab-8fc1-9a5a4c25d51f', embedding=None, metadata={'page_label': '273', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.3] TESTING INPRACTICE 255\\nproblem asthatofestimating thetenparameters inquestion, inwhich\\ncasethereisnoquestion oftesting. Butsuppose, forexample, that\\noneorbothofthefollowing considerations apply. First,theengineer\\nandhiscolleagues mayattachconsiderable personal probability tothe\\npossibility thatAisverynearlysatisfied—very nearly,thatis,interms\\nofthedispersion ofhismeasurements. Second, theadministrative,\\ncomputational, andotherincidental costsofusingtenindividual esti-\\nmatesmightbeconsiderably greaterthanthatofusingalinearformula.\\nItmightbeimpractical todealwitheitheroftheseconsiderations very\\nrigorously. Oneroughattack isfortheengineerfirsttoexamine the\\nobserved dataxandthentoproceed eitherasthoughheactually be-\\nlievedHypothesis Aorelseinsomeotherway.Theotherwaymightbe\\ntomaketheestimate according totheobjectivistic formulae thatwould\\nhavebeenusedhadtherebeennocomplicating considerations, orit\\nmighttakeintoaccount different butrelated complicating considera-\\ntionsnotexplicitly mentioned here,suchastheadvantage ofusinga\\nquadratic approximation. Itisartificial andinadequate toregardthis\\ndecision between oneclassofbasicactsoranother asatest,butthat\\niswhatincurrent practice weseemtodo.Thechoiceofwhichtest\\ntoadoptinsuchacontext isatleastpartlymotivated bythevague\\nideathatthetestshouldreadilyaccept,thatis,resultinactingasthough\\ntheextreme nullhypotheses weretrue,inthefarfetched casethatthe\\nnullhypothesis isindeedtrue,andthattheworsetheapproximation of\\nthenullhypotheses tothetruththelessprobable shouldbetheac-\\nceptance.\\nThemethod justoutlined iscrude,tosaythebest.Itisoftenmodi-\\nfiedinaccordance withcommonsense, especially sofarasthesecond\\nconsideration isconcerned. Thus, ifthemeasurements aresufficiently\\nprecise,noordinary testmightacceptthenullhypotheses, fortheex-\\nperiment willleadtoaclearandsureideaofjustwhatthedepartures\\nfromthenullhypotheses actually are.But,iftheengineer considers\\nthosedepartures unimportant forthecontext athand,hewilljustifiably\\ndecidetoneglectthem.\\nRejection ofanextreme nullhypothesis, inthesenseoftheforegoing\\ndiscussion, typically givesrisetoacomplicated subsidiary decision\\nproblem. Someaspects ofthissituation haverecently beenexplored,\\nforexamplebyPaulson [P3],[P4];Duncan [D11!,[D12];Tukey[T4],\\n(T5];Scheffé [(S7];andW.D.Fisher [F7].\\nTosummarize abstractly, Iwouldsaythat,incurrent practice, so-\\ncalledtestsofextreme hypotheses areresorted towhenatleastalittle\\ncredence isattached tothepossibility thatthenullhypothesis isvery\\nnearlytrueandwhenthereissomespecialadvantage tobehaving as\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b430f3e4-4160-4f9d-8dc7-eba9521a1bcb', embedding=None, metadata={'page_label': '274', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='256 TESTING [16.3\\nthough itweretrue.Oneotherillustration willmakeitclearthatpoint\\nestimation isnotessential tothesituation andthatbeliefintheapproxi-\\nmatetruthofthenullhypothesis alonedoesnotalways justify testing.\\nConsider thepersonnel manager ofagreatNewYorktaxicompany.\\nWishing, ofcourse, thathisdriversshouldbeasproficient aspossible,\\nhewould,undersimplecircumstances, hireexclusively fromthe na-\\ntional-extraction groupthathadobtained thehighestmeanscoresina\\nstandard proficiency examination; forwhyshouldhenotbeguidedby\\napositive indication, howeverslight? Astatistical testoftheextreme\\nHypothesis Cwouldnot,therefore, becalledfor,ashasbeenpointed\\noutingeneral termsbyBahadur andRobbins [B3].Evenstrongbe-\\nliefthatethnicdifferences areextremely smallintherespect inquestion\\nwouldnotalonebeanyreasonfordeparting fromthissimple policy,\\ndictated bytheprinciple ofadmissibility—quite incontrast totheex-\\nampleframed around Hypothesis A.If,however, publicopinion, a\\nshortage oflabor,oradministrative difficulty militates againstanydis-\\ncrimination atall,themanager mayresorttoatestbasedontheex-\\namination scores.\\nInpractice, testsofextreme hypotheses aretypically chosenfroma\\nrelatively smallarsenal ofstandard types,orfamilies, eachfamilycon-\\nsistingofoneunmixed testateverysignificance level(assizeisalways\\ncalledinthiscontext). Inpublications, itisstandard practice not\\nsimplytoreporttheresultofatest,butrathertoreportthatlevelof\\nsignificance forwhichthecorresponding testoftherelevant family\\nwouldbeontheborderline between acceptance andrejection. The\\nrationale usually givenforthisprocedure isthat1tenables eachuser\\nofthepublication tomakehisowntestatthesignificance levelhedeems\\nappropriate tohisparticular problem. Thusthesignificance level1s\\nsupposed toplaymuchthesamepractical roleasasufficient statistic.\\nAninteresting contribution tothetheoryofextreme hypotheses is\\ngivenbyBahadur[B1]inthespecialcontext ofthetwo-sided ¢-test.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='12107f30-8ec9-4ed8-80e9-182bcf43332a', embedding=None, metadata={'page_label': '275', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 17\\nInterval Estimation\\nandRelated Topics\\n1Estimates oftheaccuracy ofestimates\\nThedoctrine isoftenexpressed thatapointestimate 1soflittle,or\\nno,valueunlessaccompanied byanestimate ofitsownaccuracy. This\\ndoctrine, whichforthemoment Iwillcallthedoctrine ofaccuracy est-\\nmation,maybelittleold-fashioned, butIthinksomecritical discus-\\nsionofithereisinorderfortworeasons. Inthefirstplace,thedoctrine\\nisstillwidelyconsidered tocontainmorethanagrainoftruth.For\\nexample, manyreaders willthinkitstrange, andevenremiss, thatI\\nhavewritten alongchapter (Chapter 15)onestimation without even\\nsuggesting thatanestimate shouldbeaccompanied byanestimate of\\nitsaccuracy. Inthesecond place,itseemstomethattheconcept of\\ninterval estimation, which isthesubject ofthenextsection, haslargely\\nevolved fromthedoctrine ofaccuracy estimation andthatdiscussion\\nofthedoctrine will,forsome,pavethewayfordiscussion ofinterval\\nestimation.\\nThedoctrine ofaccuracy estimation isvague,evenbythestandards\\noftheverbalistic tradition, foritdoesnotsaywhatshouldbetaken\\nasameasure ofaccuracy, thatis,whatanestimate ofaccuracy should\\nestimate. Anymeasure wouldberatherarbitrary; atypical one,here\\nadopted fordefiniteness, istheroot-mean-square error,\\n(1)E“(1—AGP |B)={(Va|B)+[EA]BY—MP},\\nusing(15.6.23). Theroot-mean-square errorreduces tothestandard\\ndeviation, V’*(1 |B;),incasetheestimate |isunbiased.\\nFaking thedoctrine literally, itevidently leadstoendless regression,\\nforanestimate oftheaccuracy ofanestimate shouldpresumably be\\naccompanied byanestimate ofitsownaccuracy, andsoonforever.\\nEvensupposing thatthedoctrine weresomehow purgedofvagueness\\nandendless regression, itwould stillbeinclearconflictwiththebe-\\nhavioralistic concept ofestimation studied inChapter 15.Ifadecision\\n257\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c26cba3c-a702-4fa8-8baa-08b6f9bc9879', embedding=None, metadata={'page_label': '276', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='258 INTERVAL ESTIMATION ANDRELATED TOPICS (17.1\\nproblem consists indeciding onanumberinthelightofanobservation,\\nthepersonconcerned wantstoadoptan|thatis,insomesenseor\\nother,asgoodaspossible; but,sincehemustmakesomedecision, it\\ncouldatmostsatisfy idlecuriosity toknowhowgoodthebestis—\\nidle,Isay,because, hisdecision oncemade,thereisnowaytouseknowl-\\nedgeofitsaccuracy.\\nSinceitseemstomethatthekindofproblem envisaged inChapter\\n15isoffrequent occurrence andmayproperly becalledestimation,\\nIaminclined tosaythatthedoctrine ofaccuracy estimation iserrone-\\nous.However, itispossible thatsomeone shouldpointout adifferent\\nclassofproblems, alsoproperly calledproblems ofestimation, withre-\\nspecttowhichthedoctrine hassomevalidity; though, sofarasIknow,\\nthishasnotyetoccurred.\\nOnesortofsituation thatmight,through whatIwouldconsider\\nfaultyanalysis, seemtosupport thedoctrine ofaccuracy estimation is\\nillustrated bythefollowing, highlyschematized example. Aperson\\nhastoestimate thenumber nofreplacement partsofacertain sort\\nthatshouldbecarriedbyanexpedition. Hecanconduct atrialthe\\noutcome ofwhichwill,letussay,beanobservation xdistributed in\\nthePoisson distribution withmeanequaltoacn;thatis,\\n(2) P(x |n)=e*\"(acn)”/x!,\\nwhere a@isaknownconstant andc,whichthepersoncanchoose, isthe\\ncost(beyond overhead) ofthetrial.Underreasonable hypotheses,\\noncechasbeenchosenandthevaluexobserved, n(x)=x/acisagood\\nestimate ofn;andinsofarastheproblem isofthetypeenvisaged in\\nChapter 15,thatistheendofthematter.\\nButtheremaybefeatures oftheproblem thathavenotyetbeen\\nstated,though inprinciple theyshouldhavebeen.Inparticular, it\\nmaybethattheperson isfreetoconduct asecond trial,though there\\nwilltypically beahighpenalty fordoingso.Onerough,butsometimes\\nnatural andpractical, steptoward deciding whether asecondtrial is\\ncalledforistoremark that(n/ac)” isagoodestimate oftheroot-mean-\\nsquareerrorofnandmaygiveafairlygoodbasisonwhichtojudge\\nwhether theriskofmisestimation warrants theexpense ofasecond\\ntrial.\\nMyownconviction isthatweshouldfrankly regardsuchaproblem\\nashasjustbeendescribed asaspecialproblem insequential analysis\\nandtreatitasanorganic whole.Viewed thus,cistobechosen inthe\\nlightofthepossibility ofmaking asecond trial.Thedecision tobe\\nbasedonxisthecomplex oneofwhethertogototheexpenseofasecond\\ntrial;ifso,ofwhatmagnitude; and,ifnot,whatestimate ofntoadopt.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b9767afc-9d86-453d-8feb-3bdf4b7891e8', embedding=None, metadata={'page_label': '277', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.2] INTERVAL ESTIMATION 259\\nAnothersortofsituation thatseemstohavestimulated thedoctrine\\nofaccuracy estimation isthefollowing. Suppose thataresearch worker\\nhasobserved x1,---,Xn,whichareindependent andnormally distributed\\naboutthemean yuwithvariance o”given»andoc.Ifhewishestopub-\\nlishtheresultsofhisinvestigation forallconcerned touseastheirown\\nneedsandopinionsmaydictate,heshould, ideally,publishasufficient\\nstatistic ofhisobservation, statinghowitisdistributed given ypando.\\nAnyothercoursemaydeprivesomereaderofsomeinformation he\\nmightbeabletoputtouse.Sofarastheprimaryaimisconcerned, all\\nsufficient statistics areequivalent, butsecondary considerations greatly\\nnarrow theresearch worker’s choice.Toillustrate, consider thefive\\nsufficient statistics thevaluesofwhichfor{2,---,x,}are:\\n(a){21,+++,Ln}.\\n(b)Thenorderstatistics of{21,---,tn}.\\n(c)DSa,and>)2,’.\\n(d) =prDai/nand8?=p;(D2?—&D)2i)/n—1.\\n(e)Zands/n”.\\nIfnisatalllarge,(c),(d),and(e)arecheaper topublish than(a)\\nand(b).Moreover, foralmostanyusetowhichareadermightwish\\ntoputthedata,(c),(d),and(e)willsavehimaconsiderable amount\\nofcomputation. Insofarasitistruethatalmostanyreaderwhohas\\nauseforthedataatallwilluseZ,butnotnecessarily >>2;,statistics\\nlike(d)and(e)areslightly preferable to(c).There issomething tobe\\nsaidbothfor(d)andfor(e),inviewofthereadyavailability ofcertain\\ntables;but,atleastwhen7isverylarge,thereisaslightadvantage to\\n(e)forthosecalculations areader ismostlikelytoperform. Inpar-\\nticular,areaderusing(e)can,when7islarge,oftenignoretheactual\\nvalueofn.Evenifthedistributions ofthex;,---,X,arenotexactly\\nnormal, (c),(d),and(e)oftencanplayalmostthesameroleassuff-\\ncientstatistics. Itisnowonder thenthat(e)isoftenchosenasacon-\\nvenientwaytopresent data.But,inmyopinion, itisamistake to\\nlaygreattheoretical emphasis onthefactthat(e)happens toconsist\\nofwhatisordinarily agoodestimate ofu,namely z,together withwhat\\nisordinarily agoodestimate oftheroot-mean-square error ofthates-\\ntimate,namely s/n”.\\n2Interval estimation andconfidence intervals\\nTheverbalistic tradition hassuggested a procedure different from\\npointestimation butsomehowrelated toit.Thisotherprocedure, here\\ncalledinterval estimation, canbedefined asfollows,thoughthedefini-\\ntionisnecessarily vague. Wherexisanobservation subject tothe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='075c96c0-edf3-41b9-bbf3-db29dfe1a43b', embedding=None, metadata={'page_label': '278', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='260 INTERVAL ESTIMATION ANDRELATED TOPICS [17.2\\nconditional distributions P(z |B;)and(2)1safunction of7,guess\\nthatA(z)liesinsomesetM(x)(tobecalledaninterval estimate) de-\\ntermined foreachvalueofx.Itisalmostapartofthedefinition to\\nsaythatthefunction M(z)istobesochosenthatP(A(2) ¢M(x) |B;)\\nshallbenearly 1forevery7andthatM(x)shouldtendtobesmalland\\n“closeknit’”’inageometrical sense,somecompromise beingeffected be-\\ntweenthesetwoconflicting desiderata. Theparameter A(z)couldin\\nprinciple beaverygeneral function, butitwillherebeenough tosup-\\nposefordefiniteness andsimplicity thatA(z)isreal.Though more\\ngeneral possibilities arecontemplated inprinciple, thesetM(x)isin\\npractice typically abounded interval, whichcorresponds withwhatI\\nmeantinsayingthatM(x)issupposed tobe‘‘closeknit.”\\nTheideaofinterval estimation iscomplicated; anexampleisinorder.\\nSuppose that,foreach\\\\,xisarealrandom variable normally distrib-\\nutedaboutAwithunitvariance; then,asisveryeasytoseewiththe\\naidofatableofthenormaldistribution, if(x)istakentobethein-\\nterval[x—1.9600,x+1.9600], then\\n(1) P(eM(z)|a)=a,\\nwhereaisconstant andalmostequalto0.95.\\nItisusuallythought necessary towarnthenovicethatsuchanequa-\\ntionas(1)doesnotconcern theprobability thatarandom variable »\\nliesinafixed set M(x).Ofcourse,disgivenandtherefore notrandom\\ninthecontext athand;and,givenX,a@istheprobability thatM(x),\\nwhich isacontraction ofx,hasasitsvalueaninterval thatcontains X.\\nWhyseekaninterval estimate? Onesortofverbalistic answerruns\\nlikethis:Atfirstglance,theproblem ofestimation seemstorequire\\nthataperson guess,onobserving thatxtakesthevaluez,thatA(z)\\nhassomeparticular valueI(x);but,sinceit isvirtually impossible that\\nsuchaguessshouldbecorrect, itseemsbettertotrysomething else.\\nInparticular, itisoftenpossible toassertthatA(z)1sinacomparatively\\nnarrow interval M(x),chosenaccording tosuchasystemthatit1svery\\nimprobable foreach7thattheassertion willbefalse.Lessextreme ver-\\nbalistic explanations tendtogivetheimpression thatpointestimation\\nneednotbealtogether rejected, butthatinterval estimation satisfies\\naparallel need.\\nThefirstpartoftheexplanation justcitedisspecious, sincenoone\\nreallyexpects apointestimate tobecorrect,andsince,whenonereally\\nisobligedbycircumstances tomakeapointestimate inthebehavioral-\\nisticsense,thereisnoescaping it.Nonetheless,thatpartoftheex-\\nplanation doesseemtogivesomeinsightintotheappealofinterval es-\\ntimation. Thesecondpartoftheexplanation isasortoffiction; forit\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ed2b7da0-148b-490b-980d-fa0f397b5d61', embedding=None, metadata={'page_label': '279', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.2] INTERVAL ESTIMATION 261\\nwillbefoundthatwheneveritsadvocates talkofmaking assertions that\\nhavehighprobability, whether inconnection withtesting orestima-\\ntion,theydonotactually makesuchassertions themselves, butend-\\nlesslypassthebuck,sayingineffect, ‘““Thisassertion hasarisenaccord-\\ningtoasystemthatwillseldomleadyoutomakefalseassertions, if\\nyouadopt it.Asformyself, Iassertnothing buttheproperties ofthe\\nsystem.”\\nFromthebehavioralistic pointofview,Imaintain thatpointestima-\\ntionfulfilsanimportant function. Ontheotherhand,Icanciteno\\nimportant behavioralistic interpretation ofinterval estimation. More-\\nover,insuchdirectandindirect contact asIhavehadwithactualsta-\\ntistical practice, Ihave—with butoneextraordinary exception, which\\nwillsoonbediscussed—encountered noapplications ofinterval estima-\\ntionthatseemed convincing tomeasanything morethananinformal\\ndeviceforexploring dataorcrudely summarizing itforothers. In\\nshort,notbeingconvinced myself, Iaminnoposition topresent con-\\nvincing evidence fortheusefulness ofinterval estimation asadirect\\nstepindecision. Thereadershouldknow,however, thatfewareas\\npessimistic asIamaboutinterval estimation andthatmostleaders in\\nstatistical theoryhavealong-standing enthusiasm fortheidea,which\\nmayhavemoresolidgrounds thanInowknow.\\nThefollowing isaschematized example ofonesortofdecision prob-\\nlemthatdoescallforsomething likeinterval estimation. Anobserva-\\ntionxbearsontheposition )ofalifeboat, theoccupants ofwhichwill\\nbesavedorlostaccording astheboatisorisnotsightedbyasearch-\\ningaircraft before nightfall. Thedecision problem is,therefore, to\\nchoose,fromallthedomains thattheairplane couldsearchintime,one\\ndomainM(x);andthelossmust,ineffect,bereckoned as0or1accord-\\ning asM(x)doesordoesnotcontain }.Thistypeofproblem seems,\\nhowever, toorareandtoospecialtobetakenasrepresentative ofthose\\nforwhichinterval estimation issowidelyadvocated.\\nManycriteria havebeenputforward forinterval estimation, butI\\namofcourseinnoposition todiscussthemcritically. J.Neyman has\\ngoneaboutthesearchforcriteria systematically, settingupaparallel-\\nismbetween thetheory ofinterval estimation andoftesting. Inpar-\\nticular, paralleling thecriterion offixedsizefortests,hehasemphasized\\ninterval estimates suchthat\\n(2) P(d(i)¢M(x) |Bi)=@\\nforafixeda(typically closeto1)andforevery 7.Suchinterval esti-\\nmatesarecalledconfidence intervals attheconfidence levela.The\\ninterval estimate mentioned inconnection with(1)1sobviously acon-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80bfc7b7-2f67-4bdb-a67c-8e6297e38626', embedding=None, metadata={'page_label': '280', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='262 INTERVAL ESTIMATION ANDRELATED TOPICS [17.4\\nfidence interval. Wald[W3]soughttoinclude thetheoryofconfidence\\nintervals intheminimax theory,butinmyopinionhedidnotsucceed\\ningivinginterval estimation abehavioralistic interpretation.\\nThough Iaminnoposition tocriticize anycriterion ofinterval es-\\ntimation, Iventure toaskwhether (2)isnotgratuitous, asIhavemore\\npositively asserted ofitsanalogue inthetheoryoftesting.\\nChapters 19and20of[K2]willserveaskeyreferences forinterval\\nestimation.\\n3Tolerance intervals\\nTherehasrecently beenconsiderable studyofwhatarecalledtoler-\\nanceintervals (orlimits). Theyarerelated to theproblem ofguessing\\ntheactualvalueofarealrandom variable y,onthebasisofanobser-\\nvationofx.Atolerance interval foryattolerance leveloandconfi-\\ndencelevel@isaninterval-valued function Y(x)suchthat\\n(1) P[P(y«Y(x) |Bi,t)>|B)=8\\nforevery 1.\\nTheconcept expressed by(1)isaslippery one;perhaps itwillhelp\\ntoexpress itinwordsthus:ForeveryB;,thereisprobability 6thatzis\\nsuchthatywillfallinY(x)withprobability atleasta,givenB;and\\nz.Intypical applications yisindependent of z;thispermits aslight\\nsimplification ofthedefinition. Thenotionoftolerance interval seems\\ntomeatleastasunamenable tobehavioralistic interpretation asthat\\nofconfidence interval, andItherefore venture nodiscussion ofithere.\\nKeyreferences are[B22]and[W7].\\n4Fiducial probability\\nThisisnotreallyasectiononfiducial probability, butratheran\\napology fornothavingsuchasection. Theconcept offiducial proba-\\nbilityputforward andstressedbyR.A.Fisher isthemostdisputed\\ntechnical concept ofmodern statistics, and,sincetheconcept islargely\\nconcerned withinterval estimation, Iwanted todiscuss ithere.I\\nhave,however, beenprivileged toseecertainasyetunpublished manu-\\nscripts ofR.M.Williams [W12]andJ.W.Tukeywhichconvince me\\nthatsuchdiscussion bymenowwouldbepremature.\\nSomekeyreferences tofiducial probability andtotheBehrens-Fisher\\nproblem, which isthemostdisputed fieldofapplication offiducial\\nprobability, areFisher’sownpapers, especially [F5],andPapers 22,\\n25,26,27,and35ofthecollection [F6];Kendall [K2],Chapter 20;\\nYates[Y1];Owen[01];Segal[S9];Bartlett [B6];Scheffé [S6],[S5];\\nWalsh[W9];andChand[C5].*\\n+AndIcannowaddBarnard (1963),Dempster (1964),Fisher(1956,Sec-\\ntionsIII3,IV6,V5,V8,VI8,VI12),Linnik(1968,ChaptersVIII-X),\\nPatil(1965),Scheffé(1970),Tukey(1957),andWilliams (1966).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa1c8b93-4554-4a8d-85af-9d1e720c7d2f', embedding=None, metadata={'page_label': '281', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='APPENDIX 1\\nExpected Value\\nThis appendix, abrief account ofsomerelatively elementary aspects\\nofthebadly named mathematical concept, expected value, ispresented\\nfor those who might otherwise behandicapped inreading this book.\\nNo proofs are given here, but the reader who needs this appendix will\\nprobably bewilling and able toaccept the facts cited without proof,\\nespecially ifheacquires intuition forthe subject byworking the sug-\\ngested exercises. The requisite proofs are, however, given implicitly\\ninany standard work onintegration ormeasure (e.g., Chapters I-V of\\n[H2)).\\nThroughout this appendix, letSbeasetwith elements sand subsets\\nA,B,C,---onwhich a(finitely additive) probability measure Pis\\ndefined. Bounded realrandom variables, that is,bounded real-valued\\nfunctions, defined foreach s<S, will here bedenoted byx,y,---, and\\nreal numbers by 2, y, z,and lower-case Greek letters.\\nThe expected value ofx,generally written E(x), ischaracterized as\\nthe one and only function attaching areal number toevery bounded\\nrandom variable x,subject tothe following three conditions forevery\\nX,y,p,0,and B:\\n(1) E(ox+oy)=pE(x)+cE(y).\\n(2) E(x) >0 whenever P(z(s) <0)=0.\\n(3) E(c(| B))=P(B).\\nIn(3), c(|B)isthecharacteristic function ofB,that is,c(s| B)=1,\\nifs¢B,and c(s |B)=0,ifse~B. Inmathematical contexts remote\\nfrom the topics inthis book, theterm ‘characteristic function’ has at\\nleast two other meanings virtually unconnected with the one athand,\\none inconnection with linear operators onfunction spaces, and another\\ninconnection with the Fourier analysis ofdistributions.\\nOften the expected value ofxisreferred to as the integral ofxover\\nS,inwhich case itisgenerally written [2(s) dP(s).\\n263\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c8224b8a-55cf-4765-8c3f-92ad97d5c5aa', embedding=None, metadata={'page_label': '282', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='264 APPENDIX 1\\nExercises\\n1.Ifxtakesonlyafinitenumberofvalues, 21,---,tm,exceptona\\nsetofprobability zero;then\\n(4) E(x)=2.2,P(2(s)=2),\\ni=l\\nthatis,theaverage ofthez,’s,eachweighted bytheprobability ofits\\noccurrence.\\n2.IfP(x(s)<y(s))=0,E(x)=>E(y);andif,inaddition, P(a(s)>\\ny(s)+«)>0forsomee>0,thenE(x)>E(y).T\\n3.Ifxisarealrandom variable, B;apartition, p;ando;realnumbers\\nsuchthatp;<x(s)<o;fors¢B,,then\\n(5) piP(Bi)<E(x)<2o,P(Bi).\\n4. c(|ANB)=e(|A)e(|B),\\nc(|~A)=1—(A),\\nc(|AUB)=¢(|A)+e(|B)—e(|A)e(|B).\\nAsisexplained intextsonmeasure theory, theexpected valuecan\\n(atleastforcountably additive measures), andinpractice must,beex-\\ntended tomanyunbounded random variables.\\nSince,provided P(B)>0,theconditional probability, definedby\\nP(C|B)=P(CNB)/P(B), isitselfaprobability measure, theex-\\npectation ofxwithrespect toaconditional probability isameaningful\\nconcept. Thisconditional expectation iswritten E(x |B)andread\\n‘“‘theexpected valueofxgivenB.”’\\nMoreexercises\\n5.E(x|B)=E(xc(|B))/P(B). Hint:Itsufficestoverifythatthe\\nexpression ontherightsatisfies thethreeconditions parallel to(1-3)\\nthatdefineE(xB).\\n6.IfB;1sapartition ofS,then\\n(6) >c(s |B;)=1 forevery Ss.\\n7.E(x)=>E(x|B)P(B;). Hint:Usex=Ix.\\n+tTechnical note:IntheeventthatPiscountably additive, P(z(s)>y(s))>0\\nimpliestheexistence ofasuitable «,sothen«neednotbementionedatall.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e3a40a53-b7da-4112-9faf-4655724baa38', embedding=None, metadata={'page_label': '283', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='EXPECTED VALUE 265\\nSuppose yisa(not necessarily real) random variable that takes on\\nonly afinite number ofvalues. Itwill beunderstood that E(x |y)18\\nthe expected value ofxgiven that y(s) =y,provided yissuch that\\nthis event has positive probability. Furthermore, itwill beunderstood\\nthat E(x |y)isabounded real random variable that foreach stakes\\nthevalue E(x| y(s)). The definition leaves E(x |y)undefined onthe\\nnull setofthose points swhere y(s) isavalue that ytakes onwith prob-\\nability zero. Itisimmaterial how this blemish isremoved; inparticu-\\nlarE(x| y)may aswell besetequal to0,whereithasnotalready been\\ndefined.\\nStill more exercises\\n8.E(E(h| y))=E(h).\\n9.Iffisareal-valued function defined onthevalues of y; then f(y)\\nisabounded real variable, and\\n(7) E(f(y)x) =E(fQy)E(«|y)).\\n10. Ifh(x) issuch that, forall f,\\n(8) E(f(y)x) =E(fMhy)),\\nthen h(y(s)) =E(x |y(s)), except possibly onasetofs’sofprobability\\nzero.\\nExercise 9and itscorollary, 8,present themost frequently used prop-\\nerties ofconditional expectation. Exercise 10shows that the property\\npresented in9characterizes conditional expectation. Through this\\ncharacterization Kolmogoroff [K7] extends the ideas ofconditional ex-\\npectation and also ofconditional probability (for countably additive\\nmeasures) torandom variables ynot necessarily confined toafinite or\\neven denumerable setofvalues; though the definition interms of ordi-\\nnary conditional probability then breaks down completely, theproba-\\nbility that y(s)=yoften being 0forevery y.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5e256c33-5451-40e4-b934-6c301f8c1567', embedding=None, metadata={'page_label': '284', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='APPENDIX 2\\nConvex Functions\\nThisappendix givesabriefaccount ofconvexfunctions inthesame\\nspiritasthepreceding onegivesanaccount ofexpected value.Reason-\\nablefacsimiles oftheproofsomitted herearescattered through [H4],\\nwheretheymaybefoundbyanyone notcontent toskipthem.\\nAninterval isasetJofrealnumbers; suchthat,ifz,z¢7anda<y\\n<z,thenyel. Itisnotdifficult toseethatintervals canbeclassified\\naccording toTable1,whereit istobeunderstood thatz<z.\\nTABLE 1.THEVARIOUS TYPESOFINTERVALS\\n Thesetof\\nSymbolic realy’s\\ndesignation suchthat Verbaldescription\\n(—00,+00)y=y Theinfiniteinterval (thesetof\\nallrealnumbers)\\n(x,+2) a<ut\\n(—2,x) zr>y Open\\nhalf-infinite intervals\\n[z,++) aS|(—c0,2]>y Closed\\n(2,2) rey<Z Open\\nte* .s,Ss‘|Half-open boundedintervals\\n(x,z] acy<2 Closed\\n[x,x] r=y One-point intervals\\ny<y Thevacuous interval (thevacu-\\nousset)\\nAreal-valued function tdefined forzinaninterval Jisconvex,if\\nandonlyifthegraphofthefunction neverrisesaboveanychordofit-\\nself.Analytically, ifpandoarepositive, p+o=1,and2,yeJ;then\\n(1) t(pr+oy)<pt(x)+ot(y).\\n266\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fbd54e43-ebe2-4d9b-aefe-607c1a1704c7', embedding=None, metadata={'page_label': '285', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CONVEX FUNCTIONS 267\\nIfequality holds in(1)forsome p;then, asiseasily verified, itholds\\nforevery p,and tislinear, i.e., oftheform az-+ 8,inthe closed\\ninterval [z,y].Aninterval inwhich is linear will here becalled an\\ninterval oflinearity. Ifand only ifthere arenointervals oflinearity\\nother than theone-point and vacuous intervals, tisstrictly convex.\\nExercises\\n1.Verify, atleast graphically, that the following functions arecon-\\nvex inthe indicated intervals; discuss their intervals oflinearity; and\\nsay which are strictly convex.\\n[=(—0, +0):\\n(a) e°for every p, (b)x?+px+oforevery pand o,\\n(c)|x|, (d)|x|? forp>1,\\n(e) x.\\nI=(0,©):\\n(f)—log 2, (g)2?for-~<p<0.\\nI=(-—1, +1):\\n(h)(1—2?)~%, (i)1—cos(rz/2).\\n2.Inaninterval where tisconvex, ifd7(x)/dzx” exists atz,then\\nd*t(x)/dx? >0;andif, forevery z inaninterval I,d?t(x)/dz? exists and\\nisnon-negative, then tisconvex inI.\\n3.Re-explore Exercise 1inthe light of2.\\n4.LetTbeanon-vacuous set offunctions, t,t’,---, convex inJ,\\nand let\\n(2) t*(s) =sup t(s).\\nt\\nIn(2), asalways inmathematics, the sup, orsupremum, of asetof\\nnumbers isthe Jeast number, possibly ©,that isnot less than anyele-\\nment ofthe set. If¢*(s) <©forevery seJ,then t*isconvex inI.\\nExplore the proposition just stated, first graphically, especially fora\\nfinite setof linear t’s,and then analytically. What iftheelements of\\nTare allstrictly convex?\\n5.Inanopeninterval where tisconvex, itisalso continuous. What\\narethe facts forclosed and half-closed intervals?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f247085-f4a9-473b-b1ca-01200870ec1f', embedding=None, metadata={'page_label': '286', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='268 APPENDIX 2\\n6.Iftisconvex inJ,x,eI,px>0,andXp,=1,wherek=1,---,\\nr;then\\n(3) D,pat(te)>(Dpts)\\ni i\\nEquality obtains, ifandonlyifallthex;’sareinasingleinterval of\\nlinearity oft.\\n(a)Interpret thepropositions aboveintermsofprobability.\\n(b)Provethembyarithmetic induction onr.\\n(c)Whatiftisstrictlyconvex?\\nExercise 6suggests, andindeedprovesaspecialcaseof,thefollowing\\nwell-known andmostusefultheorem, whichcannotbeprovedherein\\nfullgenerality.\\nTHEOREM 1Iftisconvexandbounded intheinterval J,andx(s)eI\\nforallseS,then\\n(4) E(t(x))2U(x).\\nEquality obtains, ifandonlyifthevaluesofxarewithprobability one\\ncontained inasingleinterval oflinearity oft.Hereandthroughout this\\nappendix, suchconditions forequality aretobeunderstood toapply\\nonlyintheeventthateitherPiscountably additive ortherandom\\nvariable iswithprobability oneconfined toafinitesetofvalues;the\\ngeneral situation forfinitelyadditive measures isalittlemorecompli-\\ncated.\\nMoreexercises\\n7.Thevariance ofx,oftenwritten V(x),isdefinedthus:\\n(5) V(x)=E({x—E(x)/).\\nShowthat\\n(6) V(x)=E(x”)—E*(x)>0,\\nwithequality ifandonlyifP(a(s)=H(x))=1.\\n8.Showthat,ifxisneversmallerthansomepositivenumber,\\n(7) logE~1(x—!)<E(logx)<logE(x).\\nWhencaneitherequality obtain? Writetheanalogue of(7)suggested\\nby(3),andshowthereby that(7)isageneralization ofthefamiliar\\nfactthatthearithmetic mean(ofpositivenumbers) isatleastasgreat\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='848102a8-a334-4ff8-8969-c09cae0e251a', embedding=None, metadata={'page_label': '287', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CONVEX FUNCTIONS 269\\nasthe geometric mean and the geometric mean isatleast asgreat as\\ntheharmonic mean.\\nOne ofthemost famous of allinequalities istheSchwartz inequality,\\nwhich can, though not quite obviously, bederived from Theorem 1,\\nand which can bestated interms ofexpected values thus:\\n(8) E*(xy) <E(x’)E(y’),\\nwith equality obtaining ifand only ifforsome numbers pand onot\\nboth zero\\n(9) P(px(s) =oy(s)) =1.\\nNote that (9)expresses (perhaps toocompactly) that, except onsome\\nsetofprobability zero, either xoryvanishes identically orelse each is\\nafixed multiple ofthe other.\\nStatistically speaking, the Schwartz inequality expresses, ineffect,\\nthe familiar fact that any correlation coefficient must liebetween +1\\nand —1, one oftheextremes occurring ifand only ifatleast one ofthe\\ntwo random variables involved is alinear function ofthe other.\\nThe concept ofconvex functions and itsimplications can easily be\\nextended toreal-valued functions defined onvectors inann-dimensional\\nvector space, the role ofintervals there being replaced byconvex sub-\\nsets ofthevector space; butanunderstanding ofthis extension, though\\ndesirable, isnot absolutely essential inreading this book.\\nOne good introduction toconvex subsets ofvector spaces isSections\\n16.1-2 of[V4], and another especially adapted tostatistical applica-\\ntions isincorporated in[B18]. The standard treatise onthe topic is\\nthat ofBonnessen and Fenchel [B20].\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='598dda17-c390-4a02-bed7-df90e595fa2e', embedding=None, metadata={'page_label': '288', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='APPENDIX 3\\nBibliographic Material\\nThebibliography ofabout170itemsthatterminates thisappendix\\nlistsnotonlyallworksreferred tointhisbookbutalsosomeothers,\\nforit1sintended toservenotonlyasamechanical aidto reference but\\nalsoasabrieflyandinformally annotated listofsuggested readings in\\nthefoundations ofstatistics. Inaddition tothenotesincorporated\\nintothebibliography, information aboutmanyoftheworkslisted there\\nisgiveninotherpartsofthebook,whereitcanbefound byreferring\\ntotheauthor’s nameintheauthorindex.References thathavecometo\\nmyattention sincethefirstedition areinAppendix 4:Bibliographic\\nSupplement. Theyarecitedbytheconvention according towhichthe\\nfirstofthemiscalled(Aczél1966).\\nTodhunter hasabundant references scattered inchronological order\\nthrough [T3],emphasizing themathematical aspects ofprobability up\\nthrough theperiodofLaplace. Keynes, in[K4],givesaformal bibli-\\nography whichpurposely doesnotoverlap Todhunter’s material very\\nextensively, theemphasis beingonmorephilosophical aspects ofprob-\\nabilityandontheperiodbetween Laplace andKeynes. Carnap in\\n[C1]alsogivesaformalbibliography, whichemphasizes publications\\nsinceKeynes. Carnap promises anevenfullerbibliography inthe\\nprojected secondvolume ofhiswork,andherecommends thebibliog-\\nraphyofGeorgHenrikvonWright in[V5].\\nBibliographies ofstatistics properareofsome,though diluted, rele-\\nvance.Ofthese,themostusefulisthatofM.G.Kendall inVol.II\\nof[K2].Carnap atthebeginning ofhisbibliography givesreference to\\nsomeotherstatistical bibliographies. Theenormous workofO.K.Bu-\\nrosInstatistical bibliography, [B23],[B24],and[B25],shouldalsobe\\nmentioned. Hisvolumes bringtogether pointed excerpts fromreviews\\nofstatistical books.Burosalsodirected abibliographic department,\\nentitled ‘Statistical Methodology,” intheJournal oftheAmerican Sta-\\ntisticalAssociation fromSeptember 1945toSeptember 1948,listing cur-\\nrentarticles, books,theses,andchapters dealingwithstatistics. In\\n270\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4b6d0703-d2f7-4f13-9044-924f46788f32', embedding=None, metadata={'page_label': '289', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC MATERIAL 271\\nVolume 20(1949) oftheAnnals ofMathematical Statistics, animportant\\njournal ofstatistical theory, there aretwo cumulative indexes ofVol-\\numes 1-20, one arranged byauthor, the other bysubject.\\nBibliography\\nAitken, A.C.,and H.Silverstone\\n[Al] ‘On theestimation ofstatistical parameters,” Proceedings oftheRoyal Society\\nofEdinburgh, 61(1941-43), 186-194 (issued separately April 2,1942).\\nAllais, Maurice\\n[A2] “Le comportement del’homme rationnel devant lerisque: Critique despos-\\ntulats etaxioms del’école Americaine,’”’ Econometrica, 21(1953), 503-546.\\nAllen, 8.G., Jr.\\n[A3] “A class ofminimax tests forone-sided composite hypotheses,” Annals of\\nMathematical Statistics, 24(1953), 295-298.\\nAnscombe, F.J.\\n[A4] ““Mr. Kneale onprobability and induction,” Mind, 60(1951), 299-309.\\nSays much ofgeneral interest onthe foundations ofstatistics, inthe course\\nofcomments on[K5].\\nArrow, Kenneth J.\\n[A5] Social Choice and Individual Values, Cowles Commission Monograph No.12,\\nNew York, John Wiley &Sons, 1951. (Second edition, 1963.)\\n[A6] ‘‘Alternative approaches tothe theory ofchoice inrisk-taking situations,”\\nEconometrica, 19(1951), 404-437.\\nArrow, K.J.,David Blackwell, andM.A.Girshick\\n[A7] “Bayes andminimax solutions ofsequential decision problems,” Econometrica,\\n17(1949), 213-243.\\nBahadur, Raghu Raj\\n[B1] “Aproperty ofthe ¢-statistic,” Sankhyd, 12(1952), 79-88.\\n[B2] “Sufficiency and statistical decision functions,” Annals ofMathematical\\nStatistics, 25(1954), (toappear).\\nBahadur, Raghu Raj, and Herbert Robbins\\n[B3] “The problem ofthe greater mean,’”’ Annals ofMathematical Statistics, 21\\n(1950), 469-487.\\nBanach, S.\\n[B4] Théorie desopérations linéatres, Warsaw, Fundusz Kultury Narodowej, 1932.\\nBanach, S.,and A.Tarski\\n[B5] “Sur ladécomposition des ensembles depoints enparties respectivement\\ncongruentes,’”’ Fundamenta Mathematicae, 6(1924), 244-277.\\nBartlett, M.S.\\n[B6] “Completely simultaneous fiducial distributions,” Annals ofMathematical\\nStatistics, 10(1939), 128-138.\\nBaumol, William J.\\n[B7] “The Neumann-Morgenstern utility index—an ordinalist view,” Journal of\\nPolitical Economy, 59(1951), 61-66.\\nBayes, Thomas\\n[B8] Facsimiles ofTwo Papers byBayes: 1.AnEssay Toward Solving aProblem in\\ntheDoctrine ofChances, With Richard Price’s Foreword and Discussion; Phil.\\nTrans. Royal Soc., pp.870-418, 1768. With aCommentary byEdward C’.Molina.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='caef0f79-537e-4527-a8b4-d25366429bd4', embedding=None, metadata={'page_label': '290', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='272 APPENDIX 3\\na.ALetteronAsymptotic SeriesfromBayestoJohnCanton; pp.269-271 ofthe\\nSameVolume. WithaCommentary byW.Edwards Deming, ed.W.Edwards\\nDeming, Washington, D.C.,TheGraduate School,TheDepartment ofAgri-\\nculture, 1940;republished as(Bayes 1958).\\nThefirstofthesetwopapers, inwhichaspecialcaseofwhatisnowcalled\\nBayes’ruleisintroduced, figuresprominently incontroversies aboutthefoun-\\ndations ofprobability, forthispaperfirstputseveral ofthemajorissuesinthe\\nlimelight.\\nBell,E.T.\\n[B9]MenofMathematics, NewYork,SimonandSchuster, 1937.\\nBernoulli, Daniel\\n[B10]“Specimen theoriae novaedemensurasortis,’”’ Commentarit academiae scien-\\ntiarumimpertalis Petropolitanae (for1730and1731),5(1738),175-192.\\n[B11]DieGrundlage dermodernen Wertlehre. Versuch einerneuenTheorie der\\nWertbestimmung vonGliicksfdllen (German translation of[B10]byAlfredPrings-\\nheim,withintroduction byLudwig Frick),Leipzig,Duncker V.Humblot, 1896.\\n[Blla}“Exposition ofanewtheoryonthemeasurement ofrisk”(English trans-\\nlationof[B10]byLouiseSommer), Econometrica, 22(1954),23-26.\\nBernoulli, Jacob(=James)\\n[B12]Arsconjectandi, Basel,1713.\\n[B13]Wahrscheinlichkeitsrechnung (German translation of[B12]byR.Haussner),\\nOstwald’s Klassiker derExakten Wissenschaften, Nos.107and108,Leipzig,\\nW.Engelmann, 1899.\\nContains, besidesmuchofprimary mathematical interest, whatIunderstand\\ntobethefirstextended discussion oftheapplication ofprobability totheprob-\\nlemofinference. Unfortunately, theGermantranslation issaidtobeincom-\\nplete.\\nBirkhoff, G.,and8.MacLane\\n[B14]ASurveyofModern Algebra,NewYork,TheMacmillan Co.,1941.\\nBizley,M.T.L.\\n[B15]“Somenotesonprobability,” Journal oftheInstitute ofActuaries Students’\\nSociety, 10(1951),161-203.\\nBlackwell, David\\n[B16]“Comparison ofexperiments,’’ pp.93-102 ofProceedings oftheSecond\\n[1950]Berkeley Symposium onMathematical Statistics andProbability, ed.Jerzy\\nNeyman, Berkeley, University ofCalifornia Press,1951.\\n[B17]‘“‘Onthetranslation parameter problem fordiscrete variables,’ Annals of\\nMathematical Statistics, 22(1951),393-399.\\nBlackwell, David,andM.A.Girshick\\n[B18]TheTheory ofGamesandStatistical Decisions, NewYork,JohnWiley&\\nSons,1954.\\nBohnenblust, H.F.,S.Karlin,andL.S.Shapley\\n[B19]‘‘Solutions ofdiscrete two-person games,” pp.51-72of[K13].\\nBonnessen, T.,andW.Fenchel\\n[B20]Theorie derkonveren Kérper, Ergebnisse derMathematik undihrerGrenz-\\ngebiete, Vol.III,PartI,Berlin, J.Springer, 1934;reprinted, NewYork,Chelsea\\nPublishing Co.,1948.\\nBorel,Emile\\n[B21]‘Thetheoryofplayandintegral equations withskewsymmetric kernels;\\nOngamesthatinvolvechance andtheskilloftheplayers;Onsystems oflinear\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6fbc91ea-cb4e-4629-b031-aaa5087a0e24', embedding=None, metadata={'page_label': '291', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"BIBLIOGRAPHIC MATERIAL 273\\nformsofskewsymmetric determinant andthegeneral theory ofplay(trans-\\nlatedbyLeonard J.Savage),’’ Econometrica, 21(1953),97-124.\\nBowker, A.H.\\n[B22]‘‘Tolerance limitsfornormal distributions,” Chapter 2,pp.95-110 ofT'ech-\\nniquesofStatistical Analysis, bytheStatistical Research Group,Columbia\\nUniversity, NewYork,McGraw-Hill BookCo.,1947.\\nBuros,O.K.(ed.)\\n[B23]Research andStatistical Methodology, BooksandReviews (1933-38), New\\nBrunswick, NewJersey,Rutgers University Press,1938.\\n[B24]TheSecond Yearbook inResearch andMethodology, BooksandReviews,\\nHighland Park,NewJersey,TheGriffinPress,1941.\\n[B25]Statistical Methodology Reviews 1941-1950, NewYork,JohnWiley&Sons,\\n1951.\\nCarnap, Rudolf\\n[C1]LogicalFoundations ofProbability, Chicago, University ofChicago Press,\\n1950.\\nThisisthefirstofaprojected pairofvolumes designed todemonstrate me-\\nticulously theauthor’s contention thatacertainalmostnecessary viewofprob-\\nability isessential toscience—not denying themeaningfulness oftheobjec-\\ntivisticconcept. Reviewed bymein[S4].\\n[C2]TheNatureandApplication ofInductive Logic,Chicago, University ofChicago\\nPress,1951.\\nAreprintofselected sections of[C1].\\n[C3]TheContinuum ofInductive Methods, Chicago, University ofChicago Press,\\n1952.\\nEssentially achapter ofthesecondvolume oftheprojected pairreferred to\\nunder[Cl].\\nCentreNational deRecherche Scientifique\\n[C4]Fondements etapplications delathéoriedurisqueenéconometrie, Paris,\\nCentreNational delaRecherche Scientifique, 1954.\\nReportofaninternational econometric colloquium onrisk,inwhichtherewas\\nmuchdiscussion ofutility,heldinParis,May12-17,1952.\\nChand,Uttam\\n[C5]‘Distributions relatedtocomparison oftwomeansandtworegression coeffi-\\ncients,”Annals ofMathematical Statistics, 21(1950),507-522.\\nChapman, Douglas G.,andHerbert Robbins\\n[C6]“Minimum variance estimation without regularity assumptions,” Annals of\\nMathematical Statistics, 22(1951),581-586.\\nChernoff, Herman\\n[C7]“Remarks onaRational Selection ofaDecision Function,’ CowlesCom-\\nmissionDiscussion Paper,Statistics, No.326(January 10,1949).Unpublished.\\nChurchman, C.West\\n[C8]Theory ofExperimental Inference, NewYork,TheMacmillan Co.,1948.\\nAdiscussion ofcurrent statistics fromtheviewpoint oftechnical philosophy.\\nCramér, Harald\\n[C9]Mathematical Methods ofStatistics, Princeton, Princeton University Press,\\n1946.\\nByfarthemostcomprehensive rigorous bookonmathematical methods of\\nstatistics.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aaecf3db-546c-45be-b43c-48ef6db8f7cf', embedding=None, metadata={'page_label': '292', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"274 APPENDIX 3\\nDarmois, G.\\n[D1]“Surleslimitesdeladispersion decertains estimations,’ RevuedeI’Institut\\nInternational deStatistique, 13(1945),9-15.\\ndeFinetti,Bruno\\n[D2]‘‘Laprévision: sesloislogiques, sessources subjectives,’’ Annales deI’Institut\\nHenriPoincaré, 7(1937), 1-68.(Translated in(Kyburg andSmokler 1964).)\\nThefirsttwoandthefinalchapters ofthispapergiveastatement, whichI\\nhavefoundparticularly stimulating, oftheauthor’s view(apersonalistic one)of\\nthefoundations ofprobability. Thethreeintervening chapters aremathe-\\nmatically rathertechnical. Abibliography oftheauthor’s workonthefounda-\\ntionsofprobability uptosometime in1937isincluded.\\n[D3]‘‘Levraietleprobable,’”’ Dialectica, 3(1949),78-93.\\n[D4]‘“‘Sull’impostazione assiomatica delcalcolo delleprobabilita,” Annali Tries-\\ntint,Series2,19(1949),29-81.\\n[D5]‘La‘logicadelplausibile’ secondo laconcezione diPolya,” AttidellaXLII\\nRiunione dellaSocieta Italiana per11Progresso delleSctenze (Novembre 1949),\\nRome,Societa Italiana perilProgresso delleScienze, 1951(10pages).\\n[D6]‘Recent suggestions forthereconciliations oftheories ofprobability,” pp.\\n217-226 ofProceedings oftheSecond[1950]Berkeley Sympostum onMathematical\\nStatistics andProbability, ed.JerzyNeyman, Berkeley, University ofCalifornia\\nPress,1951.\\nEspecially through thesuggestions itmakesaboutmultipersonal problems,\\nanearlymanuscript of[D6]hasbadmuchinfluence onthisbook.\\n[D7]“Sulla preferibilita,’”’ Giornale degliEconomistt eAnnali diEconomia, 11\\n(1952),685-709.\\n[D7a]‘Lanotionde‘distribution d’opinion’ commebased’unessaid’interprétation\\ndelastatistique,’’ Publications del'Institut deStatistique del’Université de\\nParis,1(1952),1-19.\\nDelorme, 8.(ed.)\\n[D8]Colloque decalculdesprobabilités (Actualités scientifiques etindustrielles 1146),\\nParis,Hermann etCie.,1951.\\nAcollection ofpapersbyseveralauthors, mostlyonthephilosophy ofprob-\\nability,readinacolloquium heldunderthe13thInternational Congress ofthe\\nPhilosophy ofScience, inParis,1949.ThereisanoverallreviewbyM.Fréchet,\\npresident ofthecolloquium. AllpapersareinFrench, exceptoneinEnglish.\\nDialectica\\n[D9]Dialectica, Vol.3(1949),Nos.9-10.\\nThisissueofDialectica, aquarterly “international reviewofthephilosophy\\nofknowledge,” isdevoted toprobability, andmainly toitsfoundations. Itis\\ncomposed ofpapersbyseveralauthors, eachinEnglish, French, orGerman.\\nDoob,J.\\n[D10]‘‘Statistical estimation,” Transactions oftheAmerican Mathematical Society,\\n39(1936),410-421.\\nDuncan, D.B.\\n[D11]“‘Asignificance testfordifferences between rankedtreatments inananalysis\\nofvariance,” Virginia Journal ofScience, 2(1951),171-189.\\n[D12]‘Ontheproperties ofthemultiple comparisons test,’’Virginia Journal of\\nScience, 3(1952),49-57.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='788eb813-734f-4c3e-8d13-a1ee0db003f2', embedding=None, metadata={'page_label': '293', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC MATERIAL 275\\nDvoretzky, A.,A.Wald,andJ.Wolfowitz\\n[D13]“Elimination ofrandomization incertain statistical decision procedures and\\nzero-sum two-person games,’ Annals ofMathematical Statistics, 22(1951),\\n1-21.\\nElfving, G.\\n[E1]‘Sufficiency andcompleteness indecision function theory,” Annales Aca-\\ndemiaeScientarum Fennicae, SeriesA.I.,1385(1952),9pages.\\nFeller,William\\n[F1]AnIntroduction toProbability TheoryandItsApplications, Vol.1,New\\nYork,JohnWiley&Sons,1950.(Thirdedition, 1968;Vol.2,1966and1971.)\\nAsophisticated introduction tothemathematics ofprobability. Certain\\nrelatively advanced mathematical techniques areavoided byasevererestric-\\ntionofthematerial treated, which isnonethelessextensive andvaried.A\\nsecondvolume, removing therestriction, ispromised.\\nFéraud,D.\\n[F2]“Induction amplifiante etinférence statistique,’’ Dialectica, 3(1949),\\n127-152.\\nFisher,R.A.\\n[F3]Statistical Methods forResearch Workers, Edinburgh andLondon, Oliverand\\nBoyd,1925;andlatereditions.\\nTheauthor istheoutstanding member oftheBritish-American School,and\\nthisbookofhishashadfarmoreinfluence onthedevelopment ofstatistics in\\nthecurrent century thananyotherpublication.\\n[F4]TheDesignofExperiments, Edinburgh andLondon, OliverandBoyd,1935;\\nandlatereditions.\\nSecondonlyto[F3] intheextentofitsinfluence.\\n[F5]“Anoteonfiducial inference,” Annals ofMathematical Statistics, 10(1939),\\n383-388.\\n[F6]Contributions toMathematical Statistics, NewYork,JohnWiley&Sons,\\n1950.\\nAcollection ofFisher’s papersselected andannotated byhimself. Witha\\nbiography ofFisherbyP.C.Mahalanobis. Reviewed in[N4].\\nFisher,WalterD.\\n[F7]“Onapoolingproblem fromthestatistical decision viewpoint,” Econometrica\\n21(1953),567-585.\\nFréchet, Maurice\\n[F8]“Surl’extension decertains évaluations statistiques aucasdepetitséchantil-\\nlons,”’RevuedeI’Institut International deStatistique, 11(1943),183-205.\\n[F9}“Emile Borel,initiator ofthetheoryofpsychological gamesanditsapplica-\\ntion,”Econometrica 21(1953),95-96.\\nFréchet, Maurice, andJ.vonNeumann\\n[F10]“(Commentary ontheBorelnotes,’’Econometrica, 21(19538),118-127.\\nFriedman, Milton\\n[F11]‘Choice, chance,andpersonal distribution ofincome,” Journal ofPolitical\\nEconomy, 61(1953),277-290.\\nFriedman, Milton,andL.J.Savage\\n[F12]‘‘Theutilityanalysis ofchoicesinvolving risk,’”’Journal ofPolitical Economy,\\n56(1948),279-304; reprinted, withacorrection, in[S19].\\n[F13]‘‘Theexpected-utility hypothesis andthemeasurability ofutility,” Journal\\nofPolitical Economy, 60(1952),463-474.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='171978cc-5ec7-4beb-8977-67b0a972342d', embedding=None, metadata={'page_label': '294', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='276 APPENDIX 3\\nGirshick, M.A.,and L.J.Savage\\n[G1] “Bayes and minimax estimates forquadratic loss functions,” pp. 53-74 in\\nProceedings oftheSecond [1950] Berkeley Symposium onMathematical Statistics\\nand Probability, ed.Jerzy Neyman, Berkeley, University ofCalifornia Press,\\n1951.\\nGood, I.J.\\n[G2] Probability and theWeighing ofEvidence, London, Charles Griffin and Co.,\\nandNew York, Hafner Publishing Co., 1950.\\nPresents, with many interesting examples and arguments, Good’s view, a\\npersonalistic one, onthefoundations ofprobability. Reviewed bymein [S83].\\nGraves, Lawrence M.\\n[G3] The Theory ofFunctions ofReal Variables, New York, McGraw-Hill Book\\nCo., 1946.\\nHalmos, Paul R.\\n[H1] ‘“‘The foundations ofprobability,’’ American Mathematical Monthly, 51(1944),\\n493-510.\\nShort, easy exposition ofthe Kolmogoroff probability concept.\\n[H2] Measure Theory, New York, Van Nostrand Co., 1950.\\nHalmos, Paul R.,and L.J.Savage\\n[H3] “Application ofthe Radon-Nikodym theorem tothe theory ofsufficient\\nstatistics,’ Annals ofMathematical Statistics, 20(1949), 225-241.\\nHardy, G.H., J.E.Littlewood, and G.Polya\\n[H4] Inequalities, Cambridge, Cambridge University Press, 1934.\\nHildreth, Clifford\\n[H4a] ‘Alternative conditions forsocial orderings,” Econometrica, 21(1953), 81-94.\\nHodges, J.L., Jr.,and E.L.Lehmann\\n[H5] ‘Some problems inminimax point estimation,” Annals ofMathematical Sta-\\ntistics, 21(1950), 182-197.\\n[H6] “Some applications oftheCramér-Rao inequality,” pp.13-22 inProceedings\\noftheSecond [1950] Berkeley Symposium onMathematical Statistics and Prob-\\nability, ed.Jerzy Neyman, Berkeley, University ofCalifornia Press, 1951.\\nHume, David\\n[H7] An Enquiry Concerning Human Understanding, London, 1748; and later\\neditions.\\nAnearly and famous presentation ofthe philosophical problem ofinductive\\ninference, around which almost all later discussion oftheproblem pivots.\\nJeffreys, Harold\\n[J1] Theory ofProbability (Second edition), Oxford, Clarendon Press, 1948.\\nAningenious and vigorous defense ofanecessary view, similar to,but more\\nsophisticated than, Laplace’s. (Second edition, 1961.)\\nKakutani, S.\\n[K1] “‘A generalization ofBrouwer’s fixed-point theorem,” Duke Mathematical\\nJournal, 8(1941), 457-459.\\nKendall, Maurice G.\\n([K2] The Advanced Theory ofStatistics, Vol. I,1947, Vol. II,1948, London, Charles\\nGriffin and Co.\\nVirtually anencyclopedia ofstatistical theory, history, and bibliography (as\\nof1943).\\n[K3] “On the reconciliation ofthe theories ofprobability,” Biometrika, 36(1949),\\n101-116.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='215e383e-8793-45f7-b03b-760b74c60142', embedding=None, metadata={'page_label': '295', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC MATERIAL 277\\nKeynes, John Maynard\\n[K4] ATreatise onProbability, London andNew York, Macmillan &Co., 1921;\\nsecond edition, 1929.\\nAlong, but often entertaining, account ofKeynes’ view, anecessary one.\\nThehistorical passages and bibliography areofspecial interest.\\nKneale, William\\n[K5] Probability and Induction, Oxford, Clarendon Press, 1949.\\nAningenious presentation, philosophical inspirit and background, ofanew\\nview with objectivistic and necessary aspects. Reviewed in[A4].\\n[K6] “Probability and induction,” Mind, 60(1951), 310-317.\\nAreply to[A4].\\nKolmogoroff, A.N.\\n[K7] Grundbegriffe derWahrscheinlichkeitsrechnung, Berlin, J.Springer, 1933.\\n[K8] Foundations oftheTheory ofProbability (English translation of[K7] edited\\nbyNathan Morrison), New York, Chelsea Publishing Co., 1950.\\nStatement and very compact development ofthe Kolmogoroff concept of\\nmathematical probability. Excellent reading, though mathematically rather\\nadvanced.\\nKoopman, B.O.\\n[K9] “The axioms and algebra ofintuitive probability,” Annals ofMathematics,\\nSer. 2,41(1940), 269-292.\\n[K10] ‘‘The bases ofprobability,’ Bulletin oftheAmerican Mathematical Society,\\n46(1940), 763-774.\\n[K11] “Intuitive probabilities and sequences,” Annals ofMathematics, Ser. 2,42\\n(1941), 169-187.\\nThese three papers present the personalistic view that Koopman holds along\\nwith anobjectivistic one.\\n[K12] Reviews ofeleven papers, Mathematical Reviews, 7(1946), 186-193; and 8\\n(1947), 245-247.\\nAconnected sequence ofreviews ofpapers, byseveral authors, that were\\npublished asasymposium inPhilosophy and Phenomenological Research, Vols. 5\\nand 6(1945-46).\\nKuhn, H.W., and A.W.Tucker (eds. )\\n[K13] Contributions totheTheory ofGames, Vol. I(Annals ofMathematics Study\\nNo. 24), Princeton, Princeton University Press, 1950.\\n[K14] Contributions totheTheory ofGames, Vol. II(Annals ofMathematics Study\\nNo. 28), Princeton, Princeton University Press, 1953.\\nLoosely coordinated collection ofarticles onthe theory ofgames byseveral\\nauthors, with bibliographies.\\nKullback, S.,and R.A.Leibler\\n[K15] “On information and sufficiency,” Annals ofMathematical Statistics, 22\\n(1951), 79-86.\\nLaplace, Pierre Simon de\\n[L1] Essai philosophique sur lesprobabilités (First edition), Paris, 1814; and several\\nsubsequent editions, ofwhich the Fifth, 1825, was the last toberevised by\\nLaplace.\\n[L2]APhilosophical Essay onProbabilities (English translation of(L1], Second\\nedition), New York, John Wiley &Sons, 1917; reprinted, New York, Dover\\nPublications, 1952.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a8297fa-b0ff-422a-b5df-b76f55da31c6', embedding=None, metadata={'page_label': '296', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='278 APPENDIX 3\\nThis essay, published both separately and asthe preface oftheauthor’s great\\ntechnical treatise, Théorie analytique des probabilités, is,save fora few dull\\nspots, one ofthemost delightful and stimulating classics ofprobability. La-\\nplace’s view isanaive necessary one. His concept ofthedomain ofapplicability\\nofthetheory ofprobability isinspiring, ifnotalways realistic.\\nLeCam, Lucien\\n[L3] ‘On some asymptotic properties ofmaximum likelihood estimates and related\\nBayes’ estimates,” pp. 277-329 ofUniversity ofCalifornia Publications inSta-\\ntistics, Vol. 1,No. 11,Berkeley and Los Angeles, University ofCalifornia Press,\\n1953.\\nLehmann, E. L.\\n[L4] ‘Some principles ofthetheory oftesting hypotheses,’ Annals ofMathematical\\nStatistics, 21(1950), 1-26.\\n[L5] ‘‘Ageneral concept ofunbiasedness,” Annals ofMathematical Statistics, 22\\n(1951), 587-597.\\nLehmann, FE.L.,and Henry Scheffé\\n[L6] “Completeness, similar regions, and unbiased estimation, Part I,”Sankhyd,\\n10(1950), 305-340.\\nLewis, Clarence Irving, and Cooper Harold Langford\\n[L7] Symbolic Logic, New York and London, The Century Company, 1932.\\nLindley, D.V.\\n[L8] ‘‘Statistical inference,’ Journal oftheRoyal Statistical Society, Series B,15\\n(1953), 30-76.\\nExcellent reading inconnection with Chapters 14-17 ofthis book. Unfor-\\ntunately, Ididnot seethepaper intimeto reflect itscontents inthose chapters.\\nMarkowitz, Harry\\n[M1] ‘‘The utility ofwealth,” Journal ofPolitical Economy, 60(1952), 151-158.\\nMarshall, Alfred\\n[M2] Principles ofEconomics (First edition), London, Macmillan &Co., 1890; and\\nmany subsequent editions ofwhich theEighth (1927) isstandard.\\nMcKinsey, J.C. C.\\n[M3] Introduction totheTheory ofGames, New York, McGraw-Hill Book Co., 1952.\\nMosteller, Frederick, and Philip Nogee\\n[M4] ‘‘An experimental measurement ofutility,” Journal ofPolitical Economy,\\n59(1951), 371-404.\\nMourier, Edith\\n[M5] “‘Tests dechoix entre divers lois deprobabilité,’’ Trabajos deestadtstica, 2\\n(1951), 2384-259.\\nMunroe, M.E.\\n[M6] Theory ofProbability, New York, McGraw-Hill Book Co., 1951.\\nAnelementary modern text onthemathematics ofprobability. Easier and\\nmore general than [F1], but not sopenetrating.\\nNagel, Ernest\\n[N1] “Principles ofthetheory ofprobability,” International Encyclopedia ofUni-\\nfied Science, Vol. I,No. 6,Chicago, University ofChicago Press, 1939.\\nNeyman, Jerzy\\n[N2] “Outline ofatheory ofstatistical estimation based onthe classical theory of\\nprobability,” Philosophical Transactions oftheRoyal Society, Ser. A,236 (1937),\\n333-380.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fdea0e1a-b3dd-475b-ae46-a8f7fbf96079', embedding=None, metadata={'page_label': '297', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC MATERIAL 279\\n[N3] “L’estimation statistique, traitée comme unprobléme classique deproba-\\nbilité,”” pp.25-57 ofActualités scientifiques etindustrielles No. 739, Paris, Her-\\nmann etCie., 1938.\\nRepresentative works ofthe Neyman-Pearson School, amajor branch of\\nwhat inthis book iscalled theBritish-American School.\\n[N4] ‘‘Fisher’s collected papers,’’ Scientific Monthly, 42(1951), 406-408.\\nNunke, R.J.,and L.J.Savage\\n[N5] “On the setofvalues ofanonatomic, finitely additive, finite measure,” Pro-\\nceedings oftheAmerican Mathematical Society, 3(1952), 217-218.\\nOwen, A.R.G.\\n[O1] “Ancillary statistics and fiducial distributions,” Sankhyd, 9(1948-49), 1-18.\\nPareto, Vilfredo\\n[P1] Manuel d’économie politique (Second edition), Paris, Giard, 1927. (First\\nedition 1909. Based onastill earlier book inItalian.)\\nPascal, Blaise\\n[P2] Pensées, with introduction and notes byLouis Lafuma, Paris, Delmas, 1947.\\nPaulson, Edward\\n[P3] “‘Amultiple decision procedure forcertain problems inthe analysis ofvari-\\nance,” Annals ofMathematical Statistics, 20(1949), 95-98.\\n[P4] ‘On comparison ofseveral experimental categories with acontrol,’’ Annals of\\nMathematical Statistics, 23(1952), 239-246.\\nPitman, E.J.G.\\n[P5] “‘The ‘closest’ estimate ofstatistical parameters,’ Proceedings oftheCam-\\nbridge Philosophical Society, 33(1937), 212-222.\\nPolya, G.\\n[P6] “Preliminary remarks onlogic ofplausible inference,” Dialectica, 3(1949),\\n28-35.\\nRamsey, Frank P.\\n[R1] “Truth and probability” (1926), and “Further considerations” (1928), in\\nThe Foundations ofMathematics and Other Logical Essays, London, Kegan Paul,\\nandNew York, Harcourt, Brace and Co., 1931.\\nPenetrating development ofapersonalistic view ofprobability and utility.\\nRamsey’s concepts ofprobability and utility are essentially thesame asthose\\npresented in this book, but his logical development ofthem isaninteresting\\nalternative totheone used here, hisdefinitions ofprobability and utility being\\nsimultaneous and interdependent.\\nReichenbach, Hans\\n[R2] The Theory ofProbability, Berkeley and Los Angeles, University ofCali-\\nfornia Press, 1949.\\nThe most recent and complete statement ofReichenbach’s elaborately worked\\nout objectivistic view.\\nRichter, Hans\\n[R2a] “Zur Grundlegung der Wahrscheinlichkeitstheorie, I,II,III, IV,” Mathe-\\nmatische Annalen, 125 (1952), 129-139; 125 (1953), 223-224, 335-343; and 126\\n(1953), 362-374.\\nApersonalistic theory ofprobability with aphysical orientation, pertaining\\nnot tobehavior but to“feeling ofexpectation.”\\nRousseas, Stephen W.,and Albert G.Hart\\n[R3] “Experimental verification ofacomposite indifference map,’ Jeurnal of\\nPolitical Economy, 59(1951), 288-318.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4e362e8-8e6c-4519-bb24-b7d73ff98109', embedding=None, metadata={'page_label': '298', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='280 APPENDIX 3\\nRudy,Norman\\n[R4]‘Someproblems intheeconomics ofindustrial sampling inspection,” anas\\nyetunpublished dissertation submitted totheUniversity ofChicago in1952.\\nSamuelson, PaulA.\\n[S1]“Probability, utility,andtheindependence axiom,” Econometrica, 20(1952),\\n670-678.\\nSavage, L.J.\\n[S2]‘“‘Thetheoryofstatistical decision,” Journal oftheAmerican Statistical Asso-\\nciation,46(1951),55-67.\\n[S3]Review ofI.J.Good’sProbability andtheWeighing ofEvidence, inJournal of\\ntheAmerican Statistical Association, 46(1951),383-384.\\n[S4]Review ofRudolfCarnap’s LogicalFoundations ofProbability, inEconomet-\\nrica,20(1952),688-690.\\nScheffé,Henry\\n[S5]“Onsolutions oftheBehrens-Fisher problem, basedonthe¢-distribution,”’\\nAnnals ofMathematical Statistics, 14(1948),35-44.\\n[S6]“‘AnoteontheBehrens-Fisher problem,” Annals ofMathematical Statistics,\\n15(1944),430-432.\\n(S7]“Amethod ofjudging allcontrasts intheanalysis ofvariance,” Biometrika,\\n40(1953), 1-18.\\nSearle, S.R.\\n[S8]‘“Probability—the difficulties ofdefinition,” Journal oftheInstitute ofActuaries\\nStudents’ Society, 10(1951),204-212.\\nSegal,I.E.\\n[S9]‘‘Fiducial distributions ofseveralparameters withapplication toanormal\\nsystem,’’ Proceedings oftheCambridge Philosophical Society, 34(1938),41-47.\\nShackle, G.L.8.\\n[S10]Expectation inEconomics, Cambridge, Cambridge University Press,1949.\\nShannon, Claude E.,andWarrenWeaver\\n[S11]TheMathematical Theory ofCommunication, Urbana, University ofIllinois\\nPress,1949.\\nShapley, L.§.,andR.N.Snow\\n[S12]“Basicsolutions ofdiscrete games,”’ pp.27-36of[K13].\\nShohat, J.A.,andJ.D.Tamarkin\\n[S13]TheProblem ofMoments (Mathematical Surveys, No.1)American Mathe-\\nmatical Society,NewYork,1943;reprinted withsmallchanges in1950.\\nSmith,CedricA.B.\\n[S14]‘Someexamples ofdiscrimination,” Annals ofEugenics, 13(1947),272-282.\\nSobezyk, A.,andP.C.Hammer\\n[S15]“Theranges ofadditive setfunctions,’”’ DukeMathematical Journal, 11\\n(1944),847-851.\\nSprowls, R.Clay\\n[S16]“Statistical decision bythemethod ofminimum risk:anapplication,”\\nJournal oftheAmerican Statistical Association, 45(1950),238-248.\\nStatistical Research Group,Columbia University\\n[S17]Sequential Analysis ofStatistical Data:Applications, NewYork,Columbia\\nUniversity Press,1945.\\nStigler,George J.\\n[S18]‘‘Thedevelopment ofutilitytheory,” Journal ofPolitical Economy, PartI,\\n58(1950),307-327; PartII,58(1950),373-396.\\nStigler,George J.,andKenneth E.Boulding (eds.)\\n[S19]Readings inPriceTheory, Chicago, Richard D.Irwin,1952.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58bbe6aa-adeb-4e9d-b32b-c5cd58d3ea9f', embedding=None, metadata={'page_label': '299', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC MATERIAL 281\\nThrall,RobertM.,CydeH.Coombs, andRobertL.Davis(eds.)\\n[T1]Decision Processes, JohnWiley&Sons,NewYork,1954.\\nTintner, Gerhard\\n[Tia]“‘Acontribution tothenon-static theory ofchoice,’’ Quarterly Journal of\\nEconomics, 56(1942),274-306.\\nTippett, L.H.C.\\n[T2]Statistics, London, OxfordUniversity Press,1947.\\nAshort,easyintroduction totheideasofapplied statistics, withemphasis on\\nthesocialsciences.\\nTodhunter, I.\\n(T3]AHistory oftheMathematical Theory ofProbability fromtheTumeofPascal\\ntothatofLaplace, Cambridge andLondon, Macmillan &Co.,1865;reprinted, New\\nYork,G.E.Stechert, 1931;NewYork,Chelsea Publishing Co.,1949.\\nTukey,JohnW.\\n(T4]“Comparing individual meansintheanalysis ofvariance,’’ Biometrics, 5\\n(1949),99-114.\\n[T5]“Quickanddirtymethodsinstatistics. PartII.Simpleanalyses forstand-\\narddesigns,” Proceedings oftheFifthAnnual Convention oftheAmerican Society\\nforQuality Control, 1951,pp.189-197.\\nvanDantzig, D.\\n[V1]“Surl’analyse logique desrelations entrelecalculdesprobabilités etses\\napplications,” pp.49-66of[D8].\\nvonMises,Richard\\n[V2]Probability, Statistics andTruth,London, WilliamHodgeandCo.,1939.\\nEnglish translation oftheoriginalGerman. Interesting foritspresumably\\nauthoritative statement ofvonMises’unusual viewonthemathematical founda-\\ntionsofprobability. Hisviewontheintellectual foundations isobjectivistic.\\n(Second edition, 1957.)\\nvonNeumann, John\\n[V3]“ZurTheorie derGesellschaftsspiele,’”? Mathematische Annalen, 100(1928),\\n295-320.\\nvonNeumann, John,andOskarMorgenstern\\n[V4]TheoryofGamesandEconomic Behavior (Second edition), Princeton, Prince-\\ntonUniversity Press,1947.\\nContains, asadigression, theimportant newtreatment ofutilityfromwhich\\nthetreatmentofutilityin thisbookderives. Thesecondeditioncontains more\\nthanthefirstonthissubject, especially atechnical appendix. Also,theideaof\\nregarding multiple choicesassingleoverallchoices isdiscussed ingreatdetail.\\nFinally, thechapters on‘‘zero-sum two-person” gamesaremathematically in-\\ntimately connected withthestatistical minimax theory. °\\nvonWright,GeorgHenrik\\n[V5]TheLogicalProblem ofInduction (ActaPhilosophica Fennica, Fasc.3),Hel-\\nsinki,1941.(Second edition, 1957.)\\nWald,Abraham\\n[W1]OnthePrinciples ofStatistical Inference, NotreDame,Indiana, 1942.\\nAcompact exposition ofthedominant formalideasoftheBritish-American\\nSchool.\\n[W2]Sequential Analysis, NewYork,JohnWiley&Sons,1947.\\n[W3]Statistical Decision Functions, NewYork,JohnWiley&Sons,1950.\\nWald’sgreatcontributions tostatistical theoryarewellrepresented bythis\\nbook,butitisnotforbeginners.\\n[W4]“Noteontheconsistency ofthemaximum likelihood estimate,” Annals of\\nMathematical Statistics, 20(1949),595-601.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67a10390-f27a-4cec-893b-2af60ccb475f', embedding=None, metadata={'page_label': '300', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='282 APPENDIX 3\\n[W5]“Asymptotic minimax solutions ofsequential pointestimation problems,”\\npp.1-12ofProceedings oftheSecond [1950]Berkeley Symposium onMathe-\\nmatical Statistics andProbability, ed.JerzyNeyman, Berkeley, University of\\nCalifornia Press,1951.\\nWallis,W.Allen\\n[W6]‘Standard sampling-inspection procedures,” Proceedings oftheInternational\\nStatistical Conferences, 3(1947),331-350.\\n[W7]‘Tolerance intervals forlinearregression,’”’ pp.43-52ofProceedings ofthe\\nSecond [1950]Berkeley Symposium onMathematical Statistics andProbability,\\ned.JerzyNeyman, Berkeley, University ofCalifornia Press,1951.\\nWallis,W.Allen,andMiltonFriedman\\n[W8]‘‘Theempirical derivation ofindifference functions,” pp.175-189 inStudies\\ninMathematical Economics andEconometrics, ed.O.Langeetal.,Chicago, Uni-\\nversity ofChicago Press,1942.\\nWalsh,JohnE.\\n[W9]“Onthepowerfunction ofthe‘best’t-testsolution oftheBehrens-Fisher\\nproblem,” Annals ofMathematical Statistics, 20(1949),616-618.\\nWiener, Norbert\\n[W10]Cybernetics, NewYork,JohnWiley&Sons,1948.\\nWhite,PaulD.,RobertL.King,andJamesJenks,Jr.\\n[W11]“Therelation ofheartsizetothetimeintervals oftheheartbeat,with\\nparticular reference to theelephant andthewhale,” TheNewEngland Journal\\nofMedicine, 248(1953),69-70.\\nWilliams, R.M. .\\n[W12]“Theuseoffiducial distributions withspecial reference totheBehrens-\\nFisherproblem,”’ PartIIofanunpublished dissertation submitted totheUni-\\nversity ofCambridge andfiledin1949intheUniversity ofCambridge Library\\nasPh.D.1671.\\nWisdom, JohnOulton\\n[W13]Foundations ofInference inNatural Science, London, Methuen, 1952.\\nAnewbookinthephilosophical tradition, butmotivated bytheideaof\\nexamining howinductive inference isactually usedinscience. Though almost\\nentirely verbalistic inoutlook, behavioralistic ideasincluding thesure-thing\\nprinciple playanimportant roleinthefinalandculminating chapter.\\nWold,H.\\n[W14]“Ordinal preferences orcardinal utility,” Econometrica, 20(1952),661-664.\\nWolfowitz, J.\\n[W15]“Theefficiency ofsequential estimates andWald’sequation forsequential\\nprocesses,” Annals ofMathematical Statistics, 18(1947),215-230.\\n[W16]‘OnWald’s proofoftheconsistency ofthemaximum likelihood estimate,”\\nAnnals ofMathematical Statistics, 20(1949),601-602.\\n[W17]“Onecomplete classesofdecision functions,” Annals ofMathematical Sta-\\ntistics,22(1951),461-464.\\nYates,F.\\n[Y1]‘‘Anapparent inconsistency arising fromtestsofsignificance basedonfiducial\\ndistributions ofunknown parameters,” Proceedings oftheCambridge Philosophical\\nSociety,35(1939),579-591.\\n[Y2]“Principles governing theamount ofexperimentation indevelopmental\\nwork,”’Nature, 170(1952),138-140.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4c053bd8-bf40-4e74-861d-3c8268a15515', embedding=None, metadata={'page_label': '301', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='APPENDIX 4\\nBibhographic Supplement\\nSincethepublication ofthefirstedition of thisbook,theliterature\\nofthefoundations ofstatistics, likethatofallscience, hasbeengrow-\\ningwithawesome rapidity. Therelatively shortlistofabout180items\\nbelowineludes afewolderreferences overlooked inthefirstedition,\\nbutmostaremorerecent.Theyarechosen inthespiritofthoseinthe\\nfirstedition,Appendix 3:Bibliographic Material. Somesupport new\\nassertions madeinthisedition,somebringuptodatereading listsand\\nkeyreferences forcertain topics,andsomeareselected fortheirquality\\nandoriginality.\\nPagesinthisedition thatciteagivenentryinthelistbeloware\\nshownbyitalicnumbers following theentry—a neglected invention\\ngoingbackatleastto(Coolidge 1940).Where thereisneither sucha\\npagenumber noracomment, theentryissupposed tospeak foritself.\\nSomeofthenewentriesarespecialbibliographies (Edwards 1969,\\nGeorge1968 ;Greenwoodetal.1962;Joineretal.1970:Lancaster 1968,\\n1969,1970;Miller1969;Savage1970;Wasserman andSilander 1958,\\n1964).\\nBibliographies ofstatistics itself,nottomention thoseofrelated\\nfields,havesoproliferated thatLancaster (1968,1969,1970)hasal-\\nreadypublished threebibliographies ofstatistical bibliographies, one\\nofbooklength.Several important journals havepublished cumulative\\nindexesasshown bythetablebelow.\\nJournal Years Principal typesofcoverage\\n(andVolumes) \\nAnnals ofMathematical 1930-1960 Citation, author, subject, tables\\nStatistics (1-31)\\nBiometrika 1901-1950 Subject\\n(1-37)\\nBiometrika 1901-1961 Author\\n(1-48)\\n283\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d3fc5654-4ae8-4ffc-beea-903d1863823d', embedding=None, metadata={'page_label': '302', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='284 APPENDIX 4\\n Journal Years Principal typesofcoverage\\n(andVolumes)\\nEconometrica 1932-1952 Author, subject, bookreviews indexed\\n(1-20) byauthorofbook\\nJournal oftheAmerican 1888-1939 Author, subject\\nStatistical Association (1-34)\\nJournal oftheAmerican 1940-1955 Author, subject,bookreviewsindexed\\nStatistical Association (35-50) byauthorofbookandbysubject\\nRevuedeStatistique 1953-1969 Author, subject, bookreviews indexed\\nAppliquée (1-17) bysubject\\nStatistical Theoryand 1959-1966 Eachyearseparately, author, subject,\\nMethod Abstracts (1-7) bookreviews indexed byauthorof\\nbook\\nOfthese,thethirty-year indexoftheAnnals ofMathematical Sta-\\ntistics(Greenwood etal.1962)isalandmark inbibliographic technique\\nandisstillveryusefulthoughnolongerrecent.Theindexofseveral\\njournals compiled byJoineretal.(1970) istimelyandofaveryuseful\\ntype,probably muchcheaper tocompile thanthatexemplified by\\n(Greenwood etal.1962).\\nSince1964theScience Citation Index(theInstitute forScientific\\nInformation, Philadelphia) has beenpublished witheverincreasing\\ncoverage. Thisisanenormous enterprise showing whohascitedwhom\\ninabout3,000different journals, whichmakesitrelatively easytofind\\nrecentliterature onanyscientific topicforwhichoneortwoolderref-\\nerencesareknown.Though thecoverage forstatistics andrelated fields\\nmaynotyetbeverycomplete, thisfacility isalready useful.\\nAdditional bibliography\\nAczél,Janos\\n1966Lectures onFunctional Equations andThetrApplications, New\\nYork,AcademiePress.\\nSection 7.1.4isakeyreference foramathematical approach\\nthatinvestigates theconsequences offunctional equations that\\nsomehow suggestthemselves asaxiomsforprobability butdoesnot\\nseektointerpret probability.\\nAnscombe, Francis J.\\n1961“Bayesian statistics,” TheAmerican Statistician, 15,No.1,\\n21-24.\\nArchibald, G.C.\\n1959“Utility, risk,andlinearity,” Journal ofPolitical Economy, 67,\\n437-450. 104\\nBarnard, GeorgeA.\\n1947“Areviewof‘Sequential Analysis’ hyAbraham Wald,”Journal\\noftheAmerican Statistical Association, 42,658-664. Ww\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da703bb0-57d1-4191-b5f4-713e25c59b4b', embedding=None, metadata={'page_label': '303', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC SUPPLEMENT\\nBarnard, GeorgeA.\\n1963“Somelogical aspectsofthefiducialargument,” Journal ofthe\\nRoyalStatistical Society, SeriesB,25,111-114.\\nBarnard, GeorgeA.\\n1965“Theuseofthelikelihood function instatistical practice,” Pro-\\nceedings oftheFifthBerkeley Symposium onMathematical Sta-\\ntisticsandProbability, 1,27-40.\\nBarnard, G.A.,G.M.Jenkins, andC.B.Winsten\\n1962“Likelihood, inference, andtimeseries,” Journal oftheRoyal\\nStatistical Soctety, SeriesA,125,321-372.\\nBayes,Thomas\\n1958“Essaytowardsolvingaprobleminthedoctrine ofchances:\\nwithabiographical notebyG.A.Barnard,” Biometrika, 45,293-\\n315.(Alsopublished separately bytheBiometrika Office,Univer-\\nsityCollege,London.)\\nNewedition of[B8].\\nBirnbaum, Allan\\n1962“Onthefoundations ofstatistical inference,” Journal ofthe\\nAmerican Statistical Association, 57,269-306.\\nBirnbaum, Allan\\n1969“Coneepts ofstatistical evidence,” pp.112-143 inEssays in\\nHonorofErnestNagel,eds.SidneyMorgenbesser, Patrick Suppes,\\nandMortonWhite,NewYork,St.Martin’s Press.\\nBlackwell, D.,andL.Dubins\\n1962“Merging ofopinions withincreasing information,” Annals of\\nMathematical Statistics, 33,882-887.\\nBlum,Julius,andJudahRosenblatt\\n1967“Onpartialaprioriinformation instatistical inference,” Annals\\nofMathematical Statistics, 38,1671-1678.\\nSeeksacompromise between thepersonalistic andfrequentistic\\napproaches.\\nBorel,Emile\\n1924“Aproposd’untraitédeprobabilités,” RevuePhilosophique, 98,\\n321-336; reprinted inPratique etPhilosophie desProbabilités by\\nBorel,1939,Paris,Gauthier-Villars; translated in(Kyburg and\\nSmokler 1964).\\nThisreviewof[K4]contains theearliestaccount ofthemodern\\nconcept ofpersonal probability knowntome.\\nBox,GeorgeE.P.,andG.C.Tiao\\n1962“Afurther lookatrobustness viaBayes’theorem,” Biometrika,\\n49,419-433.\\nA.personalistic acconnt ofanimportant general problem in\\nstatistics.\\nBroad,C.D.\\n1969Induction, Probability, andCausation, Dordrecht, Holland, D.\\nReidelPublishing Company.\\nCollected papersofafamousphilosopher onthetitlethemes.\\nBross,IrwinD.J.\\n1963“Linguistic analvsis ofastatistical controversy,’ TheAmer-\\nicanStatistician, 17,18-21.\\nAntipersonalistie commentary.285\\n262\\nw\\nwv\\nav\\n214\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b57874d-f297-48f7-815e-6a08936c3d29', embedding=None, metadata={'page_label': '304', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='286 APPENDIX 4\\nBuhlmann, H.\\n1960“Austauschbare stochastische Variablen undihreGrenzwert-\\nsaetze,” University ofCalifornia Publications inStatistics, 3,1-36.\\nCarnap,Rudolph\\n1962“Theaimofinductive logic,”pp.303-318 in(Nagel, Suppes,\\nandTarski1962).\\nCérésole, Pierre\\n1915“L’irriductibilité deintuition desprobabilités etl’existance de\\npropositions mathématique indémontrables,” Archives dePsycho-\\nlogie,15,255-305.\\nRemarkable earlyideasaboutthesubjectivity ofprobability\\nandabouttheelusiveconcept oftheprobability ofmathematical\\npropositions.\\nChambers, Michael L.\\n1970“Asimpleproblem withstrikingly different frequentistic and\\nBayesian solutions,” Journal oftheRoyal Statistical Society,\\nSeriesB,32,278-282.\\nChao,M.T.\\n1970“Theasymptotic behavior ofBayes’estimators,” Annals of\\nMathematical Statistics, 41,601-608.\\nClarke,R.D.\\n1954“Theconcept ofprobability,” Journal oftheInstitute ofActu-\\naries,80,1-13(followed byabstract ofdiscussion, 14-31).\\nApersonalistic viewfromanactuarial standpoint.\\nCoolidge, JulianL.\\n1940AHistory ofGeometrical Methods, NewYork,Oxford Univer-\\nsityPress.(Reprinted: DoverPublications, NewYork,1963.\\nCornfield, Jerome\\n1966“Sequential trials,sequential analysis, andthehkelihood prin-\\neiple,”TheAmerican Statistician, 20,No.2,18-23.\\nCornfield, Jerome\\n1969“TheBayesian outlookanditsapplications,” Biometrics, 25,\\n617-642.\\nAsignificant statement ofthepersonalistic position instatistics.\\nCostantini, Domenico\\n1970Fondamenti delCalcolo delleProbabilita, Milano,Giangiacomo\\nFeltrinelli Editore.\\nArecentsurveyofthefoundations ofprobability byaneclec-\\nticallyinclined author.\\ndeFinetti,Bruno\\n1954“Media didecisioni enediadiopinioni,” Bulletin deUVInstitut\\nInternational deStatistique, 28thsession, 34,144-157.\\ndeFinetti,Bruno\\n1961“TheBayesian approach totherejection ofoutliers,” pp.199-\\n210inVol.1ofProceedings oftheFourth [1960]Berkeley Sym-\\nposium onMathematical Statistics andProbability, ed.Jerzy\\nNeyman,Berkeley andLosAngeles, University ofCalifornia Press.\\ndeFinetti,Bruno\\n1968“Probability: Interpretations,” pp.496-505 intheInternational\\nEncyclopedia oftheSocialSciences, NewYork,Macmillan.\\nApenetrating overview ofthephilosophy ofprobability.53\\n214\\nav\\n177\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b68c43b9-7fcf-4863-9767-91ca1f414636', embedding=None, metadata={'page_label': '305', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC SUPPLEMENT\\ndeFinetti, Bruno,andLeonard J.Savage\\n1962 “Sul mododiscegliere leprobabilita iniziali,” Suifondamenti\\ndellastatistica, Biblioteca delMetron, SeriesC,1,81-147.(A\\nfairlyextensive Englishsummary, 148-151.)\\nDiscusses atlengthmanyfacetsoftheinterpretation andappli-\\ncationofthepersonalistic position.\\ndeJouvenel, Bertrand\\n1967TheArtofConjecture, NewYork,BasicBooks.\\nAhistorical andliterary approach tointelligent andimagina-\\ntiveguessing.\\nDeZeeuw, G.,C.A.J.Viek,andW.A.Wagenaar\\n1970Subjective Probability: Theory, Experiments, Applications\\n(consists ofissueNo.2/3,Vol.34ofActaPsychologica), Ams-\\nterdam,North-Holland Publishing Co.\\nDempster, ArthurP.\\n1964“Onthedifficulties inherent inFisher’s fiducial argument,”\\nJournal oftheAmerican Statistical Association, 59,67-88.\\nDempster, ArthurP.\\n1968“Ageneralization ofBayesian inference,” Journal oftheRoyal\\nStatistical Society, SeriesB,30,205-247.\\nDickey,JamesM.\\n1971“Theweighted likelihood ratio,linearhypotheses onnormal\\nlocation parameters,’ Annals ofMathematical Statistics, 42,\\n204-223.\\nAkeyreference forthepersonalistic approach inmultivariate\\nstatistics.\\nDreze,Jacques\\n1961“Fondements logiques delaprobabilité subjective etdel’util-\\nité,”pp.73-87inLaDécision, Paris,CentreNational delaRecher-\\ncheScientifique.\\nFleesthetyranny ofconsequences thatdonotdepend onstates\\nandstatesthatdonotdepend onacts.\\nDubins, LesterE.\\n1969“Anelementary proofofBochner’s finitely additive Radon-\\nNikodymtheorem,” American Mathematical Monthly, 76,No.5,\\n520-523.\\nDubins,LesterE.,andLeonard J.Savage\\n1965HowtoGamble IfyouMust:Inequalities forStochastic Proc-\\nesses,NewYork,McGraw-Hill BookCo.\\nEdwards, A.W.F.\\n1969“Statistical methods inscientific inference,” Nature, 222,1233-\\n1237.\\nChampions anotionof prior likelihood asopposed topriorprob-\\nability. Contains interesting criticalandhistorical sidelights.\\nEdwards, Ward\\n1969 <ABibliography ofResearch onBehavior Decision Processes to\\n1968,Human Performance CenterMemorandum Report No.7,\\nAnnArbor,University ofMichigan Press.\\nEdwards, Ward,HaroldLindman, andLeonard J.Savage\\n1963“Bavesian statistical inference forpsychological research,” Psy-\\nchological Review, 70,193-242.262\\n58\\n35\\n283\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='029e3a10-dac9-4b5d-a2af-b183ecf87d12', embedding=None, metadata={'page_label': '306', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='288 APPENDIX 4\\nElementary butseriousdiscussion ofthepersonalistic position\\ninstatistics. Notparticularly confined topsychology.\\nEdwards, Ward,L.D.Philips,W.L.Hays,andB.C.Goodman\\n1968‘Probability information processing systems: Designandevalu-\\nation,”IHEETransactions onSystems Science andCybernetics,\\nSSC-4,248-265.\\nConcerned withthepractical useofpersonal probabilities in-\\nvolving theopinions ofdifferent people,eachabouthisownarea\\nofcompetence.\\nEdwards, Ward,andA.Tversky\\n1967Decision Making, Baltimore, Penguin Books.\\nAsmallanthology centering around empirical aspects ofdecis-\\nionmaking, including computer-aided improvement ofdecision.\\nEllsberg, Daniel,William Fellner,andHoward Raiffa\\n1961“Symposium: Decisions underuncertainty,” Quarterly Journal\\nofEconomics, 75,643-694.\\nAkeyreference foracertaintypeofdeparture fromthetheory\\nofpersonal probability andutilityinthisbook.\\nEricson,Wilham A.\\n1969“Subjective Bayesian models insampling finitepopulations,”\\nJournal oftheRoyalStatistical Society, SeriesB,31,195-224.\\nAkeyreference foranewlineofthinking aboutthetheoryof\\nsampling.\\nFabius, J.\\n1964“Asymptotic behavior ofBayes’estimates,” Annals ofMathe-\\nmatical Statistics, 35,846-856.\\nFishburn, PeterC.\\n1964Decision andValueTheory,NewYork,JohnWileyandSons.\\nTreatsdecisionandpreference extensively inafashionharmon-\\niouswiththisbook,andwithaviewtoapplications moretoman-\\nagement thantostatistics.\\nFishburn, PeterC.\\n1970UtilityTheoryforDecision Making,NewYork,JohnWileyand214\\nSons. 40,78,80\\nLucidly treatsagreatvariety ofaxiomatic approaches topref-\\nerence, withandwithout uncertainty, including theaxiomatic\\naspects of thisbookandlaterdevelopments.\\nFisher,RonaldA.\\n1934“Twonewproperties ofmathematical likelihood,” Proceedings\\noftheRoyalSociety, Series A,144,205-221. (Paper24of[F6].)\\nFisher,RonaldA.\\n1955“Statistical methods andscientific induction,” Journal ofthe\\nRoyalStatistical Society, SeriesB,17,69-78.\\nFisher,RonaldA.\\n1956Statistical Methods andScientific Inference, NewYork,Hafner.\\nFraser,Donald A.S.\\n1968TheStructure ofInference, NewYork,JohnWileyandSons.\\nAtheoryofstatistical inference thatseemstoderivefromideas\\noffiducial probability andofnecessarian reliance onsymmetry\\nandignorance.68\\naw\\n262\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bef21e2b-42a4-4fe6-966e-b7527fd1cd1b', embedding=None, metadata={'page_label': '307', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC SUPPLEMENT\\nFréchet, Maurice\\n1955“SurVimportance enéconométrie deladistinction entreles\\nprobabilités rationelles etirrationelles,” Econometrica, 23,303-306.\\nFreedman, David\\n1962“Invariants undermixingwhichgeneralize deFinetti’s theo-\\nrem,”Annals ofMathematical Statistics, 33,916-924.\\nFreedman, David\\n1963“Invariants undermixingwhichgeneralized deFinetti’s theo-\\nrem:Continuous timeparameter,” Annals ofMathematical Sta-\\ntistics,34,1194-1216.\\nFreedman, David\\n1965“Ontheasymptotic behavior ofBayesestimates inthediscrete\\ncaseII,”Annals ofMathematical Statistics, 36,454-456.\\nGauss,CarlFriedrich\\n1821“Theoria combinationis observationum erroribus minimis ob-\\nnoxiae,” Commentationes Societatis RegiaeScientarum Gottingen-\\nsisRecentiores, 5,33-90.InGerman translation, A.Borsechand\\nP.Simon,Abhandlungen zurMethode derkleinsten Quadrate,\\nBerlin,Westdruckerei Joachim Frickert, 1887(reprinted Wurz-\\nburg:Physica-Verlag, 1964).InFrench translation, J.Bertrand,\\nMethode desMoindres Carrés, Paris,Mallet-Bachelier, 1855.\\nGeorge,Stephen L.\\n1968AnAnnotated Bibliography ontheFoundations ofStatistical\\nInference, Institute ofStatistics Mimeograph SeriesNo.572,\\nRaleigh, NorthCarolina StateCollege Press.\\nGiere,RonaldN.\\n1969“Bayesian statistics andbiasedprocedures,” Synthese, 20,371-\\n387.\\nAphilosophic account ofdifferences between Bayesian and\\nNeyman-Pearsonian statistics, concluding infavorofthelatter.\\nGnedenko, B.V.\\n1962TheTheory ofProbability, NewYork,Chelsea Publishing Co.\\nThisedition,morethanlaterones,contains somephilosophical\\npassages, afewofthemreferring todialectical materialism.\\nGood,IrvingJohn\\n1959“Kindsofprobability,” Science, 129,443-447.\\nGood,IrvingJohn\\n1962“Subjective probability asthemeasure ofanon-measurable\\nset,”pp.319-329 in(Nagel,Suppes,andTarski1962).\\nGood,IrvingJohn\\n1965TheEstimation ofProbabilities: AnEssayonModern Bayesian\\nMethods, Cambridge, Massachusetts Institute ofTechnology Press.\\nGoodman, LeoA.\\n1953“Asimplemethod forimproving someestimators,” Annals of\\nMathematical Statistics, 24,114-117.\\nGrayson, C.Jackson, Jr.\\n1960Decisions UnderUncertainty: Drilling Decisions byOilandGas\\nOperators, Cambridge, Harvard University Press.\\nInteresting discussion ofpossible applications ofpersonal prob-\\nabilityandutilitytoakindofbusiness decision greatly affected\\nbyuncertainty.289\\n53\\n53\\n214\\n230\\n283\\n58\\n224\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9a46e77-5d6c-43b3-957d-e980536a9d58', embedding=None, metadata={'page_label': '308', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='290 APPENDIX 4\\nGreenwood, J.Arthur,Ingram Olkin,andI.Richard Savage (eds.)\\n1962Annals ofMathematical Statistics: Index toVols.1-31:\\n1930-1960, St.Paul,Minnesota, NorthCentralPublishing Co.283,284\\nAsophisticated bibliographic apparatus fortheperiodcovered,\\nincluding notonlyauthoranddetailed subject indexes forthe\\nAnnals ofMathematical Statistics, butalsoindexing citations to\\ntheAnnalsandcitations madeintheAnnals tootherjournals.\\nHaag,Jules\\n1928“Surunprobléme général deprobabilités etsesdiverses appli-\\ncations,” pp.659-674 inProceedings oftheInternational Congress\\nofMathematicians, Toronto 1924,Toronto, University ofToronto\\nPress. 52\\nHacking, Ian\\n1965LogicofStatistical Inference, Cambridge, BasicBooks.\\nHacking, Ian\\n1967“Shghtly morerealistic personal probability,” Philosophy of\\nScience, 34,311-325.\\nInteresting foritsownthesesbutalsoforitssensitive inter-\\npretation ofstatistical literature.\\nHajek,Jaroslav\\n1965“Onbasicconcepts ofstatistics,’ Proceedings oftheFifth\\n[1965/66] Berkeley Symposium onMathematical Statistics and\\nProbability, 1,139-162.\\nHakansson, NilsH.\\n1970“Friedman-Savage utilityfunctions consistent withriskaver-\\nsion,’Quarterly Journal ofEconomics, 84,472-487. 104\\nHalphen, Etienne\\n1955“Lanotiondevraisemblance,” Publication deV’Institut deSta-\\ntistique deVUniversité deParis,4,41-92.\\nThoughts ofanoriginal, andtoolittleknown, figureonthe\\nfoundations ofprobability.\\nHarrod,Roy\\n1956Foundations ofInductive Logic,NewYork,Harcourt, Brace\\nandCo.\\nHewitt, Edwin,andLeonard J.Savage\\n1955“Symmetric measures onCartesian products,” Transactions of\\ntheAmerican Mathematical Society, 80,470-501. 53\\nHildreth, C.\\n1963“Bayesian statisticians andremote clients,’ Econometrica, 31,\\n422-439.\\nBrings out,andreflectsupon,animportant pointnevermade\\nexplicit inthisbook :thepersoninpersonalistic statistics ishardly\\neverastatistician oreventhescientific investigator butisoften\\noneamongthepublicwhoappraises ascientific publication.\\nHill,BruceM.\\n1963“Thethree-parameter lognormal distribution andBayesian\\nanalysis ofapoint-source epidemic,” Journal oftheAmerican\\nStatistical Association, 58,72-85.\\nHill,BruceM.\\n1969“Foundations forthetheoryofleastsquares,” Journal ofthe\\n“I\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ef55a84b-c20f-47d0-abd5-646ba3624cac', embedding=None, metadata={'page_label': '309', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC SUPPLEMENT 291\\nRoyalStatistical Society, SeriesB,31,89-97.\\nPersonalistic treatment ofatopiccentraltomodernstatistics.\\nHilpinen, Risto\\n1968RulesofAcceptance andInductive Logic(ActaPhilosophica\\nFennica, Issue2),Amsterdam, North-Holland Publishing Com-\\npany.\\nHirshleifer, Jack\\n1961“TheBayesian approachtostatistical decision:Anexposition,”\\nTheJournal ofBusiness, 34,471-489.\\nHoadley, Bruce\\n1970“ABayesian lookatinverse linearregression,” Journal ofthe\\nAmerican Statistical Association, 65,356-369.\\nAnexample ofpersonalistic ideasinthestudyofaspecific prob-\\nleminstatistics.\\nHuber,PeterJ.\\n1964“Robust estimation ofalocation parameter,” AnnalsofMathe-\\nmatical Statistics, 35,73-101.\\nAnimportant nonpersonalistic advance inthecentralproblem\\nofstatistical robustness.\\nJeffrey,Richard C.\\n1965TheLogicofDecision, NewYork,McGraw-Hill BookCo.\\nAninteresting departure fromthetheory ofpersonal prob-\\nabilityandutilityasrepresented bythisbook.\\nJeffreys, Harold\\n1955“Thepresent position ofthetheory ofprobahility,” British\\nJournal forthePhilosophy ofScience, 5,275-289.\\nJoiner,BrianL.,N.F.Laubscher, Eleanor S.Brown,andBertLevy\\n1970AnAuthor andPermuted TitleIndex toSelected Statistical\\nJournals, National Bureau ofStandards SpecialPublication 321,\\nWashington, United StatesDepartment ofCommerce. 283,284\\nBrings thecumulative.indexes ofsevenjournals upthrough a\\nlittlelaterthan1968inaformthatisconvenient andpowerful.\\nKendall, M.G.,andA.Stuart\\n1958TheAdvanced Theory ofStatistics: Vol.1,Distribution Theory,\\nLondon, Charles GriffenandCo.\\nKendall, M.G.,andA.Stuart\\n1961TheAdvanced Theory ofStatistics: Vol.2,Inference andRela-\\ntionship, London, Charles GriffenandCo.\\nThisitemandtheonebefore, together withaprojected third\\nvolume, Planning andAnalysis, andTime-Series, willconstitute\\naradically neweditionof[K2].\\nKraft,C.H.,JohnW.Pratt,andA.Seidenberg\\n1959“Intuitive probability onfinitesets,”Annals ofMathematical\\nStatistics, 30,408-419. 40\\nKullback, Solomon\\n1961Information TheoryandStatistics, NewYork,JohnWileyand\\nSons. . 50\\nKyburg, HenrvE.,Jr.\\n1961Probability andtheLogicofRational Belief,Middleton, Conn.,\\nWesleyan University Press.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1fced1c-ded5-4303-b1aa-3860bb3c15e2', embedding=None, metadata={'page_label': '310', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='292 APPENDIX 4\\nKyburg, HenryE.,Jr.,andErnestNagel(eds.)\\n1963Induction: SomeCurrent Issues,Middletown, Wesleyan Uni-\\nversity Press.\\nKyburg,HenryE.,Jr.,andHoward E.Smokler\\n1964Studies inSubjective Probability, NewYork,JohnWileyand\\nSons.\\nAnanthology consisting ofanextractfromJohnVenn,trans-\\nlationsof(Borel1924)and[D2],andreproductions of[R1],\\n[K10],and(Savage 1961).Hasalargebibliography.\\nLancaster, HenryO.\\n1968Bibliography ofStatistical Bibliographies, Edinburgh, Oliver\\nandBoyd.\\nLancaster, HenryO.\\n1969“Abibliography ofstatistical bibliographies: Asecond list,”\\nReview oftheInternational Statistical Institute, 37,57-67.\\nLancaster, HenryO.\\n1970“Abibliography ofstatistical bibliographies: Athirdlist,”\\nReview oftheInternational Statistical Institute, 38,258-267.\\nLeCam,Lucien\\n1964“Sufficiency andapproximate sufficiency,” Annals ofMathe-\\nmatical Statistics, 35,1419-1455.\\nLehmann, E.L.\\n1958“Significance levelandpower,” Annals ofMathematical Sta-\\ntistics,29,1167-1176.\\nLehmann, E.L.\\n1959Testing Statistical Hypotheses, NewYork,JohnWileyand\\nSons.\\nExcellent illustration ofNeyman-Pearson theory, showing\\nolderandneweraspects intensionwitheachother.\\nLindley, DennisV.\\n1958“Professor Hogben’s ‘Crisis’-—A surveyofthefoundations of\\nstatistics,” Applied Statistics, 7,186-198.\\nLindley, DennisV.\\n1965Introduction toProbability andStatistics: FromaBayesian\\nViewpoint: Part1,Probability; Part2,Inference, Cambridge,\\nCambridge University Press.\\nLinnik,Yu.V.\\n1968Statistical Problems withNuisance Parameters, Translations\\nofMathematical Monographs, 20,Providence, American Mathe-\\nmatical Society.\\nLuce,R.Duncan, RobertR.Bush,andEugeneGalanter (eds.)\\n1965Readings inMathematical Psychology, Volume II,NewYork,\\nJohnWileyandSons.\\nPartsVandVIcontain relevant readings.\\nLuce,R.Duncan, andHoward Raiffa\\n1957GamesandDecision, NewYork,JohnWileyandSons.\\nGoodaccount ofthetheoryofgamesanditscontacts withthe\\nnormative theoryofdecision.\\nLuce,R.Duncan, andPatrickSuppes\\n1965“Preference, utility,andsubjective probability,” pp.249-410a4\\n283\\n283\\n283\\n134\\naw\\n262\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6952bb98-8416-4a4d-b630-585cc76346d9', embedding=None, metadata={'page_label': '311', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC SUPPLEMENT\\ninVol.3ofHandbook ofMathematical Psychology, eds.R.Dun-\\neanLuce,Robert R.Bush,andEugene Galanter, NewYork,\\nLondon,andSydney,JohnWileyandSons.\\nValuable foritselfandasakeyreference.\\nLukasiewicz, Jan\\n1970“Logical foundations ofprobability theory,” inJanLukasie-\\nwicz:Selected Works, ed.L.Borkowski, Amsterdam andLondon,\\nNorth-Holland Publishing Company, andWarsaw,PWN,Polish\\nScientific Publishers.\\nEnglish translations oftheoriginal Gerinan version of1913.\\nEloquently presents anearlyformal-logic approach toprobability.\\nValuable forinsightsandhistory.\\nLusted,LeeB.\\n1968Introduction toMedical Decision Making, Springfield, Illinois,\\nC.C.Thomas.\\nHopeful butrealistic discussion ofapromising fieldofapphi-\\neationforpersonalistic ideas.\\nMaritz, J.S.\\n1970Empirical BayesMethods, London, Methuen.\\nKeyreference forabranch offrequentistic statistics with\\nBayesian elements.\\nMarkowitz, HarryM.\\n1959Portfolio Selection, CowlesCommission Monograph No.16,\\nNewYork,JohnWiley and Sons.\\nApplication ofthetheoryofutilitytothetheoryofinvestment.\\nMarschak, Jacob\\n1968“Decision making: Economie aspects,” pp.42-45intheInter-\\nnational Encyclopedia oftheSocialSciences, NewYork,Mac-\\nmillan.\\nMilher-Gruzewska, Halina\\n1949“Onthelawofprobability andthecharacteristic function of\\nthestandardized sumofequivalent variables,” Towarzystwo Nau-\\nkoweWarzawshkie, 42,99-142.\\nMilier-Gruzewska, Halina\\n1950“Sullaleggelimitedellevariabili casuali equivalenti,” Att:\\nMemorie delluAccademia Nazionale deiLincei, Series 8,2(First\\nsection), 25-33.\\nMiller,RobertB.\\n1969 <ASelected Bayesian Statistics Bibliography, Department of\\nStatistics Technical PaperNo.214,Madison, University ofWis-\\nconsinPress.\\nMorse,Norman, andRichard Sacksteder\\n1966“Statistical isomorphisin,” Annals ofMathematical Statistics,\\n37,203-214.\\nMosteller, Frederick, andDavidL.Wallace\\n1964Inference andDisputed Authorship: TheFederalist, Reading,\\nMassachusetts, Addison-Wesley.\\nAthorough andilluminating application ofstatistics illustrat-\\ningbothpersonalistic andnonpersonalistic approaches.293\\n53\\n53\\n283\\n152\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f5c0c3b-0d91-4304-b50c-02513b42aea9', embedding=None, metadata={'page_label': '312', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='294 APPENDIX 4\\nNagel,Ernest,PatrickSuppes,andAlfredTarski (eds.)\\n1962Logic,Methodology andPhilosophy ofScience, Stanford, Stan-\\nfordUniversity Press.\\nNeyman, Jerzy\\n1952Lectures andConferences onMathematical Statistics andProb-\\nability,Washington, Graduate SchooloftheUnitedStatesDepart-\\nmentofAgriculture. (Firstedition1938,mimeographed. )\\nNeyman, Jerzy\\n1957“Current problems ofmathematical statistics,” pp.349-370 in\\nVol.1ofProceedings oftheInternational Congress ofMathe-\\nmaticians [Amsterdam, 1954],Groningen, E.P.Nordhoff.\\nNeyinan, Jerzy\\n1967<ASelection ofEarlyStatistical Pupers ofJ.Neyman, Cam-\\nbridge,Cambridge University Press.\\nNeyman, Jerzy,andEgonS.Pearson\\n1967JointStatistical Papers byJ.Neyman andE.8.Pearson,\\nBerkeley, University ofCalifornia Press.\\nPatil,Venkutai H.\\n1965“Approximation totheBehrens-Fisher distributions,” Biomet-\\nrika,52,267-271.\\nPearson, EgonS.\\n1966TheSelected Papers ofE.S.Peurson, Berkeley andLosAn-\\ngeles,University ofCalifornia Press.\\nPfanzagl, Johann\\n1968Theory ofMeasurement, NewYork,JohnWileyandSons.\\nMeasurement ofutilityandpersonal probability istypicalof\\ntheconcerns ofthishighlymathematical book.\\nPlackett, RobertL.\\n1966 “Current trendsinstatistical inference,” Journal oftheRoyal\\nStatistical Society, SeriesA,129,249-267.\\nPolya,G.\\n1954Mathematics andPlausible Reasoning. Vol.I,Induction and\\nAnalogy inMathematics. Vol.II,Patterns ofPlausible Inference,\\nPrinceton, Princeton University Press,andLondon, OxfordUni-\\nversity Press.\\nPopper, Karl\\n1959TheLogicofScientific Discovery, London, Hutchinson. (Re-\\nprinted: ScienceEditions, NewYork,1961.)\\nPratt,JohnW.\\n1961“Review ofE.H.Lehmann’s ‘Testing Statistical Hypotheses, ”\\nJournal oftheAmerican Statistical Association, 56,163-167.\\nApersonalistic review ofthehighly respected frequentistic\\nwork(Lehmann 1959.)\\nPratt,JohnW.\\n1964“Riskaversion inthesmallandinthelarge,”Econometrica, 32,\\n1-2,122-136.\\nAvaluable contribution tothetheoryofutility.\\nPratt,JohnW.\\n1965“Bayesian interpretation ofstandard inference statements,”\\nJournal oftheRoyalStatistical Society, SeriesB,27,169-203.262\\n“J\\nav\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='739c1f20-fd34-479f-9520-cd09dae25c35', embedding=None, metadata={'page_label': '313', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC SUPPLEMENT\\nQuine,W.V.\\n1951“Twodogmas ofempiricism,” Philosophical Review, 60,20-43.\\n(Reprinted inFromaLogicalPointofView,1953,Cambridge,\\nHarvard University Press.)\\nRaiffa,Howard\\n1968Decision Analysis: Introductory Lectures onChoices underUn-\\ncertainty, Reading, Massachusetts, Addison-Wesley.\\nRaiffa,Howard, andRobertSchlaifer\\n1961Applied Statistical Decision Theory, Cambridge, Harvard Uni-\\nversity Press.\\nAratheradvanced textbook inpersonalistic statistics.\\nRényi,Alfréd\\n1970Foundations ofProbability, SanFrancisco, Holden-Day.\\nMoremathematical thanphilosophical butimportant forthe\\nseriousphilosophical students of probability.\\nRényi,Alfréd,andP.Révész\\n1963“Astudyofsequences ofequivalent eventsasspecial stable se-\\nquences,” Publicationes Mathematecae, Debrecen, 10,319-325.\\nRoberts,HarryV.\\n1963“Risk,ambiguity, andtheSavageaxioms:Comment,” Quarterly\\nJournal ofEconomics, 77,327-342.\\nRyll-Nardzewski, C.\\n1957“Onstationary sequences ofrandom variables andthedeFin-\\netti’sequivalence,” Colloquium Mathematicum, 4,149-156.\\nSalmon,Wesley C.\\n1966TheFoundations ofScientific Inference, Pittsburgh, University\\nofPittsburgh Press.\\nLucidreviewandstudyoftheproblemofinduction bequeathed\\ntousbyHume.\\nSavage, I.Richard\\n1968Statistics: Uncertainty andBehavior, Boston, Houghton Mif-\\nflinCompany.\\nAnelementary statistical textbook inthepersonalistic Bayesian\\nspirit.\\nSavage,Leonard J.\\n1960“Recent tendencies inthefoundations ofstatistics,” pp.540-544\\ninProceedings oftheInternational Congress ofMathematicians\\n[Edinburgh, 1958],Cambridge, Cambridge University Press.\\nSavage,Leonard J.\\n1961“Thefoundations ofstatistics reconsidered,” pp.575-586 inVol.\\nIofProceedings oftheFourth [1960]Berkeley Symposium on\\nMathematical Statistics andProbability, ed.JerzyNeyman, Berk-\\neley,University ofCalifornia Press.\\nSavage, LeonardJ.\\n1962“Bayesian statistics,” pp.161-194 inRecentDevelopments in\\nDecision andInformation Processes, eds.Robert E.Machol and\\nPaulGray,NewYork,Macnullan Co.\\nSavage,Leonard J.\\n1962“Subjective probability andstatistical practice,” pp.9-35in\\n(Savage, etal,1962).295\\n25\\n53\\n53\\nw\\n0)\\n217\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97a829d8-042a-4bef-bedf-da7ecda87618', embedding=None, metadata={'page_label': '314', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='296 APPENDIX 4\\nSavage, Leonard J.\\n1967 “Difficulties inthetheory ofpersonal probability,” Philosophy\\nofScience, 34,305-310.\\nSavage, Leonard J.\\n1967 “Implications ofpersonal probability for induction,” Journal\\nofPhilosophy, 64,593-607.\\nSavage, Leonard J.\\n1970 “Reading suggestions for the foundations ofstatistics,’ The\\nAmerican Statistician, 24,No. 4,23-27.\\nSomewhat overlaps, but ismuch shorter than, the present Bib-\\nhiographic Supplement.\\nSavage, Leonard J.\\n1971 “Elicitation ofpersonal probabilities and expectations,” Jour-\\nnal oftheAmerican Statistical Association, 66,783-801.\\nSee (Staél von Holstein 1970).\\nSavage, Leonard J.,etal.\\n1962 The Foundations ofStatistical Inference: ASymposium, New\\nYork, John Wiley and Sons.\\nValuable for the interchange ofideas among statisticians of\\ndiverse experience and outlook.\\nScheffé, Henry\\n1970 “Practical solutions ofthe Behrens-Fisher problem,” Journal\\noftheAmerican Statistical Association, 65,1501-1598.\\nSchelling, Thomas C.\\n1960 The Strategy of Conflict, Cambridge, Harvard University\\nPress.\\nPertinent because conflict and group decision areaspects ofthe\\nsame thing. Extramathematical and particularly stimulating.\\nSchlaifer, Robert\\n1959 Probability and Statistics for Business Decisions, New York,\\nMcGraw-Hill Book Co.\\nSchmitt, Samuel A.\\n1969 Measuring Uncertainty: An Elementary Introduction toBay-\\nestan Statistics, Reading, Massachusetts, Addison-Wesley.\\nShelly, Maynard W., II,and Glenn L.Bryan (eds.)\\n1964 Human Judgments and Optimality, New York, John Wiley and\\nSons.\\nAn organized collection of essays bymany authors. Interesting\\ninitself and useful asanextensive key reference.\\nSmith, Cedric A.B.\\n1961 ‘Consistency instatistical inference and decision,” Journal of\\ntheRoyal Statistical Society, Series B,23, 1-25.\\nSmith, Cedric A.B.\\n1965 “Personal probability and statistical analysis,” Journal ofthe\\nRoyal Statistical Society, Series A,128, 469-489.\\nStaél von Holstein, Carl-Axel S.\\n1970 Assessment and Evaluation ofSubjective Probability Distribu-\\ntions, Stockholm, The Economic Research Institute atthe Stock-\\nholm School ofEconomies.\\nExcellent monograph onhowtoelicit personal probabilities and\\nwhat todowith them. Reviews and enriches aconsiderable litera-\\nture. Related references are (Savage 1971; Winkler 1966).283\\n262\\n58\\n177\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b7564f1-10d9-459b-9d4c-3cc2a5a5e0d9', embedding=None, metadata={'page_label': '315', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHIC SUPPLEMENT\\nStone, Mervyn\\n1970 “The role ofexperimental randomization inBayesian statistics :\\nFinite sampling and two Bayesians,” Biometrika, 56,681-683.\\nStrawderman, William E.\\n1971 “Proper Bayes minimax estimators ofthe multivariate normal\\nmean,” Annals ofMathematical Statistics, 42,385-388.\\nKey reference forachallenging theoretical development initi-\\nated byCharles Stein.\\nSuppes, Patrick\\n1960 “Some open problems inthefoundations ofsubjective proba-\\nbility,” pp. 162-169 inInformation and Decision Processes, ed.\\nRobert Machol, New York, McGraw-Hill Book Co.\\nTavaneg, P.V., ed.\\n1970 Problems oftheLogic ofScientific Knowledge, translated by\\nT.J.Blakely, New York, Humanities Press.\\nArare opportunity toread inEnglish theideas ofsome modern\\nSoviet philosophers about probability and induction.\\nTribe, Laurence H.\\n1971 “Trial bymathematics: Precision and ritual inthe legal proc-\\ness,” Harvard Law Reriew, 84,1329-1393.\\nAkey reference onthepossible applicability ofprobabilistic ideas\\ninthecourts, which theauthor does notfind promising.\\nTnbus, Myron\\n1969 Rational Descriptions, Decisions and Designs, New York, Per-\\nganion Press.\\nAnecessarian approach.\\nTukey, John W.\\n1957 “Some examples with fiducial relevance,” Annals ofMathema-\\ntical Statistics, 28,687-695.\\nTukey, John W.\\n1962 “The future ofdata analysis,” Annuls ofMathematical Statis-\\ntics, 33,1-67.\\nUlam, Stanislaw\\n1930 “Zur Masstheorie inderallgemeinen Mengenlehre,” Fundamenta\\nMathematicae, 16,140-150.\\nvan Dantzig, David\\n1950-1 “Review ofCarnap’s Logicul Foundations ofProbability,”\\nSynthese, 8,459-470.\\nvan Dantzig, David\\n1957 “Statistical priesthood: Savage onpersonal probabilities,” Sta-\\ntistica Neerlandica, 11,1-16.\\nvanDantzig, David\\n1957 “Statistical priesthood II: SirRonald onscientific inference,”\\nStatistica Neerlandica, 11,185-200.\\nThe three preceding references review three different views of\\nthefoundations ofprobahility and statistics from thestandpoint\\nofafourth.\\nVetter, Hermann\\n1967 Wahrscheinichkeit und logischer Spielraum, Tubingen, J.C.B.\\nMohr (Pau! Siebeck).297\\n262\\n41\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='64af8ee3-cc9d-4d04-a635-e6e4c8dedad5', embedding=None, metadata={'page_label': '316', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='298 APPENDIX 4\\nVillegas, C.\\n1964 “On qualitative probability g-algebras,” Annals ofMathema-\\ntical Statistics, 35,1787-1796.\\nvon Mises, Richard\\n1942 “On thecorrect useofBayes’ formula,” Annals ofMathematical\\nStatistics, 13,156-165.\\nIllustrates anapproach unusual forafrequentist.\\nvon Wright, Georg Henrik\\n1962 “Remarks onepistemology ofsubjective probability,” pp. 330-\\n339 in(Nagel, Suppes, and Tarski 1962).\\nWald, Abraham (ed.)\\n1955 Selected Papers inStatistics and Probability, New York, Me-\\nGraw-Hill Book Co.\\nWasserman, Paul, andFred 8.Silander\\n1958 Decision Making: AnAnnotated Bibliography, Ithaca, Cornell\\nUniversity Press.\\nWasserman, Paul, andFred S.Silander\\n1964 Decision-Making: An Annotated Bibliography: Supplement,\\n1958-63, Ithaca, Cornell University Press.\\nWatts, Donald G.(ed.)\\n1967 The Future ofStatistics, Proceedings ofaConference onthe\\nFuture ofStatistics held atthe University ofWisconsin, June\\n1967, New York and London, Academie Press.\\nWetherill, G.B.\\n1961 “Bayesian sequential analysis,” Biometrika, 48,281-292.\\nWhittle, Peter\\n1957 “Curve and periodogram smoothing,” Journal ofthe Royal\\nStatistical Society, Series B,19,38-47.\\nWhittle, Peter\\n1958 “On thesmoothing ofprobability density functions,” Journal\\noftheRoyal Statistical Society, Series B,20,334-343.\\nThese two references aresuggestive forpersonalistic technique.\\nWilliams, J.S.\\n1966 “The role of probability in fiducial inference,” Sankhya,\\nSeries A,28,271-296.\\nWinkler, Robert L.\\n1968 “The consensus ofsubjective probahility distributions,’ Man-\\nagement Science, 15, 2,B61-B75.\\nWolfowitz, J.\\n1962 “Bayesian inference and axioms ofconsistent decision,” Eco-\\nnometrica, 30,471-479.\\nWolfowitz, J.\\n1970 “Reflections onthe future ofmathematical statistics,” pp. 739-\\n750 inEssays inProbability and Statistics, eds. R.C.Boseet al.,\\nChapel Hill, University ofNorth Carolina Press.43\\n283\\n283\\n262\\n177\\naw\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4805187-d99f-4717-94f3-e804f20f3f6b', embedding=None, metadata={'page_label': '317', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Technical Symbols\\nThis index isintended tolead tothe definitions ofalltechnical symbols that are\\ndefined inthe text and used extensively. Some symbols have more than one page\\nreference, corresponding totheir use inmore than one sense, depending oncontext.\\nA,B,C,11,263\\nB,(B], 85\\nc(s |B),e(|B),263\\nE(x), 263\\nE(x| B),264\\nE(x| y),E(x| y),265\\nF,14\\nf,9,h,14,82\\nF,f g;h85\\nF,14\\nf,g,h,14,82\\nf,f, 85\\nf,g,h,71\\nGLB, 80\\nHo, Hi,247\\nH()\\\\; x),236\\ninf,80\\nI(£; 7),163, 173\\n1,75\\nJ(F; 7),J,148\\nJ,235\\nk,123\\nLUB, 80\\nL*,164, 174, 180, 184\\nLx,180, 184\\nL(r; 2),L,178\\nLif; g),179, 180, 184, 186\\nL(f; 1),163, 174, 180, 187\\nL(r; g),187\\ni(g), 127\\n1,1,221\\nT(n), 241\\nm(r), 149\\nN,247(1\\nP,33\\nR,R,[r], 195\\nR,S,130\\nr,135\\nsup, 267\\nS,8,11,263\\nS,85\\n5,3’,85\\nT,T’,193\\nT—, 194\\nU,193\\nU,69\\nV(x), 268\\nv,82\\nv(F 8),123\\no(F 8,2),oF |x),125\\nB,B(2), 121\\nB*,149\\nB(x), 125\\nBi |x),124\\ne,e,11\\n=p, 47\\nf,263\\n=,2,C,11\\n~,11\\nU,A,1W\\n<,18,25,31,72\\n<+,18\\n>,<,>,+,19\\n=,25\\n<=ra\\n?,i\\n299\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f2d075e3-2f45-4ae7-a84a-1bf56ecc3d18', embedding=None, metadata={'page_label': '318', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbe5bda0-d733-4738-95be-bb1e19bf917f', embedding=None, metadata={'page_label': '319', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Author Index\\nThisindexisintended toleadtoeveryreference madeinthetext toanauthor’s\\nworksoropinions. Onlyafewoftheauthorsreferred todonothaveworkslistedin\\nthebibliography (p.271).\\nAfewexamples illustrate theuseofthisindex:F.J.Anscombeisnotreferred to\\ninthetextproper,butthereisareference tohim,beyond themerelisting of his\\nname,inthebibliography under[A4].ApaperofwhichDavidBlackwell isaco-\\nauthor,butwhose firstlistedauthor isKenneth J.Arrow, issomewhere referred to\\nwithout mention ofBlackwell’s name,butonlyabibliographic symbol oftheform\\n[An].AworkofS.R.Searleislistedinthebibliography, butnototherwise mentioned.\\nAitken, A.C.,238\\nAllais,Maurice, 29,97,101,102\\nAllen,S.G.,Jr.,252\\nAnscombe, F.J.,[A4]\\nArrow,Kenneth J.,91,142,146, 175,\\n216\\nBahadur, RaghuRaj,131,134,140,256\\nBanach, &S.,41,42,78\\nBaumol, William J.,97\\nBayes,Thomas, [B8]\\nBell,E.T.,93\\nBernoulli, Daniel, 63,81,91,92andff.,\\n99,102,155\\nBernoulli, Jacob(=James), 1,92,[B13]\\nBernoulli, Nicholas, 93\\nBirkhoff, G.,193\\nBizley,M.T.L.,64\\nBlackwell, David, 149, 153, 178,184,187,\\n199\\nseealsoKenneth J.Arrow\\nBohnenblust, H.F.,148,189, 191, 218\\nBonnessen, T.,121,269\\nBorel,Emil,178,179\\nBoulding, Kenneth E.,seeStigler in\\nbibliography\\nBowker, A.H.,262\\nBrambilla, Francesco, 90\\nBuros,O.K.,270\\nCarnap, Rudolf, 56,61,62,160,[C1],\\n[C2],[C3]\\nChapman, Douglas G.,238 Chernoff, Herman, 205,206\\nChurchman, C.West,[C8]\\nCoombs, ClydeH.,seeRobertM.Thrall\\nCramer, Gabriel, 81,92,94,95\\nCramér, Harald, 131,238,241,248,[C8]\\nD’Alembert, JeanleRond,65\\nDarmois, G.,238\\nDavis,Robert L.,seeRobertM.Thrall\\nDeFinetti,Bruno, 4,7,28,38,38,40,43,\\n51,52, 58, 60, 62, 175,177,[D2],[D6]\\nDelorme, S.,[D8]\\nDeming, W.Edwards, [B8]\\nDoob,J.,238\\nDuncan, D.B.,255\\nDvoretzky, A.,219\\nElfving,G.,134\\nFeller,William, viii,49,[F1]\\nFenchel, W.,seeT.Bonnessen\\nFéraud, D.,62\\nFisher, Irving,96\\nFisher,R.A.,50,116,134,236,244,\\n251,262,[F3],[F4],[F6]\\nFisher,WalterD.,255\\nFréchet, Maurice, 178,238,[D8]\\nFrick,Ludwig, [B11]\\nFriedman, Milton, 83,97,104\\nFry,Thornton C.,30\\nGirshick, M.A.,208,243\\nseealsoDavidBlackwell\\n301\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58401300-d37f-41aa-aa89-6b2464ee18b7', embedding=None, metadata={'page_label': '320', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='302\\nGood,I.J.,60,[G2]\\nGraves,Lawrence M.,73\\nHalinos, PaulR.,ix,41,131,134, 263,\\n(H1)\\nHammer, P.C.,35\\nHardy,G.H.,266\\nHaussner, R.,[B13]\\nHildreth, Clifford, 175\\nHodges, J.L.,Jr.,200, 201, 203,238\\nHume,David,[H7]\\nJeffreys, Harold, [J1]\\nJenks,James, Jr.,seePaulD.White\\nKakutani, 8.,218\\nKarlin, 8.,189\\nseealsoH.F.Bohnenblust\\nKendall, Maurice G.,67,140,221, 241,\\n246, 262, 270,[K2]\\nKeynes, JohnMaynard, 1,39,56,61,\\n270,[K4]\\nKing,Robert L., seePaulD.White\\nKneale, William, [A4]\\nKolmogoroff, A.N.,3,52,264,[K8]\\nKoopman, B.O.,38, 39, 48, 44, 56,60,\\n62,[K11],[K12]\\nKuhn,H.W.,[K13],[K14]\\nKullback, S.,50,134\\nLangford, CooperHarold, 12\\nLaplace, PierreSimonde,95,[L2]\\nLeCam,Lucien, 241,243,244\\nLehmann, E.L.,131,134,140,200,201,\\n203, 221, 246, 249, 250,251,252\\nseealsoJ.L.Hodges, Jr.\\nLeibler, R.A.,50,134\\nseealsoS.Kullback\\nLeibniz, 1\\nLewis,Clarence Irving,12\\nLindley, D.V.,[L8]\\nLittlewood, J.E.,seeG.H.Hardy\\nMaclLane, 8.,193\\nseealsoG.Birkhoff\\nMahalanobis, P.C.,[F6]\\nMarkowitz, Harry,104\\nMarshall, Alfred,95\\nMcKinsey, J.C.C.,178,184\\nMolina,Edward C.,[B8] AUTHOR INDEX\\nMorgenstern, Oskar, 5,15,69, 75, 94,\\n96,97,98, 99, 121,269\\nseealsoJohnvonNeumann\\nMorlat, Georges, 101\\nMorrison, Nathan, [K8]\\nMosteller, Frederick C.,29\\nMourier, Edith,238\\nMunroe, M.E.,viii,[M6]\\nNagel,Ernest, 56,62\\nNeyman, Jerzy,140,156,159,240,241,\\n244, 261, (F6],[N3]\\nNogee, Philip,29\\nNunke,R.J.,35\\nPareto, Vilfredo, 96\\nPascal, Blaise,81\\nPaulson, Edward, 255\\nPearson, E.8.,140,156\\nPitman, E.J.G.,245\\nPolya,G.,7\\nseealsoG.H.Hardy\\nPrice,Richard, [B8]\\nPringsheim, Alfred, [B11]\\nRamsey, FrankP.,7,60,96, 97,[R1]\\nRao,C.R.,238\\nReichenbach, Hans,58,61,[R2]\\nRichter, Hans,(R2a]\\nRobbins, Herbert, 256\\nRubin,Herman, 203\\nRudy,Norman, 252\\nSamuelson, PaulA., 97,101\\nSavage, L.J.,100,166,[B21]\\nseealsoMilton Friedman, M.A.\\nGirshick, PaulR.Halmos, andR.J.\\nNunke\\nScheffé,Henry,255\\nseealsoE.L.Lehmann\\nSearle, 8.R.,biblzography\\nShackle, G.L.S.,bibliography\\nShannon, Claude E.,50\\nShapley, L.S.,148, 188, 189\\nseealsoH.F.Bohnenblust\\nSherman, S.,148\\nShohat, J.A.,53,55,152\\nSilverstone, H.,238\\nSmith,CedricA.B.,140\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02bcf962-0537-4d97-b12c-745005fa0d37', embedding=None, metadata={'page_label': '321', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='AUTHOR INDEX\\nSnow,R.N.,188\\nSobezyk, A.,35\\nSommer, Louise, [B11a]\\nSprowles, R.Clay,252\\nStatistical Research Group, Columbia\\nUniversity, 142,146\\nStigler,George J.,91,96\\nTamarkin, J.D.,seeJ.A.Shohat\\nTarski, A.,seeS.Banach\\nThrall,RobertM.,bibliography\\nTintner, Gerhard, 97\\nTippett, L.H.C.,vii,[T2]\\nTodhunter, I.,65,270\\nTucker, A.W.,seeH.W.Kuhn\\nTukey,JohnW.,255,262\\nvanDantzig, D.,62\\nvonMises,Richard, 3 303\\nvonNeumann, John,5,15,69,75,94,\\n96,97, 98, 99, 178,179,184,187,202\\nvonWright, GeorgHenrik, 270\\nWald,Abraham, viii,26,61,114,116,\\n138,142, 146, 156, 159,162, 164, 165,\\n168,169, 170, 182, 198,216,220,221,\\n238,241,251,262\\nseealsoA.Dvoretzky\\nWallis,W.Allen,xi,29,253,262\\nWeaver, Warren, seeClaudeE.Shannon\\nWhite,PaulD.,254\\nWiener, Norbert, 50\\nWilliams, R.M.,262\\nWisdom, JohnOulton, [W13]\\nWold,H.,97,100\\nWolfowitz, J.,142,205,238,241\\nseealsoA.Dvoretsky\\nWoodbury, Max,58\\nYates,F.,234\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0dbe6f6f-5642-4970-994c-524506636d13', embedding=None, metadata={'page_label': '322', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6cbdbff1-23fa-40dc-8b96-17e7fe9183ae', embedding=None, metadata={'page_label': '323', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='General Index\\nSeealsoTechnical Symbols, p.299,andIndexofAuthors, pp.301-3.\\nAcceptance sampling, 253\\nAccepting, 247\\nAccuracy estimation, doctrine of,257ff\\nAct,definition of,14\\nexamples of,14\\ngenericsymbols for,14,15\\nproperinterpretation of,15\\nActs,constant, 25\\nequivalent (orindifferent), 19\\ngenericsymbolforsetsof,14\\ninfinite setsof,18\\nActuarial value,94\\nAdmissibility, 115\\nintheoryofgames,197\\nofatest,148\\nprinciple of,26,165,231\\ngroup,174\\nAgreement between acts,onevents,22\\nAgreement between aprobability meas-\\nureandaquantitative prob-\\nability,34\\nAgreement between people, 26,66ff,114,\\n126, 127, 217\\nastojudgment, 156\\nastoutility,155\\ncomplete, 7\\nAlmostequivalent events,37\\nAlmost exactscience, 101\\nAlmostuniform partition, 34\\nAlternative hypothesis, 247\\nAnalysis ofvariance, 116\\nAnnals ofMathematical Statistics, 272\\nAposteriori probability, 47\\nApproach tocertainty, 141,176ff,214,\\n226\\nAprioriprobability, 47\\nAristotle, 1\\nArsconjectandi, 1,2,92\\nAsymptotic normality, 227\\nAsymptotic variance, 227 Banach-Tarski paradox, 42\\nBasicact,106\\ndefinition of,110\\nBasicdecision problem, 106,208\\nBayes’rule(ortheorem), 45\\nBehavioral interrogation, 28\\nstrictly empirical, 28,29\\nBehavioralistic andverbalistic outlooks,\\n17\\nBehavioralistic outlook, 60,159ff,220,\\n261\\napplied topointestimation, 229ff\\nBehrens-Fisher problem, 251,262\\nBets,63,64\\nBetweenness, 19\\nBibliographies, 270,271\\nBinomial distribution, 131,146ff,222\\ndefinition of,203\\nBoolean algebra, 10,11\\nduality principle of,12\\nexercises in,11ff\\nBorelfield,42\\nBounded act,79\\nBoundedutility, 81,82\\nBritish-American School, 3,4,26,154,\\n155,159\\nCertainty, approach to,141,176ff,214,\\n226\\nCharacteristic function, 263\\nCogent reason, principle of,64\\nCommunication, 68\\nComplement ofanevent,11\\nCompound problem, 214\\nConcave function, 94\\nConditional expectation, 264\\nConditional preference, 22\\namongconsequences superfluous, 25,\\n26\\nConditional probability, 43ff\\n305\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a8196f4-6d9a-4ae4-b3c0-d7d5b0758f5c', embedding=None, metadata={'page_label': '324', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='306\\nConditional probability, widesense,52\\nConfidence interval, 261\\nConfidence level,261\\nofatolerance interval, 262\\nConsequence, 13,14\\ngenericsymbols for,14\\nvariety of,14\\nConsequences, ignorance of,15\\nsymbolforsetof,14\\nConsideration, costof,30\\nConsistent sequence ofestimates, 226\\nConstant acts,25\\nContaining events, 11\\nContraction, 128ff\\nofanobservation, 112\\nofasetofacts,113\\nConvex function, 94,266ff\\nstrictly, 267\\nConvexsetofgambles, 75\\nConvex sets,269\\nCorrect act,164\\nCorrect estimate, 230\\nCostofconsideration, 30\\nCostofobservation, 116,118,214,215\\nCountable additivity, 40,43,78\\nCramér-Rao inequality, 238\\nDecision, 13\\nafterobservation, 23\\nlogicand,6\\nDecision problem, group,172ff\\nandobservation, 210\\nobjectivistic, 172ff\\nDecisions, consecutive, 15,16\\nDefinitive observation, 127,133,212\\nDegree ofconviction, 30\\nDemocracy, 175\\nDeMorgan’s theorem, 13\\ngeneral, 13\\nDerived act,106\\ndefinition of,111\\nDerived decision problem, 106\\nDerived problem, 209\\nDesign ofexperiments, 16,105,116\\nDichotomy, 121\\nDifferential information, 236ff\\nDisagreement between people, 67,68\\nDominance, 115\\nintheoryofgames,197\\nofonetestbyanother, 148 GENERAL INDEX\\nDualistic viewsonprobability, 2,51,62,\\n63\\nDuality principle, 185\\nofBoolean algebra, 12\\nofpersonal probability, 78\\noftheoryofgames, 185,186\\nEfficient sequence ofestimates, 227,\\n242ff\\nEmpirical interpretation ofpostulates,\\n19,20\\nEpsilon, Porson, 11\\nvertical, 11\\nEqualevents, 11\\nEquity, 63,92\\nEquivalence, ofsetsofacts,113\\noftests,148\\nEquivalent acts,19\\nEquivalent observations, 112\\nEquivalent sequence ofevents,52\\nError,meansquare,224\\nseealsoRoot-mean square errorand\\nSquared error\\nofanestimate, definition of,227\\nErrors offirstandsecond kind,140,\\n247\\nEstimation, interval, 259\\npoint,220ff\\ndefinition of,221\\nEstimation decision problem, 229ff\\nEvent,complement of,11\\ndefinition of,10\\nexamplesof,10\\ngenericsymbols for,11\\nnull(orvirtually impossible), 24\\nuniversal, 10\\nvacuous, 10\\nEvents, almostequivalent, 37\\ncontaining, 11\\nequal,11\\nintersection of,11\\nunionof,11\\nExpectation, conditional, 264\\nExpected value,263ff\\ndefinition of,263\\nExperience, 44,46, 55,62\\nExperiment andobservation, 117,118\\nExtension, ofanobservation, 112\\nofasetofacts,113\\nExtreme £8,129\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97604d2a-3594-4cf9-b236-ab114e709c6e', embedding=None, metadata={'page_label': '325', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='GENERAL INDEX\\nFactorability criterion forsufficiency,\\n130ff\\nFaircoin,33\\nFiducial probability, 262\\nFine,37,40\\nFoundations ofsciences, roleof,1\\nFoundations ofstatistics, deep,5\\nhistory of,1ff\\nshallow, 5\\nGamble, 70,71\\nGambling, 638,64,91,94\\nGambling apparatus, 66\\nGame,abstract, 184ff\\nbilinear, 186ff\\nstandard, 178ff\\ntwo-person, 178ff\\nGames, inrelation tominimax theories\\nofdecision, 180ff\\nmathematics of,184ff\\ntheory of,156,178ff\\nGiven,22,44\\nGrandworld,84\\nGreekfonts,11\\nGroup,mathematical, 193\\nGroupaction,105\\nGroupdecision problem, 172ff\\nandobservation, 210\\nGroupminimaxrule,207\\nHausdorff moment problem, 53,55,152\\nHomogeneous coordinates, 136\\nHyper-utility, 75\\nHypothesis, alternative, 247\\nextreme null,254\\nnull,247\\nIncome, 163\\nnegative, 164,169,170\\nandloss,182,200\\npersonal, 173\\nInconsistency, 20, 21,57\\nIndecision, 21\\nIndependence inqualitative probability,\\n44,91\\nIndependent events,44\\nIndependent random variables, 46\\nIndifference, 17,59\\ndifficulty oftesting, 17\\nInductive behavior, 159 307\\nInductive inference, 2\\nInexact science, 59\\nInfimum, 80\\nInfinite setsinappliedmathematics, 39,\\n77\\nInfinite utility,81\\nInformation, 50,153,235ff\\ndifferential, 236ff\\nInformation inequality, 238\\nInsufficient reason, principle of,64,65,\\n193\\nIntegral, 263\\nInterrogation, behavioral, 28\\nintermediate modeof,28\\nstrictly empirical, 28,29\\nIntersection ofevents, 11\\nInterval, 266\\nInterval estimation, 257\\ndefinition of,259,260\\nInterval ofgambles, 75\\nInterval oflinearity, 267\\nInvariance ofagame,194ff\\nInvariant minimax, 197,198\\nIrrelevant, 126\\nutterly, 126\\nIrrelevant event,44\\nJournal ofAmerican Statistical Associa-\\ntion,270\\nJudgment, 156\\nLargenumbers, stronglawof,54\\nweaklawof,49,54,91\\nLearning, 44,55\\nseealsoExperience\\nLebesgue measure, 41\\nLikelihood ratio,48,135ff,225\\nLikelihood-ratio test,1389,213\\nLinearfunction, 267\\nLogic,3\\ndecision and,6\\nempirical interpretation of,20\\ncriticism of,20\\nincompleteness of,59\\nnormative interpretation of,20\\nLogicalbehavior, implications of,7,8,20\\n“Lookbeforeyouleapprinciple,” 16\\ncriticism of,16,17\\nLoss,163,164,169,170\\npersonal, 174\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='527ac97a-61fe-4091-a53b-178c1d762580', embedding=None, metadata={'page_label': '326', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='308\\nLoss,uniformity of,166,174\\nLossandnegative income, 182,200\\nMarginal utility, 103,104\\ndiminishing, 94ff\\nMathematical expectation, principle of,\\n91,92\\nMaximin, 184\\nMaximum-likelihood estimate, 140,203,\\n222ff,241\\ndefinition of,225\\nMean-square error,224\\nseealsoRootmean-square errorand\\nSquared error\\nMeasurable random variable, 45\\nMedian, 228\\nMicrocosm, 86\\nMinimax, 184\\nMinimaxact,164\\nMinimax equality, 179,187\\nMinimaxestimate, 232,240,241\\nMinimaxrule,157,180ff\\nandsimpleordering, 205\\ngroup, 174ff,207\\nobjectivistic, 164ff\\ndefinition of,164\\nillustrations of,164ff\\nobjectivistic motivation of,168,169\\nMinimaxrules, criticism of,200ff\\nMinimaxtest,249,250\\nMinimax theories, mathematics of,184ff\\nMinimax theory, 156\\nobjectivistic, definition of,165\\nobjectivistic approach to,158ff\\nMinimax theoryandobservation, 208\\nMinimax value,164\\nMixedact,162,163\\ningroupdecision problem, 173\\nMixedactsinstatistics, 213,216,217ff\\nMixture ofgambles, 71\\nMoment problem, Hausdorff, 53,55,\\n152\\nMoralexpectation, 93,94\\nMoralworth,93ff\\nMultipersonal considerations, 122,124,\\n126,127,148,154ff,172ff\\nseealsoAgreement, Certainty, and\\nDisagreement\\nMultiple observation (orstatistic), 111\\ncounting of,133 GENERAL INDEX\\nNecessary statistic, 137,224\\nNecessary viewsofprobability, 3,60,61,\\n67\\nNegative income, 164,169,170\\nandloss,182,200\\nNeyman-Pearson school,140\\nNeyman-Pearson theory oftesting, 252\\nnon-Archimedean probability, 39\\nNormal distribution, 132,222\\nNormative interpretation, ofpostulates,\\n19ff\\noftheoryofutility,97\\nNormative theory, 102\\nNuisance parameter, 223\\nNullevent,24,26\\nNullhypothesis, 247\\nextreme, 254\\nNullobservation, 112\\nObjectivistic decision problem, 159\\nObjectivistic observational problem, 208\\nObjectivistic viewsofprobability, 3,60,\\n61, 67,253,254\\ncentral difficulty of,4\\nprobability ofisolated propositions\\nunder,4\\nObservation, 105ff,125ff\\ncostof,116,118,169,214,215\\ndecision after,23\\ndefinition of,110\\nObservational problem, objectivistic, 208\\nObservation andexperiment, 117,118\\nObserved value,110\\nObtains, 10\\nOperating characteristic, 248\\nOptimism, 68\\nOrderstatistic, 132\\nParameter, 221\\nnuisance, 223\\nPartialordering, 21\\nPartition, 24\\nalmostuniform, 34\\nPartition formula, 45\\nPartition problems, 120ff\\nPersonalistic view,56\\ndifficulties with,57\\npossible incompleteness of,59\\nPersonalistic viewsof probability, 3,67\\nPersonal probability, 27,30\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36ffc3e1-716b-4f09-be38-020cb6e4fef5', embedding=None, metadata={'page_label': '327', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='GENERAL INDEX\\nPersonal probability, criticism ofverbal-\\nisticapproach to,27,28\\nothertermsfor,30\\nPersonaseconomic unit,8\\nPessimism, 68\\nPlanasasingledecision, 16\\ncriticism of,16,17\\nPointestimation, 220ff\\ndefinition of,221\\nPoisson distribution, 222\\nPowerfunction, 248\\nPreference, 17\\nassimpleordering, 18\\naspartialordering, 21\\nconditional, 22\\nsuperfluous forconsequences, 25,26\\nirreflexivity of,17\\ntransitivity of,18\\nPreference amongconsequences, 25\\ndistinguished frompreference among\\nacts,25\\nPre-statistics, 5\\nPrimary act,163\\nPrize,31\\nProbabilities ofhigherorder,58\\nProbability, mathematical properties of,\\n2,3\\nunknown, superfluousness ofinperson-\\nalistictheory, 50,51\\nviewson,dualistic, 2,51,62,63\\nnecessary, 3,60,61,67\\nobjectivistic, 3,60,61,67,253,254\\npersonalistic, 3,67\\nseealsoPersonalistic view\\nProbability measure, 33\\nProbability space,45\\nPropositions, probability of,underob-\\njectivistic views,4,27,61,62\\nPseudo-microcosm, 86\\nPsychological probability, 30\\nQualitative probability, definition of,32\\nexample, 28\\nfinebutnottight,41\\nneither finenortight,41\\ntightbutnotfine,41\\nQuantitative probability, 33\\nRandomization, 66,168,216,217\\nRandom numbers, 67 309\\nRandom variable, 45\\nreal,263\\nRational behavior, 7\\nRay,135\\nRegret, 163\\nRejecting, 247\\nRoot-mean-square error,257\\nseealsoMean-square errorandSquared\\nerror\\nSt.Petersburg paradox, 93ff\\nSchwartz inequality, 269\\nScience, almostexact,101\\nSequential analysis, 116,142ff,215,216\\nSequential observational program, 142\\nSequential probability ratioprocedure\\n146\\nSignificance level,252\\nreporting of,256\\nSignificance tests,246ff\\nSimple dichotomy, 138,145,146,148,\\n212,213,252\\nSimpleordering, 18\\nandtheminimax rule,205\\nexercises on,19\\nSizeofatest,250\\nSmallworld, 9,16,82ff\\nSquarederror, 81,234\\nseealsoMean-square errorandRoot\\nmean-square error\\nStandard deviation, 257\\nStandard game,178ff\\nStandard sequence ofobservations, 227\\n|State,9\\ntrue,9\\nStates,genericsymbolsfor,11\\nStatistic, 128\\nStatistics, othernamesfor,2\\nscopeof,2\\nStatistics proper, 5,105,114,121\\ndefinition of,154\\nStrategy function, 111\\nStrictly convexfunction, 267\\nSubjective probability, 30\\nSufficient statistic, 129ff,212,224,230,\\n237,246,256,259\\nfactorability criterion for,130ff\\nSupremum, 80,267\\nSurepersonal probabilities, 57,58,66\\nSure-thing principle, 21ff,114,207\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='edee3a25-fec8-4d4c-84ec-5c1887e96e57', embedding=None, metadata={'page_label': '328', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='310\\nSymmetric dual,78\\nSymmetric sequenceofevents, 50ff\\nSymmetry, 232,246\\ninprobability, 63ff\\nofgames, 193ff\\nTastes, 155\\nTeammate,132\\nTest,definition of,247\\nofhypotheses, 246ff\\nTesting, 221\\nTesting problem, 247\\nTiesinrank,219\\nTight,37,40\\nTimeintheory ofdecision, 10,17,23,\\n44\\nTolerance interval, 262\\nTolerance level,262\\nTopological assumptions possible fora\\nsimpleordering, 18\\nTransitivity, 19\\nTruestate,9\\nUnbiased estimate, 203,224,244,245\\ndefinition of,226\\nUnbiased test,249\\ncriticism of,250\\nUniform distributicn, 131\\nUnionofevents, 11\\nUniversal event,10\\nsymbolfor,11\\nUtile,82  GENERAL INDEX\\nUtility,69\\nandtheminimaxrules,201ff\\nbounded, 95\\ncriticism of,91ff\\ndefinition of,73\\nhistory of,91ff\\nlogarithmic, 94,95\\nprobability-less, 91,95,96\\nUtterly irrelevant observation, 126,212,\\n237\\nVacillation, 21\\nVacuous event,10\\nsymbolfor,10,11\\nVagueness, 59,168,169\\nValueofobservation, 151\\nVariance, 268\\nVenndiagram, 12\\nVerbalistic andbehavioralistic outlooks,\\n17\\nVerbalistic outlook, 159ff,220,260,261\\ninadequacyofindefinition ofpersonal\\nprobability, 27,28\\nVirtual extension, 148\\nVirtually equivalent acts,148\\nVirtually impossible event,24\\nWorld,choiceof,9\\ndefinition of,9\\nexamples of,8\\ngrand,84\\nsmall,9,16,82ff\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6f83cdae-3bd0-40ad-bad3-f9d1b0f985cd', embedding=None, metadata={'page_label': '329', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='83f540da-bb35-43f2-9679-20530fe10a47', embedding=None, metadata={'page_label': '330', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Postulates ofaPersonalistic\\nThesevenpostulates (P1through P7)scattered through thefirst\\nfivechapters ofthisbookarereproduced hereforreadyreference along\\nwithaminimum ofexplanatory material. Thelanguageofthepostu-\\nlatesisherechangedsomewhat forconciseness andtoshowanalterna-\\ntivemodeofexpression, butthelogicalcontent ofeachpostulate is\\nleftunaltered.\\nTheformalsubjectmatterofthetheory\\nThestates,asetSofelementss,s’,--+withsubsetsA,B,C,--+(page11).\\nTheconsequences, asetFofelements f,g,h,--+(page14).\\nActs,arbitrary functions f,g,h,---fromStoF(page14).\\nTherelation ‘‘isnotpreferred to”between acts,<(page18).\\nThepostulates, anddefinitions onwhichtheydepend\\nDefinitions oftermsnotingeneralmathematical usearegivenhere\\nasD1throughD5;forothersconsult theGeneral Index(page289)\\nandtheTechnical Symbols (page283).\\nPlTherelation<isasimpleordering (page18).\\nD1 f<ggivenB,ifandonlyiff’<g’forevery f’andg’that\\nagreewithfandg,respectively, onBandwitheachotheron~B\\nandg’<f’eitherforallsuchpairsorfornone(page22).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b6f9a19-d68b-4e12-b255-e427ee9fdfcd', embedding=None, metadata={'page_label': '331', 'file_name': '1972-savage-foundationsofstatistics.pdf', 'file_path': '/content/data/1972-savage-foundationsofstatistics.pdf', 'file_type': 'application/pdf', 'file_size': 28637068, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Theory ofDecision\\nP2  ~==Foreveryf,g,andB,f<ggivenBorg<fgivenB(page23).\\nD2g<qg’;if andonlyiff<f’,whenf(s)=g,f’(s)=g’forevery\\nseS(page25).\\nD3 _sC@Bisnull,ifandonlyiff<ggivenBforeveryf,g(page24).\\nP3Iff(s)=g,f’(s)=g’foreveryseB,andBisnotnull;then\\nf<f’givenB,ifandonlyifg<g’(page26).\\nD4A<B; ifandonlyiff4<fgorg<g’foreveryfa,fz,g,g’\\nsuchthat: fa(s) =gforseA,fa(s)=g’forse~A, fa(s)=g,for\\nseB,f(s)=g’forse~B(page31).\\nP4Forevery A,B,A<BorB<A (page31).\\nP5 ‘Itisfalsethat,foreveryf,f’,f<f’(page31).\\nP6Supposeitfalsethatg<h;then,foreveryf,thereisa(finite)\\npartition ofSsuchthat,ifg’agreeswithgandh’agreeswithhexcept\\non anarbitrary element ofthepartition, g’andh’beingequaltof\\nthere,thenitwillbefalsethatg’<horg<h’(page39).\\nD5f<ggivenB(g<fgivenB);ifandonlyiff<hgivenB\\n(h<fgivenB),whenh(s)=gforeverys(page72).\\nP7Iff<g(s)givenB(g(s)<fgivenB)foreveryse¢B,then\\nf<ggivenB(g<fgivenB)(page77).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00f873b4-7c2d-4640-9fd4-1011ded8a6a9', embedding=None, metadata={'page_label': '1', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\\nThis major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\\n›springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie • Tibshirani • Friedman\\nSecond Edition', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='345f2459-2795-4d30-b125-76ceb8588cdc', embedding=None, metadata={'page_label': '2', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page v\\nPrinter: Opaque this\\nTo our parents:\\nValerie and Patrick Hastie\\nVera and Sami Tibshirani\\nFlorence and Harry Friedman\\nand to our families:\\nSamantha, Timothy, and Lynda\\nCharlie, Ryan, Julie, and Cheryl\\nMelanie, Dora, Monika, and Ildiko', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4c438a6-5adb-421b-8110-92cb8eb18dd3', embedding=None, metadata={'page_label': '3', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='vi', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec3b2432-59c8-4608-87c2-5db4b13620ad', embedding=None, metadata={'page_label': '4', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page vii\\nPrinter: Opaque this\\nPreface to the Second Edition\\nIn God we trust, all others bring data.\\n–William Edwards Deming (1900-1993)1\\nWe have been gratiﬁed by the popularity of the ﬁrst edition of The\\nElements of Statistical Learning. This, along with the fast pace of research\\nin the statistical learning ﬁeld, motivated us to update our book with a\\nsecond edition.\\nWe have added four new chapters and updated some of the existing\\nchapters. Because many readers are familiar with the layout of the ﬁrst\\nedition, we have tried to change it as little as possible. Here is a summary\\nof the main changes:\\n1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\\nHayden; however Professor Hayden told us that he can claim no credit for this quote,\\nand ironically we could ﬁnd no “data” conﬁrming that Deming a ctually said this.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='29218c0f-90b8-437c-a6d3-3abd1f9e1f5b', embedding=None, metadata={'page_label': '5', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='viii Preface to the Second Edition\\nChapter What’s new\\n1.Introduction\\n2.Overview of Supervised Learning\\n3.Linear Methods for Regression LAR algorithm and generalizations\\nof the lasso\\n4.Linear Methods for Classiﬁcation Lasso path for logistic regression\\n5.Basis Expansions and Regulariza-\\ntionAdditional illustrations of RKHS\\n6.Kernel Smoothing Methods\\n7.Model Assessment and Selection Strengths and pitfalls of cross-\\nvalidation\\n8.Model Inference and Averaging\\n9.Additive Models, Trees, and\\nRelated Methods\\n10.Boosting and Additive Trees New example from ecology; some\\nmaterial split oﬀ to Chapter 16.\\n11.Neural Networks Bayesian neural nets and the NIPS\\n2003 challenge\\n12.Support Vector Machines and\\nFlexible DiscriminantsPath algorithm for SVM classiﬁer\\n13. Prototype Methods and\\nNearest-Neighbors\\n14.Unsupervised Learning Spectral clustering, kernel PCA,\\nsparse PCA, non-negative matrix\\nfactorization archetypal analysis,\\nnonlinear dimension reduction,\\nGoogle page rank algorithm, a\\ndirect approach to ICA\\n15.Random Forests New\\n16.Ensemble Learning New\\n17.Undirected Graphical Models New\\n18.High-Dimensional Problems New\\nSome further notes:\\n•Our ﬁrst edition was unfriendly to colorblind readers; in particular,\\nwe tended to favor red/green contrasts which are particularly trou-\\nblesome. We have changed the color palette in this edition to a large\\nextent, replacing the above with an orange /bluecontrast.\\n•We have changed the name of Chapter 6 from “Kernel Methods” to\\n“Kernel Smoothing Methods”, to avoid confusion with the machine-\\nlearning kernel method that is discussed in the context of support vec-\\ntor machines (Chapter 11) and more generally in Chapters 5 and 14.\\n•In the ﬁrst edition, the discussion of error-rate estimation in Chap-\\nter 7 was sloppy, as we did not clearly diﬀerentiate the notions of\\nconditional error rates (conditional on the training set) and uncondi-\\ntional rates. We have ﬁxed this in the new edition.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53a218c9-7d48-4474-8d24-d5224712d6ea', embedding=None, metadata={'page_label': '6', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface to the Second Edition ix\\n•Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\\nters are probably best read in that order.\\n•In Chapter 17, we have not attempted a comprehensive treatment\\nof graphical models, and discuss only undirected models and some\\nnew methods for their estimation. Due to a lack of space, we have\\nspeciﬁcally omitted coverage of directed graphical models.\\n•Chapter 18 explores the “ p≫N” problem, which is learning in high-\\ndimensional feature spaces. These problems arise in many areas, in-\\ncluding genomic and proteomic studies, and document classiﬁcation.\\nWe thank the many readers who have found the (too numerous) errors in\\nthe ﬁrst edition. We apologize for those and have done our best to avoid er-\\nrors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\\nWasserman for comments on some of the new chapters, and many Stanford\\ngraduate and post-doctoral students who oﬀered comments, in particular\\nMohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Maleki, Donal\\nMcMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\\nHui Zou. We thank John Kimmel for his patience in guiding us through this\\nnew edition. RT dedicates this edition to the memory of Anna McPhee.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nAugust 2008', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ecddc47-ab2b-40f9-bb52-709311b36221', embedding=None, metadata={'page_label': '7', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='x Preface to the Second Edition', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='af9bd189-e7cf-4e64-a12a-02477a306fe8', embedding=None, metadata={'page_label': '8', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page xi\\nPrinter: Opaque this\\nPreface to the First Edition\\nWe are drowning in information and starving for knowledge.\\n–Rutherford D. Roger\\nThe ﬁeld of Statistics is constantly challenged by the problems that science\\nand industry brings to its door. In the early days, these problems often came\\nfrom agricultural and industrial experiments and were relatively small in\\nscope. With the advent of computers and the information age, statistical\\nproblems have exploded both in size and complexity. Challenges in the\\nareas of data storage, organization and searching have led to the new ﬁeld\\nof “data mining”; statistical and computational problems in biology and\\nmedicine have created “bioinformatics.” Vast amounts of data are being\\ngenerated in many ﬁelds, and the statistician’s job is to make sense of it\\nall: to extract important patterns and trends, and understand “what the\\ndata says.” We call this learning from data .\\nThe challenges in learning from data have led to a revolution in the sta-\\ntistical sciences. Since computation plays such a key role, it is not surprising\\nthat much of this new development has been done by researchers in other\\nﬁelds such as computer science and engineering.\\nThe learning problems that we consider can be roughly categorized as\\neither supervised orunsupervised . In supervised learning, the goal is to pre-\\ndict the value of an outcome measure based on a number of input measures;\\nin unsupervised learning, there is no outcome measure, and the goal is to\\ndescribe the associations and patterns among a set of input measures.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ffc167ba-af83-4d78-aa61-57c0ab028e1a', embedding=None, metadata={'page_label': '9', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xii Preface to the First Edition\\nThis book is our attempt to bring together many of the important new\\nideas in learning, and explain them in a statistical framework. While some\\nmathematical details are needed, we emphasize the methods and their con-\\nceptual underpinnings rather than their theoretical properties. As a result,\\nwe hope that this book will appeal not just to statisticians but also to\\nresearchers and practitioners in a wide variety of ﬁelds.\\nJust as we have learned a great deal from researchers outside of the ﬁeld\\nof statistics, our statistical viewpoint may help others to better understa nd\\ndiﬀerent aspects of learning:\\nThere is no true interpretation of anything; interpretatio n is a\\nvehicle in the service of human comprehension. The value of\\ninterpretation is in enabling others to fruitfully think ab out an\\nidea.\\n–Andreas Buja\\nWe would like to acknowledge the contribution of many people to the\\nconception and completion of this book. David Andrews, Leo Breiman,\\nAndreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton, Werner\\nStuetzle, and John Tukey have greatly inﬂuenced our careers. Balasub-\\nramanian Narasimhan gave us advice and help on many computational\\nproblems, and maintained an excellent computing environment. Shin-Ho\\nBang helped in the production of a number of the ﬁgures. Lee Wilkinson\\ngave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Ma ya\\nGupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\\ndan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\\nZhu, two reviewers and many students read parts of the manuscript and\\noﬀered helpful suggestions. John Kimmel was supportive, patient and help-\\nful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\\nproduction team at Springer. Trevor Hastie would like to thank the statis-\\ntics department at the University of Cape Town for their hospitality during\\nthe ﬁnal stages of this book. We gratefully acknowledge NSF and NIH for\\ntheir support of this work. Finally, we would like to thank our families and\\nour parents for their love and support.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nMay 2001\\nThe quiet statisticians have changed our world; not by disco v-\\nering new facts or technical developments, but by changing t he\\nways that we reason, experiment and form our opinions ....\\n–Ian Hacking', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de9d0c1c-4705-4ed8-97b5-90f7e9b80d67', embedding=None, metadata={'page_label': '10', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page xiii\\nPrinter: Opaque this\\nContents\\nPreface to the Second Edition vii\\nPreface to the First Edition xi\\n1 Introduction 1\\n2 Overview of Supervised Learning 9\\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\\n2.3 Two Simple Approaches to Prediction:\\nLeast Squares and Nearest Neighbors . . . . . . . . . . . 11\\n2.3.1 Linear Models and Least Squares . . . . . . . . 11\\n2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\\n2.3.3 From Least Squares to Nearest Neighbors . . . . 16\\n2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\\n2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\\n2.6 Statistical Models, Supervised Learning\\nand Function Approximation . . . . . . . . . . . . . . . . 28\\n2.6.1 A Statistical Model\\nfor the Joint Distribution Pr( X,Y) . . . . . . . 28\\n2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\\n2.6.3 Function Approximation . . . . . . . . . . . . . 29\\n2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\\n2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4aa01a5e-8ace-4bdd-a03b-deefdb499367', embedding=None, metadata={'page_label': '11', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xiv Contents\\n2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\\n2.8.1 Roughness Penalty and Bayesian Methods . . . 34\\n2.8.2 Kernel Methods and Local Regression . . . . . . 34\\n2.8.3 Basis Functions and Dictionary Methods . . . . 35\\n2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3 Linear Methods for Regression 43\\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n3.2 Linear Regression Models and Least Squares . . . . . . . 44\\n3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\\n3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51\\n3.2.3 Multiple Regression\\nfrom Simple Univariate Regression . . . . . . . . 52\\n3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\\n3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\\n3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\\n3.3.2 Forward- and Backward-Stepwise Selection . . . 58\\n3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\\n3.3.4 Prostate Cancer Data Example (Continued) . . 61\\n3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\\n3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\\n3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\\n3.4.3 Discussion: Subset Selection, Ridge Regression\\nand the Lasso . . . . . . . . . . . . . . . . . . . 69\\n3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\\n3.5 Methods Using Derived Input Directions . . . . . . . . . 79\\n3.5.1 Principal Components Regression . . . . . . . . 79\\n3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\\n3.6 Discussion: A Comparison of the Selection\\nand Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\\n3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\\n3.8 More on the Lasso and Related Path Algorithms . . . . . 86\\n3.8.1 Incremental Forward Stagewise Regression . . . 86\\n3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\\n3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\\n3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\\n3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\\n3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\\n3.9 Computational Considerations . . . . . . . . . . . . . . . 93\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4743b4ba-ecd9-47eb-bfb2-388f21ed3a87', embedding=None, metadata={'page_label': '12', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents xv\\n4 Linear Methods for Classiﬁcation 101\\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\\n4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\\n4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\\n4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\\n4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\\n4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\\n4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\\n4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\\n4.4.2 Example: South African Heart Disease . . . . . 122\\n4.4.3 Quadratic Approximations and Inference . . . . 124\\n4.4.4 L1Regularized Logistic Regression . . . . . . . . 125\\n4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\\n4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130\\n4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n5 Basis Expansions and Regularization 139\\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\\n5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\\n5.2.2 Example: South African Heart Disease (Continued)146\\n5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\\n5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\\n5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\\n5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\\n5.5 Automatic Selection of the Smoothing Parameters . . . . 156\\n5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\\n5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158\\n5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\\n5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\\n5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\\n5.8.1 Spaces of Functions Generated by Kernels . . . 168\\n5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\\n5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\\n5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\\n5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nAppendix: Computational Considerations for Splines . . . . . . 186\\nAppendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186\\nAppendix: Computations for Smoothing Splines . . . . . 189', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee73a6bc-ba23-466c-936b-67856dab5b66', embedding=None, metadata={'page_label': '13', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xvi Contents\\n6 Kernel Smoothing Methods 191\\n6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\\n6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\\n6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\\n6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\\n6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\\n6.4 Structured Local Regression Models in IRp. . . . . . . . 201\\n6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\\n6.4.2 Structured Regression Functions . . . . . . . . . 203\\n6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\\n6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 208\\n6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\\n6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210\\n6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210\\n6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\\n6.8 Mixture Models for Density Estimation and Classiﬁcation 214\\n6.9 Computational Considerations . . . . . . . . . . . . . . . 216\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\\n7 Model Assessment and Selection 219\\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\\n7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\\n7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223\\n7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226\\n7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\\n7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\\n7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232\\n7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\\n7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\\n7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237\\n7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\\n7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\\n7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241\\n7.10.2 The Wrong and Right Way\\nto Do Cross-validation . . . . . . . . . . . . . . . 245\\n7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\\n7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\\n7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\\n7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\\n8 Model Inference and Averaging 261\\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c612595-a4c9-4c11-b9a2-264386603cd5', embedding=None, metadata={'page_label': '14', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents xvii\\n8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\\n8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\\n8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\\n8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\\n8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\\n8.4 Relationship Between the Bootstrap\\nand Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\\n8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\\n8.5.1 Two-Component Mixture Model . . . . . . . . . 272\\n8.5.2 The EM Algorithm in General . . . . . . . . . . 276\\n8.5.3 EM as a Maximization–Maximization Procedure 277\\n8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\\n8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\\n8.7.1 Example: Trees with Simulated Data . . . . . . 283\\n8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\\n8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\\n9 Additive Models, Trees, and Related Methods 295\\n9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\\n9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\\n9.1.2 Example: Additive Logistic Regression . . . . . 299\\n9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\\n9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\\n9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\\n9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\\n9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308\\n9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\\n9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\\n9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\\n9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\\n9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321\\n9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\\n9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\\n9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\\n9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\\n9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\\n9.7 Computational Considerations . . . . . . . . . . . . . . . 334\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n10 Boosting and Additive Trees 337\\n10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\\n10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20cc0c22-20c2-407a-afbc-e4a2438a7cba', embedding=None, metadata={'page_label': '15', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xviii Contents\\n10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\\n10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\\n10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\\n10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\\n10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\\n10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 350\\n10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\\n10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\\n10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\\n10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\\n10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\\n10.10.3 Implementations of Gradient Boosting . . . . . . 360\\n10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\\n10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\\n10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\\n10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\\n10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\\n10.13.1 Relative Importance of Predictor Variables . . . 367\\n10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\\n10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\\n10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\\n10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\n11 Neural Networks 389\\n11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\\n11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\\n11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\\n11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\\n11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\\n11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\\n11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398\\n11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\\n11.5.4 Number of Hidden Units and Layers . . . . . . . 400\\n11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\\n11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\\n11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\\n11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\\n11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\\n11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\\n11.9.2 Performance Comparisons . . . . . . . . . . . . 412\\n11.10 Computational Considerations . . . . . . . . . . . . . . . 414\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d968517-af97-4e11-b023-a868c3028c09', embedding=None, metadata={'page_label': '16', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents xix\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\\n12 Support Vector Machines and\\nFlexible Discriminants 417\\n12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\\n12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417\\n12.2.1 Computing the Support Vector Classiﬁer . . . . 420\\n12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\\n12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\\n12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423\\n12.3.2 The SVM as a Penalization Method . . . . . . . 426\\n12.3.3 Function Estimation and Reproducing Kernels . 428\\n12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\\n12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432\\n12.3.6 Support Vector Machines for Regression . . . . . 434\\n12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\\n12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\\n12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438\\n12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\\n12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\\n12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\\n12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\\n12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\\n13 Prototype Methods and Nearest-Neighbors 459\\n13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\\n13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\\n13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460\\n13.2.2 Learning Vector Quantization . . . . . . . . . . 462\\n13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\\n13.3 k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463\\n13.3.1 Example: A Comparative Study . . . . . . . . . 468\\n13.3.2 Example: k-Nearest-Neighbors\\nand Image Scene Classiﬁcation . . . . . . . . . . 470\\n13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\\n13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\\n13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\\n13.4.2 Global Dimension Reduction\\nfor Nearest-Neighbors . . . . . . . . . . . . . . . 479\\n13.5 Computational Considerations . . . . . . . . . . . . . . . 480\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='64979c84-f6a5-4379-9275-40ad0a09bc18', embedding=None, metadata={'page_label': '17', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xx Contents\\n14 Unsupervised Learning 485\\n14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\\n14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\\n14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\\n14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\\n14.2.3 Example: Market Basket Analysis . . . . . . . . 492\\n14.2.4 Unsupervised as Supervised Learning . . . . . . 495\\n14.2.5 Generalized Association Rules . . . . . . . . . . 497\\n14.2.6 Choice of Supervised Learning Method . . . . . 499\\n14.2.7 Example: Market Basket Analysis (Continued) . 499\\n14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\\n14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\\n14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\\n14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\\n14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\\n14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\\n14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509\\n14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\\n14.3.8 Example: Human Tumor Microarray Data . . . 512\\n14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\\n14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515\\n14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\\n14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\\n14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\\n14.5 Principal Components, Curves and Surfaces . . . . . . . . 534\\n14.5.1 Principal Components . . . . . . . . . . . . . . . 534\\n14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\\n14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\\n14.5.4 Kernel Principal Components . . . . . . . . . . . 547\\n14.5.5 Sparse Principal Components . . . . . . . . . . . 550\\n14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\\n14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\\n14.7 Independent Component Analysis\\nand Exploratory Projection Pursuit . . . . . . . . . . . . 557\\n14.7.1 Latent Variables and Factor Analysis . . . . . . 558\\n14.7.2 Independent Component Analysis . . . . . . . . 560\\n14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\\n14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\\n14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\\n14.9 Nonlinear Dimension Reduction\\nand Local Multidimensional Scaling . . . . . . . . . . . . 572\\n14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67695ab9-cb4c-4a0f-a99d-f5b9b6eaf122', embedding=None, metadata={'page_label': '18', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents xxi\\n15 Random Forests 587\\n15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\\n15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587\\n15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\\n15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\\n15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\\n15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\\n15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596\\n15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\\n15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597\\n15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\\n15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\\n16 Ensemble Learning 605\\n16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\\n16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\\n16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\\n16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610\\n16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\\n16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\\n16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\\n16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\\n17 Undirected Graphical Models 625\\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\\n17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\\n17.3 Undirected Graphical Models for Continuous Variables . 630\\n17.3.1 Estimation of the Parameters\\nwhen the Graph Structure is Known . . . . . . . 631\\n17.3.2 Estimation of the Graph Structure . . . . . . . . 635\\n17.4 Undirected Graphical Models for Discrete Variables . . . 638\\n17.4.1 Estimation of the Parameters\\nwhen the Graph Structure is Known . . . . . . . 639\\n17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\\n17.4.3 Estimation of the Graph Structure . . . . . . . . 642\\n17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\\n18 High-Dimensional Problems: p≫N 649\\n18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='993ffb4f-e4e4-40b1-9166-364d9f80f61d', embedding=None, metadata={'page_label': '19', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xxii Contents\\n18.2 Diagonal Linear Discriminant Analysis\\nand Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\\n18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654\\n18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\\n18.3.2 Logistic Regression\\nwith Quadratic Regularization . . . . . . . . . . 657\\n18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657\\n18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\\n18.3.5 Computational Shortcuts When p≫N. . . . . 659\\n18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661\\n18.4.1 Application of Lasso\\nto Protein Mass Spectroscopy . . . . . . . . . . 664\\n18.4.2 The Fused Lasso for Functional Data . . . . . . 666\\n18.5 Classiﬁcation When Features are Unavailable . . . . . . . 668\\n18.5.1 Example: String Kernels\\nand Protein Classiﬁcation . . . . . . . . . . . . . 668\\n18.5.2 Classiﬁcation and Other Models Using\\nInner-Product Kernels and Pairwise Distances . 670\\n18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672\\n18.6 High-Dimensional Regression:\\nSupervised Principal Components . . . . . . . . . . . . . 674\\n18.6.1 Connection to Latent-Variable Modeling . . . . 678\\n18.6.2 Relationship with Partial Least Squares . . . . . 680\\n18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\\n18.7 Feature Assessment and the Multiple-Testing Problem . . 683\\n18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\\n18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\\n18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\\n18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\\nReferences 699\\nAuthor Index 729\\nIndex 737', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='615df8a8-3b9e-4668-bd69-cc8569ba7219', embedding=None, metadata={'page_label': '20', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 1\\nPrinter: Opaque this\\n1\\nIntroduction\\nStatistical learning plays a key role in many areas of science, ﬁnance and\\nindustry. Here are some examples of learning problems:\\n•Predict whether a patient, hospitalized due to a heart attack, will\\nhave a second heart attack. The prediction is to be based on demo-\\ngraphic, diet and clinical measurements for that patient.\\n•Predict the price of a stock in 6 months from now, on the basis of\\ncompany performance measures and economic data.\\n•Identify the numbers in a handwritten ZIP code, from a digitized\\nimage.\\n•Estimate the amount of glucose in the blood of a diabetic person,\\nfrom the infrared absorption spectrum of that person’s blood.\\n•Identify the risk factors for prostate cancer, based on clinical and\\ndemographic variables.\\nThe science of learning plays a key role in the ﬁelds of statistics, data\\nmining and artiﬁcial intelligence, intersecting with areas of engineering and\\nother disciplines.\\nThis book is about learning from data. In a typical scenario, we have\\nan outcome measurement, usually quantitative (such as a stock price) or\\ncategorical (such as heart attack/no heart attack), that we wish to predict\\nbased on a set of features (such as diet and clinical measurements). We\\nhave a training set of data, in which we observe the outcome and feature', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06f911e8-7c56-47cc-87a7-daf63c1c4be5', embedding=None, metadata={'page_label': '21', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2 1. Introduction\\nTABLE 1.1. Average percentage of words or characters in an email message\\nequal to the indicated word or character. We have chosen the wo rds and characters\\nshowing the largest diﬀerence between spamandemail.\\ngeorge you your hp free hpl ! our re edu remove\\nspam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\\nemail 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\\nmeasurements for a set of objects (such as people). Using this data we build\\na prediction model, or learner , which will enable us to predict the outcome\\nfor new unseen objects. A good learner is one that accurately predicts such\\nan outcome.\\nThe examples above describe what is called the supervised learning prob-\\nlem. It is called “supervised” because of the presence of the outcome vari-\\nable to guide the learning process. In the unsupervised learning problem ,\\nwe observe only the features and have no measurements of the outcome.\\nOur task is rather to describe how the data are organized or clustered. We\\ndevote most of this book to supervised learning; the unsupervised problem\\nis less developed in the literature, and is the focus of Chapter 14.\\nHere are some examples of real learning problems that are discussed in\\nthis book.\\nExample 1: Email Spam\\nThe data for this example consists of information from 4601 email mes-\\nsages, in a study to try to predict whether the email was junk email, or\\n“spam.” The objective was to design an automatic spam detector that\\ncould ﬁlter out spam before clogging the users’ mailboxes. For all 4601\\nemail messages, the true outcome (email type) email orspamis available,\\nalong with the relative frequencies of 57 of the most commonly occurring\\nwords and punctuation marks in the email message. This is a supervised\\nlearning problem, with the outcome the class variable email/spam. It is also\\ncalled a classiﬁcation problem.\\nTable 1.1 lists the words and characters showing the largest average\\ndiﬀerence between spamandemail.\\nOur learning method has to decide which features to use and how: for\\nexample, we might use a rule such as\\nif (%george <0.6) & (%you>1.5) then spam\\nelseemail.\\nAnother form of a rule might be:\\nif (0.2≤%you−0.3≤%george )>0 then spam\\nelseemail.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b85b7ef-61c0-4e1b-a7a3-021dd7b8e304', embedding=None, metadata={'page_label': '22', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. Introduction 3\\nlpsa−1 1 2 3 4\\noooooo ooo ooo o oooo o oooo oooooooooo oooooooooooooooooooooooooooooo oo oooo ooooooooooooooooooooooooooooo\\nooo ooo ooooo oooooooooooooooooooooooooo o ooooooooo oo ooooooo oooooooooooo ooooooooooooooooooooooooooooo40 50 60 70 80\\noo oooo ooo ooo oooooo o ooooooooooooo oooooooooooooooo o o oooooo oo oooo oooooooooooooooooooo ooooooooooooooo\\noo o ooo o o o oo ooooo o oo o o o ooooo ooo ooo oo o oo oo oo o o ooo o o oo ooo o o o ooooo o o o ooo o o o o oooo oo oo ooooo o o oo o o oooooo0.0 0.4 0.8\\noo o ooo ooo oo o oooooooooooooooooooo oooooo o ooooooo o oo oooooooooooo o o o oooo oo o o oooo oo o ooo o oo o o ooo o oooooo\\noo o ooo ooo oo o ooooooooo o o o oooooo oo oooo o o oooooooo o oo ooooo ooooooo ooo ooooooo ooooooo o ooooooo oooo ooooooo6.0 7.0 8.0 9.0\\noo oooo ooo oo o oooooooooooooooooooo oooooooo o ooooo o oo ooooooooooooo ooooooooooo o ooooooooo o ooooooooooooo\\n0 1 2 3 4 5oo oooo ooo oo o ooooooooooo o oo o o o ooo oooooooo o ooooo o oo o o o oooooooooo oo oo o ooo ooo o ooo o oooooooo oooo oooooo o−1 1 2 3 4ooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nlcavol\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noo oo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\nooo o\\noo\\no oo\\nooooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\no oooo\\noo oo\\noooo\\noo\\nooo\\noo\\nooo\\no oo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\no oooo\\noo o o\\noo o o\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\nooo o\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\nooooo\\noo o o\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\no oo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\nooo o\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\no oo\\nooooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\no o\\noo o\\nooo\\nooooo\\noo oo\\noooo\\noo\\nooo\\noo\\nooo\\no oo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\nooooooo\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooolweight\\noo\\nooooooo\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\no ooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\noo o\\nooo\\noo\\noooooo o\\noo o\\nooo\\nooo\\noo\\no oooo\\nooo\\nooo\\no\\noooo\\no\\noo\\noooo o\\noooo\\no ooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooo oooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\no oo oo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooo oooo\\noo\\noo\\nooo\\nooo\\n2.5 3.5 4.5oo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooo oo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noo oooo\\no\\noo\\noo\\noo o\\noo\\nooooooo\\noo\\noo\\nooo\\nooo40 50 60 70 80ooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noo oooooo ooo oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooooo\\nooo\\no\\noo\\noooooooooo ooo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\nage\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooooo o oo\\noo oo\\noo\\nooo\\no oo\\noo\\no\\nooo\\nooo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooo oo ooo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\noo o ooooo\\no ooo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\nooo\\noo\\noooo\\no\\noooo oooo\\no o oo\\no oooo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no ooo\\no\\no oo ooooo\\noo oo\\no o ooo\\nooo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no ooo\\no\\no o oo oooo\\no ooo\\no oo oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\noooo\\nooo\\noo\\nooo\\noo\\no o oo\\nolbph\\no o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\no o o o o ooo\\no o oo\\noo ooo\\nooo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no ooo\\no\\no o oo o ooo\\no o oo\\no o ooo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\n−1 0 1 2o o oo o ooo\\no o oo\\noo o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\noooo\\nooo\\noo\\no oo\\noo\\noooo\\no0.0 0.4 0.8oo o ooo o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o oo\\no o o o o o oo\\no o o o o o o o o o o o o oo\\noo\\no o o o o oo\\noo oo o\\no oo\\no o oo\\no oo\\nooo o\\noooooo o\\noooo oo o oo o oo o oooo oo o o oo oo ooo o oo o oo o o ooo\\no o oo oooo\\no oo o oo o oooo o ooo\\noo\\noo ooo oo\\noo o o o\\no oo\\noo oo\\nooo\\noo oo\\nooo oo o o\\no oo ooo o o oo o oo o oo ooo oooo ooo ooo o oooo o oo oo\\noo oo oo oo\\noo o o oo oo oo o oo oo\\noo\\noo oo o oo\\nooo oo\\no oo\\noo oo\\nooo\\noo oo\\nooo oo oo\\no o oo oo ooo ooo o oo o oo o oo o o o o ooo oo o o oo o o ooo\\no o oo o ooo\\no o o oo o oo ooo oo oo\\noo\\noo o oooo\\nooo o o\\no oo\\noooo\\nooo\\nooo o\\noo oo o o o\\no o o o o o o oo o o oo o o o oo o oo oo o ooo ooo o oo o o oooo\\no o o oo o oo\\no o oo o oo o oo oo o oo\\noo\\no o o oo oo\\noo oo o\\nooo\\no o oo\\no oo\\noo o o\\noo o o o oo\\nsvi\\no o o o o o o o o o o o oo oo o oo o o oo oo o oo o ooo o o oo ooo\\noo o o oooo\\noo oo o o oo o oo o o oo\\noo\\no o ooo oo\\noo o o o\\nooo\\no o oo\\nooo\\noo oo\\noo o oo o o\\no o oo o o o o o o o o o o oo oo o o o oo oo o o o oo o o o o o o ooo\\no ooo o o oo\\noo o oo o o o o oo o ooo\\noo\\no o o oo oo\\noo oo o\\no oo\\no o oo\\no oo\\noo o o\\noo o o o o o\\no o oo o o o o o o o o oo o o oo o o o oo oo o oo oo o o o o oo o oo\\no oooo o oo\\noo o oo ooo oo o o ooo\\noo\\no o ooooo\\noo oo o\\no oo\\noo oo\\nooo\\noo o o\\noo ooo oo\\noo o ooo o o o oo oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\noooo oo o oo o ooo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no ooo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\nooo\\no o ooooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\nooooo\\noo\\no oo ooo o o oo o oo\\noo\\nooo\\no ooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no oo oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\nooooo\\noo\\no o oo oo ooo oooo\\noo\\nooo\\no ooo\\noo\\no oo\\nooo\\no\\no ooo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\noooo\\no oo\\no oo oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\nooooo\\noo\\no o o o o o o oo o o oo\\noo\\nooo\\no ooo\\noo\\nooo\\nooo\\no\\noo oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\noo o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\no oo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\nlcp\\no o oo o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\noooo\\no oo\\no o ooooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\n−1 0 1 2 3o o oo o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o ooooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo6.0 7.0 8.0 9.0ooo\\nooo o o o oo oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo o o oo\\no o o o o o o o oo\\no o\\nooo o\\noooooo o\\nooo\\no oo o oo o ooo oo\\noo\\no o o oo\\noo\\no ooo o\\noo o oo ooo\\no ooo\\no\\nooooo\\no\\nooo\\noo o ooo\\noo o\\nooo\\no\\noo oo\\noo oo oo\\no oo o o oo o oo\\no o\\noo oo\\nooo oo o o\\no oo\\nooo o o oo o oo o o\\noo\\noo ooo\\noo\\nooooo\\no oooo ooo\\no ooo\\no\\nooo oo\\no\\no oo\\noo oo oo\\nooo\\nooo\\no\\noo oo\\noooo oo\\noo o oo oo o oo\\noo\\noo oo\\nooo oo oo\\no oo\\no oo ooo oooo oo\\noo\\no o ooo\\noo\\no ooo o\\no o o oo ooo\\no ooo\\no\\noo ooo\\no\\no oo\\noo oo oo\\nooo\\nooo\\no\\noo o o\\nooo ooo\\no o o ooooo oo\\no o\\nooo o\\noo oo o o o\\no oo\\no o o o oo o o oo o o\\noo\\no o ooo\\noo\\nooo oo\\no o oo o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\nooo o oo\\noo o\\nooo\\no\\noo o o\\noo o ooo\\no o ooo o o ooo\\no o\\noo o o\\noo o o o oo\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no ooo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo oo oo\\no o o o oo o o oo\\no o\\noo o o\\noo o o o o o\\no oo\\no o o o o o o o ooo o\\noo\\noo o oo\\noo\\no ooo o\\no oo o o ooo\\no ooo\\no\\nooooo\\no\\no oo\\noo oo o o\\noo o\\nooo\\no\\noo oo\\noo oo oo\\no ooo oo o ooo\\no o\\noo oo\\noo o oo o ogleason\\no oo\\no o o o o o o o ooo o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no ooo\\no\\noo o oo\\no\\no oo\\noooo oo\\noo o\\nooo\\no\\noo oo\\noo oo oo\\no o oo ooo ooo\\no o\\noo o o\\noo ooo oo\\n0 1 2 3 4 5ooo\\nooo o o o oo oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\nooo\\no oo o oo o ooo\\noooo\\no o o oo\\noo\\no oo\\noo\\noo o oo ooooo\\noo\\no\\nooooo\\no\\nooo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n2.5 3.5 4.5o oo\\nooo o o oo o oo\\no ooo\\noo ooo\\noo\\nooo\\noo\\no oooo ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no oo ooo oooo\\noooo\\no o ooo\\noo\\no oo\\noo\\no o o oo ooooo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n−1 0 1 2o oo\\no o o o oo o o oo\\no ooo\\no o ooo\\noo\\nooo\\noo\\no o oo o ooooo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n−1 0 1 2 3o oo\\no o o o o o o o oo\\no ooo\\noo o oo\\noo\\no oo\\noo\\no oo o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooooo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n0 20 60 100\\n0 20 60 100pgg45\\nFIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ﬁrst row sho ws\\nthe response against each of the predictors in turn. Two of the pr edictors, sviand\\ngleason , are categorical.\\nFor this problem not all errors are equal; we want to avoid ﬁltering out\\ngood email, while letting spam get through is not desirable but less serious\\nin its consequences. We discuss a number of diﬀerent methods for tackling\\nthis learning problem in the book.\\nExample 2: Prostate Cancer\\nThe data for this example, displayed in Figure 1.11, come from a study\\nby Stamey et al. (1989) that examined the correlation between the level of\\n1There was an error in these data in the ﬁrst edition of this boo k. Subject 32 had\\na value of 6.1 for lweight , which translates to a 449 gm prostate! The correct value is\\n44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='812a3909-13a0-4900-b9ea-7377e424e965', embedding=None, metadata={'page_label': '23', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4 1. Introduction\\nFIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.\\nprostate speciﬁc antigen (PSA) and a number of clinical measures, in 97\\nmen who were about to receive a radical prostatectomy.\\nThe goal is to predict the log of PSA ( lpsa) from a number of measure-\\nments including log cancer volume ( lcavol ), log prostate weight lweight ,\\nage, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-\\nvasionsvi, log of capsular penetration lcp, Gleason score gleason , and\\npercent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix\\nof the variables. Some correlations with lpsaare evident, but a good pre-\\ndictive model is diﬃcult to construct by eye.\\nThis is a supervised learning problem, known as a regression problem ,\\nbecause the outcome measurement is quantitative.\\nExample 3: Handwritten Digit Recognition\\nThe data from this example come from the handwritten ZIP codes on\\nenvelopes from U.S. postal mail. Each image is a segment from a ﬁve digi t\\nZIP code, isolating a single digit. The images are 16 ×16 eight-bit grayscale\\nmaps, with each pixel ranging in intensity from 0 to 255. Some sample\\nimages are shown in Figure 1.2.\\nThe images have been normalized to have approximately the same size\\nand orientation. The task is to predict, from the 16 ×16 matrix of pixel\\nintensities, the identity of each image (0 ,1,... ,9) quickly and accurately. If\\nit is accurate enough, the resulting algorithm would be used as part of an\\nautomatic sorting procedure for envelopes. This is a classiﬁcation problem\\nfor which the error rate needs to be kept very low to avoid misdirection of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='962d6a5e-5547-466b-958b-df1de59ce82e', embedding=None, metadata={'page_label': '24', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. Introduction 5\\nmail. In order to achieve this low error rate, some objects can be assigned\\nto a “don’t know” category, and sorted instead by hand.\\nExample 4: DNA Expression Microarrays\\nDNA stands for deoxyribonucleic acid, and is the basic material that makes\\nup human chromosomes. DNA microarrays measure the expression of a\\ngene in a cell by measuring the amount of mRNA (messenger ribonucleic\\nacid) present for that gene. Microarrays are considered a breakthrough\\ntechnology in biology, facilitating the quantitative study of thousands of\\ngenes simultaneously from a single sample of cells.\\nHere is how a DNA microarray works. The nucleotide sequences for a few\\nthousand genes are printed on a glass slide. A target sample and a reference\\nsample are labeled with red and green dyes, and each are hybridized with\\nthe DNA on the slide. Through ﬂuoroscopy, the log (red/green) intensities\\nof RNA hybridizing at each site is measured. The result is a few thousand\\nnumbers, typically ranging from say −6 to 6, measuring the expression level\\nof each gene in the target relative to the reference sample. Positive values\\nindicate higher expression in the target versus the reference, and vice versa\\nfor negative values.\\nA gene expression dataset collects together the expression values from a\\nseries of DNA microarray experiments, with each column representing an\\nexperiment. There are therefore several thousand rows representing individ-\\nual genes, and tens of columns representing samples: in the particular ex-\\nample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),\\nalthough for clarity only a random sample of 100 rows are shown. The ﬁg-\\nure displays the data set as a heat map, ranging from green (negative) to\\nred (positive). The samples are 64 cancer tumors from diﬀerent patients.\\nThe challenge here is to understand how the genes and samples are or-\\nganized. Typical questions include the following:\\n(a) which samples are most similar to each other, in terms of their expres-\\nsion proﬁles across genes?\\n(b) which genes are most similar to each other, in terms of their expression\\nproﬁles across samples?\\n(c) do certain genes show very high (or low) expression for certain cancer\\nsamples?\\nWe could view this task as a regression problem, with two categorical\\npredictor variables—genes and samples—with the response variable being\\nthe level of expression. However, it is probably more useful to view it as\\nunsupervised learning problem. For example, for question (a) above, we\\nthink of the samples as points in 6830–dimensional space, which we want\\ntocluster together in some way.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4cc72111-567a-42b8-8490-4855b9ee87ec', embedding=None, metadata={'page_label': '25', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6 1. Introduction\\nSID42354SID31984SID301902SIDW128368SID375990SID360097SIDW325120ESTsChr.10SIDW365099SID377133SID381508SIDW308182SID380265SIDW321925ESTsChr.15SIDW362471SIDW417270SIDW298052SID381079SIDW428642TUPLE1TUP1ERLUMENSIDW416621SID43609ESTsSID52979SIDW357197SIDW366311ESTsSMALLNUCSIDW486740ESTsSID297905SID485148SID284853ESTsChr.15SID200394SIDW322806ESTsChr.2SIDW257915SID46536SIDW488221ESTsChr.5SID280066SIDW376394ESTsChr.15SIDW321854WASWiskottHYPOTHETICALSIDW376776SIDW205716SID239012SIDW203464HLACLASSISIDW510534SIDW279664SIDW201620SID297117SID377419SID114241ESTsCh31SIDW376928SIDW310141SIDW298203PTPRCSID289414SID127504ESTsChr.3SID305167SID488017SIDW296310ESTsChr.6SID47116MITOCHONDRIAL60ChrSIDW376586HomosapiensSIDW487261SIDW470459SID167117SIDW31489SID375812DNAPOLYMERSID377451ESTsChr.1MYBPROTOSID471915ESTsSIDW469884HumanmRNASIDW377402ESTsSID207172RASGTPASESID325394H.sapiensmRNAGNALSID73161SIDW380102SIDW299104BREAST\\nRENAL\\nMELANOMAMELANOMA\\nMCF7D-repro\\nCOLONCOLON\\nK562B-repro\\nCOLON\\nNSCLC\\nLEUKEMIA\\nRENAL\\nMELANOMA\\nBREAST\\nCNSCNS\\nRENAL\\nMCF7A-repro\\nNSCLC\\nK562A-repro\\nCOLON\\nCNS\\nNSCLCNSCLC\\nLEUKEMIA\\nCNS\\nOVARIAN\\nBREAST\\nLEUKEMIA\\nMELANOMAMELANOMA\\nOVARIANOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nOVARIANOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nLEUKEMIA\\nCOLON\\nBREAST\\nLEUKEMIA\\nCOLON\\nCNS\\nMELANOMA\\nNSCLC\\nPROSTATE\\nNSCLC\\nRENALRENAL\\nNSCLC\\nRENAL\\nLEUKEMIA\\nOVARIAN\\nPROSTATE\\nCOLON\\nBREAST\\nRENAL\\nUNKNOWNFIGURE 1.3. DNA microarray data: expression matrix of 6830genes (rows)\\nand64samples (columns), for the human tumor data. Only a random sampl e\\nof100rows are shown. The display is a heat map, ranging from bright gre en\\n(negative, under expressed) to bright red (positive, over expre ssed). Missing values\\nare gray. The rows and columns are displayed in a randomly chosen order.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93610a9a-12d1-4d7a-8414-b09456521727', embedding=None, metadata={'page_label': '26', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. Introduction 7\\nWho Should Read this Book\\nThis book is designed for researchers and students in a broad variety of\\nﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We\\nexpect that the reader will have had at least one elementary course in\\nstatistics, covering basic topics including linear regression.\\nWe have not attempted to write a comprehensive catalog of learning\\nmethods, but rather to describe some of the most important techniques.\\nEqually notable, we describe the underlying concepts and considerations\\nby which a researcher can judge a learning method. We have tried to write\\nthis book in an intuitive fashion, emphasizing concepts rather than math-\\nematical details.\\nAs statisticians, our exposition will naturally reﬂect our backgrounds and\\nareas of expertise. However in the past eight years we have been attending\\nconferences in neural networks, data mining and machine learning, and our\\nthinking has been heavily inﬂuenced by these exciting ﬁelds. This inﬂuence\\nis evident in our current research, and in this book.\\nHow This Book is Organized\\nOur view is that one must understand simple methods before trying to\\ngrasp more complex ones. Hence, after giving an overview of the supervis-\\ning learning problem in Chapter 2 , we discuss linear methods for regression\\nand classiﬁcation in Chapters 3 and4. InChapter 5 we describe splines,\\nwavelets and regularization/penalization methods for a single predictor,\\nwhile Chapter 6 covers kernel methods and local regression. Both of these\\nsets of methods are important building blocks for high-dimensional learn-\\ning techniques. Model assessment and selection is the topic of Chapter 7 ,\\ncovering the concepts of bias and variance, overﬁtting and methods such as\\ncross-validation for choosing models. Chapter 8 discusses model inference\\nand averaging, including an overview of maximum likelihood, Bayesian in-\\nference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,\\nA related procedure called boosting is the focus of Chapter 10 .\\nInChapters 9–13 we describe a series of structured methods for su-\\npervised learning, with Chapters 9 and 11 covering regression and Chap-\\nters 12 and 13 focusing on classiﬁcation. Chapter 14 describes methods for\\nunsupervised learning. Two recently proposed techniques, random forests\\nand ensemble learning, are discussed in Chapters 15 and 16 . We describe\\nundirected graphical models in Chapter 17 and ﬁnally we study high-\\ndimensional problems in Chapter 18 .\\nAt the end of each chapter we discuss computational considerations im-\\nportant for data mining applications, including how the computations scale\\nwith the number of observations and predictors. Each chapter ends with\\nBibliographic Notes giving background references for the material.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='148d33b4-b5f6-40e1-a0c4-eb9452c7a71e', embedding=None, metadata={'page_label': '27', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8 1. Introduction\\nWe recommend that Chapters 1–4 be ﬁrst read in sequence. Chapter 7\\nshould also be considered mandatory, as it covers central concepts that\\npertain to all learning methods. With this in mind, the rest of the book\\ncan be read sequentially, or sampled, depending on the reader’s interest.\\nThe symbol\\n indicates a technically diﬃcult section, one that can\\nbe skipped without interrupting the ﬂow of the discussion.\\nBook Website\\nThe website for this book is located at\\nhttp://www-stat.stanford.edu/ElemStatLearn\\nIt contains a number of resources, including many of the datasets used in\\nthis book.\\nNote for Instructors\\nWe have successively used the ﬁrst edition of this book as the basis for a\\ntwo-quarter course, and with the additional materials in this second edition,\\nit could even be used for a three-quarter sequence. Exercises are provided at\\nthe end of each chapter. It is important for students to have access to good\\nsoftware tools for these topics. We used the R and S-PLUS programming\\nlanguages in our courses.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a8054fdf-b115-45c2-b020-ac28d7732411', embedding=None, metadata={'page_label': '28', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 9\\nPrinter: Opaque this\\n2\\nOverview of Supervised Learning\\n2.1 Introduction\\nThe ﬁrst three examples described in Chapter 1 have several components\\nin common. For each there is a set of variables that might be denoted as\\ninputs , which are measured or preset. These have some inﬂuence on one or\\nmoreoutputs . For each example the goal is to use the inputs to predict the\\nvalues of the outputs. This exercise is called supervised learning .\\nWe have used the more modern language of machine learning. In the\\nstatistical literature the inputs are often called the predictors , a term we\\nwill use interchangeably with inputs, and more classically the independent\\nvariables . In the pattern recognition literature the term features is preferred,\\nwhich we use as well. The outputs are called the responses , or classically\\nthedependent variables .\\n2.2 Variable Types and Terminology\\nThe outputs vary in nature among the examples. In the glucose prediction\\nexample, the output is a quantitative measurement, where some measure-\\nments are bigger than others, and measurements close in value are close\\nin nature. In the famous Iris discrimination example due to R. A. Fisher,\\nthe output is qualitative (species of Iris) and assumes values in a ﬁnite set\\nG={Virginica ,Setosa andVersicolor }. In the handwritten digit example\\nthe output is one of 10 diﬀerent digit classes :G={0,1,... ,9}. In both of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3743d399-b9cb-4221-ae53-cd845feeb829', embedding=None, metadata={'page_label': '29', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10 2. Overview of Supervised Learning\\nthese there is no explicit ordering in the classes, and in fact often descrip-\\ntive labels rather than numbers are used to denote the classes. Qualitative\\nvariables are also referred to as categorical ordiscrete variables as well as\\nfactors .\\nFor both types of outputs it makes sense to think of using the inputs to\\npredict the output. Given some speciﬁc atmospheric measurements today\\nand yesterday, we want to predict the ozone level tomorrow. Given the\\ngrayscale values for the pixels of the digitized image of the handwritten\\ndigit, we want to predict its class label.\\nThis distinction in output type has led to a naming convention for the\\nprediction tasks: regression when we predict quantitative outputs, and clas-\\nsiﬁcation when we predict qualitative outputs. We will see that these two\\ntasks have a lot in common, and in particular both can be viewed as a task\\nin function approximation.\\nInputs also vary in measurement type; we can have some of each of qual-\\nitative and quantitative input variables. These have also led to distinctio ns\\nin the types of methods that are used for prediction: some methods are\\ndeﬁned most naturally for quantitative inputs, some most naturally for\\nqualitative and some for both.\\nA third variable type is ordered categorical , such as small, medium and\\nlarge, where there is an ordering between the values, but no metric notion\\nis appropriate (the diﬀerence between medium and small need not be the\\nsame as that between large and medium). These are discussed further in\\nChapter 4.\\nQualitative variables are typically represented numerically by codes. The\\neasiest case is when there are only two classes or categories, such as “suc-\\ncess” or “failure,” “survived” or “died.” These are often represented by a\\nsingle binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will\\nbecome apparent, such numeric codes are sometimes referred to as targets.\\nWhen there are more than two categories, several alternatives are available.\\nThe most useful and commonly used coding is via dummy variables . Here a\\nK-level qualitative variable is represented by a vector of Kbinary variables\\nor bits, only one of which is “on” at a time. Although more compact coding\\nschemes are possible, dummy variables are symmetric in the levels of the\\nfactor.\\nWe will typically denote an input variable by the symbol X. IfXis\\na vector, its components can be accessed by subscripts Xj. Quantitative\\noutputs will be denoted by Y, and qualitative outputs by G(for group).\\nWe use uppercase letters such as X,YorGwhen referring to the generic\\naspects of a variable. Observed values are written in lowercase; hence the\\nith observed value of Xis written as xi(where xiis again a scalar or\\nvector). Matrices are represented by bold uppercase letters; for example, a\\nset of Ninput p-vectors xi, i= 1,... ,N would be represented by the N×p\\nmatrix X. In general, vectors will not be bold, except when they have N\\ncomponents; this convention distinguishes a p-vector of inputs xifor the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1970d0a9-c1e7-4521-811c-0b6a24e0a292', embedding=None, metadata={'page_label': '30', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Least Squares and Nearest Neighbors 11\\nith observation from the N-vector xjconsisting of all the observations on\\nvariable Xj. Since all vectors are assumed to be column vectors, the ith\\nrow of XisxT\\ni, the vector transpose of xi.\\nFor the moment we can loosely state the learning task as follows: given\\nthe value of an input vector X, make a good prediction of the output Y,\\ndenoted by ˆY(pronounced “y-hat”). If Ytakes values in IR then so should\\nˆY; likewise for categorical outputs, ˆGshould take values in the same set G\\nassociated with G.\\nFor a two-class G, one approach is to denote the binary coded target\\nasY, and then treat it as a quantitative output. The predictions ˆYwill\\ntypically lie in [0 ,1], and we can assign to ˆGthe class label according to\\nwhether ˆ y >0.5. This approach generalizes to K-level qualitative outputs\\nas well.\\nWe need data to construct prediction rules, often a lot of it. We thus\\nsuppose we have available a set of measurements ( xi,yi) or ( xi,gi), i=\\n1,... ,N , known as the training data , with which to construct our prediction\\nrule.\\n2.3 Two Simple Approaches to Prediction: Least\\nSquares and Nearest Neighbors\\nIn this section we develop two simple but powerful prediction methods: the\\nlinear model ﬁt by least squares and the k-nearest-neighbor prediction rule.\\nThe linear model makes huge assumptions about structure and yields stable\\nbut possibly inaccurate predictions. The method of k-nearest neighbors\\nmakes very mild structural assumptions: its predictions are often accurate\\nbut can be unstable.\\n2.3.1 Linear Models and Least Squares\\nThe linear model has been a mainstay of statistics for the past 30 years\\nand remains one of our most important tools. Given a vector of inputs\\nXT= (X1,X2,... ,X p), we predict the output Yvia the model\\nˆY=ˆβ0+p∑\\nj=1Xjˆβj. (2.1)\\nThe term ˆβ0is the intercept, also known as the biasin machine learning.\\nOften it is convenient to include the constant variable 1 in X, include ˆβ0in\\nthe vector of coeﬃcients ˆβ, and then write the linear model in vector form\\nas an inner product\\nˆY=XTˆβ, (2.2)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bad48be3-c8e2-4f89-9ff2-d1a6d636a664', embedding=None, metadata={'page_label': '31', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12 2. Overview of Supervised Learning\\nwhere XTdenotes vector or matrix transpose ( Xbeing a column vector).\\nHere we are modeling a single output, so ˆYis a scalar; in general ˆYcan be\\naK–vector, in which case βwould be a p×Kmatrix of coeﬃcients. In the\\n(p+ 1)-dimensional input–output space, ( X,ˆY) represents a hyperplane.\\nIf the constant is included in X, then the hyperplane includes the origin\\nand is a subspace; if not, it is an aﬃne set cutting the Y-axis at the point\\n(0,ˆβ0). From now on we assume that the intercept is included in ˆβ.\\nViewed as a function over the p-dimensional input space, f(X) =XTβ\\nis linear, and the gradient f′(X) =βis a vector in input space that points\\nin the steepest uphill direction.\\nHow do we ﬁt the linear model to a set of training data? There are\\nmany diﬀerent methods, but by far the most popular is the method of\\nleast squares . In this approach, we pick the coeﬃcients βto minimize the\\nresidual sum of squares\\nRSS(β) =N∑\\ni=1(yi−xT\\niβ)2. (2.3)\\nRSS(β) is a quadratic function of the parameters, and hence its minimum\\nalways exists, but may not be unique. The solution is easiest to characterize\\nin matrix notation. We can write\\nRSS(β) = (y−Xβ)T(y−Xβ), (2.4)\\nwhere Xis an N×pmatrix with each row an input vector, and yis an\\nN-vector of the outputs in the training set. Diﬀerentiating w.r.t. βwe get\\nthenormal equations\\nXT(y−Xβ) = 0. (2.5)\\nIfXTXis nonsingular, then the unique solution is given by\\nˆβ= (XTX)−1XTy, (2.6)\\nand the ﬁtted value at the ith input xiis ˆyi= ˆy(xi) =xT\\niˆβ. At an arbi-\\ntrary input x0the prediction is ˆ y(x0) =xT\\n0ˆβ. The entire ﬁtted surface is\\ncharacterized by the pparameters ˆβ. Intuitively, it seems that we do not\\nneed a very large data set to ﬁt such a model.\\nLet’s look at an example of the linear model in a classiﬁcation context.\\nFigure 2.1 shows a scatterplot of training data on a pair of inputs X1and\\nX2. The data are simulated, and for the present the simulation model is\\nnot important. The output class variable Ghas the values BLUEorORANGE ,\\nand is represented as such in the scatterplot. There are 100 points in each\\nof the two classes. The linear regression model was ﬁt to these data, with\\nthe response Ycoded as 0 for BLUEand 1 for ORANGE . The ﬁtted values ˆY\\nare converted to a ﬁtted class variable ˆGaccording to the rule\\nˆG={\\nORANGE ifˆY >0.5,\\nBLUE ifˆY≤0.5.(2.7)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f845999e-5448-4864-be1e-ff4ce3ef1f51', embedding=None, metadata={'page_label': '32', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Least Squares and Nearest Neighbors 13\\nLinear Regression of 0/1 Response\\n.. . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.1. A classiﬁcation example in two dimensions. The classes are coded\\nas a binary variable ( BLUE= 0,ORANGE = 1), and then ﬁt by linear regression.\\nThe line is the decision boundary deﬁned by xTˆβ= 0.5. The orange shaded region\\ndenotes that part of input space classiﬁed as ORANGE , while the blue region is\\nclassiﬁed as BLUE.\\nThe set of points in IR2classiﬁed as ORANGE corresponds to {x:xTˆβ >0.5},\\nindicated in Figure 2.1, and the two predicted classes are separated by the\\ndecision boundary {x:xTˆβ= 0.5}, which is linear in this case. We see\\nthat for these data there are several misclassiﬁcations on both sides of the\\ndecision boundary. Perhaps our linear model is too rigid— or are such errors\\nunavoidable? Remember that these are errors on the training data itself,\\nand we have not said where the constructed data came from. Consider the\\ntwo possible scenarios:\\nScenario 1: The training data in each class were generated from bivariate\\nGaussian distributions with uncorrelated components and diﬀerent\\nmeans.\\nScenario 2: The training data in each class came from a mixture of 10 low-\\nvariance Gaussian distributions, with individual means themselves\\ndistributed as Gaussian.\\nA mixture of Gaussians is best described in terms of the generative\\nmodel. One ﬁrst generates a discrete variable that determines which of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='29c909b0-4d82-4147-a48a-007c023ab1ef', embedding=None, metadata={'page_label': '33', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14 2. Overview of Supervised Learning\\nthe component Gaussians to use, and then generates an observation from\\nthe chosen density. In the case of one Gaussian per class, we will see in\\nChapter 4 that a linear decision boundary is the best one can do, and that\\nour estimate is almost optimal. The region of overlap is inevitable, and\\nfuture data to be predicted will be plagued by this overlap as well.\\nIn the case of mixtures of tightly clustered Gaussians the story is dif-\\nferent. A linear decision boundary is unlikely to be optimal, and in fact is\\nnot. The optimal decision boundary is nonlinear and disjoint, and as such\\nwill be much more diﬃcult to obtain.\\nWe now look at another classiﬁcation and regression procedure that is\\nin some sense at the opposite end of the spectrum to the linear model, and\\nfar better suited to the second scenario.\\n2.3.2 Nearest-Neighbor Methods\\nNearest-neighbor methods use those observations in the training set Tclos-\\nest in input space to xto form ˆY. Speciﬁcally, the k-nearest neighbor ﬁt\\nforˆYis deﬁned as follows:\\nˆY(x) =1\\nk∑\\nxi∈Nk(x)yi, (2.8)\\nwhere Nk(x) is the neighborhood of xdeﬁned by the kclosest points xiin\\nthe training sample. Closeness implies a metric, which for the moment we\\nassume is Euclidean distance. So, in words, we ﬁnd the kobservations with\\nxiclosest to xin input space, and average their responses.\\nIn Figure 2.2 we use the same training data as in Figure 2.1, and use\\n15-nearest-neighbor averaging of the binary coded response as the method\\nof ﬁtting. Thus ˆYis the proportion of ORANGE ’s in the neighborhood, and\\nso assigning class ORANGE toˆGifˆY >0.5 amounts to a majority vote in\\nthe neighborhood. The colored regions indicate all those points in input\\nspace classiﬁed as BLUEorORANGE by such a rule, in this case found by\\nevaluating the procedure on a ﬁne grid in input space. We see that the\\ndecision boundaries that separate the BLUEfrom the ORANGE regions are far\\nmore irregular, and respond to local clusters where one class dominates.\\nFigure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: ˆYis\\nassigned the value yℓof the closest point xℓtoxin the training data. In\\nthis case the regions of classiﬁcation can be computed relatively easily, and\\ncorrespond to a Voronoi tessellation of the training data. Each point xi\\nhas an associated tile bounding the region for which it is the closest input\\npoint. For all points xin the tile, ˆG(x) =gi. The decision boundary is even\\nmore irregular than before.\\nThe method of k-nearest-neighbor averaging is deﬁned in exactly the\\nsame way for regression of a quantitative output Y, although k= 1 would\\nbe an unlikely choice.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f38dd6c1-cb64-4384-90ca-16da88327d19', embedding=None, metadata={'page_label': '34', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Least Squares and Nearest Neighbors 15\\n15-Nearest Neighbor Classifier\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .\\n... .. .. .. .. . .. . .. . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE = 1)and\\nthen ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted class i s hence\\nchosen by majority vote amongst the 15-nearest neighbors.\\nIn Figure 2.2 we see that far fewer training observations are misclassiﬁed\\nthan in Figure 2.1. This should not give us too much comfort, though, since\\nin Figure 2.3 noneof the training data are misclassiﬁed. A little thought\\nsuggests that for k-nearest-neighbor ﬁts, the error on the training data\\nshould be approximately an increasing function of k, and will always be 0\\nfork= 1. An independent test set would give us a more satisfactory means\\nfor comparing the diﬀerent methods.\\nIt appears that k-nearest-neighbor ﬁts have a single parameter, the num-\\nber of neighbors k, compared to the pparameters in least-squares ﬁts. Al-\\nthough this is the case, we will see that the eﬀective number of parameters\\nofk-nearest neighbors is N/kand is generally bigger than p, and decreases\\nwith increasing k. To get an idea of why, note that if the neighborhoods\\nwere nonoverlapping, there would be N/kneighborhoods and we would ﬁt\\none parameter (a mean) in each neighborhood.\\nIt is also clear that we cannot use sum-of-squared errors on the training\\nset as a criterion for picking k, since we would always pick k= 1! It would\\nseem that k-nearest-neighbor methods would be more appropriate for the\\nmixture Scenario 2 described above, while for Gaussian data the decision\\nboundaries of k-nearest neighbors would be unnecessarily noisy.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f17f608e-b2bd-4036-91e8-5b013c5c06cf', embedding=None, metadata={'page_label': '35', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16 2. Overview of Supervised Learning\\n1−Nearest Neighbor Classifier\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE = 1), and\\nthen predicted by 1-nearest-neighbor classiﬁcation.\\n2.3.3 From Least Squares to Nearest Neighbors\\nThe linear decision boundary from least squares is very smooth, and ap-\\nparently stable to ﬁt. It does appear to rely heavily on the assumption\\nthat a linear decision boundary is appropriate. In language we will develop\\nshortly, it has low variance and potentially high bias.\\nOn the other hand, the k-nearest-neighbor procedures do not appear to\\nrely on any stringent assumptions about the underlying data, and can adapt\\nto any situation. However, any particular subregion of the decision bound-\\nary depends on a handful of input points and their particular positions,\\nand is thus wiggly and unstable—high variance and low bias.\\nEach method has its own situations for which it works best; in particular\\nlinear regression is more appropriate for Scenario 1 above, while nearest\\nneighbors are more suitable for Scenario 2. The time has come to expose\\nthe oracle! The data in fact were simulated from a model somewhere be-\\ntween the two, but closer to Scenario 2. First we generated 10 means mk\\nfrom a bivariate Gaussian distribution N((1,0)T,I) and labeled this class\\nBLUE. Similarly, 10 more were drawn from N((0,1)T,I) and labeled class\\nORANGE . Then for each class we generated 100 observations as follows: for\\neach observation, we picked an mkat random with probability 1 /10, and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b0c07268-a7ae-428b-bb19-ad7609d97eef', embedding=None, metadata={'page_label': '36', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Least Squares and Nearest Neighbors 17\\nDegrees of Freedom − N/kTest Error\\n0.10 0.15 0.20 0.25 0.30\\n  2   3   5   8  12  18  29  67 200151 101  69  45  31  21  11   7   5   3   1\\nTrain\\nTest\\nBayesk −  Number of Nearest Neighbors\\nLinear\\nFIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Fi g-\\nures 2.1, 2.2 and 2.3. A single training sample of size 200was used, and a test\\nsample of size 10,000. The orange curves are test and the blue are training er-\\nror for k-nearest-neighbor classiﬁcation. The results for linear regres sion are the\\nbigger orange and blue squares at three degrees of freedom. The purple line is the\\noptimal Bayes error rate.\\nthen generated a N(mk,I/5), thus leading to a mixture of Gaussian clus-\\nters for each class. Figure 2.4 shows the results of classifying 10,000 new\\nobservations generated from the model. We compare the results for least\\nsquares and those for k-nearest neighbors for a range of values of k.\\nA large subset of the most popular techniques in use today are variants of\\nthese two simple procedures. In fact 1-nearest-neighbor, the simplest of all,\\ncaptures a large percentage of the market for low-dimensional problems.\\nThe following list describes some ways in which these simple procedures\\nhave been enhanced:\\n•Kernel methods use weights that decrease smoothly to zero with dis-\\ntance from the target point, rather than the eﬀective 0 /1 weights used\\nbyk-nearest neighbors.\\n•In high-dimensional spaces the distance kernels are modiﬁed to em-\\nphasize some variable more than others.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f347176-e6dc-4ba0-b3c9-dcc0e85edf43', embedding=None, metadata={'page_label': '37', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18 2. Overview of Supervised Learning\\n•Local regression ﬁts linear models by locally weighted least squares,\\nrather than ﬁtting constants locally.\\n•Linear models ﬁt to a basis expansion of the original inputs allow\\narbitrarily complex models.\\n•Projection pursuit and neural network models consist of sums of non-\\nlinearly transformed linear models.\\n2.4 Statistical Decision Theory\\nIn this section we develop a small amount of theory that provides a frame-\\nwork for developing models such as those discussed informally so far. We\\nﬁrst consider the case of a quantitative output, and place ourselves in the\\nworld of random variables and probability spaces. Let X∈IRpdenote a\\nreal valued random input vector, and Y∈IR a real valued random out-\\nput variable, with joint distribution Pr( X,Y). We seek a function f(X)\\nfor predicting Ygiven values of the input X. This theory requires a loss\\nfunction L(Y,f(X)) for penalizing errors in prediction, and by far the most\\ncommon and convenient is squared error loss :L(Y,f(X)) = ( Y−f(X))2.\\nThis leads us to a criterion for choosing f,\\nEPE(f) = E( Y−f(X))2(2.9)\\n=∫\\n[y−f(x)]2Pr(dx,dy ), (2.10)\\nthe expected (squared) prediction error . By conditioning1onX, we can\\nwrite EPE as\\nEPE(f) = E XEY|X(\\n[Y−f(X)]2|X)\\n(2.11)\\nand we see that it suﬃces to minimize EPE pointwise:\\nf(x) = argmincEY|X(\\n[Y−c]2|X=x)\\n. (2.12)\\nThe solution is\\nf(x) = E( Y|X=x), (2.13)\\nthe conditional expectation, also known as the regression function. Thus\\nthe best prediction of Yat any point X=xis the conditional mean, when\\nbest is measured by average squared error.\\nThe nearest-neighbor methods attempt to directly implement this recipe\\nusing the training data. At each point x, we might ask for the average of all\\n1Conditioning here amounts to factoring the joint density Pr (X, Y) = Pr( Y|X)Pr(X)\\nwhere Pr( Y|X) = Pr( Y, X)/Pr(X), and splitting up the bivariate integral accordingly.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02521b13-d6b3-4480-b555-c68b31ef05a3', embedding=None, metadata={'page_label': '38', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Statistical Decision Theory 19\\nthose yis with input xi=x. Since there is typically at most one observation\\nat any point x, we settle for\\nˆf(x) = Ave( yi|xi∈Nk(x)), (2.14)\\nwhere “Ave” denotes average, and Nk(x) is the neighborhood containing\\nthekpoints in T closest to x. Two approximations are happening here:\\n•expectation is approximated by averaging over sample data;\\n•conditioning at a point is relaxed to conditioning on some region\\n“close” to the target point.\\nFor large training sample size N, the points in the neighborhood are likely\\nto be close to x, and as kgets large the average will get more stable.\\nIn fact, under mild regularity conditions on the joint probability distri-\\nbution Pr( X,Y), one can show that as N,k→ ∞ such that k/N→0,\\nˆf(x)→E(Y|X=x). In light of this, why look further, since it seems\\nwe have a universal approximator? We often do not have very large sam-\\nples. If the linear or some more structured model is appropriate, then we\\ncan usually get a more stable estimate than k-nearest neighbors, although\\nsuch knowledge has to be learned from the data as well. There are other\\nproblems though, sometimes disastrous. In Section 2.5 we see that as the\\ndimension pgets large, so does the metric size of the k-nearest neighbor-\\nhood. So settling for nearest neighborhood as a surrogate for conditioning\\nwill fail us miserably. The convergence above still holds, but the rateof\\nconvergence decreases as the dimension increases.\\nHow does linear regression ﬁt into this framework? The simplest explana-\\ntion is that one assumes that the regression function f(x) is approximately\\nlinear in its arguments:\\nf(x)≈xTβ. (2.15)\\nThis is a model-based approach—we specify a model for the regression func-\\ntion. Plugging this linear model for f(x) into EPE (2.9) and diﬀerentiating\\nwe can solve for βtheoretically:\\nβ= [E(XXT)]−1E(XY). (2.16)\\nNote we have notconditioned on X; rather we have used our knowledge\\nof the functional relationship to poolover values of X. The least squares\\nsolution (2.6) amounts to replacing the expectation in (2.16) by averages\\nover the training data.\\nSo both k-nearest neighbors and least squares end up approximating\\nconditional expectations by averages. But they diﬀer dramatically in terms\\nof model assumptions:\\n•Least squares assumes f(x) is well approximated by a globally linear\\nfunction.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='275550ed-e548-48e2-b71f-8effe7079b66', embedding=None, metadata={'page_label': '39', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20 2. Overview of Supervised Learning\\n•k-nearest neighbors assumes f(x) is well approximated by a locally\\nconstant function.\\nAlthough the latter seems more palatable, we have already seen that we\\nmay pay a price for this ﬂexibility.\\nMany of the more modern techniques described in this book are model\\nbased, although far more ﬂexible than the rigid linear model. For example,\\nadditive models assume that\\nf(X) =p∑\\nj=1fj(Xj). (2.17)\\nThis retains the additivity of the linear model, but each coordinate function\\nfjis arbitrary. It turns out that the optimal estimate for the additive model\\nuses techniques such as k-nearest neighbors to approximate univariate con-\\nditional expectations simultaneously for each of the coordinate functions.\\nThus the problems of estimating a conditional expectation in high dimen-\\nsions are swept away in this case by imposing some (often unrealistic) model\\nassumptions, in this case additivity.\\nAre we happy with the criterion (2.11)? What happens if we replace the\\nL2loss function with the L1:E|Y−f(X)|? The solution in this case is the\\nconditional median,\\nˆf(x) = median( Y|X=x), (2.18)\\nwhich is a diﬀerent measure of location, and its estimates are more robust\\nthan those for the conditional mean. L1criteria have discontinuities in\\ntheir derivatives, which have hindered their widespread use. Other more\\nresistant loss functions will be mentioned in later chapters, but squared\\nerror is analytically convenient and the most popular.\\nWhat do we do when the output is a categorical variable G? The same\\nparadigm works here, except we need a diﬀerent loss function for penalizing\\nprediction errors. An estimate ˆGwill assume values in G, the set of possible\\nclasses. Our loss function can be represented by a K×Kmatrix L, where\\nK= card( G).Lwill be zero on the diagonal and nonnegative elsewhere,\\nwhere L(k,ℓ) is the price paid for classifying an observation belonging to\\nclassGkasGℓ. Most often we use the zero–one loss function, where all\\nmisclassiﬁcations are charged a single unit. The expected prediction error\\nis\\nEPE = E[ L(G,ˆG(X))], (2.19)\\nwhere again the expectation is taken with respect to the joint distribution\\nPr(G,X). Again we condition, and can write EPE as\\nEPE = E XK∑\\nk=1L[Gk,ˆG(X)]Pr(Gk|X) (2.20)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16bf98d3-4a6b-43f4-acfe-7155ad522ca9', embedding=None, metadata={'page_label': '40', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Statistical Decision Theory 21\\nBayes Optimal Classifier\\n... .. . . .. . . . .. . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.5. The optimal Bayes decision boundary for the simulation exampl e\\nof Figures 2.1, 2.2 and 2.3. Since the generating density is known for each class,\\nthis boundary can be calculated exactly (Exercise 2.2).\\nand again it suﬃces to minimize EPE pointwise:\\nˆG(x) = argming∈GK∑\\nk=1L(Gk,g)Pr(Gk|X=x). (2.21)\\nWith the 0–1 loss function this simpliﬁes to\\nˆG(x) = argming∈G[1−Pr(g|X=x)] (2.22)\\nor simply\\nˆG(X) =Gkif Pr(Gk|X=x) = max\\ng∈GPr(g|X=x). (2.23)\\nThis reasonable solution is known as the Bayes classiﬁer , and says that\\nwe classify to the most probable class, using the conditional (discrete) dis-\\ntribution Pr( G|X). Figure 2.5 shows the Bayes-optimal decision boundary\\nfor our simulation example. The error rate of the Bayes classiﬁer is called\\ntheBayes rate .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0be9ef29-8a13-4c37-a299-77fafe3ad893', embedding=None, metadata={'page_label': '41', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='22 2. Overview of Supervised Learning\\nAgain we see that the k-nearest neighbor classiﬁer directly approximates\\nthis solution—a majority vote in a nearest neighborhood amounts to ex-\\nactly this, except that conditional probability at a point is relaxed to con-\\nditional probability within a neighborhood of a point, and probabilities ar e\\nestimated by training-sample proportions.\\nSuppose for a two-class problem we had taken the dummy-variable ap-\\nproach and coded Gvia a binary Y, followed by squared error loss estima-\\ntion. Then ˆf(X) = E( Y|X) = Pr( G=G1|X) ifG1corresponded to Y= 1.\\nLikewise for a K-class problem, E( Yk|X) = Pr( G=Gk|X). This shows\\nthat our dummy-variable regression procedure, followed by classiﬁcation to\\nthe largest ﬁtted value, is another way of representing the Bayes classiﬁer.\\nAlthough this theory is exact, in practice problems can occur, depending\\non the regression model used. For example, when linear regression is used,\\nˆf(X) need not be positive, and we might be suspicious about using it as\\nan estimate of a probability. We will discuss a variety of approaches to\\nmodeling Pr( G|X) in Chapter 4.\\n2.5 Local Methods in High Dimensions\\nWe have examined two learning techniques for prediction so far: the stable\\nbut biased linear model and the less stable but apparently less biased class\\nofk-nearest-neighbor estimates. It would seem that with a reasonably large\\nset of training data, we could always approximate the theoretically optimal\\nconditional expectation by k-nearest-neighbor averaging, since we should\\nbe able to ﬁnd a fairly large neighborhood of observations close to any x\\nand average them. This approach and our intuition breaks down in high\\ndimensions, and the phenomenon is commonly referred to as the curse\\nof dimensionality (Bellman, 1961). There are many manifestations of this\\nproblem, and we will examine a few here.\\nConsider the nearest-neighbor procedure for inputs uniformly distributed\\nin ap-dimensional unit hypercube, as in Figure 2.6. Suppose we send out a\\nhypercubical neighborhood about a target point to capture a fraction rof\\nthe observations. Since this corresponds to a fraction rof the unit volume,\\nthe expected edge length will be ep(r) =r1/p. In ten dimensions e10(0.01) =\\n0.63 and e10(0.1) = 0 .80, while the entire range for each input is only 1 .0.\\nSo to capture 1% or 10% of the data to form a local average, we must cover\\n63% or 80% of the range of each input variable. Such neighborhoods are no\\nlonger “local.” Reducing rdramatically does not help much either, since\\nthe fewer observations we average, the higher is the variance of our ﬁt.\\nAnother consequence of the sparse sampling in high dimensions is that\\nall sample points are close to an edge of the sample. Consider Ndata points\\nuniformly distributed in a p-dimensional unit ball centered at the origin.\\nSuppose we consider a nearest-neighbor estimate at the origin. The median', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c4f6a465-aa87-43c8-bb73-9fb4736e52a7', embedding=None, metadata={'page_label': '42', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.5 Local Methods in High Dimensions 23\\n1\\n10Unit Cube\\nFraction of VolumeDistance\\n0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0p=1p=2p=3p=10\\nNeighborhood\\nFIGURE 2.6. The curse of dimensionality is well illustrated by a subcubica l\\nneighborhood for uniform data in a unit cube. The ﬁgure on the righ t shows the\\nside-length of the subcube needed to capture a fraction rof the volume of the data,\\nfor diﬀerent dimensions p. In ten dimensions we need to cover 80%of the range\\nof each coordinate to capture 10%of the data.\\ndistance from the origin to the closest data point is given by the expression\\nd(p,N) =(\\n1−1\\n21/N)1/p\\n(2.24)\\n(Exercise 2.3). A more complicated expression exists for the mean distance\\nto the closest point. For N= 500, p= 10 , d(p,N)≈0.52, more than\\nhalfway to the boundary. Hence most data points are closer to the boundary\\nof the sample space than to any other data point. The reason that this\\npresents a problem is that prediction is much more diﬃcult near the edges\\nof the training sample. One must extrapolate from neighboring sample\\npoints rather than interpolate between them.\\nAnother manifestation of the curse is that the sampling density is pro-\\nportional to N1/p, where pis the dimension of the input space and Nis the\\nsample size. Thus, if N1= 100 represents a dense sample for a single input\\nproblem, then N10= 10010is the sample size required for the same sam-\\npling density with 10 inputs. Thus in high dimensions all feasible training\\nsamples sparsely populate the input space.\\nLet us construct another uniform example. Suppose we have 1000 train-\\ning examples xigenerated uniformly on [ −1,1]p. Assume that the true\\nrelationship between XandYis\\nY=f(X) =e−8||X||2,\\nwithout any measurement error. We use the 1-nearest-neighbor rule to\\npredict y0at the test-point x0= 0. Denote the training set by T. We can', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6a77e313-7d68-4834-a166-7b513b12048b', embedding=None, metadata={'page_label': '43', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='24 2. Overview of Supervised Learning\\ncompute the expected prediction error at x0for our procedure, averaging\\nover all such samples of size 1000. Since the problem is deterministic, this\\nis the mean squared error (MSE) for estimating f(0):\\nMSE( x0) = E T[f(x0)−ˆy0]2\\n= E T[ˆy0−ET(ˆy0)]2+ [ET(ˆy0)−f(x0)]2\\n= Var T(ˆy0) + Bias2(ˆy0). (2.25)\\nFigure 2.7 illustrates the setup. We have broken down the MSE into two\\ncomponents that will become familiar as we proceed: variance and squared\\nbias. Such a decomposition is always possible and often useful, and is known\\nas the bias–variance decomposition . Unless the nearest neighbor is at 0,\\nˆy0will be smaller than f(0) in this example, and so the average estimate\\nwill be biased downward. The variance is due to the sampling variance of\\nthe 1-nearest neighbor. In low dimensions and with N= 1000, the nearest\\nneighbor is very close to 0, and so both the bias and variance are small. As\\nthe dimension increases, the nearest neighbor tends to stray further from\\nthe target point, and both bias and variance are incurred. By p= 10, for\\nmore than 99% of the samples the nearest neighbor is a distance greater\\nthan 0 .5 from the origin. Thus as pincreases, the estimate tends to be 0\\nmore often than not, and hence the MSE levels oﬀ at 1 .0, as does the bias,\\nand the variance starts dropping (an artifact of this example).\\nAlthough this is a highly contrived example, similar phenomena occur\\nmore generally. The complexity of functions of many variables can grow\\nexponentially with the dimension, and if we wish to be able to estimate\\nsuch functions with the same accuracy as function in low dimensions, then\\nwe need the size of our training set to grow exponentially as well. In this\\nexample, the function is a complex interaction of all pvariables involved.\\nThe dependence of the bias term on distance depends on the truth, and\\nit need not always dominate with 1-nearest neighbor. For example, if the\\nfunction always involves only a few dimensions as in Figure 2.8, then the\\nvariance can dominate instead.\\nSuppose, on the other hand, that we know that the relationship between\\nYandXis linear,\\nY=XTβ+ε, (2.26)\\nwhere ε∼N(0,σ2) and we ﬁt the model by least squares to the train-\\ning data. For an arbitrary test point x0, we have ˆ y0=xT\\n0ˆβ, which can\\nbe written as ˆ y0=xT\\n0β+∑N\\ni=1ℓi(x0)εi, where ℓi(x0) is the ith element\\nofX(XTX)−1x0. Since under this model the least squares estimates are', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9ce9f6fb-74b5-430a-b426-6759de974eb9', embedding=None, metadata={'page_label': '44', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.5 Local Methods in High Dimensions 25\\nXf(X)\\n-1.0 -0.5 0.0 0.5 1.00.0 0.2 0.4 0.6 0.8 1.0•1-NN in One Dimension\\nX1X2\\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•••\\n•••\\n••\\n•\\n•••\\n•••\\n••\\n••\\n•\\n•\\n•••\\n••1-NN in One vs. Two Dimensions\\nDimensionAverage Distance to Nearest Neighbor\\n2 4 6 8 100.0 0.2 0.4 0.6 0.8••••••••••Distance to 1-NN vs. Dimension\\nDimensionMse\\n2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0• •••••••••\\n• •••••••• • • •••••••••MSE vs. Dimension\\n•MSE\\n•Variance\\n•Sq. Bias\\nFIGURE 2.7. A simulation example, demonstrating the curse of dimensional-\\nity and its eﬀect on MSE, bias and variance. The input features are u niformly\\ndistributed in [−1,1]pforp= 1, . . . ,10The top left panel shows the target func-\\ntion (no noise) in I R:f(X) =e−8||X||2, and demonstrates the error that 1-nearest\\nneighbor makes in estimating f(0). The training point is indicated by the blue tick\\nmark. The top right panel illustrates why the radius of the 1-nearest neighborhood\\nincreases with dimension p. The lower left panel shows the average radius of the\\n1-nearest neighborhoods. The lower-right panel shows the MSE, sq uared bias and\\nvariance curves as a function of dimension p.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='129a2ff3-d11c-43b8-94c7-07a808bff314', embedding=None, metadata={'page_label': '45', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='26 2. Overview of Supervised Learning\\nXf(X)\\n-1.0 -0.5 0.0 0.5 1.00 1 2 3 4•1-NN in One Dimension\\nDimensionMSE\\n2 4 6 8 100.0 0.05 0.10 0.15 0.20 0.25••••••••••\\n••••••••••\\n• •• • ••••••MSE  vs. Dimension\\n•MSE\\n•Variance\\n•Sq. Bias\\nFIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here\\nthe function is constant in all but one dimension: F(X) =1\\n2(X1+ 1)3. The\\nvariance dominates.\\nunbiased, we ﬁnd that\\nEPE(x0) = E y0|x0ET(y0−ˆy0)2\\n= Var( y0|x0) + E T[ˆy0−ETˆy0]2+ [ETˆy0−xT\\n0β]2\\n= Var( y0|x0) + Var T(ˆy0) + Bias2(ˆy0)\\n=σ2+ ETxT\\n0(XTX)−1x0σ2+ 02. (2.27)\\nHere we have incurred an additional variance σ2in the prediction error,\\nsince our target is not deterministic. There is no bias, and the variance\\ndepends on x0. IfNis large and Twere selected at random, and assuming\\nE(X) = 0, then XTX→NCov(X) and\\nEx0EPE(x0)∼Ex0xT\\n0Cov(X)−1x0σ2/N+σ2\\n= trace[Cov( X)−1Cov(x0)]σ2/N+σ2\\n=σ2(p/N) +σ2. (2.28)\\nHere we see that the expected EPE increases linearly as a function of p,\\nwith slope σ2/N. IfNis large and/or σ2is small, this growth in vari-\\nance is negligible (0 in the deterministic case). By imposing some heavy\\nrestrictions on the class of models being ﬁtted, we have avoided the curse\\nof dimensionality. Some of the technical details in (2.27) and (2.28) are\\nderived in Exercise 2.5.\\nFigure 2.9 compares 1-nearest neighbor vs. least squares in two situa-\\ntions, both of which have the form Y=f(X) +ε,Xuniform as before,\\nandε∼N(0,1). The sample size is N= 500. For the orange curve, f(x)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c6c1193e-8343-4179-94ad-b18b2e51ee91', embedding=None, metadata={'page_label': '46', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.5 Local Methods in High Dimensions 27\\nDimensionEPE Ratio\\n2 4 6 8 101.6 1.7 1.8 1.9 2.0 2.1••••••••••••••••••••Expected Prediction Error of 1NN vs. OLS\\n•Linear\\n•Cubic\\nFIGURE 2.9. The curves show the expected prediction error (at x0= 0) for\\n1-nearest neighbor relative to least squares for the model Y=f(X) +ε. For the\\norange curve, f(x) =x1, while for the blue curve f(x) =1\\n2(x1+ 1)3.\\nis linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8.\\nShown is the relative EPE of 1-nearest neighbor to least squares, which\\nappears to start at around 2 for the linear case. Least squares is unbiased\\nin this case, and as discussed above the EPE is slightly above σ2= 1.\\nThe EPE for 1-nearest neighbor is always above 2, since the variance of\\nˆf(x0) in this case is at least σ2, and the ratio increases with dimension as\\nthe nearest neighbor strays from the target point. For the cubic case, least\\nsquares is biased, which moderates the ratio. Clearly we could manufacture\\nexamples where the bias of least squares would dominate the variance, and\\nthe 1-nearest neighbor would come out the winner.\\nBy relying on rigid assumptions, the linear model has no bias at all and\\nnegligible variance, while the error in 1-nearest neighbor is substantially\\nlarger. However, if the assumptions are wrong, all bets are oﬀ and the\\n1-nearest neighbor may dominate. We will see that there is a whole spec-\\ntrum of models between the rigid linear models and the extremely ﬂexible\\n1-nearest-neighbor models, each with their own assumptions and biases,\\nwhich have been proposed speciﬁcally to avoid the exponential growth in\\ncomplexity of functions in high dimensions by drawing heavily on these\\nassumptions.\\nBefore we delve more deeply, let us elaborate a bit on the concept of\\nstatistical models and see how they ﬁt into the prediction framework.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bb19ccf7-b306-40e9-b9ae-0ced89341b74', embedding=None, metadata={'page_label': '47', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='28 2. Overview of Supervised Learning\\n2.6 Statistical Models, Supervised Learning and\\nFunction Approximation\\nOur goal is to ﬁnd a useful approximation ˆf(x) to the function f(x) that\\nunderlies the predictive relationship between the inputs and outputs. In the\\ntheoretical setting of Section 2.4, we saw that squared error loss lead us\\nto the regression function f(x) = E( Y|X=x) for a quantitative response.\\nThe class of nearest-neighbor methods can be viewed as direct estimates\\nof this conditional expectation, but we have seen that they can fail in at\\nleast two ways:\\n•if the dimension of the input space is high, the nearest neighbors need\\nnot be close to the target point, and can result in large errors;\\n•if special structure is known to exist, this can be used to reduce both\\nthe bias and the variance of the estimates.\\nWe anticipate using other classes of models for f(x), in many cases specif-\\nically designed to overcome the dimensionality problems, and here we dis-\\ncuss a framework for incorporating them into the prediction problem.\\n2.6.1 A Statistical Model for the Joint Distribution Pr(X,Y)\\nSuppose in fact that our data arose from a statistical model\\nY=f(X) +ε, (2.29)\\nwhere the random error εhas E( ε) = 0 and is independent of X. Note that\\nfor this model, f(x) = E( Y|X=x), and in fact the conditional distribution\\nPr(Y|X) depends on Xonlythrough the conditional mean f(x).\\nThe additive error model is a useful approximation to the truth. For\\nmost systems the input–output pairs ( X,Y) will not have a deterministic\\nrelationship Y=f(X). Generally there will be other unmeasured variables\\nthat also contribute to Y, including measurement error. The additive model\\nassumes that we can capture all these departures from a deterministic re-\\nlationship via the error ε.\\nFor some problems a deterministic relationship does hold. Many of the\\nclassiﬁcation problems studied in machine learning are of this form, where\\nthe response surface can be thought of as a colored map deﬁned in IRp.\\nThe training data consist of colored examples from the map {xi,gi}, and\\nthe goal is to be able to color any point. Here the function is deterministic,\\nand the randomness enters through the xlocation of the training points.\\nFor the moment we will not pursue such problems, but will see that they\\ncan be handled by techniques appropriate for the error-based models.\\nThe assumption in (2.29) that the errors are independent and identically\\ndistributed is not strictly necessary, but seems to be at the back of our mind', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1437a74-b08f-4127-a926-673202fc049d', embedding=None, metadata={'page_label': '48', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.6 Statistical Models, Supervised Learning and Function Approxi mation 29\\nwhen we average squared errors uniformly in our EPE criterion. With such\\na model it becomes natural to use least squares as a data criterion for\\nmodel estimation as in (2.1). Simple modiﬁcations can be made to avoid\\nthe independence assumption; for example, we can have Var( Y|X=x) =\\nσ(x), and now both the mean and variance depend on X. In general the\\nconditional distribution Pr( Y|X) can depend on Xin complicated ways,\\nbut the additive error model precludes these.\\nSo far we have concentrated on the quantitative response. Additive error\\nmodels are typically not used for qualitative outputs G; in this case the tar-\\nget function p(X)isthe conditional density Pr( G|X), and this is modeled\\ndirectly. For example, for two-class data, it is often reasonable to assume\\nthat the data arise from independent binary trials, with the probability of\\none particular outcome being p(X), and the other 1 −p(X). Thus if Yis\\nthe 0–1 coded version of G, then E( Y|X=x) =p(x), but the variance\\ndepends on xas well: Var( Y|X=x) =p(x)[1−p(x)].\\n2.6.2 Supervised Learning\\nBefore we launch into more statistically oriented jargon, we present the\\nfunction-ﬁtting paradigm from a machine learning point of view. Suppose\\nfor simplicity that the errors are additive and that the model Y=f(X)+ε\\nis a reasonable assumption. Supervised learning attempts to learn fby\\nexample through a teacher . One observes the system under study, both\\nthe inputs and outputs, and assembles a training set of observations T=\\n(xi,yi), i= 1,... ,N . The observed input values to the system xiare also\\nfed into an artiﬁcial system, known as a learning algorithm (usually a com-\\nputer program), which also produces outputs ˆf(xi) in response to the in-\\nputs. The learning algorithm has the property that it can modify its in-\\nput/output relationship ˆfin response to diﬀerences yi−ˆf(xi) between the\\noriginal and generated outputs. This process is known as learning by exam-\\nple. Upon completion of the learning process the hope is that the artiﬁcial\\nand real outputs will be close enough to be useful for all sets of inputs likely\\nto be encountered in practice.\\n2.6.3 Function Approximation\\nThe learning paradigm of the previous section has been the motivation\\nfor research into the supervised learning problem in the ﬁelds of machine\\nlearning (with analogies to human reasoning) and neural networks (with\\nbiological analogies to the brain). The approach taken in applied mathe-\\nmatics and statistics has been from the perspective of function approxima-\\ntion and estimation. Here the data pairs {xi,yi}are viewed as points in a\\n(p+ 1)-dimensional Euclidean space. The function f(x) has domain equal\\nto the p-dimensional input subspace, and is related to the data via a model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b738bbd-c620-408c-af8d-335ebd93186f', embedding=None, metadata={'page_label': '49', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='30 2. Overview of Supervised Learning\\nsuch as yi=f(xi) +εi. For convenience in this chapter we will assume the\\ndomain is IRp, ap-dimensional Euclidean space, although in general the\\ninputs can be of mixed type. The goal is to obtain a useful approximation\\ntof(x) for all xin some region of IRp, given the representations in T.\\nAlthough somewhat less glamorous than the learning paradigm, treating\\nsupervised learning as a problem in function approximation encourages the\\ngeometrical concepts of Euclidean spaces and mathematical concepts of\\nprobabilistic inference to be applied to the problem. This is the approach\\ntaken in this book.\\nMany of the approximations we will encounter have associated a set of\\nparameters θthat can be modiﬁed to suit the data at hand. For example,\\nthe linear model f(x) =xTβhasθ=β. Another class of useful approxi-\\nmators can be expressed as linear basis expansions\\nfθ(x) =K∑\\nk=1hk(x)θk, (2.30)\\nwhere the hkare a suitable set of functions or transformations of the input\\nvector x. Traditional examples are polynomial and trigonometric expan-\\nsions, where for example hkmight be x2\\n1,x1x2\\n2, cos(x1) and so on. We\\nalso encounter nonlinear expansions, such as the sigmoid transformation\\ncommon to neural network models,\\nhk(x) =1\\n1 + exp( −xTβk). (2.31)\\nWe can use least squares to estimate the parameters θinfθas we did\\nfor the linear model, by minimizing the residual sum-of-squares\\nRSS(θ) =N∑\\ni=1(yi−fθ(xi))2(2.32)\\nas a function of θ. This seems a reasonable criterion for an additive error\\nmodel. In terms of function approximation, we imagine our parameterized\\nfunction as a surface in p+ 1 space, and what we observe are noisy re-\\nalizations from it. This is easy to visualize when p= 2 and the vertical\\ncoordinate is the output y, as in Figure 2.10. The noise is in the output\\ncoordinate, so we ﬁnd the set of parameters such that the ﬁtted surface\\ngets as close to the observed points as possible, where close is measured by\\nthe sum of squared vertical errors in RSS( θ).\\nFor the linear model we get a simple closed form solution to the mini-\\nmization problem. This is also true for the basis function methods, if the\\nbasis functions themselves do not have any hidden parameters. Otherwise\\nthe solution requires either iterative methods or numerical optimization.\\nWhile least squares is generally very convenient, it is not the only crite-\\nrion used and in some cases would not make much sense. A more general', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc4f220c-4e65-4c7f-8f38-5bb536949768', embedding=None, metadata={'page_label': '50', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.6 Statistical Models, Supervised Learning and Function Approxi mation 31\\n•••\\n•••\\n••••\\n••\\n•••\\n••••\\n•••\\n••\\n•••\\n•\\n•\\n••\\n•\\n•• ••••\\n••\\n•••\\n•••\\n•\\n••\\n••\\n•••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n•\\n••\\nFIGURE 2.10. Least squares ﬁtting of a function of two inputs. The parameters\\noffθ(x)are chosen so as to minimize the sum-of-squared vertical erro rs.\\nprinciple for estimation is maximum likelihood estimation . Suppose we have\\na random sample yi, i= 1,... ,N from a density Pr θ(y) indexed by some\\nparameters θ. The log-probability of the observed sample is\\nL(θ) =N∑\\ni=1log Pr θ(yi). (2.33)\\nThe principle of maximum likelihood assumes that the most reasonable\\nvalues for θare those for which the probability of the observed sample is\\nlargest. Least squares for the additive error model Y=fθ(X) +ε, with\\nε∼N(0,σ2), is equivalent to maximum likelihood using the conditional\\nlikelihood\\nPr(Y|X,θ) =N(fθ(X),σ2). (2.34)\\nSo although the additional assumption of normality seems more restrictive,\\nthe results are the same. The log-likelihood of the data is\\nL(θ) =−N\\n2log(2π)−Nlogσ−1\\n2σ2N∑\\ni=1(yi−fθ(xi))2, (2.35)\\nand the only term involving θis the last, which is RSS( θ) up to a scalar\\nnegative multiplier.\\nA more interesting example is the multinomial likelihood for the regres-\\nsion function Pr( G|X) for a qualitative output G. Suppose we have a model\\nPr(G=Gk|X=x) =pk,θ(x), k= 1,... ,K for the conditional probabil-\\nity of each class given X, indexed by the parameter vector θ. Then the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1d376261-b68b-46db-be19-55c6d1157beb', embedding=None, metadata={'page_label': '51', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='32 2. Overview of Supervised Learning\\nlog-likelihood (also referred to as the cross-entropy) is\\nL(θ) =N∑\\ni=1logpgi,θ(xi), (2.36)\\nand when maximized it delivers values of θthat best conform with the data\\nin this likelihood sense.\\n2.7 Structured Regression Models\\nWe have seen that although nearest-neighbor and other local methods focus\\ndirectly on estimating the function at a point, they face problems in high\\ndimensions. They may also be inappropriate even in low dimensions in\\ncases where more structured approaches can make more eﬃcient use of the\\ndata. This section introduces classes of such structured approaches. Before\\nwe proceed, though, we discuss further the need for such classes.\\n2.7.1 Diﬃculty of the Problem\\nConsider the RSS criterion for an arbitrary function f,\\nRSS(f) =N∑\\ni=1(yi−f(xi))2. (2.37)\\nMinimizing (2.37) leads to inﬁnitely many solutions: any function ˆfpassing\\nthrough the training points ( xi,yi) is a solution. Any particular solution\\nchosen might be a poor predictor at test points diﬀerent from the training\\npoints. If there are multiple observation pairs xi,yiℓ, ℓ= 1,... ,N iat each\\nvalue of xi, the risk is limited. In this case, the solutions pass through\\nthe average values of the yiℓat each xi; see Exercise 2.6. The situation is\\nsimilar to the one we have already visited in Section 2.4; indeed, (2.37) is\\nthe ﬁnite sample version of (2.11) on page 18. If the sample size Nwere\\nsuﬃciently large such that repeats were guaranteed and densely arranged,\\nit would seem that these solutions might all tend to the limiting conditional\\nexpectation.\\nIn order to obtain useful results for ﬁnite N, we must restrict the eligible\\nsolutions to (2.37) to a smaller set of functions. How to decide on the\\nnature of the restrictions is based on considerations outside of the data.\\nThese restrictions are sometimes encoded via the parametric representation\\noffθ, or may be built into the learning method itself, either implicitly or\\nexplicitly. These restricted classes of solutions are the major topic of this\\nbook. One thing should be clear, though. Any restrictions imposed on f\\nthat lead to a unique solution to (2.37) do not really remove the ambiguity', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ff9f31e9-e7a6-4aa2-8290-e7310350cbfd', embedding=None, metadata={'page_label': '52', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.8 Classes of Restricted Estimators 33\\ncaused by the multiplicity of solutions. There are inﬁnitely many possible\\nrestrictions, each leading to a unique solution, so the ambiguity has simply\\nbeen transferred to the choice of constraint.\\nIn general the constraints imposed by most learning methods can be\\ndescribed as complexity restrictions of one kind or another. This usually\\nmeans some kind of regular behavior in small neighborhoods of the input\\nspace. That is, for all input points xsuﬃciently close to each other in\\nsome metric, ˆfexhibits some special structure such as nearly constant,\\nlinear or low-order polynomial behavior. The estimator is then obtained by\\naveraging or polynomial ﬁtting in that neighborhood.\\nThe strength of the constraint is dictated by the neighborhood size. The\\nlarger the size of the neighborhood, the stronger the constraint, and the\\nmore sensitive the solution is to the particular choice of constraint. For\\nexample, local constant ﬁts in inﬁnitesimally small neighborhoods is no\\nconstraint at all; local linear ﬁts in very large neighborhoods is almost a\\nglobally linear model, and is very restrictive.\\nThe nature of the constraint depends on the metric used. Some methods,\\nsuch as kernel and local regression and tree-based methods, directly specify\\nthe metric and size of the neighborhood. The nearest-neighbor methods\\ndiscussed so far are based on the assumption that locally the function is\\nconstant; close to a target input x0, the function does not change much, and\\nso close outputs can be averaged to produce ˆf(x0). Other methods such\\nas splines, neural networks and basis-function methods implicitly deﬁne\\nneighborhoods of local behavior. In Section 5.4.1 we discuss the concept\\nof anequivalent kernel (see Figure 5.8 on page 157), which describes this\\nlocal dependence for any method linear in the outputs. These equivalent\\nkernels in many cases look just like the explicitly deﬁned weighting kernels\\ndiscussed above—peaked at the target point and falling away smoothly\\naway from it.\\nOne fact should be clear by now. Any method that attempts to pro-\\nduce locally varying functions in small isotropic neighborhoods will run\\ninto problems in high dimensions—again the curse of dimensionality. And\\nconversely, all methods that overcome the dimensionality problems have an\\nassociated—and often implicit or adaptive—metric for measuring neighbor-\\nhoods, which basically does not allow the neighborhood to be simultane-\\nously small in all directions.\\n2.8 Classes of Restricted Estimators\\nThe variety of nonparametric regression techniques or learning methods fall\\ninto a number of diﬀerent classes depending on the nature of the restrictions\\nimposed. These classes are not distinct, and indeed some methods fall in\\nseveral classes. Here we give a brief summary, since detailed descriptions', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='11ff93c1-cfdf-4d2a-bb8f-405b83a21f57', embedding=None, metadata={'page_label': '53', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='34 2. Overview of Supervised Learning\\nare given in later chapters. Each of the classes has associated with it one\\nor more parameters, sometimes appropriately called smoothing parameters,\\nthat control the eﬀective size of the local neighborhood. Here we describe\\nthree broad classes.\\n2.8.1 Roughness Penalty and Bayesian Methods\\nHere the class of functions is controlled by explicitly penalizing RSS( f)\\nwith a roughness penalty\\nPRSS( f;λ) = RSS( f) +λJ(f). (2.38)\\nThe user-selected functional J(f) will be large for functions fthat vary too\\nrapidly over small regions of input space. For example, the popular cubic\\nsmoothing spline for one-dimensional inputs is the solution to the penalized\\nleast-squares criterion\\nPRSS( f;λ) =N∑\\ni=1(yi−f(xi))2+λ∫\\n[f′′(x)]2dx. (2.39)\\nThe roughness penalty here controls large values of the second derivative\\noff, and the amount of penalty is dictated by λ≥0. For λ= 0 no penalty\\nis imposed, and any interpolating function will do, while for λ=∞only\\nfunctions linear in xare permitted.\\nPenalty functionals Jcan be constructed for functions in any dimension,\\nand special versions can be created to impose special structure. For ex-\\nample, additive penalties J(f) =∑p\\nj=1J(fj) are used in conjunction with\\nadditive functions f(X) =∑p\\nj=1fj(Xj) to create additive models with\\nsmooth coordinate functions. Similarly, projection pursuit regression mod-\\nels have f(X) =∑M\\nm=1gm(αT\\nmX) for adaptively chosen directions αm, and\\nthe functions gmcan each have an associated roughness penalty.\\nPenalty function, or regularization methods, express our prior belief that\\nthe type of functions we seek exhibit a certain type of smooth behavior, and\\nindeed can usually be cast in a Bayesian framework. The penalty Jcorre-\\nsponds to a log-prior, and PRSS( f;λ) the log-posterior distribution, and\\nminimizing PRSS( f;λ) amounts to ﬁnding the posterior mode. We discuss\\nroughness-penalty approaches in Chapter 5 and the Bayesian paradigm in\\nChapter 8.\\n2.8.2 Kernel Methods and Local Regression\\nThese methods can be thought of as explicitly providing estimates of the re-\\ngression function or conditional expectation by specifying the nature of the\\nlocal neighborhood, and of the class of regular functions ﬁtted locally. The\\nlocal neighborhood is speciﬁed by a kernel function Kλ(x0,x) which assigns', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d4402cde-56d7-4433-a946-1dcaadafeb91', embedding=None, metadata={'page_label': '54', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.8 Classes of Restricted Estimators 35\\nweights to points xin a region around x0(see Figure 6.1 on page 192). For\\nexample, the Gaussian kernel has a weight function based on the Gaussian\\ndensity function\\nKλ(x0,x) =1\\nλexp[\\n−||x−x0||2\\n2λ]\\n(2.40)\\nand assigns weights to points that die exponentially with their squared\\nEuclidean distance from x0. The parameter λcorresponds to the variance\\nof the Gaussian density, and controls the width of the neighborhood. The\\nsimplest form of kernel estimate is the Nadaraya–Watson weighted averag e\\nˆf(x0) =∑N\\ni=1Kλ(x0,xi)yi∑N\\ni=1Kλ(x0,xi). (2.41)\\nIn general we can deﬁne a local regression estimate of f(x0) asfˆθ(x0),\\nwhere ˆθminimizes\\nRSS(fθ,x0) =N∑\\ni=1Kλ(x0,xi)(yi−fθ(xi))2, (2.42)\\nandfθis some parameterized function, such as a low-order polynomial.\\nSome examples are:\\n•fθ(x) =θ0, the constant function; this results in the Nadaraya–\\nWatson estimate in (2.41) above.\\n•fθ(x) =θ0+θ1xgives the popular local linear regression model.\\nNearest-neighbor methods can be thought of as kernel methods having a\\nmore data-dependent metric. Indeed, the metric for k-nearest neighbors is\\nKk(x,x0) =I(||x−x0|| ≤ ||x(k)−x0||),\\nwhere x(k)is the training observation ranked kth in distance from x0, and\\nI(S) is the indicator of the set S.\\nThese methods of course need to be modiﬁed in high dimensions, to avoid\\nthe curse of dimensionality. Various adaptations are discussed in Chapter 6.\\n2.8.3 Basis Functions and Dictionary Methods\\nThis class of methods includes the familiar linear and polynomial expan-\\nsions, but more importantly a wide variety of more ﬂexible models. The\\nmodel for fis a linear expansion of basis functions\\nfθ(x) =M∑\\nm=1θmhm(x), (2.43)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0156c830-22b1-43bb-ab54-393abcfb5e7b', embedding=None, metadata={'page_label': '55', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='36 2. Overview of Supervised Learning\\nwhere each of the hmis a function of the input x, and the term linear here\\nrefers to the action of the parameters θ. This class covers a wide variety of\\nmethods. In some cases the sequence of basis functions is prescribed, such\\nas a basis for polynomials in xof total degree M.\\nFor one-dimensional x, polynomial splines of degree Kcan be represented\\nby an appropriate sequence of Mspline basis functions, determined in turn\\nbyM−Kknots. These produce functions that are piecewise polynomials\\nof degree Kbetween the knots, and joined up with continuity of degree\\nK−1 at the knots. As an example consider linear splines, or piecewise\\nlinear functions. One intuitively satisfying basis consists of the functions\\nb1(x) = 1, b2(x) =x, and bm+2(x) = ( x−tm)+,m= 1,... ,M −2,\\nwhere tmis the mth knot, and z+denotes positive part. Tensor products\\nof spline bases can be used for inputs with dimensions larger than one\\n(see Section 5.2, and the CART and MARS models in Chapter 9.) The\\nparameter θcan be the total degree of the polynomial or the number of\\nknots in the case of splines.\\nRadial basis functions are symmetric p-dimensional kernels located at\\nparticular centroids,\\nfθ(x) =M∑\\nm=1Kλm(θm,x)θm; (2.44)\\nfor example, the Gaussian kernel Kλ(θ,x) =e−||x−θ||2/2λis popular.\\nRadial basis functions have centroids θmand scales λmthat have to\\nbe determined. The spline basis functions have knots. In general we would\\nlike the data to dictate them as well. Including these as parameters changes\\nthe regression problem from a straightforward linear problem to a combi-\\nnatorially hard nonlinear problem. In practice, shortcuts such as greedy\\nalgorithms or two stage processes are used. Section 6.7 describes some such\\napproaches.\\nA single-layer feed-forward neural network model with linear output\\nweights can be thought of as an adaptive basis function method. The model\\nhas the form\\nfθ(x) =M∑\\nm=1βmσ(αT\\nmx+bm), (2.45)\\nwhere σ(x) = 1 /(1 +e−x) is known as the activation function. Here, as\\nin the projection pursuit model, the directions αmand the biasterms bm\\nhave to be determined, and their estimation is the meat of the computation.\\nDetails are give in Chapter 11.\\nThese adaptively chosen basis function methods are also known as dictio-\\nnarymethods, where one has available a possibly inﬁnite set or dictionary\\nDof candidate basis functions from which to choose, and models are built\\nup by employing some kind of search mechanism.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6154514c-7105-41e1-b36c-907791dcd7fa', embedding=None, metadata={'page_label': '56', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.9 Model Selection and the Bias–Variance Tradeoﬀ 37\\n2.9 Model Selection and the Bias–Variance\\nTradeoﬀ\\nAll the models described above and many others discussed in later chapters\\nhave a smoothing orcomplexity parameter that has to be determined:\\n•the multiplier of the penalty term;\\n•the width of the kernel;\\n•or the number of basis functions.\\nIn the case of the smoothing spline, the parameter λindexes models ranging\\nfrom a straight line ﬁt to the interpolating model. Similarly a local degr ee-\\nmpolynomial model ranges between a degree- mglobal polynomial when\\nthe window size is inﬁnitely large, to an interpolating ﬁt when the window\\nsize shrinks to zero. This means that we cannot use residual sum-of-squares\\non the training data to determine these parameters as well, since we would\\nalways pick those that gave interpolating ﬁts and hence zero residuals. Such\\na model is unlikely to predict future data well at all.\\nThek-nearest-neighbor regression ﬁt ˆfk(x0) usefully illustrates the com-\\npeting forces that eﬀect the predictive ability of such approximations. Sup-\\npose the data arise from a model Y=f(X) +ε, with E( ε) = 0 and\\nVar(ε) =σ2. For simplicity here we assume that the values of xiin the\\nsample are ﬁxed in advance (nonrandom). The expected prediction error\\natx0, also known as testorgeneralization error, can be decomposed:\\nEPE k(x0) = E[( Y−ˆfk(x0))2|X=x0]\\n=σ2+ [Bias2(ˆfk(x0)) + Var T(ˆfk(x0))] (2.46)\\n=σ2+[\\nf(x0)−1\\nkk∑\\nℓ=1f(x(ℓ))]2\\n+σ2\\nk. (2.47)\\nThe subscripts in parentheses ( ℓ) indicate the sequence of nearest neighbors\\ntox0.\\nThere are three terms in this expression. The ﬁrst term σ2is the ir-\\nreducible error—the variance of the new test target—and is beyond our\\ncontrol, even if we know the true f(x0).\\nThe second and third terms are under our control, and make up the\\nmean squared error ofˆfk(x0) in estimating f(x0), which is broken down\\ninto a bias component and a variance component. The bias term is the\\nsquared diﬀerence between the true mean f(x0) and the expected value of\\nthe estimate—[E T(ˆfk(x0))−f(x0)]2—where the expectation averages the\\nrandomness in the training data. This term will most likely increase with\\nk, if the true function is reasonably smooth. For small kthe few closest\\nneighbors will have values f(x(ℓ)) close to f(x0), so their average should', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1a4b7df7-15fd-4055-bc4f-2aba092a2914', embedding=None, metadata={'page_label': '57', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='38 2. Overview of Supervised Learning\\nHigh Bias\\nLow VarianceLow Bias\\nHigh VariancePrediction Error\\nModel ComplexityTraining SampleTest Sample\\nLow High\\nFIGURE 2.11. Test and training error as a function of model complexity.\\nbe close to f(x0). As kgrows, the neighbors are further away, and then\\nanything can happen.\\nThe variance term is simply the variance of an average here, and de-\\ncreases as the inverse of k. So as kvaries, there is a bias–variance tradeoﬀ.\\nMore generally, as the model complexity of our procedure is increased,\\nthe variance tends to increase and the squared bias tends to decreases.\\nThe opposite behavior occurs as the model complexity is decreased. For\\nk-nearest neighbors, the model complexity is controlled by k.\\nTypically we would like to choose our model complexity to trade bias\\noﬀ with variance in such a way as to minimize the test error. An obvious\\nestimate of test error is the training error1\\nN∑\\ni(yi−ˆyi)2. Unfortunately\\ntraining error is not a good estimate of test error, as it does not properly\\naccount for model complexity.\\nFigure 2.11 shows the typical behavior of the test and training error, as\\nmodel complexity is varied. The training error tends to decrease whenever\\nwe increase the model complexity, that is, whenever we ﬁt the data harder.\\nHowever with too much ﬁtting, the model adapts itself too closely to the\\ntraining data, and will not generalize well (i.e., have large test error). In\\nthat case the predictions ˆf(x0) will have large variance, as reﬂected in the\\nlast term of expression (2.46). In contrast, if the model is not complex\\nenough, it will underﬁt and may have large bias, again resulting in poor\\ngeneralization. In Chapter 7 we discuss methods for estimating the test\\nerror of a prediction method, and hence estimating the optimal amount of\\nmodel complexity for a given prediction method and training set.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9203a56a-b9ce-415b-b6cd-0e2b1433285d', embedding=None, metadata={'page_label': '58', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 39\\nBibliographic Notes\\nSome good general books on the learning problem are Duda et al. (2000),\\nBishop (1995),(Bishop, 2006), Ripley (1996), Cherkassky and Mulier (2 007)\\nand Vapnik (1996). Parts of this chapter are based on Friedman (1994b).\\nExercises\\nEx. 2.1 Suppose each of K-classes has an associated target tk, which is a\\nvector of all zeros, except a one in the kth position. Show that classifying to\\nthe largest element of ˆ yamounts to choosing the closest target, min k||tk−\\nˆy||, if the elements of ˆ ysum to one.\\nEx. 2.2 Show how to compute the Bayes decision boundary for the simula-\\ntion example in Figure 2.5.\\nEx. 2.3 Derive equation (2.24).\\nEx. 2.4 The edge eﬀect problem discussed on page 23 is not peculiar to\\nuniform sampling from bounded domains. Consider inputs drawn from a\\nspherical multinormal distribution X∼N(0,Ip). The squared distance\\nfrom any sample point to the origin has a χ2\\npdistribution with mean p.\\nConsider a prediction point x0drawn from this distribution, and let a=\\nx0/||x0||be an associated unit vector. Let zi=aTxibe the projection of\\neach of the training points on this direction.\\nShow that the ziare distributed N(0,1) with expected squared distance\\nfrom the origin 1, while the target point has expected squared distance p\\nfrom the origin.\\nHence for p= 10, a randomly drawn test point is about 3 .1 standard\\ndeviations from the origin, while all the training points are on average\\none standard deviation along direction a. So most prediction points see\\nthemselves as lying on the edge of the training set.\\nEx. 2.5\\n(a) Derive equation (2.27). The last line makes use of (3.8) through a\\nconditioning argument.\\n(b) Derive equation (2.28), making use of the cyclic property of the trace\\noperator [trace( AB) = trace( BA)], and its linearity (which allows us\\nto interchange the order of trace and expectation).\\nEx. 2.6 Consider a regression problem with inputs xiand outputs yi, and a\\nparameterized model fθ(x) to be ﬁt by least squares. Show that if there are\\nobservations with tiedoridentical values of x, then the ﬁt can be obtained\\nfrom a reduced weighted least squares problem.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='296051ba-6239-4785-b291-cabf1055b3e0', embedding=None, metadata={'page_label': '59', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='40 2. Overview of Supervised Learning\\nEx. 2.7 Suppose we have a sample of Npairs xi,yidrawn i.i.d. from the\\ndistribution characterized as follows:\\nxi∼h(x),the design density\\nyi=f(xi) +εi, fis the regression function\\nεi∼(0,σ2) (mean zero, variance σ2)\\nWe construct an estimator for flinear in the yi,\\nˆf(x0) =N∑\\ni=1ℓi(x0;X)yi,\\nwhere the weights ℓi(x0;X) do not depend on the yi, but do depend on the\\nentire training sequence of xi, denoted here by X.\\n(a) Show that linear regression and k-nearest-neighbor regression are mem-\\nbers of this class of estimators. Describe explicitly the weights ℓi(x0;X)\\nin each of these cases.\\n(b) Decompose the conditional mean-squared error\\nEY|X(f(x0)−ˆf(x0))2\\ninto a conditional squared bias and a conditional variance component.\\nLikeX,Yrepresents the entire training sequence of yi.\\n(c) Decompose the (unconditional) mean-squared error\\nEY,X(f(x0)−ˆf(x0))2\\ninto a squared bias and a variance component.\\n(d) Establish a relationship between the squared biases and variances in\\nthe above two cases.\\nEx. 2.8 Compare the classiﬁcation performance of linear regression and k–\\nnearest neighbor classiﬁcation on the zipcode data. In particular, consider\\nonly the 2’s and3’s, and k= 1,3,5,7 and 15. Show both the training and\\ntest error for each choice. The zipcode data are available from the book\\nwebsitewww-stat.stanford.edu/ElemStatLearn .\\nEx. 2.9 Consider a linear regression model with pparameters, ﬁt by least\\nsquares to a set of training data ( x1,y1),... ,(xN,yN) drawn at random\\nfrom a population. Let ˆβbe the least squares estimate. Suppose we have\\nsome test data (˜ x1,˜y1),... ,(˜xM,˜yM) drawn at random from the same pop-\\nulation as the training data. If Rtr(β) =1\\nN∑N\\n1(yi−βTxi)2andRte(β) =\\n1\\nM∑M\\n1(˜yi−βT˜xi)2, prove that\\nE[Rtr(ˆβ)]≤E[Rte(ˆβ)],', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6cf3cf09-a520-46f1-9a6f-dcb3b4e6042f', embedding=None, metadata={'page_label': '60', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 41\\nwhere the expectations are over all that is random in each expression. [This\\nexercise was brought to our attention by Ryan Tibshirani, from a homework\\nassignment given by Andrew Ng.]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e37e0f2-1c86-4bbe-96a2-cf8e4a28a35c', embedding=None, metadata={'page_label': '61', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='42 2. Overview of Supervised Learning', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b49227e-cd72-4f37-8bcd-a2a4a8c4572c', embedding=None, metadata={'page_label': '62', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 43\\nPrinter: Opaque this\\n3\\nLinear Methods for Regression\\n3.1 Introduction\\nA linear regression model assumes that the regression function E( Y|X) is\\nlinear in the inputs X1,... ,X p. Linear models were largely developed in\\nthe precomputer age of statistics, but even in today’s computer era there\\nare still good reasons to study and use them. They are simple and often\\nprovide an adequate and interpretable description of how the inputs aﬀect\\nthe output. For prediction purposes they can sometimes outperform fancier\\nnonlinear models, especially in situations with small numbers of training\\ncases, low signal-to-noise ratio or sparse data. Finally, linear methods can be\\napplied to transformations of the inputs and this considerably expands their\\nscope. These generalizations are sometimes called basis-function methods,\\nand are discussed in Chapter 5.\\nIn this chapter we describe linear methods for regression, while in the\\nnext chapter we discuss linear methods for classiﬁcation. On some topics we\\ngo into considerable detail, as it is our ﬁrm belief that an understanding\\nof linear methods is essential for understanding nonlinear ones. In fact,\\nmany nonlinear techniques are direct generalizations of the linear methods\\ndiscussed here.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4995dd3b-2e91-4c25-8954-69e99168e89a', embedding=None, metadata={'page_label': '63', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='44 3. Linear Methods for Regression\\n3.2 Linear Regression Models and Least Squares\\nAs introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\\nand want to predict a real-valued output Y. The linear regression model\\nhas the form\\nf(X) =β0+p∑\\nj=1Xjβj. (3.1)\\nThe linear model either assumes that the regression function E( Y|X) is\\nlinear, or that the linear model is a reasonable approximation. Here the\\nβj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\\nfrom diﬀerent sources:\\n•quantitative inputs;\\n•transformations of quantitative inputs, such as log, square-root or\\nsquare;\\n•basis expansions, such as X2=X2\\n1,X3=X3\\n1, leading to a polynomial\\nrepresentation;\\n•numeric or “dummy” coding of the levels of qualitative inputs. For\\nexample, if Gis a ﬁve-level factor input, we might create Xj, j=\\n1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\\nsents the eﬀect of Gby a set of level-dependent constants, since in∑5\\nj=1Xjβj, one of the Xjs is one, and the others are zero.\\n•interactions between variables, for example, X3=X1≤X2.\\nNo matter the source of the Xj, the model is linear in the parameters.\\nTypically we have a set of training data ( x1,y1)...(xN,yN) from which\\nto estimate the parameters β. Each xi= (xi1,xi2,... ,x ip)Tis a vector\\nof feature measurements for the ith case. The most popular estimation\\nmethod is least squares , in which we pick the coeﬃcients β= (β0,β1,... ,β p)T\\nto minimize the residual sum of squares\\nRSS(β) =N∑\\ni=1(yi−f(xi))2\\n=N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n. (3.2)\\nFrom a statistical point of view, this criterion is reasonable if the tr aining\\nobservations ( xi,yi) represent independent random draws from their popu-\\nlation. Even if the xi’s were not drawn randomly, the criterion is still valid\\nif the yi’s are conditionally independent given the inputs xi. Figure 3.1\\nillustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d04f985b-aa2e-4fdd-8b88-9fc215cc9e38', embedding=None, metadata={'page_label': '64', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 Linear Regression Models and Least Squares 45\\n•• •\\n••••\\n••••\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•••\\n••••••\\n••••\\n•••\\n••\\n••\\n•\\n••\\n••\\n••••\\n••\\n••\\n•\\n•••\\n•\\n••••\\n••••\\n••\\n••\\nX1X2Y\\nFIGURE 3.1. Linear least squares ﬁtting with X∈I R2. We seek the linear\\nfunction of Xthat minimizes the sum of squared residuals from Y.\\nspace occupied by the pairs ( X,Y). Note that (3.2) makes no assumptions\\nabout the validity of model (3.1); it simply ﬁnds the best linear ﬁt to the\\ndata. Least squares ﬁtting is intuitively satisfying no matter how the data\\narise; the criterion measures the average lack of ﬁt.\\nHow do we minimize (3.2)? Denote by XtheN×(p+ 1) matrix with\\neach row an input vector (with a 1 in the ﬁrst position), and similarly let\\nybe the N-vector of outputs in the training set. Then we can write the\\nresidual sum-of-squares as\\nRSS(β) = (y−Xβ)T(y−Xβ). (3.3)\\nThis is a quadratic function in the p+ 1 parameters. Diﬀerentiating with\\nrespect to βwe obtain\\n∂RSS\\n∂β=−2XT(y−Xβ)\\n∂2RSS\\n∂β∂βT= 2XTX.(3.4)\\nAssuming (for the moment) that Xhas full column rank, and hence XTX\\nis positive deﬁnite, we set the ﬁrst derivative to zero\\nXT(y−Xβ) = 0 (3.5)\\nto obtain the unique solution\\nˆβ= (XTX)−1XTy. (3.6)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cc89c5cc-8677-4a18-8694-204ad21a6133', embedding=None, metadata={'page_label': '65', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='46 3. Linear Methods for Regression\\nx1x2y\\nˆ y\\nFIGURE 3.2. TheN-dimensional geometry of least squares regression with two\\npredictors. The outcome vector yis orthogonally projected onto the hyperplane\\nspanned by the input vectors x1andx2. The projection ˆyrepresents the vector\\nof the least squares predictions\\nThe predicted values at an input vector x0are given by ˆf(x0) = (1 : x0)Tˆβ;\\nthe ﬁtted values at the training inputs are\\nˆy=Xˆβ=X(XTX)−1XTy, (3.7)\\nwhere ˆ yi=ˆf(xi). The matrix H=X(XTX)−1XTappearing in equation\\n(3.7) is sometimes called the “hat” matrix because it puts the hat on y.\\nFigure 3.2 shows a diﬀerent geometrical representation of the least squares\\nestimate, this time in IRN. We denote the column vectors of Xbyx0,x1,... ,xp,\\nwithx0≡1. For much of what follows, this ﬁrst column is treated like any\\nother. These vectors span a subspace of IRN, also referred to as the column\\nspace of X. We minimize RSS( β) =∥y−Xβ∥2by choosing ˆβso that the\\nresidual vector y−ˆyis orthogonal to this subspace. This orthogonality is\\nexpressed in (3.5), and the resulting estimate ˆyis hence the orthogonal pro-\\njection ofyonto this subspace. The hat matrix Hcomputes the orthogonal\\nprojection, and hence it is also known as a projection matrix.\\nIt might happen that the columns of Xare not linearly independent, so\\nthatXis not of full rank. This would occur, for example, if two of the\\ninputs were perfectly correlated, (e.g., x2= 3x1). Then XTXis singular\\nand the least squares coeﬃcients ˆβare not uniquely deﬁned. However,\\nthe ﬁtted values ˆy=Xˆβare still the projection of yonto the column\\nspace of X; there is just more than one way to express that projection\\nin terms of the column vectors of X. The non-full-rank case occurs most\\noften when one or more qualitative inputs are coded in a redundant fashion.\\nThere is usually a natural way to resolve the non-unique representation,\\nby recoding and/or dropping redundant columns in X. Most regression\\nsoftware packages detect these redundancies and automatically implement', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91f3ce14-eea9-4930-8202-06235e5368fb', embedding=None, metadata={'page_label': '66', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 Linear Regression Models and Least Squares 47\\nsome strategy for removing them. Rank deﬁciencies can also occur in signal\\nand image analysis, where the number of inputs pcan exceed the number\\nof training cases N. In this case, the features are typically reduced by\\nﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and\\nChapter 18).\\nUp to now we have made minimal assumptions about the true distribu-\\ntion of the data. In order to pin down the sampling properties of ˆβ, we now\\nassume that the observations yiare uncorrelated and have constant vari-\\nanceσ2, and that the xiare ﬁxed (non random). The variance–covariance\\nmatrix of the least squares parameter estimates is easily derived from (3.6 )\\nand is given by\\nVar(ˆβ) = (XTX)−1σ2. (3.8)\\nTypically one estimates the variance σ2by\\nˆσ2=1\\nN−p−1N∑\\ni=1(yi−ˆyi)2.\\nTheN−p−1 rather than Nin the denominator makes ˆ σ2an unbiased\\nestimate of σ2: E(ˆσ2) =σ2.\\nTo draw inferences about the parameters and the model, additional as-\\nsumptions are needed. We now assume that (3.1) is the correct model for\\nthe mean; that is, the conditional expectation of Yis linear in X1,... ,X p.\\nWe also assume that the deviations of Yaround its expectation are additive\\nand Gaussian. Hence\\nY= E( Y|X1,... ,X p) +ε\\n=β0+p∑\\nj=1Xjβj+ε, (3.9)\\nwhere the error εis a Gaussian random variable with expectation zero and\\nvariance σ2, written ε∼N(0,σ2).\\nUnder (3.9), it is easy to show that\\nˆβ∼N(β,(XTX)−1σ2). (3.10)\\nThis is a multivariate normal distribution with mean vector and variance–\\ncovariance matrix as shown. Also\\n(N−p−1)ˆσ2∼σ2χ2\\nN−p−1, (3.11)\\na chi-squared distribution with N−p−1 degrees of freedom. In addition ˆβ\\nand ˆσ2are statistically independent. We use these distributional properties\\nto form tests of hypothesis and conﬁdence intervals for the parameters βj.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='910a5d4d-eeb2-4f44-9856-e68b881041d3', embedding=None, metadata={'page_label': '67', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='48 3. Linear Methods for Regression\\nZTail Probabilities\\n2.0 2.2 2.4 2.6 2.8 3.00.01 0.02 0.03 0.04 0.05 0.06t30\\nt100\\nnormal\\nFIGURE 3.3. The tail probabilities Pr(|Z|> z)for three distributions, t30,t100\\nand standard normal. Shown are the appropriate quantiles for test ing signiﬁcance\\nat the p= 0.05and0.01levels. The diﬀerence between tand the standard normal\\nbecomes negligible for Nbigger than about 100.\\nTo test the hypothesis that a particular coeﬃcient βj= 0, we form the\\nstandardized coeﬃcient or Z-score\\nzj=ˆβj\\nˆσ√vj, (3.12)\\nwhere vjis the jth diagonal element of ( XTX)−1. Under the null hypothesis\\nthatβj= 0,zjis distributed as tN−p−1(atdistribution with N−p−1\\ndegrees of freedom), and hence a large (absolute) value of zjwill lead to\\nrejection of this null hypothesis. If ˆ σis replaced by a known value σ, then\\nzjwould have a standard normal distribution. The diﬀerence between the\\ntail quantiles of a t-distribution and a standard normal become negligible\\nas the sample size increases, and so we typically use the normal quantiles\\n(see Figure 3.3).\\nOften we need to test for the signiﬁcance of groups of coeﬃcients simul-\\ntaneously. For example, to test if a categorical variable with klevels can\\nbe excluded from a model, we need to test whether the coeﬃcients of the\\ndummy variables used to represent the levels can all be set to zero. Here\\nwe use the Fstatistic,\\nF=(RSS 0−RSS1)/(p1−p0)\\nRSS1/(N−p1−1), (3.13)\\nwhere RSS 1is the residual sum-of-squares for the least squares ﬁt of the big-\\nger model with p1+1 parameters, and RSS 0the same for the nested smaller\\nmodel with p0+1 parameters, having p1−p0parameters constrained to be', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a39f9a81-1d0b-47b5-ac8e-ee5126444928', embedding=None, metadata={'page_label': '68', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 Linear Regression Models and Least Squares 49\\nzero. The Fstatistic measures the change in residual sum-of-squares per\\nadditional parameter in the bigger model, and it is normalized by an esti-\\nmate of σ2. Under the Gaussian assumptions, and the null hypothesis that\\nthe smaller model is correct, the Fstatistic will have a Fp1−p0,N−p1−1dis-\\ntribution. It can be shown (Exercise 3.1) that the zjin (3.12) are equivalent\\nto the Fstatistic for dropping the single coeﬃcient βjfrom the model. For\\nlargeN, the quantiles of the Fp1−p0,N−p1−1approach those of the χ2\\np1−p0.\\nSimilarly, we can isolate βjin (3.10) to obtain a 1 −2αconﬁdence interval\\nforβj:\\n(ˆβj−z(1−α)v1\\n2\\njˆσ,ˆβj+z(1−α)v1\\n2\\njˆσ). (3.14)\\nHerez(1−α)is the 1 −αpercentile of the normal distribution:\\nz(1−0.025)= 1.96,\\nz(1−.05)= 1.645,etc.\\nHence the standard practice of reporting ˆβ±2≤se(ˆβ) amounts to an ap-\\nproximate 95% conﬁdence interval. Even if the Gaussian error assumption\\ndoes not hold, this interval will be approximately correct, with its coverage\\napproaching 1 −2αas the sample size N→ ∞.\\nIn a similar fashion we can obtain an approximate conﬁdence set for the\\nentire parameter vector β, namely\\nCβ={β|(ˆβ−β)TXTX(ˆβ−β)≤ˆσ2χ2\\np+1(1−α)}, (3.15)\\nwhere χ2\\nℓ(1−α)is the 1 −αpercentile of the chi-squared distribution on ℓ\\ndegrees of freedom: for example, χ2\\n5(1−0.05)= 11.1,χ2\\n5(1−0.1)= 9.2. This\\nconﬁdence set for βgenerates a corresponding conﬁdence set for the true\\nfunction f(x) =xTβ, namely {xTβ|β∈Cβ}(Exercise 3.2; see also Fig-\\nure 5.4 in Section 5.2.2 for examples of conﬁdence bands for functions).\\n3.2.1 Example: Prostate Cancer\\nThe data for this example come from a study by Stamey et al. (1989). They\\nexamined the correlation between the level of prostate-speciﬁc antigen and\\na number of clinical measures in men who were about to receive a radical\\nprostatectomy. The variables are log cancer volume ( lcavol ), log prostate\\nweight ( lweight ),age, log of the amount of benign prostatic hyperplasia\\n(lbph), seminal vesicle invasion ( svi), log of capsular penetration ( lcp),\\nGleason score ( gleason ), and percent of Gleason scores 4 or 5 ( pgg45).\\nThe correlation matrix of the predictors given in Table 3.1 shows many\\nstrong correlations. Figure 1.1 (page 3) of Chapter 1 is a scatterplot matr ix\\nshowing every pairwise plot between the variables. We see that sviis a\\nbinary variable, and gleason is an ordered categorical variable. We see, for', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f753aea6-a827-4be0-8d85-b0195dd83efe', embedding=None, metadata={'page_label': '69', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='50 3. Linear Methods for Regression\\nTABLE 3.1. Correlations of predictors in the prostate cancer data.\\nlcavol lweight age lbph svi lcp gleason\\nlweight 0.300\\nage 0.286 0.317\\nlbph 0.063 0.437 0.287\\nsvi 0.593 0.181 0.129 −0.139\\nlcp 0.692 0.157 0.173 −0.089 0.671\\ngleason 0.426 0.024 0.366 0.033 0.307 0.476\\npgg45 0.483 0.074 0.276 −0.030 0.481 0.663 0.757\\nTABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the\\ncoeﬃcient divided by its standard error (3.12). Roughly a Zscore larger than two\\nin absolute value is signiﬁcantly nonzero at the p= 0.05level.\\nTerm Coeﬃcient Std. Error ZScore\\nIntercept 2.46 0.09 27.60\\nlcavol 0.68 0.13 5.37\\nlweight 0.26 0.10 2.75\\nage −0.14 0.10 −1.40\\nlbph 0.21 0.10 2.06\\nsvi 0.31 0.12 2.47\\nlcp −0.29 0.15 −1.87\\ngleason −0.02 0.15 −0.15\\npgg45 0.27 0.15 1.74\\nexample, that both lcavol andlcpshow a strong relationship with the\\nresponse lpsa, and with each other. We need to ﬁt the eﬀects jointly to\\nuntangle the relationships between the predictors and the response.\\nWe ﬁt a linear model to the log of prostate-speciﬁc antigen, lpsa, after\\nﬁrst standardizing the predictors to have unit variance. We randomly split\\nthe dataset into a training set of size 67 and a test set of size 30. We ap-\\nplied least squares estimation to the training set, producing the estimates,\\nstandard errors and Z-scores shown in Table 3.2. The Z-scores are deﬁned\\nin (3.12), and measure the eﬀect of dropping that variable from the model.\\nAZ-score greater than 2 in absolute value is approximately signiﬁcant at\\nthe 5% level. (For our example, we have nine parameters, and the 0 .025 tail\\nquantiles of the t67−9distribution are ±2.002!) The predictor lcavol shows\\nthe strongest eﬀect, with lweight andsvialso strong. Notice that lcpis\\nnot signiﬁcant, once lcavol is in the model (when used in a model without\\nlcavol ,lcpis strongly signiﬁcant). We can also test for the exclusion of\\na number of terms at once, using the F-statistic (3.13). For example, we\\nconsider dropping all the non-signiﬁcant terms in Table 3.2, namely age,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4294f6df-f5cf-44eb-8738-c42233377048', embedding=None, metadata={'page_label': '70', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 Linear Regression Models and Least Squares 51\\nlcp,gleason , andpgg45. We get\\nF=(32.81−29.43)/(9−5)\\n29.43/(67−9)= 1.67, (3.16)\\nwhich has a p-value of 0 .17 (Pr( F4,58>1.67) = 0 .17), and hence is not\\nsigniﬁcant.\\nThe mean prediction error on the test data is 0 .521. In contrast, predic-\\ntion using the mean training value of lpsahas a test error of 1 .057, which\\nis called the “base error rate.” Hence the linear model reduces the base\\nerror rate by about 50%. We will return to this example later to compare\\nvarious selection and shrinkage methods.\\n3.2.2 The Gauss–Markov Theorem\\nOne of the most famous results in statistics asserts that the least squares\\nestimates of the parameters βhave the smallest variance among all linear\\nunbiased estimates. We will make this precise here, and also make clear\\nthat the restriction to unbiased estimates is not necessarily a wise one. This\\nobservation will lead us to consider biased estimates such as ridge regression\\nlater in the chapter. We focus on estimation of any linear combination of\\nthe parameters θ=aTβ; for example, predictions f(x0) =xT\\n0βare of this\\nform. The least squares estimate of aTβis\\nˆθ=aTˆβ=aT(XTX)−1XTy. (3.17)\\nConsidering Xto be ﬁxed, this is a linear function cT\\n0yof the response\\nvector y. If we assume that the linear model is correct, aTˆβis unbiased\\nsince\\nE(aTˆβ) = E( aT(XTX)−1XTy)\\n=aT(XTX)−1XTXβ\\n=aTβ. (3.18)\\nThe Gauss–Markov theorem states that if we have any other linear estima-\\ntor˜θ=cTythat is unbiased for aTβ, that is, E( cTy) =aTβ, then\\nVar(aTˆβ)≤Var(cTy). (3.19)\\nThe proof (Exercise 3.3) uses the triangle inequality. For simplicity we hav e\\nstated the result in terms of estimation of a single parameter aTβ, but with\\na few more deﬁnitions one can state it in terms of the entire parameter\\nvector β(Exercise 3.3).\\nConsider the mean squared error of an estimator ˜θin estimating θ:\\nMSE( ˜θ) = E( ˜θ−θ)2\\n= Var( ˜θ) + [E( ˜θ)−θ]2. (3.20)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b8e840d-778b-4ffe-919f-09eb366c6f77', embedding=None, metadata={'page_label': '71', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='52 3. Linear Methods for Regression\\nThe ﬁrst term is the variance, while the second term is the squared bias.\\nThe Gauss-Markov theorem implies that the least squares estimator has the\\nsmallest mean squared error of all linear estimators with no bias. However ,\\nthere may well exist a biased estimator with smaller mean squared error.\\nSuch an estimator would trade a little bias for a larger reduction in varia nce.\\nBiased estimates are commonly used. Any method that shrinks or sets to\\nzero some of the least squares coeﬃcients may result in a biased estimate.\\nWe discuss many examples, including variable subset selection and ridge\\nregression, later in this chapter. From a more pragmatic point of view, m ost\\nmodels are distortions of the truth, and hence are biased; picking the right\\nmodel amounts to creating the right balance between bias and variance.\\nWe go into these issues in more detail in Chapter 7.\\nMean squared error is intimately related to prediction accuracy, as dis-\\ncussed in Chapter 2. Consider the prediction of the new response at input\\nx0,\\nY0=f(x0) +ε0. (3.21)\\nThen the expected prediction error of an estimate ˜f(x0) =xT\\n0˜βis\\nE(Y0−˜f(x0))2=σ2+ E(xT\\n0˜β−f(x0))2\\n=σ2+ MSE( ˜f(x0)). (3.22)\\nTherefore, expected prediction error and mean squared error diﬀer only by\\nthe constant σ2, representing the variance of the new observation y0.\\n3.2.3 Multiple Regression from Simple Univariate Regressi on\\nThe linear model (3.1) with p >1 inputs is called the multiple linear\\nregression model . The least squares estimates (3.6) for this model are best\\nunderstood in terms of the estimates for the univariate (p= 1) linear\\nmodel, as we indicate in this section.\\nSuppose ﬁrst that we have a univariate model with no intercept, that is,\\nY=Xβ+ε. (3.23)\\nThe least squares estimate and residuals are\\nˆβ=∑N\\n1xiyi∑N\\n1x2\\ni,\\nri=yi−xiˆβ.(3.24)\\nIn convenient vector notation, we let y= (y1,... ,y N)T,x= (x1,... ,x N)T\\nand deﬁne\\n⟨x,y⟩=N∑\\ni=1xiyi,\\n=xTy, (3.25)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3765c187-3ff5-4677-a8b2-c0c37ea8b075', embedding=None, metadata={'page_label': '72', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 Linear Regression Models and Least Squares 53\\ntheinner product between xandy1. Then we can write\\nˆβ=⟨x,y⟩\\n⟨x,x⟩,\\nr=y−xˆβ.(3.26)\\nAs we will see, this simple univariate regression provides the building block\\nfor multiple linear regression. Suppose next that the inputs x1,x2,... ,xp\\n(the columns of the data matrix X) are orthogonal; that is ⟨xj,xk⟩= 0\\nfor all j̸=k. Then it is easy to check that the multiple least squares esti-\\nmates ˆβjare equal to ⟨xj,y⟩/⟨xj,xj⟩—the univariate estimates. In other\\nwords, when the inputs are orthogonal, they have no eﬀect on each other’s\\nparameter estimates in the model.\\nOrthogonal inputs occur most often with balanced, designed experiments\\n(where orthogonality is enforced), but almost never with observational\\ndata. Hence we will have to orthogonalize them in order to carry this idea\\nfurther. Suppose next that we have an intercept and a single input x. Then\\nthe least squares coeﬃcient of xhas the form\\nˆβ1=⟨x−¯x1,y⟩\\n⟨x−¯x1,x−¯x1⟩, (3.27)\\nwhere ¯ x=∑\\nixi/N, and1=x0, the vector of Nones. We can view the\\nestimate (3.27) as the result of two applications of the simple regression\\n(3.26). The steps are:\\n1. regress xon1to produce the residual z=x−¯x1;\\n2. regress yon the residual zto give the coeﬃcient ˆβ1.\\nIn this procedure, “regress bona” means a simple univariate regression of b\\nonawith no intercept, producing coeﬃcient ˆ γ=⟨a,b⟩/⟨a,a⟩and residual\\nvector b−ˆγa. We say that bis adjusted for a, or is “orthogonalized” with\\nrespect to a.\\nStep 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple\\nunivariate regression, using the orthogonal predictors 1andz. Figure 3.4\\nshows this process for two general inputs x1andx2. The orthogonalization\\ndoes not change the subspace spanned by x1andx2, it simply produces an\\northogonal basis for representing it.\\nThis recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.\\nNote that the inputs z0,... ,zj−1in step 2 are orthogonal, hence the simple\\nregression coeﬃcients computed there are in fact also the multiple regres-\\nsion coeﬃcients.\\n1The inner-product notation is suggestive of generalizatio ns of linear regression to\\ndiﬀerent metric spaces, as well as to probability spaces.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac404d90-cbb9-44bc-abf5-7d832d488cb0', embedding=None, metadata={'page_label': '73', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='54 3. Linear Methods for Regression\\nx1x2y\\nˆ yz z z z z\\nFIGURE 3.4. Least squares regression by orthogonalization of the inputs. The\\nvector x2is regressed on the vector x1, leaving the residual vector z. The regres-\\nsion of yonzgives the multiple regression coeﬃcient of x2. Adding together the\\nprojections of yon each of x1andzgives the least squares ﬁt ˆy.\\nAlgorithm 3.1 Regression by Successive Orthogonalization.\\n1. Initialize z0=x0=1.\\n2. For j= 1,2,... ,p\\nRegress xjonz0,z1,... ,,zj−1to produce coeﬃcients ˆ γℓj=\\n⟨zℓ,xj⟩/⟨zℓ,zℓ⟩,ℓ= 0,... ,j −1 and residual vector zj=\\nxj−∑j−1\\nk=0ˆγkjzk.\\n3. Regress yon the residual zpto give the estimate ˆβp.\\nThe result of this algorithm is\\nˆβp=⟨zp,y⟩\\n⟨zp,zp⟩. (3.28)\\nRe-arranging the residual in step 2, we can see that each of the xjis a linear\\ncombination of the zk, k≤j. Since the zjare all orthogonal, they form\\na basis for the column space of X, and hence the least squares projection\\nonto this subspace is ˆy. Since zpalone involves xp(with coeﬃcient 1), we\\nsee that the coeﬃcient (3.28) is indeed the multiple regression coeﬃcient of\\nyonxp. This key result exposes the eﬀect of correlated inputs in multiple\\nregression. Note also that by rearranging the xj, any one of them could\\nbe in the last position, and a similar results holds. Hence stated more\\ngenerally, we have shown that the jth multiple regression coeﬃcient is the\\nunivariate regression coeﬃcient of yonxj≤012...(j−1)(j+1)...,p, the residual\\nafter regressing xjonx0,x1,... ,xj−1,xj+1,... ,xp:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6bc7c4c3-7fb9-467d-a67b-61b5606548a9', embedding=None, metadata={'page_label': '74', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 Linear Regression Models and Least Squares 55\\nThe multiple regression coeﬃcient ˆβjrepresents the additional\\ncontribution of xjony, afterxjhas been adjusted for x0,x1,... ,xj−1,\\nxj+1,... ,xp.\\nIfxpis highly correlated with some of the other xk’s, the residual vector\\nzpwill be close to zero, and from (3.28) the coeﬃcient ˆβpwill be very\\nunstable. This will be true for all the variables in the correlated set. In\\nsuch situations, we might have all the Z-scores (as in Table 3.2) be smal l—\\nany one of the set can be deleted—yet we cannot delete them all. From\\n(3.28) we also obtain an alternate formula for the variance estimates ( 3.8),\\nVar(ˆβp) =σ2\\n⟨zp,zp⟩=σ2\\n∥zp∥2. (3.29)\\nIn other words, the precision with which we can estimate ˆβpdepends on\\nthe length of the residual vector zp; this represents how much of xpis\\nunexplained by the other xk’s.\\nAlgorithm 3.1 is known as the Gram–Schmidt procedure for multiple\\nregression, and is also a useful numerical strategy for computing the esti-\\nmates. We can obtain from it not just ˆβp, but also the entire multiple least\\nsquares ﬁt, as shown in Exercise 3.4.\\nWe can represent step 2 of Algorithm 3.1 in matrix form:\\nX=ZΓ, (3.30)\\nwhereZhas as columns the zj(in order), and Γis the upper triangular ma-\\ntrix with entries ˆ γkj. Introducing the diagonal matrix Dwithjth diagonal\\nentry Djj=∥zj∥, we get\\nX=ZD−1DΓ\\n=QR, (3.31)\\nthe so-called QRdecomposition of X. Here Qis anN×(p+1) orthogonal\\nmatrix, QTQ=I, andRis a (p+ 1)×(p+ 1) upper triangular matrix.\\nTheQRdecomposition represents a convenient orthogonal basis for the\\ncolumn space of X. It is easy to see, for example, that the least squares\\nsolution is given by\\nˆβ=R−1QTy, (3.32)\\nˆy=QQTy. (3.33)\\nEquation (3.32) is easy to solve because Ris upper triangular\\n(Exercise 3.4).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ff6ad990-0191-4cb5-bf7e-eb780a5863a9', embedding=None, metadata={'page_label': '75', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='56 3. Linear Methods for Regression\\n3.2.4 Multiple Outputs\\nSuppose we have multiple outputs Y1,Y2,... ,Y Kthat we wish to predict\\nfrom our inputs X0,X1,X2,... ,X p. We assume a linear model for each\\noutput\\nYk=β0k+p∑\\nj=1Xjβjk+εk (3.34)\\n=fk(X) +εk. (3.35)\\nWith Ntraining cases we can write the model in matrix notation\\nY=XB+E. (3.36)\\nHereYis the N×Kresponse matrix, with ikentry yik,Xis the N×(p+1)\\ninput matrix, Bis the ( p+ 1)×Kmatrix of parameters and Eis the\\nN×Kmatrix of errors. A straightforward generalization of the univariat e\\nloss function (3.2) is\\nRSS(B) =K∑\\nk=1N∑\\ni=1(yik−fk(xi))2(3.37)\\n= tr[( Y−XB)T(Y−XB)]. (3.38)\\nThe least squares estimates have exactly the same form as before\\nˆB= (XTX)−1XTY. (3.39)\\nHence the coeﬃcients for the kth outcome are just the least squares es-\\ntimates in the regression of ykonx0,x1,... ,xp. Multiple outputs do not\\naﬀect one another’s least squares estimates.\\nIf the errors ε= (ε1,... ,ε K) in (3.34) are correlated, then it might seem\\nappropriate to modify (3.37) in favor of a multivariate version. Speciﬁca lly,\\nsuppose Cov( ε) =Σ, then the multivariate weighted criterion\\nRSS(B;Σ) =N∑\\ni=1(yi−f(xi))TΣ−1(yi−f(xi)) (3.40)\\narises naturally from multivariate Gaussian theory. Here f(x) is the vector\\nfunction ( f1(x),... ,f K(x)), and yithe vector of Kresponses for observa-\\ntioni. However, it can be shown that again the solution is given by (3.39);\\nKseparate regressions that ignore the correlations (Exercise 3.11). If the Σi\\nvary among observations, then this is no longer the case, and the solution\\nforBno longer decouples.\\nIn Section 3.7 we pursue the multiple outcome problem, and consider\\nsituations where it does pay to combine the regressions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13083e6b-4c9f-45c9-9c60-cc41724134ff', embedding=None, metadata={'page_label': '76', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3 Subset Selection 57\\n3.3 Subset Selection\\nThere are two reasons why we are often not satisﬁed with the least squares\\nestimates (3.6).\\n•The ﬁrst is prediction accuracy : the least squares estimates often have\\nlow bias but large variance. Prediction accuracy can sometimes be\\nimproved by shrinking or setting some coeﬃcients to zero. By doing\\nso we sacriﬁce a little bit of bias to reduce the variance of the predicted\\nvalues, and hence may improve the overall prediction accuracy.\\n•The second reason is interpretation . With a large number of predic-\\ntors, we often would like to determine a smaller subset that exhibit\\nthe strongest eﬀects. In order to get the “big picture,” we are willing\\nto sacriﬁce some of the small details.\\nIn this section we describe a number of approaches to variable subset selec-\\ntion with linear regression. In later sections we discuss shrinkage and hybrid\\napproaches for controlling variance, as well as other dimension-reduction\\nstrategies. These all fall under the general heading model selection . Model\\nselection is not restricted to linear models; Chapter 7 covers this topic in\\nsome detail.\\nWith subset selection we retain only a subset of the variables, and elim-\\ninate the rest from the model. Least squares regression is used to estimate\\nthe coeﬃcients of the inputs that are retained. There are a number of dif-\\nferent strategies for choosing the subset.\\n3.3.1 Best-Subset Selection\\nBest subset regression ﬁnds for each k∈ {0,1,2,... ,p }the subset of size k\\nthat gives smallest residual sum of squares (3.2). An eﬃcient algorithm—\\ntheleaps and bounds procedure (Furnival and Wilson, 1974)—makes this\\nfeasible for pas large as 30 or 40. Figure 3.5 shows all the subset models\\nfor the prostate cancer example. The lower boundary represents the models\\nthat are eligible for selection by the best-subsets approach. Note that the\\nbest subset of size 2, for example, need not include the variable that was\\nin the best subset of size 1 (for this example all the subsets are nested).\\nThe best-subset curve (red lower boundary in Figure 3.5) is necessarily\\ndecreasing, so cannot be used to select the subset size k. The question of\\nhow to choose kinvolves the tradeoﬀ between bias and variance, along with\\nthe more subjective desire for parsimony. There are a number of criteria\\nthat one may use; typically we choose the smallest model that minimizes\\nan estimate of the expected prediction error.\\nMany of the other approaches that we discuss in this chapter are similar,\\nin that they use the training data to produce a sequence of models varying\\nin complexity and indexed by a single parameter. In the next section we use', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2013abef-12a4-4ede-9a93-07dca592fdd4', embedding=None, metadata={'page_label': '77', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='58 3. Linear Methods for Regression\\nSubset Size kResidual Sum−of−Squares\\n0 20 40 60 80 100\\n0 1 2 3 4 5 6 7 8•\\n••••••••\\n••••••••••••••••••••••••••\\n•••••••••••••••••••••••••••••••••••••••••••••••••\\n••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•••••••••••••••••••••••••••••••••••••••••••••\\n••••••••••••••••••••••••••\\n••••••••\\n••\\n•\\n•••••• •\\nFIGURE 3.5. All possible subset models for the prostate cancer example. At\\neach subset size is shown the residual sum-of-squares for ea ch model of that size.\\ncross-validation to estimate prediction error and select k; the AIC criterion\\nis a popular alternative. We defer more detailed discussion of these and\\nother approaches to Chapter 7.\\n3.3.2 Forward- and Backward-Stepwise Selection\\nRather than search through all possible subsets (which becomes infeasible\\nforpmuch larger than 40), we can seek a good path through them. Forward-\\nstepwise selection starts with the intercept, and then sequentially adds into\\nthe model the predictor that most improves the ﬁt. With many candidate\\npredictors, this might seem like a lot of computation; however, clever up-\\ndating algorithms can exploit the QR decomposition for the current ﬁt to\\nrapidly establish the next candidate (Exercise 3.9). Like best-subset re-\\ngression, forward stepwise produces a sequence of models indexed by k, the\\nsubset size, which must be determined.\\nForward-stepwise selection is a greedy algorithm , producing a nested se-\\nquence of models. In this sense it might seem sub-optimal compared to\\nbest-subset selection. However, there are several reasons why it might be\\npreferred:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c7bd0cde-232b-4371-a10d-2a8f709fab86', embedding=None, metadata={'page_label': '78', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3 Subset Selection 59\\n•Computational; for large pwe cannot compute the best subset se-\\nquence, but we can always compute the forward stepwise sequence\\n(even when p≫N).\\n•Statistical; a price is paid in variance for selecting the best subset\\nof each size; forward stepwise is a more constrained search, and will\\nhave lower variance, but perhaps more bias.\\n0 5 10 15 20 25 300.65 0.70 0.75 0.80 0.85 0.90 0.95Best Subset\\nForward Stepwise\\nBackward Stepwise\\nForward StagewiseE||ˆβ(k)−β||2\\nSubset Size k\\nFIGURE 3.6. Comparison of four subset-selection techniques on a simulat ed lin-\\near regression problem Y=XTβ+ε. There are N= 300 observations on p= 31\\nstandard Gaussian variables, with pairwise correlations all equal to 0.85. For10of\\nthe variables, the coeﬃcients are drawn at random from a N(0,0.4)distribution;\\nthe rest are zero. The noise ε∼N(0,6.25), resulting in a signal-to-noise ratio of\\n0.64. Results are averaged over 50simulations. Shown is the mean-squared error\\nof the estimated coeﬃcient ˆβ(k)at each step from the true β.\\nBackward-stepwise selection starts with the full model, and sequentially\\ndeletes the predictor that has the least impact on the ﬁt. The candidate for\\ndropping is the variable with the smallest Z-score (Exercise 3.10). Backw ard\\nselection can only be used when N > p , while forward stepwise can always\\nbe used.\\nFigure 3.6 shows the results of a small simulation study to compare\\nbest-subset regression with the simpler alternatives forward and backward\\nselection. Their performance is very similar, as is often the case. Included in\\nthe ﬁgure is forward stagewise regression (next section), which takes longer\\nto reach minimum error.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9555a751-a702-40d3-9ee6-81c44e7cb094', embedding=None, metadata={'page_label': '79', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='60 3. Linear Methods for Regression\\nOn the prostate cancer example, best-subset, forward and backward se-\\nlection all gave exactly the same sequence of terms.\\nSome software packages implement hybrid stepwise-selection strategies\\nthat consider both forward and backward moves at each step, and select\\nthe “best” of the two. For example in the Rpackage the stepfunction uses\\nthe AIC criterion for weighing the choices, which takes proper account of\\nthe number of parameters ﬁt; at each step an add or drop will be performed\\nthat minimizes the AIC score.\\nOther more traditional packages base the selection on F-statistics, adding\\n“signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out\\nof fashion, since they do not take proper account of the multiple testing\\nissues. It is also tempting after a model search to print out a summary of\\nthe chosen model, such as in Table 3.2; however, the standard errors are\\nnot valid, since they do not account for the search process. The bootstrap\\n(Section 8.2) can be useful in such settings.\\nFinally, we note that often variables come in groups (such as the dummy\\nvariables that code a multi-level categorical predictor). Smart stepwise pro-\\ncedures (such as stepinR) will add or drop whole groups at a time, taking\\nproper account of their degrees-of-freedom.\\n3.3.3 Forward-Stagewise Regression\\nForward-stagewise regression (FS) is even more constrained than forward-\\nstepwise regression. It starts like forward-stepwise regression, wit h an in-\\ntercept equal to ¯ y, and centered predictors with coeﬃcients initially all 0.\\nAt each step the algorithm identiﬁes the variable most correlated with the\\ncurrent residual. It then computes the simple linear regression coeﬃcient\\nof the residual on this chosen variable, and then adds it to the current co-\\neﬃcient for that variable. This is continued till none of the variables have\\ncorrelation with the residuals—i.e. the least-squares ﬁt when N > p .\\nUnlike forward-stepwise regression, none of the other variables are ad-\\njusted when a term is added to the model. As a consequence, forward\\nstagewise can take many more than psteps to reach the least squares ﬁt,\\nand historically has been dismissed as being ineﬃcient. It turns out that\\nthis “slow ﬁtting” can pay dividends in high-dimensional problems. We\\nsee in Section 3.8.1 that both forward stagewise and a variant which is\\nslowed down even further are quite competitive, especially in very high-\\ndimensional problems.\\nForward-stagewise regression is included in Figure 3.6. In this example it\\ntakes over 1000 steps to get all the correlations below 10−4. For subset size\\nk, we plotted the error for the last step for which there where knonzero\\ncoeﬃcients. Although it catches up with the best ﬁt, it takes longer to\\ndo so.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26a030d8-fc16-4803-9754-f07ac0f18ec7', embedding=None, metadata={'page_label': '80', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 61\\n3.3.4 Prostate Cancer Data Example (Continued)\\nTable 3.3 shows the coeﬃcients from a number of diﬀerent selection and\\nshrinkage methods. They are best-subset selection using an all-subsets search,\\nridge regression , thelasso,principal components regression andpartial least\\nsquares . Each method has a complexity parameter, and this was chosen to\\nminimize an estimate of prediction error based on tenfold cross-validation;\\nfull details are given in Section 7.10. Brieﬂy, cross-validation works by divid-\\ning the training data randomly into ten equal parts. The learning method\\nis ﬁt—for a range of values of the complexity parameter—to nine-tenths of\\nthe data, and the prediction error is computed on the remaining one-tenth.\\nThis is done in turn for each one-tenth of the data, and the ten prediction\\nerror estimates are averaged. From this we obtain an estimated prediction\\nerror curve as a function of the complexity parameter.\\nNote that we have already divided these data into a training set of size\\n67 and a test set of size 30. Cross-validation is applied to the training set,\\nsince selecting the shrinkage parameter is part of the training process. The\\ntest set is there to judge the performance of the selected model.\\nThe estimated prediction error curves are shown in Figure 3.7. Many of\\nthe curves are very ﬂat over large ranges near their minimum. Included\\nare estimated standard error bands for each estimated error rate, based on\\nthe ten error estimates computed by cross-validation. We have used the\\n“one-standard-error” rule—we pick the most parsimonious model within\\none standard error of the minimum (Section 7.10, page 244). Such a rule\\nacknowledges the fact that the tradeoﬀ curve is estimated with error, and\\nhence takes a conservative approach.\\nBest-subset selection chose to use the two predictors lcvol andlweight .\\nThe last two lines of the table give the average prediction error (and its\\nestimated standard error) over the test set.\\n3.4 Shrinkage Methods\\nBy retaining a subset of the predictors and discarding the rest, subset selec-\\ntion produces a model that is interpretable and has possibly lower predic-\\ntion error than the full model. However, because it is a discrete process—\\nvariables are either retained or discarded—it often exhibits high variance,\\nand so doesn’t reduce the prediction error of the full model. Shrinkage\\nmethods are more continuous, and don’t suﬀer as much from high\\nvariability.\\n3.4.1 Ridge Regression\\nRidge regression shrinks the regression coeﬃcients by imposing a penalty\\non their size. The ridge coeﬃcients minimize a penalized residual sum of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='003f2175-d0be-4de9-a316-af129ee84f64', embedding=None, metadata={'page_label': '81', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='62 3. Linear Methods for Regression\\nSubset SizeCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•••••••All Subsets\\nDegrees of FreedomCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•\\n••••••Ridge Regression\\nShrinkage Factor sCV Error\\n0.0 0.2 0.4 0.6 0.8 1.00.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•\\n••••••Lasso\\nNumber of DirectionsCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n••••••••Principal Components Regression\\nNumber of  DirectionsCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n••••••• •Partial Least Squares\\nFIGURE 3.7. Estimated prediction error curves and their standard errors f or\\nthe various selection and shrinkage methods. Each curve is plo tted as a function\\nof the corresponding complexity parameter for that method. The horizontal axis\\nhas been chosen so that the model complexity increases as we mov e from left to\\nright. The estimates of prediction error and their standard er rors were obtained by\\ntenfold cross-validation; full details are given in Section 7. 10. The least complex\\nmodel within one standard error of the best is chosen, indicated b y the purple\\nvertical broken lines.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='461304f5-1054-493c-9e3e-605c1daab67d', embedding=None, metadata={'page_label': '82', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 63\\nTABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent subs et\\nand shrinkage methods applied to the prostate data. The blank ent ries correspond\\nto variables omitted.\\nTerm LS Best Subset Ridge Lasso PCR PLS\\nIntercept 2.465 2.477 2.452 2.468 2.497 2.452\\nlcavol 0.680 0.740 0.420 0.533 0.543 0.419\\nlweight 0.263 0.316 0.238 0.169 0.289 0.344\\nage −0.141 −0.046 −0.152 −0.026\\nlbph 0.210 0.162 0.002 0.214 0.220\\nsvi 0.305 0.227 0.094 0.315 0.243\\nlcp −0.288 0.000 −0.051 0.079\\ngleason −0.021 0.040 0.232 0.011\\npgg45 0.267 0.133 −0.056 0.084\\nTest Error 0.521 0.492 0.492 0.479 0.449 0.528\\nStd Error 0.179 0.143 0.165 0.164 0.105 0.152\\nsquares,\\nˆβridge= argmin\\nβ{N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1β2\\nj}\\n. (3.41)\\nHereλ≥0 is a complexity parameter that controls the amount of shrink-\\nage: the larger the value of λ, the greater the amount of shrinkage. The\\ncoeﬃcients are shrunk toward zero (and each other). The idea of penaliz-\\ning by the sum-of-squares of the parameters is also used in neural networks,\\nwhere it is known as weight decay (Chapter 11).\\nAn equivalent way to write the ridge problem is\\nˆβridge= argmin\\nβN∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n,\\nsubject top∑\\nj=1β2\\nj≤t,(3.42)\\nwhich makes explicit the size constraint on the parameters. There is a one-\\nto-one correspondence between the parameters λin (3.41) and tin (3.42).\\nWhen there are many correlated variables in a linear regression model,\\ntheir coeﬃcients can become poorly determined and exhibit high variance.\\nA wildly large positive coeﬃcient on one variable can be canceled by a\\nsimilarly large negative coeﬃcient on its correlated cousin. By imposing a\\nsize constraint on the coeﬃcients, as in (3.42), this problem is alleviated.\\nThe ridge solutions are not equivariant under scaling of the inputs, and\\nso one normally standardizes the inputs before solving (3.41). In addition,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='03ebd2da-aa02-4f52-a833-27c31d2b4477', embedding=None, metadata={'page_label': '83', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='64 3. Linear Methods for Regression\\nnotice that the intercept β0has been left out of the penalty term. Penal-\\nization of the intercept would make the procedure depend on the origin\\nchosen for Y; that is, adding a constant cto each of the targets yiwould\\nnot simply result in a shift of the predictions by the same amount c. It\\ncan be shown (Exercise 3.5) that the solution to (3.41) can be separated\\ninto two parts, after reparametrization using centered inputs: each xijgets\\nreplaced by xij−¯xj. We estimate β0by ¯y=1\\nN∑N\\n1yi. The remaining co-\\neﬃcients get estimated by a ridge regression without intercept, using the\\ncentered xij. Henceforth we assume that this centering has been done, so\\nthat the input matrix Xhasp(rather than p+ 1) columns.\\nWriting the criterion in (3.41) in matrix form,\\nRSS(λ) = (y−Xβ)T(y−Xβ) +λβTβ, (3.43)\\nthe ridge regression solutions are easily seen to be\\nˆβridge= (XTX+λI)−1XTy, (3.44)\\nwhereIis the p×pidentity matrix. Notice that with the choice of quadratic\\npenalty βTβ, the ridge regression solution is again a linear function of\\ny. The solution adds a positive constant to the diagonal of XTXbefore\\ninversion. This makes the problem nonsingular, even if XTXis not of full\\nrank, and was the main motivation for ridge regression when it was ﬁrst\\nintroduced in statistics (Hoerl and Kennard, 1970). Traditional descriptions\\nof ridge regression start with deﬁnition (3.44). We choose to motivat e it via\\n(3.41) and (3.42), as these provide insight into how it works.\\nFigure 3.8 shows the ridge coeﬃcient estimates for the prostate can-\\ncer example, plotted as functions of df( λ), the eﬀective degrees of freedom\\nimplied by the penalty λ(deﬁned in (3.50) on page 68). In the case of or-\\nthonormal inputs, the ridge estimates are just a scaled version of the least\\nsquares estimates, that is, ˆβridge=ˆβ/(1 +λ).\\nRidge regression can also be derived as the mean or mode of a poste-\\nrior distribution, with a suitably chosen prior distribution. In detail, sup-\\nposeyi∼N(β0+xT\\niβ,σ2), and the parameters βjare each distributed as\\nN(0,τ2), independently of one another. Then the (negative) log-posterior\\ndensity of β, with τ2andσ2assumed known, is equal to the expression\\nin curly braces in (3.41), with λ=σ2/τ2(Exercise 3.6). Thus the ridge\\nestimate is the mode of the posterior distribution; since the distribution is\\nGaussian, it is also the posterior mean.\\nThesingular value decomposition (SVD) of the centered input matrix X\\ngives us some additional insight into the nature of ridge regression. This de-\\ncomposition is extremely useful in the analysis of many statistical metho ds.\\nThe SVD of the N×pmatrix Xhas the form\\nX=UDVT. (3.45)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='853583fd-99ce-4d0b-836e-aa1c86319145', embedding=None, metadata={'page_label': '84', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 65Coefficients\\n0 2 4 6 8−0.2 0.0 0.2 0.4 0.6•\\n•••••\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n••••••\\n•lcavol\\n••••••••••••••••••••••••\\n•lweight\\n•••••••••••••••••••••••••\\nage••••••••••••••••••••••••\\n•lbph••••••••••••••••••••••••\\n•svi\\n••••••••••••••••••••••••\\n•\\nlcp••••••••••••••••••••••••\\n•gleason•\\n•••••••••••••••••••••••\\n•pgg45\\ndf(λ)\\nFIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, as\\nthe tuning parameter λis varied. Coeﬃcients are plotted versus df(λ), the eﬀective\\ndegrees of freedom. A vertical line is drawn at df = 5 .0, the value chosen by\\ncross-validation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0fa4038-26d7-4944-9985-020bfc297ee3', embedding=None, metadata={'page_label': '85', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='66 3. Linear Methods for Regression\\nHereUandVareN×pandp×porthogonal matrices, with the columns\\nofUspanning the column space of X, and the columns of Vspanning the\\nrow space. Dis ap×pdiagonal matrix, with diagonal entries d1≥d2≥\\n≤≤≤ ≥ dp≥0 called the singular values of X. If one or more values dj= 0,\\nXis singular.\\nUsing the singular value decomposition we can write the least squares\\nﬁtted vector as\\nXˆβls=X(XTX)−1XTy\\n=UUTy, (3.46)\\nafter some simpliﬁcation. Note that UTyare the coordinates of ywith\\nrespect to the orthonormal basis U. Note also the similarity with (3.33);\\nQandUare generally diﬀerent orthogonal bases for the column space of\\nX(Exercise 3.8).\\nNow the ridge solutions are\\nXˆβridge=X(XTX+λI)−1XTy\\n=U D(D2+λI)−1D UTy\\n=p∑\\nj=1ujd2\\nj\\nd2\\nj+λuT\\njy, (3.47)\\nwhere the ujare the columns of U. Note that since λ≥0, we have d2\\nj/(d2\\nj+\\nλ)≤1. Like linear regression, ridge regression computes the coordinates of\\nywith respect to the orthonormal basis U. It then shrinks these coordinates\\nby the factors d2\\nj/(d2\\nj+λ). This means that a greater amount of shrinkage\\nis applied to the coordinates of basis vectors with smaller d2\\nj.\\nWhat does a small value of d2\\njmean? The SVD of the centered matrix\\nXis another way of expressing the principal components of the variables\\ninX. The sample covariance matrix is given by S=XTX/N, and from\\n(3.45) we have\\nXTX=VD2VT, (3.48)\\nwhich is the eigen decomposition ofXTX(and of S, up to a factor N).\\nThe eigenvectors vj(columns of V) are also called the principal compo-\\nnents (or Karhunen–Loeve) directions of X. The ﬁrst principal component\\ndirection v1has the property that z1=Xv1has the largest sample vari-\\nance amongst all normalized linear combinations of the columns of X. This\\nsample variance is easily seen to be\\nVar(z1) = Var( Xv1) =d2\\n1\\nN, (3.49)\\nand in fact z1=Xv1=u1d1. The derived variable z1is called the ﬁrst\\nprincipal component of X, and hence u1is the normalized ﬁrst principal', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d2a09df3-d419-4dec-bcd5-b4a4f19f8fbe', embedding=None, metadata={'page_label': '86', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 67\\n-4 -2 0 2 4-4 -2 0 2 4ooo\\nooooooo\\no\\no\\nooo\\noo\\noo\\noo\\nooo\\no\\no\\noooo\\noo\\no\\nooo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\nooo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\no\\noooo\\nooo\\no\\noo\\no\\noo\\noo\\no\\noo\\noooooo\\nooo\\noo\\nooo\\noo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\nooo\\noo\\no\\noooo\\no\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\noo\\noo\\no\\noo\\noo\\noooo\\no\\nooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooLargest Principal\\nComponent\\nSmallest Principal\\nComponent\\nX1X2\\nFIGURE 3.9. Principal components of some input data points. The largest prin-\\ncipal component is the direction that maximizes the variance of t he projected data,\\nand the smallest principal component minimizes that variance. Rid ge regression\\nprojects yonto these components, and then shrinks the coeﬃcients of the low–\\nvariance components more than the high-variance components.\\ncomponent. Subsequent principal components zjhave maximum variance\\nd2\\nj/N, subject to being orthogonal to the earlier ones. Conversely the last\\nprincipal component has minimum variance. Hence the small singular val-\\nuesdjcorrespond to directions in the column space of Xhaving small\\nvariance, and ridge regression shrinks these directions the most.\\nFigure 3.9 illustrates the principal components of some data points in\\ntwo dimensions. If we consider ﬁtting a linear surface over this domain\\n(theY-axis is sticking out of the page), the conﬁguration of the data allow\\nus to determine its gradient more accurately in the long direction than\\nthe short. Ridge regression protects against the potentially high variance\\nof gradients estimated in the short directions. The implicit assumption is\\nthat the response will tend to vary most in the directions of high variance\\nof the inputs. This is often a reasonable assumption, since predictors are\\noften chosen for study because they vary with the response variable, but\\nneed not hold in general.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ad67a2b9-23d2-4a80-91a1-6a71ec109111', embedding=None, metadata={'page_label': '87', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='68 3. Linear Methods for Regression\\nIn Figure 3.7 we have plotted the estimated prediction error versus the\\nquantity\\ndf(λ) = tr[ X(XTX+λI)−1XT],\\n= tr( Hλ)\\n=p∑\\nj=1d2\\nj\\nd2\\nj+λ. (3.50)\\nThis monotone decreasing function of λis the eﬀective degrees of freedom\\nof the ridge regression ﬁt. Usually in a linear-regression ﬁt with pvariables,\\nthe degrees-of-freedom of the ﬁt is p, the number of free parameters. The\\nidea is that although all pcoeﬃcients in a ridge ﬁt will be non-zero, they\\nare ﬁt in a restricted fashion controlled by λ. Note that df( λ) =pwhen\\nλ= 0 (no regularization) and df( λ)→0 asλ→ ∞ . Of course there\\nis always an additional one degree of freedom for the intercept, which was\\nremoved apriori . This deﬁnition is motivated in more detail in Section 3.4.4\\nand Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df( λ) = 5 .0.\\nTable 3.3 shows that ridge regression reduces the test error of the full least\\nsquares estimates by a small amount.\\n3.4.2 The Lasso\\nThe lasso is a shrinkage method like ridge, with subtle but important dif-\\nferences. The lasso estimate is deﬁned by\\nˆβlasso= argmin\\nβN∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\nsubject top∑\\nj=1|βj| ≤t. (3.51)\\nJust as in ridge regression, we can re-parametrize the constant β0by stan-\\ndardizing the predictors; the solution for ˆβ0is ¯y, and thereafter we ﬁt a\\nmodel without an intercept (Exercise 3.5). In the signal processing litera-\\nture, the lasso is also known as basis pursuit (Chen et al., 1998).\\nWe can also write the lasso problem in the equivalent Lagrangian form\\nˆβlasso= argmin\\nβ{1\\n2N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1|βj|}\\n.(3.52)\\nNotice the similarity to the ridge regression problem (3.42) or (3.41) : the\\nL2ridge penalty∑p\\n1β2\\njis replaced by the L1lasso penalty∑p\\n1|βj|. This\\nlatter constraint makes the solutions nonlinear in the yi, and there is no\\nclosed form expression as in ridge regression. Computing the lasso solution', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f2ae9c4b-88d3-4cec-b984-fd48507cc670', embedding=None, metadata={'page_label': '88', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 69\\nis a quadratic programming problem, although we see in Section 3.4.4 that\\neﬃcient algorithms are available for computing the entire path of solutions\\nasλis varied, with the same computational cost as for ridge regression.\\nBecause of the nature of the constraint, making tsuﬃciently small will\\ncause some of the coeﬃcients to be exactly zero. Thus the lasso does a kind\\nof continuous subset selection. If tis chosen larger than t0=∑p\\n1|ˆβj|(where\\nˆβj=ˆβls\\nj, the least squares estimates), then the lasso estimates are the ˆβj’s.\\nOn the other hand, for t=t0/2 say, then the least squares coeﬃcients are\\nshrunk by about 50% on average. However, the nature of the shrinkage\\nis not obvious, and we investigate it further in Section 3.4.4 below. Like\\nthe subset size in variable subset selection, or the penalty parameter in\\nridge regression, tshould be adaptively chosen to minimize an estimate of\\nexpected prediction error.\\nIn Figure 3.7, for ease of interpretation, we have plotted the lasso pre-\\ndiction error estimates versus the standardized parameter s=t/∑p\\n1|ˆβj|.\\nA value ˆ s≈0.36 was chosen by 10-fold cross-validation; this caused four\\ncoeﬃcients to be set to zero (ﬁfth column of Table 3.3). The resulting\\nmodel has the second lowest test error, slightly lower than the full least\\nsquares model, but the standard errors of the test error estimates (last line\\nof Table 3.3) are fairly large.\\nFigure 3.10 shows the lasso coeﬃcients as the standardized tuning pa-\\nrameter s=t/∑p\\n1|ˆβj|is varied. At s= 1.0 these are the least squares\\nestimates; they decrease to 0 as s→0. This decrease is not always strictly\\nmonotonic, although it is in this example. A vertical line is drawn at\\ns= 0.36, the value chosen by cross-validation.\\n3.4.3 Discussion: Subset Selection, Ridge Regression and t he\\nLasso\\nIn this section we discuss and compare the three approaches discussed so far\\nfor restricting the linear regression model: subset selection, ridge regression\\nand the lasso.\\nIn the case of an orthonormal input matrix Xthe three procedures have\\nexplicit solutions. Each method applies a simple transformation to the leas t\\nsquares estimate ˆβj, as detailed in Table 3.4.\\nRidge regression does a proportional shrinkage. Lasso translates each\\ncoeﬃcient by a constant factor λ, truncating at zero. This is called “soft\\nthresholding,” and is used in the context of wavelet-based smoothing in Sec-\\ntion 5.9. Best-subset selection drops all variables with coeﬃcients smaller\\nthan the Mth largest; this is a form of “hard-thresholding.”\\nBack to the nonorthogonal case; some pictures help understand their re-\\nlationship. Figure 3.11 depicts the lasso (left) and ridge regression (righ t)\\nwhen there are only two parameters. The residual sum of squares has ellip-\\ntical contours, centered at the full least squares estimate. The constraint', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='746519e7-b3e7-4773-811d-ee4421e93ba9', embedding=None, metadata={'page_label': '89', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='70 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.0−0.2 0.0 0.2 0.4 0.6\\nShrinkage Factor sCoefficientslcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\nFIGURE 3.10. Proﬁles of lasso coeﬃcients, as the tuning parameter tis varied.\\nCoeﬃcients are plotted versus s=t/Pp\\n1|ˆβj|. A vertical line is drawn at s= 0.36,\\nthe value chosen by cross-validation. Compare Figure 3.8 on p age 65; the lasso\\nproﬁles hit zero, while those for ridge do not. The proﬁles are pi ece-wise linear,\\nand so are computed only at the points displayed; see Section 3.4. 4 for details.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6224ef71-b492-4555-9c1c-cda824661f3d', embedding=None, metadata={'page_label': '90', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 71\\nTABLE 3.4. Estimators of βjin the case of orthonormal columns of X.Mandλ\\nare constants chosen by the corresponding techniques; signdenotes the sign of its\\nargument ( ±1), and x+denotes “positive part” of x. Below the table, estimators\\nare shown by broken red lines. The 45◦line in gray shows the unrestricted estimate\\nfor reference.\\nEstimator Formula\\nBest subset (size M)ˆβj≤I(|ˆβj| ≥ |ˆβ(M)|)\\nRidge ˆβj/(1 +λ)\\nLasso sign( ˆβj)(|ˆβj| −λ)+\\n(0,0) (0,0) (0,0)|ˆβ(M)|λBest Subset Ridge Lasso\\nβ^β^ 2. . β\\n1β2\\nβ1β\\nFIGURE 3.11. Estimation picture for the lasso (left) and ridge regression\\n(right). Shown are contours of the error and constraint functions. T he solid blue\\nareas are the constraint regions |β1|+|β2| ≤tandβ2\\n1+β2\\n2≤t2, respectively,\\nwhile the red ellipses are the contours of the least squares er ror function.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c36fca01-9b67-4bc3-990d-4975baa9a28f', embedding=None, metadata={'page_label': '91', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='72 3. Linear Methods for Regression\\nregion for ridge regression is the disk β2\\n1+β2\\n2≤t, while that for lasso is\\nthe diamond |β1|+|β2| ≤t. Both methods ﬁnd the ﬁrst point where the\\nelliptical contours hit the constraint region. Unlike the disk, the diamond\\nhas corners; if the solution occurs at a corner, then it has one parameter\\nβjequal to zero. When p >2, the diamond becomes a rhomboid, and has\\nmany corners, ﬂat edges and faces; there are many more opportunities for\\nthe estimated parameters to be zero.\\nWe can generalize ridge regression and the lasso, and view them as Bayes\\nestimates. Consider the criterion\\n˜β= argmin\\nβ{N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1|βj|q}\\n(3.53)\\nforq≥0. The contours of constant value of∑\\nj|βj|qare shown in Fig-\\nure 3.12, for the case of two inputs.\\nThinking of |βj|qas the log-prior density for βj, these are also the equi-\\ncontours of the prior distribution of the parameters. The value q= 0 corre-\\nsponds to variable subset selection, as the penalty simply counts the number\\nof nonzero parameters; q= 1 corresponds to the lasso, while q= 2 to ridge\\nregression. Notice that for q≤1, the prior is not uniform in direction, but\\nconcentrates more mass in the coordinate directions. The prior correspond-\\ning to the q= 1 case is an independent double exponential (or Laplace)\\ndistribution for each input, with density (1 /2τ)exp(−|β|/τ) and τ= 1/λ.\\nThe case q= 1 (lasso) is the smallest qsuch that the constraint region\\nis convex; non-convex constraint regions make the optimization problem\\nmore diﬃcult.\\nIn this view, the lasso, ridge regression and best subset selection are\\nBayes estimates with diﬀerent priors. Note, however, that they are derived\\nas posterior modes, that is, maximizers of the posterior. It is more com mon\\nto use the mean of the posterior as the Bayes estimate. Ridge regression is\\nalso the posterior mean, but the lasso and best subset selection are not.\\nLooking again at the criterion (3.53), we might try using other values\\nofqbesides 0, 1, or 2. Although one might consider estimating qfrom\\nthe data, our experience is that it is not worth the eﬀort for the extra\\nvariance incurred. Values of q∈(1,2) suggest a compromise between the\\nlasso and ridge regression. Although this is the case, with q >1,|βj|qis\\ndiﬀerentiable at 0, and so does not share the ability of lasso ( q= 1) for\\nq= 4 q= 2 q= 1 q= 0.5 q= 0.1\\nFIGURE 3.12. Contours of constant value ofP\\nj|βj|qfor given values of q.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c7fe72a8-df67-4126-8d72-e8ac998ddc73', embedding=None, metadata={'page_label': '92', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 73\\nq= 1.2 α= 0.2\\nLq Elastic Net\\nFIGURE 3.13. Contours of constant value ofP\\nj|βj|qforq= 1.2(left plot),\\nand the elastic-net penaltyP\\nj(αβ2\\nj+(1−α)|βj|)forα= 0.2(right plot). Although\\nvisually very similar, the elastic-net has sharp (non-diﬀerent iable) corners, while\\ntheq= 1.2penalty does not.\\nsetting coeﬃcients exactly to zero. Partly for this reason as well as for\\ncomputational tractability, Zou and Hastie (2005) introduced the elastic-\\nnetpenalty\\nλp∑\\nj=1(\\nαβ2\\nj+ (1−α)|βj|)\\n, (3.54)\\na diﬀerent compromise between ridge and lasso. Figure 3.13 compares the\\nLqpenalty with q= 1.2 and the elastic-net penalty with α= 0.2; it is\\nhard to detect the diﬀerence by eye. The elastic-net selects variables like\\nthe lasso, and shrinks together the coeﬃcients of correlated predictors like\\nridge. It also has considerable computational advantages over the Lqpenal-\\nties. We discuss the elastic-net further in Section 18.4.\\n3.4.4 Least Angle Regression\\nLeast angle regression (LAR) is a relative newcomer (Efron et al., 2004) ,\\nand can be viewed as a kind of “democratic” version of forward stepwise\\nregression (Section 3.3.2). As we will see, LAR is intimately connected\\nwith the lasso, and in fact provides an extremely eﬃcient algorithm for\\ncomputing the entire lasso path as in Figure 3.10.\\nForward stepwise regression builds a model sequentially, adding one vari-\\nable at a time. At each step, it identiﬁes the best variable to include in the\\nactive set , and then updates the least squares ﬁt to include all the active\\nvariables.\\nLeast angle regression uses a similar strategy, but only enters “as much”\\nof a predictor as it deserves. At the ﬁrst step it identiﬁes the variable\\nmost correlated with the response. Rather than ﬁt this variable completely,\\nLAR moves the coeﬃcient of this variable continuously toward its least-\\nsquares value (causing its correlation with the evolving residual to decrease\\nin absolute value). As soon as another variable “catches up” in terms of\\ncorrelation with the residual, the process is paused. The second variable\\nthen joins the active set, and their coeﬃcients are moved together in a way\\nthat keeps their correlations tied and decreasing. This process is continued', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6ef1483-ff04-4866-904f-49aaaf148ac4', embedding=None, metadata={'page_label': '93', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='74 3. Linear Methods for Regression\\nuntil all the variables are in the model, and ends at the full least-squares\\nﬁt. Algorithm 3.2 provides the details. The termination condition in step 5\\nrequires some explanation. If p > N −1, the LAR algorithm reaches a zero\\nresidual solution after N−1 steps (the −1 is because we have centered the\\ndata).\\nAlgorithm 3.2 Least Angle Regression.\\n1. Standardize the predictors to have mean zero and unit norm. Start\\nwith the residual r=y−¯y,β1,β2,... ,β p= 0.\\n2. Find the predictor xjmost correlated with r.\\n3. Move βjfrom 0 towards its least-squares coeﬃcient ⟨xj,r⟩, until some\\nother competitor xkhas as much correlation with the current residual\\nas does xj.\\n4. Move βjandβkin the direction deﬁned by their joint least squares\\ncoeﬃcient of the current residual on ( xj,xk), until some other com-\\npetitor xlhas as much correlation with the current residual.\\n5. Continue in this way until all ppredictors have been entered. After\\nmin(N−1,p) steps, we arrive at the full least-squares solution.\\nSuppose Akis the active set of variables at the beginning of the kth\\nstep, and let βAkbe the coeﬃcient vector for these variables at this step;\\nthere will be k−1 nonzero values, and the one just entered will be zero. If\\nrk=y−XAkβAkis the current residual, then the direction for this step is\\nδk= (XT\\nAkXAk)−1XT\\nAkrk. (3.55)\\nThe coeﬃcient proﬁle then evolves as βAk(α) =βAk+α≤δk. Exercise 3.23\\nveriﬁes that the directions chosen in this fashion do what is claimed: keep\\nthe correlations tied and decreasing. If the ﬁt vector at the beginning of\\nthis step is ˆfk, then it evolves as ˆfk(α) =ˆfk+α≤uk, where uk=XAkδk\\nis the new ﬁt direction. The name “least angle” arises from a geometrical\\ninterpretation of this process; ukmakes the smallest (and equal) angle\\nwith each of the predictors in Ak(Exercise 3.24). Figure 3.14 shows the\\nabsolute correlations decreasing and joining ranks with each step of the\\nLAR algorithm, using simulated data.\\nBy construction the coeﬃcients in LAR change in a piecewise linear fash-\\nion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a\\nfunction of their L1arc length2. Note that we do not need to take small\\n2TheL1arc-length of a diﬀerentiable curve β(s) fors∈[0, S] is given by TV( β, S) =RS\\n0||˙β(s)||1ds,where ˙β(s) =∂β(s)/∂s. For the piecewise-linear LAR coeﬃcient proﬁle,\\nthis amounts to summing the L1norms of the changes in coeﬃcients from step to step.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5082b68d-9b60-40b2-8e69-18c3afb850b8', embedding=None, metadata={'page_label': '94', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 75\\n0 5 10 150.0 0.1 0.2 0.3 0.4v2 v6 v4 v5 v3 v1\\nL1Arc LengthAbsolute Correlations\\nFIGURE 3.14. Progression of the absolute correlations during each step of t he\\nLAR procedure, using a simulated data set with six predictors . The labels at the\\ntop of the plot indicate which variables enter the active set at each step. The step\\nlength are measured in units of L1arc length.\\n0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Least Angle Regression\\n0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Lasso\\nL1Arc Length L1Arc Length\\nCoeﬃcientsCoeﬃcients\\nFIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated\\ndata, as a function of the L1arc length. The right panel shows the Lasso proﬁle.\\nThey are identical until the dark-blue coeﬃcient crosses zero a t an arc length of\\nabout 18.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aeb1f629-cf14-4d16-8e42-8bd83dd1cd03', embedding=None, metadata={'page_label': '95', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='76 3. Linear Methods for Regression\\nsteps and recheck the correlations in step 3; using knowledge of the covari-\\nance of the predictors and the piecewise linearity of the algorithm, we can\\nwork out the exact step length at the beginning of each step (Exercise 3.25).\\nThe right panel of Figure 3.15 shows the lasso coeﬃcient proﬁles on the\\nsame data. They are almost identical to those in the left panel, and diﬀer\\nfor the ﬁrst time when the blue coeﬃcient passes back through zero. For the\\nprostate data, the LAR coeﬃcient proﬁle turns out to be identical to the\\nlasso proﬁle in Figure 3.10, which never crosses zero. These observations\\nlead to a simple modiﬁcation of the LAR algorithm that gives the entire\\nlasso path, which is also piecewise-linear.\\nAlgorithm 3.2a Least Angle Regression: Lasso Modiﬁcation .\\n4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set\\nof variables and recompute the current joint least squares direction.\\nThe LAR(lasso) algorithm is extremely eﬃcient, requiring the same order\\nof computation as that of a single least squares ﬁt using the ppredictors.\\nLeast angle regression always takes psteps to get to the full least squares\\nestimates. The lasso path can have more than psteps, although the two\\nare often quite similar. Algorithm 3.2 with the lasso modiﬁcation 3. 2a is\\nan eﬃcient way of computing the solution to any lasso problem, especially\\nwhen p≫N. Osborne et al. (2000a) also discovered a piecewise-linear path\\nfor computing the lasso, which they called a homotopy algorithm.\\nWe now give a heuristic argument for why these procedures are so similar.\\nAlthough the LAR algorithm is stated in terms of correlations, if the input\\nfeatures are standardized, it is equivalent and easier to work with inner-\\nproducts. Suppose Ais the active set of variables at some stage in the\\nalgorithm, tied in their absolute inner-product with the current residuals\\ny−Xβ. We can express this as\\nxT\\nj(y−Xβ) =γ≤sj,∀j∈ A (3.56)\\nwhere sj∈ {−1,1}indicates the sign of the inner-product, and γis the\\ncommon value. Also |xT\\nk(y−Xβ)| ≤γ∀k̸∈ A. Now consider the lasso\\ncriterion (3.52), which we write in vector form\\nR(β) =1\\n2||y−Xβ||2\\n2+λ||β||1. (3.57)\\nLetBbe the active set of variables in the solution for a given value of λ.\\nFor these variables R(β) is diﬀerentiable, and the stationarity conditions\\ngive\\nxT\\nj(y−Xβ) =λ≤sign(βj),∀j∈ B (3.58)\\nComparing (3.58) with (3.56), we see that they are identical only if the\\nsign of βjmatches the sign of the inner product. That is why the LAR', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f207365-b4e8-4508-8e60-78b38ea7691e', embedding=None, metadata={'page_label': '96', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Shrinkage Methods 77\\nalgorithm and lasso start to diﬀer when an active coeﬃcient passes through\\nzero; condition (3.58) is violated for that variable, and it is kicked out o f the\\nactive set B. Exercise 3.23 shows that these equations imply a piecewise-\\nlinear coeﬃcient proﬁle as λdecreases. The stationarity conditions for the\\nnon-active variables require that\\n|xT\\nk(y−Xβ)| ≤λ,∀k̸∈ B, (3.59)\\nwhich again agrees with the LAR algorithm.\\nFigure 3.16 compares LAR and lasso to forward stepwise and stagewise\\nregression. The setup is the same as in Figure 3.6 on page 59, except here\\nN= 100 here rather than 300, so the problem is more diﬃcult. We see\\nthat the more aggressive forward stepwise starts to overﬁt quite earl y (well\\nbefore the 10 true variables can enter the model), and ultimately performs\\nworse than the slower forward stagewise regression. The behavior of LAR\\nand lasso is similar to that of forward stagewise regression. Increment al\\nforward stagewise is similar to LAR and lasso, and is described in Sec-\\ntion 3.8.1.\\nDegrees-of-Freedom Formula for LAR and Lasso\\nSuppose that we ﬁt a linear model via the least angle regression procedure,\\nstopping at some number of steps k < p, or equivalently using a lasso bound\\ntthat produces a constrained version of the full least squares ﬁt. How many\\nparameters, or “degrees of freedom” have we used?\\nConsider ﬁrst a linear regression using a subset of kfeatures. If this subset\\nis prespeciﬁed in advance without reference to the training data, then the\\ndegrees of freedom used in the ﬁtted model is deﬁned to be k. Indeed, in\\nclassical statistics, the number of linearly independent parameters is what\\nis meant by “degrees of freedom.” Alternatively, suppose that we carry out\\na best subset selection to determine the “optimal” set of kpredictors. Then\\nthe resulting model has kparameters, but in some sense we have used up\\nmore than kdegrees of freedom.\\nWe need a more general deﬁnition for the eﬀective degrees of freedom of\\nan adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted\\nvector ˆy= (ˆy1,ˆy2,... ,ˆyN) as\\ndf(ˆy) =1\\nσ2N∑\\ni=1Cov(ˆyi,yi). (3.60)\\nHere Cov(ˆ yi,yi) refers to the sampling covariance between the predicted\\nvalue ˆ yiand its corresponding outcome value yi. This makes intuitive sense:\\nthe harder that we ﬁt to the data, the larger this covariance and hence\\ndf(ˆy). Expression (3.60) is a useful notion of degrees of freedom, one that\\ncan be applied to any model prediction ˆy. This includes models that are', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d3519b39-a019-45b1-bedf-7441076625a0', embedding=None, metadata={'page_label': '97', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='78 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.00.55 0.60 0.65Forward Stepwise\\nLAR\\nLasso\\nForward Stagewise\\nIncremental Forward StagewiseE||ˆβ(k)−β||2\\nFraction of L1arc-length\\nFIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward\\nstagewise (FS) and incremental forward stagewise (FS 0) regression. The setup\\nis the same as in Figure 3.6, except N= 100 here rather than 300. Here the\\nslower FS regression ultimately outperforms forward stepw ise. LAR and lasso\\nshow similar behavior to FS and FS 0. Since the procedures take diﬀerent numbers\\nof steps (across simulation replicates and methods), we plot the MSE as a function\\nof the fraction of total L1arc-length toward the least-squares ﬁt.\\nadaptively ﬁtted to the training data. This deﬁnition is motivated and\\ndiscussed further in Sections 7.4–7.6.\\nNow for a linear regression with kﬁxed predictors, it is easy to show\\nthat df( ˆy) =k. Likewise for ridge regression, this deﬁnition leads to the\\nclosed-form expression (3.50) on page 68: df( ˆy) = tr( Sλ). In both these\\ncases, (3.60) is simple to evaluate because the ﬁt ˆy=Hλyis linear in y.\\nIf we think about deﬁnition (3.60) in the context of a best subset selection\\nof size k, it seems clear that df( ˆy) will be larger than k, and this can be\\nveriﬁed by estimating Cov(ˆ yi,yi)/σ2directly by simulation. However there\\nis no closed form method for estimating df( ˆy) for best subset selection.\\nFor LAR and lasso, something magical happens. These techniques are\\nadaptive in a smoother way than best subset selection, and hence estimation\\nof degrees of freedom is more tractable. Speciﬁcally it can be shown that\\nafter the kth step of the LAR procedure, the eﬀective degrees of freedom of\\nthe ﬁt vector is exactly k. Now for the lasso, the (modiﬁed) LAR procedure', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aac73526-6684-46ec-81d7-207b7193f2e3', embedding=None, metadata={'page_label': '98', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.5 Methods Using Derived Input Directions 79\\noften takes more than psteps, since predictors can drop out. Hence the\\ndeﬁnition is a little diﬀerent; for the lasso, at any stage df( ˆy) approximately\\nequals the number of predictors in the model. While this approximation\\nworks reasonably well anywhere in the lasso path, for each kit works best\\nat the lastmodel in the sequence that contains kpredictors. A detailed\\nstudy of the degrees of freedom for the lasso may be found in Zou et al.\\n(2007).\\n3.5 Methods Using Derived Input Directions\\nIn many situations we have a large number of inputs, often very correlated.\\nThe methods in this section produce a small number of linear combinations\\nZm, m= 1,... ,M of the original inputs Xj, and the Zmare then used in\\nplace of the Xjas inputs in the regression. The methods diﬀer in how the\\nlinear combinations are constructed.\\n3.5.1 Principal Components Regression\\nIn this approach the linear combinations Zmused are the principal com-\\nponents as deﬁned in Section 3.4.1 above.\\nPrincipal component regression forms the derived input columns zm=\\nXvm, and then regresses yonz1,z2,... ,zMfor some M≤p. Since the zm\\nare orthogonal, this regression is just a sum of univariate regressions:\\nˆypcr\\n(M)= ¯y1+M∑\\nm=1ˆθmzm, (3.61)\\nwhere ˆθm=⟨zm,y⟩/⟨zm,zm⟩. Since the zmare each linear combinations\\nof the original xj, we can express the solution (3.61) in terms of coeﬃcients\\nof thexj(Exercise 3.13):\\nˆβpcr(M) =M∑\\nm=1ˆθmvm. (3.62)\\nAs with ridge regression, principal components depend on the scaling of\\nthe inputs, so typically we ﬁrst standardize them. Note that if M=p, we\\nwould just get back the usual least squares estimates, since the columns of\\nZ=UDspan the column space of X. ForM < p we get a reduced regres-\\nsion. We see that principal components regression is very similar to ridge\\nregression: both operate via the principal components of the input ma-\\ntrix. Ridge regression shrinks the coeﬃcients of the principal components\\n(Figure 3.17), shrinking more depending on the size of the corresponding\\neigenvalue; principal components regression discards the p−Msmallest\\neigenvalue components. Figure 3.17 illustrates this.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9566d306-8d7f-4423-b47e-50de0feec0e1', embedding=None, metadata={'page_label': '99', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='80 3. Linear Methods for Regression\\nIndexShrinkage Factor\\n2 4 6 80.0 0.2 0.4 0.6 0.8 1.0•\\n••\\n••••\\n•• • • • • • •\\n• •ridge\\npcr\\nFIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the pri n-\\ncipal components, using shrinkage factors d2\\nj/(d2\\nj+λ)as in (3.47). Principal\\ncomponent regression truncates them. Shown are the shrinkage and t runcation\\npatterns corresponding to Figure 3.7, as a function of the princip al component\\nindex.\\nIn Figure 3.7 we see that cross-validation suggests seven terms; the re-\\nsulting model has the lowest test error in Table 3.3.\\n3.5.2 Partial Least Squares\\nThis technique also constructs a set of linear combinations of the inputs\\nfor regression, but unlike principal components regression it uses y(in ad-\\ndition to X) for this construction. Like principal component regression,\\npartial least squares (PLS) is not scale invariant, so we assume that eac h\\nxjis standardized to have mean 0 and variance 1. PLS begins by com-\\nputing ˆ ϕ1j=⟨xj,y⟩for each j. From this we construct the derived input\\nz1=∑\\njˆϕ1jxj, which is the ﬁrst partial least squares direction. Hence\\nin the construction of each zm, the inputs are weighted by the strength\\nof their univariate eﬀect on y3. The outcome yis regressed on z1giving\\ncoeﬃcient ˆθ1, and then we orthogonalize x1,... ,xpwith respect to z1. We\\ncontinue this process, until M≤pdirections have been obtained. In this\\nmanner, partial least squares produces a sequence of derived, orthogonal\\ninputs or directions z1,z2,... ,zM. As with principal-component regres-\\nsion, if we were to construct all M=pdirections, we would get back a\\nsolution equivalent to the usual least squares estimates; using M < p di-\\nrections produces a reduced regression. The procedure is described fully in\\nAlgorithm 3.3.\\n3Since the xjare standardized, the ﬁrst directions ˆ ϕ1jare the univariate regression\\ncoeﬃcients (up to an irrelevant constant); this is not the ca se for subsequent directions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='525d3396-c528-46fe-9ac5-357825c864dd', embedding=None, metadata={'page_label': '100', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.5 Methods Using Derived Input Directions 81\\nAlgorithm 3.3 Partial Least Squares.\\n1. Standardize each xjto have mean zero and variance one. Set ˆy(0)=\\n¯y1, andx(0)\\nj=xj, j= 1,... ,p .\\n2. For m= 1,2,... ,p\\n(a)zm=∑p\\nj=1ˆϕmjx(m−1)\\nj, where ˆ ϕmj=⟨x(m−1)\\nj,y⟩.\\n(b)ˆθm=⟨zm,y⟩/⟨zm,zm⟩.\\n(c)ˆy(m)=ˆy(m−1)+ˆθmzm.\\n(d) Orthogonalize each x(m−1)\\nj with respect to zm:x(m)\\nj=x(m−1)\\nj−\\n[⟨zm,x(m−1)\\nj⟩/⟨zm,zm⟩]zm,j= 1,2,... ,p .\\n3. Output the sequence of ﬁtted vectors {ˆy(m)}p\\n1. Since the {zℓ}m\\n1are\\nlinear in the original xj, so is ˆy(m)=Xˆβpls(m). These linear coeﬃ-\\ncients can be recovered from the sequence of PLS transformations.\\nIn the prostate cancer example, cross-validation chose M= 2 PLS direc-\\ntions in Figure 3.7. This produced the model given in the rightmost column\\nof Table 3.3.\\nWhat optimization problem is partial least squares solving? Since it uses\\nthe response yto construct its directions, its solution path is a nonlinear\\nfunction of y. It can be shown (Exercise 3.15) that partial least squares\\nseeks directions that have high variance andhave high correlation with the\\nresponse, in contrast to principal components regression which keys only\\non high variance (Stone and Brooks, 1990; Frank and Friedman, 1993). In\\nparticular, the mth principal component direction vmsolves:\\nmax αVar(Xα) (3.63)\\nsubject to ||α||= 1, αTSvℓ= 0, ℓ= 1,... ,m −1,\\nwhereSis the sample covariance matrix of the xj. The conditions αTSvℓ=\\n0 ensures that zm=Xαis uncorrelated with all the previous linear com-\\nbinations zℓ=Xvℓ. The mth PLS direction ˆ ϕmsolves:\\nmax αCorr2(y,Xα)Var(Xα) (3.64)\\nsubject to ||α||= 1, αTSˆϕℓ= 0, ℓ= 1,... ,m −1.\\nFurther analysis reveals that the variance aspect tends to dominate, and\\nso partial least squares behaves much like ridge regression and principal\\ncomponents regression. We discuss this further in the next section.\\nIf the input matrix Xis orthogonal, then partial least squares ﬁnds the\\nleast squares estimates after m= 1 steps. Subsequent steps have no eﬀect', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a061d2a-eb56-4eee-976d-3027014b1b60', embedding=None, metadata={'page_label': '101', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='82 3. Linear Methods for Regression\\nsince the ˆ ϕmjare zero for m >1 (Exercise 3.14). It can also be shown that\\nthe sequence of PLS coeﬃcients for m= 1,2,... ,p represents the conjugate\\ngradient sequence for computing the least squares solutions (Exercise 3.18).\\n3.6 Discussion: A Comparison of the Selection and\\nShrinkage Methods\\nThere are some simple settings where we can understand better the rela-\\ntionship between the diﬀerent methods described above. Consider an exam-\\nple with two correlated inputs X1andX2, with correlation ρ. We assume\\nthat the true regression coeﬃcients are β1= 4 and β2= 2. Figure 3.18\\nshows the coeﬃcient proﬁles for the diﬀerent methods, as their tuning pa-\\nrameters are varied. The top panel has ρ= 0.5, the bottom panel ρ=−0.5.\\nThe tuning parameters for ridge and lasso vary over a continuous range,\\nwhile best subset, PLS and PCR take just two discrete steps to the least\\nsquares solution. In the top panel, starting at the origin, ridge regression\\nshrinks the coeﬃcients together until it ﬁnally converges to least squares.\\nPLS and PCR show similar behavior to ridge, although are discrete and\\nmore extreme. Best subset overshoots the solution and then backtracks.\\nThe behavior of the lasso is intermediate to the other methods. When the\\ncorrelation is negative (lower panel), again PLS and PCR roughly track\\nthe ridge path, while all of the methods are more similar to one another.\\nIt is interesting to compare the shrinkage behavior of these diﬀerent\\nmethods. Recall that ridge regression shrinks all directions, but shrinks\\nlow-variance directions more. Principal components regression leaves M\\nhigh-variance directions alone, and discards the rest. Interestingly, it can\\nbe shown that partial least squares also tends to shrink the low-variance\\ndirections, but can actually inﬂate some of the higher variance directions.\\nThis can make PLS a little unstable, and cause it to have slightly higher\\nprediction error compared to ridge regression. A full study is given in Frank\\nand Friedman (1993). These authors conclude that for minimizing predic-\\ntion error, ridge regression is generally preferable to variable subset selec-\\ntion, principal components regression and partial least squares. However\\nthe improvement over the latter two methods was only slight.\\nTo summarize, PLS, PCR and ridge regression tend to behave similarly.\\nRidge regression may be preferred because it shrinks smoothly, rather than\\nin discrete steps. Lasso falls somewhere between ridge regression and best\\nsubset regression, and enjoys some of the properties of each.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce0f131a-83e2-4257-b911-025996304c74', embedding=None, metadata={'page_label': '102', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83\\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\\n0Ridge\\nLasso\\nBest SubsetPLS PCR\\n•\\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\\nRidge\\nBest Subset\\nPLS\\nPCRLasso•\\n0ρ= 0.5\\nρ=−0.5\\nβ1β1β2 β2\\nFIGURE 3.18. Coeﬃcient proﬁles from diﬀerent methods for a simple problem:\\ntwo inputs with correlation ±0.5, and the true regression coeﬃcients β= (4,2).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='159c5a6d-c46b-475b-8bf2-61efe37176a7', embedding=None, metadata={'page_label': '103', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='84 3. Linear Methods for Regression\\n3.7 Multiple Outcome Shrinkage and Selection\\nAs noted in Section 3.2.4, the least squares estimates in a multiple-output\\nlinear model are simply the individual least squares estimates for each of\\nthe outputs.\\nTo apply selection and shrinkage methods in the multiple output case,\\none could apply a univariate technique individually to each outcome or si-\\nmultaneously to all outcomes. With ridge regression, for example, we could\\napply formula (3.44) to each of the Kcolumns of the outcome matrix Y,\\nusing possibly diﬀerent parameters λ, or apply it to all columns using the\\nsame value of λ. The former strategy would allow diﬀerent amounts of\\nregularization to be applied to diﬀerent outcomes but require estimation\\nofkseparate regularization parameters λ1,... ,λ k, while the latter would\\npermit all koutputs to be used in estimating the sole regularization pa-\\nrameter λ.\\nOther more sophisticated shrinkage and selection strategies that exploit\\ncorrelations in the diﬀerent responses can be helpful in the multiple output\\ncase. Suppose for example that among the outputs we have\\nYk=f(X) +εk (3.65)\\nYℓ=f(X) +εℓ; (3.66)\\ni.e., (3.65) and (3.66) share the same structural part f(X) in their models.\\nIt is clear in this case that we should pool our observations on YkandYl\\nto estimate the common f.\\nCombining responses is at the heart of canonical correlation analysis\\n(CCA), a data reduction technique developed for the multiple output case.\\nSimilar to PCA, CCA ﬁnds a sequence of uncorrelated linear combina-\\ntionsXvm, m= 1,... ,M of the xj, and a corresponding sequence of\\nuncorrelated linear combinations Yumof the responses yk, such that the\\ncorrelations\\nCorr2(Yum,Xvm) (3.67)\\nare successively maximized. Note that at most M= min( K,p) directions\\ncan be found. The leading canonical response variates are those linear com-\\nbinations (derived responses) best predicted by the xj; in contrast, the\\ntrailing canonical variates can be poorly predicted by the xj, and are can-\\ndidates for being dropped. The CCA solution is computed using a general-\\nized SVD of the sample cross-covariance matrix YTX/N(assuming Yand\\nXare centered; Exercise 3.20).\\nReduced-rank regression (Izenman, 1975; van der Merwe and Zidek, 1980)\\nformalizes this approach in terms of a regression model that explicitly pool s\\ninformation. Given an error covariance Cov( ε) =Σ, we solve the following', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a1fce183-d675-4963-b0b5-a2f240102445', embedding=None, metadata={'page_label': '104', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.7 Multiple Outcome Shrinkage and Selection 85\\nrestricted multivariate regression problem:\\nˆBrr(m) = argmin\\nrank(B)=mN∑\\ni=1(yi−BTxi)TΣ−1(yi−BTxi). (3.68)\\nWithΣreplaced by the estimate YTY/N, one can show (Exercise 3.21)\\nthat the solution is given by a CCA of YandX:\\nˆBrr(m) =ˆBUmU−\\nm, (3.69)\\nwhereUmis the K×msub-matrix of Uconsisting of the ﬁrst mcolumns,\\nandUis the K×Mmatrix of leftcanonical vectors u1,u2,... ,u M.U−\\nm\\nis its generalized inverse. Writing the solution as\\nˆBrr(M) = (XTX)−1XT(YU m)U−\\nm, (3.70)\\nwe see that reduced-rank regression performs a linear regression on the\\npooled response matrix YU m, and then maps the coeﬃcients (and hence\\nthe ﬁts as well) back to the original response space. The reduced-rank ﬁts\\nare given by\\nˆYrr(m) =X(XTX)−1XTYU mU−\\nm\\n=HYP m,(3.71)\\nwhere His the usual linear regression projection operator, and Pmis the\\nrank-mCCA response projection operator. Although a better estimate of\\nΣwould be ( Y−XˆB)T(Y−XˆB)/(N−pK), one can show that the solution\\nremains the same (Exercise 3.22).\\nReduced-rank regression borrows strength among responses by truncat-\\ning the CCA. Breiman and Friedman (1997) explored with some success\\nshrinkage of the canonical variates between XandY, a smooth version of\\nreduced rank regression. Their proposal has the form (compare (3.69))\\nˆBc+w=ˆBUΛU−1, (3.72)\\nwhere Λis a diagonal shrinkage matrix (the “c+w” stands for “Curds\\nand Whey,” the name they gave to their procedure). Based on optimal\\nprediction in the population setting, they show that Λhas diagonal entries\\nλm=c2\\nm\\nc2m+p\\nN(1−c2m), m= 1,... ,M, (3.73)\\nwhere cmis the mth canonical correlation coeﬃcient. Note that as the ratio\\nof the number of input variables to sample size p/Ngets small, the shrink-\\nage factors approach 1. Breiman and Friedman (1997) proposed modiﬁed\\nversions of Λbased on training data and cross-validation, but the general\\nform is the same. Here the ﬁtted response has the form\\nˆYc+w=HYSc+w, (3.74)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c3401dd-3ad8-4e53-945e-c865eda7fcfe', embedding=None, metadata={'page_label': '105', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='86 3. Linear Methods for Regression\\nwhere Sc+w=UΛU−1is the response shrinkage operator.\\nBreiman and Friedman (1997) also suggested shrinking in both the Y\\nspace and Xspace. This leads to hybrid shrinkage models of the form\\nˆYridge,c+w=AλYSc+w, (3.75)\\nwhereAλ=X(XTX+λI)−1XTis the ridge regression shrinkage operator,\\nas in (3.46) on page 66. Their paper and the discussions thereof contain\\nmany more details.\\n3.8 More on the Lasso and Related Path\\nAlgorithms\\nSince the publication of the LAR algorithm (Efron et al., 2004) there has\\nbeen a lot of activity in developing algorithms for ﬁtting regularization\\npaths for a variety of diﬀerent problems. In addition, L1regularization has\\ntaken on a life of its own, leading to the development of the ﬁeld compressed\\nsensing in the signal-processing literature. (Donoho, 2006a; Candes, 2006).\\nIn this section we discuss some related proposals and other path algorithms,\\nstarting oﬀ with a precursor to the LAR algorithm.\\n3.8.1 Incremental Forward Stagewise Regression\\nHere we present another LAR-like algorithm, this time focused on forward\\nstagewise regression. Interestingly, eﬀorts to understand a ﬂexible nonlinear\\nregression procedure (boosting) led to a new algorithm for linear models\\n(LAR). In reading the ﬁrst edition of this book and the forward stagewise\\nAlgorithm 3.4 Incremental Forward Stagewise Regression—FS ǫ.\\n1. Start with the residual requal to yandβ1,β2,... ,β p= 0. All the\\npredictors are standardized to have mean zero and unit norm.\\n2. Find the predictor xjmost correlated with r\\n3. Update βj←βj+δj, where δj=ǫ≤sign[⟨xj,r⟩] and ǫ >0 is a small\\nstep size, and set r←r−δjxj.\\n4. Repeat steps 2 and 3 many times, until the residuals are uncorrelated\\nwith all the predictors.\\nAlgorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with\\n4In the ﬁrst edition, this was Algorithm 10.4 in Chapter 10.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='03fc3f97-5d48-4c56-b8df-97978e623e90', embedding=None, metadata={'page_label': '106', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 More on the Lasso and Related Path Algorithms 87−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0 50 100 150 200\\n−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0.0 0.5 1.0 1.5 2.0FSǫ FS0\\nIteration\\nCoeﬃcientsCoeﬃcients\\nL1Arc-length of Coeﬃcients\\nFIGURE 3.19. Coeﬃcient proﬁles for the prostate data. The left panel shows\\nincremental forward stagewise regression with step size ǫ= 0.01. The right panel\\nshows the inﬁnitesimal version FS 0obtained letting ǫ→0. This proﬁle was ﬁt by\\nthe modiﬁcation 3.2b to the LAR Algorithm 3.2. In this example t he FS 0proﬁles\\nare monotone, and hence identical to those of lasso and LAR.\\nlinear models, one could explicitly construct the piecewise-linear lasso paths\\nof Figure 3.10. This led him to propose the LAR procedure of Section 3.4.4,\\nas well as the incremental version of forward-stagewise regression presented\\nhere.\\nConsider the linear-regression version of the forward-stagewise boosting\\nalgorithm 16.1 proposed in Section 16.1 (page 608). It generates a coeﬃcient\\nproﬁle by repeatedly updating (by a small amount ǫ) the coeﬃcient of the\\nvariable most correlated with the current residuals. Algorithm 3.4 gives\\nthe details. Figure 3.19 (left panel) shows the progress of the algorithm on\\nthe prostate data with step size ǫ= 0.01. If δj=⟨xj,r⟩(the least-squares\\ncoeﬃcient of the residual on jth predictor), then this is exactly the usual\\nforward stagewise procedure (FS) outlined in Section 3.3.3.\\nHere we are mainly interested in small values of ǫ. Letting ǫ→0 gives\\nthe right panel of Figure 3.19, which in this case is identical to the lasso\\npath in Figure 3.10. We call this limiting procedure inﬁnitesimal forward\\nstagewise regression or FS 0. This procedure plays an important role in\\nnon-linear, adaptive methods like boosting (Chapters 10 and 16) and is the\\nversion of incremental forward stagewise regression that is most amenabl e\\nto theoretical analysis. B¨ uhlmann and Hothorn (2007) refer to the same\\nprocedure as “L2boost”, because of its connections to boosting.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34fe1fd1-fb5e-4a80-ab03-01bc256cf7f1', embedding=None, metadata={'page_label': '107', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='88 3. Linear Methods for Regression\\nEfron originally thought that the LAR Algorithm 3.2 was an implemen-\\ntation of FS 0, allowing each tied predictor a chance to update their coeﬃ-\\ncients in a balanced way, while remaining tied in correlation. However, he\\nthen realized that the LAR least-squares ﬁt amongst the tied predictors\\ncan result in coeﬃcients moving in the opposite direction to their correla-\\ntion, which cannot happen in Algorithm 3.4. The following modiﬁcation of\\nthe LAR algorithm implements FS 0:\\nAlgorithm 3.2b Least Angle Regression: FS 0Modiﬁcation .\\n4. Find the new direction by solving the constrained least squares prob-\\nlem\\nmin\\nb||r−XAb||2\\n2subject to bjsj≥0, j∈ A,\\nwhere sjis the sign of ⟨xj,r⟩.\\nThe modiﬁcation amounts to a non-negative least squares ﬁt, keeping the\\nsigns of the coeﬃcients the same as those of the correlations. One can show\\nthat this achieves the optimal balancing of inﬁnitesimal “update turns”\\nfor the variables tied for maximal correlation (Hastie et al., 2007) . Like\\nlasso, the entire FS 0path can be computed very eﬃciently via the LAR\\nalgorithm.\\nAs a consequence of these results, if the LAR proﬁles are monotone non-\\nincreasing or non-decreasing, as they are in Figure 3.19, then all three\\nmethods—LAR, lasso, and FS 0—give identical proﬁles. If the proﬁles are\\nnot monotone but do not cross the zero axis, then LAR and lasso are\\nidentical.\\nSince FS 0is diﬀerent from the lasso, it is natural to ask if it optimizes\\na criterion. The answer is more complex than for lasso; the FS 0coeﬃcient\\nproﬁle is the solution to a diﬀerential equation. While the lasso makes op-\\ntimal progress in terms of reducing the residual sum-of-squares per unit\\nincrease in L1-norm of the coeﬃcient vector β, FS0is optimal per unit\\nincrease in L1arc-length traveled along the coeﬃcient path. Hence its co-\\neﬃcient path is discouraged from changing directions too often.\\nFS0is more constrained than lasso, and in fact can be viewed as a mono-\\ntone version of the lasso; see Figure 16.3 on page 614 for a dramatic exa m-\\nple. FS 0may be useful in p≫Nsituations, where its coeﬃcient proﬁles\\nare much smoother and hence have less variance than those of lasso. More\\ndetails on FS 0are given in Section 16.2.3 and Hastie et al. (2007). Fig-\\nure 3.16 includes FS 0where its performance is very similar to that of the\\nlasso.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='412fb64e-3a61-4cf2-b369-8dba792ed5e5', embedding=None, metadata={'page_label': '108', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 More on the Lasso and Related Path Algorithms 89\\n3.8.2 Piecewise-Linear Path Algorithms\\nThe least angle regression procedure exploits the piecewise linear nature of\\nthe lasso solution paths. It has led to similar “path algorithms” for o ther\\nregularized problems. Suppose we solve\\nˆβ(λ) = argminβ[R(β) +λJ(β)], (3.76)\\nwith\\nR(β) =N∑\\ni=1L(yi,β0+p∑\\nj=1xijβj), (3.77)\\nwhere both the loss function Land the penalty function Jare convex.\\nThen the following are suﬃcient conditions for the solution path ˆβ(λ) to\\nbe piecewise linear (Rosset and Zhu, 2007):\\n1.Ris quadratic or piecewise-quadratic as a function of β, and\\n2.Jis piecewise linear in β.\\nThis also implies (in principle) that the solution path can be eﬃciently\\ncomputed. Examples include squared- and absolute-error loss, “Huberized”\\nlosses, and the L1,L∞penalties on β. Another example is the “hinge loss”\\nfunction used in the support vector machine. There the loss is piecewise\\nlinear, and the penalty is quadratic. Interestingly, this leads to a piecewise-\\nlinear path algorithm in the dual space ; more details are given in Sec-\\ntion 12.3.5.\\n3.8.3 The Dantzig Selector\\nCandes and Tao (2007) proposed the following criterion:\\nminβ||β||1subject to ||XT(y−Xβ)||∞≤s. (3.78)\\nThey call the solution the Dantzig selector (DS). It can be written equiva-\\nlently as\\nminβ||XT(y−Xβ)||∞subject to ||β||1≤t. (3.79)\\nHere|| ≤ ||∞denotes the L∞norm, the maximum absolute value of the\\ncomponents of the vector. In this form it resembles the lasso, replacing\\nsquared error loss by the maximum absolute value of its gradient. Note\\nthat as tgets large, both procedures yield the least squares solution if\\nN < p . Ifp≥N, they both yield the least squares solution with minimum\\nL1norm. However for smaller values of t, the DS procedure produces a\\ndiﬀerent path of solutions than the lasso.\\nCandes and Tao (2007) show that the solution to DS is a linear pro-\\ngramming problem; hence the name Dantzig selector, in honor of the late', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='298f2767-e784-47b9-8a06-9e8abff91c61', embedding=None, metadata={'page_label': '109', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='90 3. Linear Methods for Regression\\nGeorge Dantzig, the inventor of the simplex method for linear program-\\nming. They also prove a number of interesting mathematical properties for\\nthe method, related to its ability to recover an underlying sparse coeﬃ-\\ncient vector. These same properties also hold for the lasso, as shown later\\nby Bickel et al. (2008).\\nUnfortunately the operating properties of the DS method are somewhat\\nunsatisfactory. The method seems similar in spirit to the lasso, especiall y\\nwhen we look at the lasso’s stationary conditions (3.58). Like the LAR a l-\\ngorithm, the lasso maintains the same inner product (and correlation) with\\nthe current residual for all variables in the active set, and moves their co-\\neﬃcients to optimally decrease the residual sum of squares. In the process,\\nthis common correlation is decreased monotonically (Exercise 3.23), and at\\nall times this correlation is larger than that for non-active variables. The\\nDantzig selector instead tries to minimize the maximum inner product of\\nthe current residual with all the predictors. Hence it can achieve a smaller\\nmaximum than the lasso, but in the process a curious phenomenon can\\noccur. If the size of the active set is m, there will be mvariables tied with\\nmaximum correlation. However, these need not coincide with the active set!\\nHence it can include a variable in the model that has smaller correlation\\nwith the current residual than some of the excluded variables (Efron et\\nal., 2007). This seems unreasonable and may be responsible for its some-\\ntimes inferior prediction accuracy. Efron et al. (2007) also show that DS\\ncan yield extremely erratic coeﬃcient paths as the regularization parameter\\nsis varied.\\n3.8.4 The Grouped Lasso\\nIn some problems, the predictors belong to pre-deﬁned groups; for example\\ngenes that belong to the same biological pathway, or collections of indicator\\n(dummy) variables for representing the levels of a categorical predictor. In\\nthis situation it may be desirable to shrink and select the members of a\\ngroup together. The grouped lasso is one way to achieve this. Suppose that\\ntheppredictors are divided into Lgroups, with pℓthe number in group\\nℓ. For ease of notation, we use a matrix Xℓto represent the predictors\\ncorresponding to the ℓth group, with corresponding coeﬃcient vector βℓ.\\nThe grouped-lasso minimizes the convex criterion\\nmin\\nβ∈I Rp(\\n||y−β01−L∑\\nℓ=1Xℓβℓ||2\\n2+λL∑\\nℓ=1√pℓ||βℓ||2)\\n, (3.80)\\nwhere the√pℓterms accounts for the varying group sizes, and || ≤ ||2is\\nthe Euclidean norm (not squared). Since the Euclidean norm of a vector\\nβℓis zero only if all of its components are zero, this procedure encourages\\nsparsity at both the group and individual levels. That is, for some values of\\nλ, an entire group of predictors may drop out of the model. This procedure', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f092a79-34f5-4ac4-a0b8-e5968950768d', embedding=None, metadata={'page_label': '110', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 More on the Lasso and Related Path Algorithms 91\\nwas proposed by Bakin (1999) and Lin and Zhang (2006), and studied and\\ngeneralized by Yuan and Lin (2007). Generalizations include more general\\nL2norms ||η||K= (ηTKη)1/2, as well as allowing overlapping groups of\\npredictors (Zhao et al., 2008). There are also connections to methods for\\nﬁtting sparse additive models (Lin and Zhang, 2006; Ravikumar et al.,\\n2008).\\n3.8.5 Further Properties of the Lasso\\nA number of authors have studied the ability of the lasso and related pro-\\ncedures to recover the correct model, as Nandpgrow. Examples of this\\nwork include Knight and Fu (2000), Greenshtein and Ritov (2004), Tropp\\n(2004), Donoho (2006b), Meinshausen (2007), Meinshausen and B¨ uhlmann\\n(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006), and Bunea\\net al. (2007). For example Donoho (2006b) focuses on the p > N case and\\nconsiders the lasso solution as the bound tgets large. In the limit this gives\\nthe solution with minimum L1norm among all models with zero training\\nerror. He shows that under certain assumptions on the model matrix X, if\\nthe true model is sparse, this solution identiﬁes the correct predictors with\\nhigh probability.\\nMany of the results in this area assume a condition on the model matrix\\nof the form\\n||(XSTXS)−1XSTXSc||∞≤(1−ǫ) for some ǫ∈(0,1]. (3.81)\\nHereSindexes the subset of features with non-zero coeﬃcients in the true\\nunderlying model, and XSare the columns of Xcorresponding to those\\nfeatures. Similarly Scare the features with true coeﬃcients equal to zero,\\nandXScthe corresponding columns. This says that the least squares coef-\\nﬁcients for the columns of XSconXSare not too large, that is, the “good”\\nvariables Sare not too highly correlated with the nuisance variables Sc.\\nRegarding the coeﬃcients themselves, the lasso shrinkage causes the esti-\\nmates of the non-zero coeﬃcients to be biased towards zero, and in general\\nthey are not consistent5. One approach for reducing this bias is to run\\nthe lasso to identify the set of non-zero coeﬃcients, and then ﬁt an un-\\nrestricted linear model to the selected set of features. This is not always\\nfeasible, if the selected set is large. Alternatively, one can use the lasso to\\nselect the set of non-zero predictors, and then apply the lasso again, but\\nusing only the selected predictors from the ﬁrst step. This is known as the\\nrelaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to\\nestimate the initial penalty parameter for the lasso, and then again for a\\nsecond penalty parameter applied to the selected set of predictors. Since\\n5Statistical consistency means as the sample size grows, the estimates converge to\\nthe true values.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee58535f-2900-42f0-af63-8da0418fd159', embedding=None, metadata={'page_label': '111', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='92 3. Linear Methods for Regression\\nthe variables in the second step have less “competition” from noise vari-\\nables, cross-validation will tend to pick a smaller value for λ, and hence\\ntheir coeﬃcients will be shrunken less than those in the initial estimate.\\nAlternatively, one can modify the lasso penalty function so that larger co-\\neﬃcients are shrunken less severely; the smoothly clipped absolute deviation\\n(SCAD) penalty of Fan and Li (2005) replaces λ|β|byJa(β,λ), where\\ndJa(β,λ)\\ndβ=λ≤sign(β)[\\nI(|β| ≤λ) +(aλ− |β|)+\\n(a−1)λI(|β|> λ)]\\n(3.82)\\nfor some a≥2. The second term in square-braces reduces the amount of\\nshrinkage in the lasso for larger values of β, with ultimately no shrinkage\\nasa→ ∞. Figure 3.20 shows the SCAD penalty, along with the lasso and\\n−4 −2 0 2 40 1 2 3 4 5\\n−4 −2 0 2 40.0 0.5 1.0 1.5 2.0 2.5\\n−4 −2 0 2 40.5 1.0 1.5 2.0|β| SCAD |β|1−ν\\nβ β β\\nFIGURE 3.20. The lasso and two alternative non-convex penalties designed to\\npenalize large coeﬃcients less. For SCAD we use λ= 1anda= 4, and ν=1\\n2in\\nthe last panel.\\n|β|1−ν. However this criterion is non-convex, which is a drawback since it\\nmakes the computation much more diﬃcult. The adaptive lasso (Zou, 2006)\\nuses a weighted penalty of the form∑p\\nj=1wj|βj|where wj= 1/|ˆβj|ν,ˆβjis\\nthe ordinary least squares estimate and ν >0. This is a practical approxi-\\nmation to the |β|qpenalties ( q= 1−νhere) discussed in Section 3.4.3. The\\nadaptive lasso yields consistent estimates of the parameters while retaining\\nthe attractive convexity property of the lasso.\\n3.8.6 Pathwise Coordinate Optimization\\nAn alternate approach to the LARS algorithm for computing the lasso\\nsolution is simple coordinate descent. This idea was proposed by Fu (1998)\\nand Daubechies et al. (2004), and later studied and generalized by Friedman\\net al. (2007), Wu and Lange (2008) and others. The idea is to ﬁx the penalty\\nparameter λin the Lagrangian form (3.52) and optimize successively over\\neach parameter, holding the other parameters ﬁxed at their current values.\\nSuppose the predictors are all standardized to have mean zero and unit\\nnorm. Denote by ˜βk(λ) the current estimate for βkat penalty parameter', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='827df7af-5e05-4b84-a644-1855cc0808f4', embedding=None, metadata={'page_label': '112', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.9 Computational Considerations 93\\nλ. We can rearrange (3.52) to isolate βj,\\nR(˜β(λ),βj) =1\\n2N∑\\ni=1(\\nyi−∑\\nk̸=jxik˜βk(λ)−xijβj)2\\n+λ∑\\nk̸=j|˜βk(λ)|+λ|βj|,\\n(3.83)\\nwhere we have suppressed the intercept and introduced a factor1\\n2for con-\\nvenience. This can be viewed as a univariate lasso problem with response\\nvariable the partial residual yi−˜y(j)\\ni=yi−∑\\nk̸=jxik˜βk(λ). This has an\\nexplicit solution, resulting in the update\\n˜βj(λ)←S(N∑\\ni=1xij(yi−˜y(j)\\ni),λ)\\n. (3.84)\\nHereS(t,λ) = sign( t)(|t|−λ)+is the soft-thresholding operator in Table 3.4\\non page 71. The ﬁrst argument to S(≤) is the simple least-squares coeﬃcient\\nof the partial residual on the standardized variable xij. Repeated iteration\\nof (3.84)—cycling through each variable in turn until convergence—yields\\nthe lasso estimate ˆβ(λ).\\nWe can also use this simple algorithm to eﬃciently compute the lasso\\nsolutions at a grid of values of λ. We start with the smallest value λmax\\nfor which ˆβ(λmax) = 0, decrease it a little and cycle through the variables\\nuntil convergence. Then λis decreased again and the process is repeated,\\nusing the previous solution as a “warm start” for the new value of λ. This\\ncan be faster than the LARS algorithm, especially in large problems. A\\nkey to its speed is the fact that the quantities in (3.84) can be updated\\nquickly as jvaries, and often the update is to leave ˜βj= 0. On the other\\nhand, it delivers solutions over a grid of λvalues, rather than the entire\\nsolution path. The same kind of algorithm can be applied to the elastic\\nnet, the grouped lasso and many other models in which the penalty is a\\nsum of functions of the individual parameters (Friedman et al., 2010). It\\ncan also be applied, with some substantial modiﬁcations, to the fused lasso\\n(Section 18.4.2); details are in Friedman et al. (2007).\\n3.9 Computational Considerations\\nLeast squares ﬁtting is usually done via the Cholesky decomposition of\\nthe matrix XTXor a QR decomposition of X. With Nobservations and p\\nfeatures, the Cholesky decomposition requires p3+Np2/2 operations, while\\nthe QR decomposition requires Np2operations. Depending on the relative\\nsize of Nandp, the Cholesky can sometimes be faster; on the other hand,\\nit can be less numerically stable (Lawson and Hansen, 1974). Computation\\nof the lasso via the LAR algorithm has the same order of computation as\\na least squares ﬁt.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c3b6752f-4674-48fa-ac40-595784cae230', embedding=None, metadata={'page_label': '113', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='94 3. Linear Methods for Regression\\nBibliographic Notes\\nLinear regression is discussed in many statistics books, for example, Seber\\n(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression wa s\\nintroduced by Hoerl and Kennard (1970), while the lasso was proposed by\\nTibshirani (1996). Around the same time, lasso-type penalties were pro-\\nposed in the basis pursuit method for signal processing (Chen et al., 1998).\\nThe least angle regression procedure was proposed in Efron et al. (2004);\\nrelated to this is the earlier homotopy procedure of Osborne et al. (2000a)\\nand Osborne et al. (2000b). Their algorithm also exploits the piecewise\\nlinearity used in the LAR/lasso algorithm, but lacks its transparency. The\\ncriterion for the forward stagewise criterion is discussed in Hastie et a l.\\n(2007). Park and Hastie (2007) develop a path algorithm similar to l east\\nangle regression for generalized regression models. Partial least squares\\nwas introduced by Wold (1975). Comparisons of shrinkage methods may\\nbe found in Copas (1983) and Frank and Friedman (1993).\\nExercises\\nEx. 3.1 Show that the Fstatistic (3.13) for dropping a single coeﬃcient\\nfrom a model is equal to the square of the corresponding z-score (3.12).\\nEx. 3.2 Given data on two variables XandY, consider ﬁtting a cubic\\npolynomial regression model f(X) =∑3\\nj=0βjXj. In addition to plotting\\nthe ﬁtted curve, you would like a 95% conﬁdence band about the curve.\\nConsider the following two approaches:\\n1. At each point x0, form a 95% conﬁdence interval for the linear func-\\ntionaTβ=∑3\\nj=0βjxj\\n0.\\n2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\\nconﬁdence intervals for f(x0).\\nHow do these approaches diﬀer? Which band is likely to be wider? Conduct\\na small simulation experiment to compare the two methods.\\nEx. 3.3 Gauss–Markov theorem:\\n(a) Prove the Gauss–Markov theorem: the least squares estimate of a\\nparameter aTβhas variance no bigger than that of any other linear\\nunbiased estimate of aTβ(Section 3.2.2).\\n(b) The matrix inequality B⪯Aholds if A−Bis positive semideﬁnite.\\nShow that if ˆVis the variance-covariance matrix of the least squares\\nestimate of βand˜Vis the variance-covariance matrix of any other\\nlinear unbiased estimate, then ˆV⪯˜V.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b5e3b869-d873-43fe-9b22-0cbc2a1320e2', embedding=None, metadata={'page_label': '114', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 95\\nEx. 3.4 Show how the vector of least squares coeﬃcients can be obtained\\nfrom a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Rep-\\nresent your solution in terms of the QR decomposition of X.\\nEx. 3.5 Consider the ridge regression problem (3.41). Show that this prob-\\nlem is equivalent to the problem\\nˆβc= argmin\\nβc{N∑\\ni=1[\\nyi−βc\\n0−p∑\\nj=1(xij−¯xj)βc\\nj]2+λp∑\\nj=1βc\\nj2}\\n.(3.85)\\nGive the correspondence between βcand the original βin (3.41). Char-\\nacterize the solution to this modiﬁed criterion. Show that a similar result\\nholds for the lasso.\\nEx. 3.6 Show that the ridge regression estimate is the mean (and mode)\\nof the posterior distribution, under a Gaussian prior β∼N(0,τI), and\\nGaussian sampling model y∼N(Xβ,σ2I). Find the relationship between\\nthe regularization parameter λin the ridge formula, and the variances τ\\nandσ2.\\nEx. 3.7 Assume yi∼N(β0+xT\\niβ,σ2),i= 1,2,... ,N , and the parameters\\nβjare each distributed as N(0,τ2), independently of one another. Assuming\\nσ2andτ2are known, show that the (minus) log-posterior density of βis\\nproportional to∑N\\ni=1(yi−β0−∑\\njxijβj)2+λ∑p\\nj=1β2\\njwhere λ=σ2/τ2.\\nEx. 3.8 Consider the QR decomposition of the uncentered N×(p+ 1)\\nmatrix X(whose ﬁrst column is all ones), and the SVD of the N×p\\ncentered matrix ˜X. Show that Q2andUspan the same subspace, where\\nQ2is the sub-matrix of Qwith the ﬁrst column removed. Under what\\ncircumstances will they be the same, up to sign ﬂips?\\nEx. 3.9 Forward stepwise regression. Suppose we have the QR decomposi-\\ntion for the N×qmatrix X1in a multiple regression problem with response\\ny, and we have an additional p−qpredictors in the matrix X2. Denote the\\ncurrent residual by r. We wish to establish which one of these additional\\nvariables will reduce the residual-sum-of squares the most when included\\nwith those in X1. Describe an eﬃcient procedure for doing this.\\nEx. 3.10 Backward stepwise regression. Suppose we have the multiple re-\\ngression ﬁt of yonX, along with the standard errors and Z-scores as in\\nTable 3.2. We wish to establish which variable, when dropped, will increase\\nthe residual sum-of-squares the least. How would you do this?\\nEx. 3.11 Show that the solution to the multivariate linear regression prob-\\nlem (3.40) is given by (3.39). What happens if the covariance matrices Σi\\nare diﬀerent for each observation?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f98437f-d5c4-412a-8932-307745cccbe8', embedding=None, metadata={'page_label': '115', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='96 3. Linear Methods for Regression\\nEx. 3.12 Show that the ridge regression estimates can be obtained by\\nordinary least squares regression on an augmented data set. We augment\\nthe centered matrix Xwithpadditional rows√\\nλI, and augment ywithp\\nzeros. By introducing artiﬁcial data having response value zero, the ﬁtting\\nprocedure is forced to shrink the coeﬃcients toward zero. This is related to\\nthe idea of hintsdue to Abu-Mostafa (1995), where model constraints are\\nimplemented by adding artiﬁcial data examples that satisfy them.\\nEx. 3.13 Derive the expression (3.62), and show that ˆβpcr(p) =ˆβls.\\nEx. 3.14 Show that in the orthogonal case, PLS stops after m= 1 steps,\\nbecause subsequent ˆ ϕmjin step 2 in Algorithm 3.3 are zero.\\nEx. 3.15 Verify expression (3.64), and hence show that the partial least\\nsquares directions are a compromise between the ordinary regression coef-\\nﬁcient and the principal component directions.\\nEx. 3.16 Derive the entries in Table 3.4, the explicit forms for estimators\\nin the orthogonal case.\\nEx. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in\\nChapter 1.\\nEx. 3.18 Read about conjugate gradient algorithms (Murray et al., 1981, for\\nexample), and establish a connection between these algorithms and partial\\nleast squares.\\nEx. 3.19 Show that ∥ˆβridge∥increases as its tuning parameter λ→0. Does\\nthe same property hold for the lasso and partial least squares estimates?\\nFor the latter, consider the “tuning parameter” to be the successive steps\\nin the algorithm.\\nEx. 3.20 Consider the canonical-correlation problem (3.67). Show that the\\nleading pair of canonical variates u1andv1solve the problem\\nmax\\nuT(YTY)u=1\\nvT(XTX)v=1uT(YTX)v, (3.86)\\na generalized SVD problem. Show that the solution is given by u1=\\n(YTY)−1\\n2u∗\\n1, and v1= (XTX)−1\\n2v∗\\n1, where u∗\\n1andv∗\\n1are the leading left\\nand right singular vectors in\\n(YTY)−1\\n2(YTX)(XTX)−1\\n2=U∗D∗V∗T. (3.87)\\nShow that the entire sequence um, vm, m= 1,... ,min(K,p) is also given\\nby (3.87).\\nEx. 3.21 Show that the solution to the reduced-rank regression problem\\n(3.68), with Σestimated by YTY/N, is given by (3.69). Hint: Transform', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='efaa9c6a-e01f-4de6-8cfd-62ff71667935', embedding=None, metadata={'page_label': '116', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 97\\nYtoY∗=YΣ−1\\n2, and solved in terms of the canonical vectors u∗\\nm. Show\\nthatUm=Σ−1\\n2U∗\\nm, and a generalized inverse is U−\\nm=U∗\\nmTΣ1\\n2.\\nEx. 3.22 Show that the solution in Exercise 3.21 does not change if Σis\\nestimated by the more natural quantity ( Y−XˆB)T(Y−XˆB)/(N−pK).\\nEx. 3.23 Consider a regression problem with all variables and response hav-\\ning mean zero and standard deviation one. Suppose also that each variable\\nhas identical absolute correlation with the response:\\n1\\nN|⟨xj,y⟩|=λ, j= 1,... ,p.\\nLetˆβbe the least-squares coeﬃcient of yonX, and let u(α) =αXˆβfor\\nα∈[0,1] be the vector that moves a fraction αtoward the least squares ﬁt\\nu. LetRSSbe the residual sum-of-squares from the full least squares ﬁt.\\n(a) Show that\\n1\\nN|⟨xj,y−u(α)⟩|= (1−α)λ, j= 1,... ,p,\\nand hence the correlations of each xjwith the residuals remain equal\\nin magnitude as we progress toward u.\\n(b) Show that these correlations are all equal to\\nλ(α) =(1−α)√\\n(1−α)2+α(2−α)\\nN≤RSS≤λ,\\nand hence they decrease monotonically to zero.\\n(c) Use these results to show that the LAR algorithm in Section 3.4.4\\nkeeps the correlations tied and monotonically decreasing, as claimed\\nin (3.55).\\nEx. 3.24 LAR directions. Using the notation around equation (3.55) on\\npage 74, show that the LAR direction makes an equal angle with each of\\nthe predictors in Ak.\\nEx. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2). Starting at the be-\\nginning of the kth step of the LAR algorithm, derive expressions to identify\\nthe next variable to enter the active set at step k+1, and the value of αat\\nwhich this occurs (using the notation around equation (3.55) on page 74).\\nEx. 3.26 Forward stepwise regression enters the variable at each step that\\nmost reduces the residual sum-of-squares. LAR adjusts variables that have\\nthe most (absolute) correlation with the current residuals. Show that these\\ntwo entry criteria are not necessarily the same. [Hint: let xj.Abe the jth', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d0ee2e4-69b8-498d-a976-5640168892c7', embedding=None, metadata={'page_label': '117', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='98 3. Linear Methods for Regression\\nvariable, linearly adjusted for all the variables currently in the model. Show\\nthat the ﬁrst criterion amounts to identifying the jfor which Cor( xj.A,r)\\nis largest in magnitude.\\nEx. 3.27 Lasso and LAR : Consider the lasso problem in Lagrange multiplier\\nform: with L(β) =1\\n2∑\\ni(yi−∑\\njxijβj)2, we minimize\\nL(β) +λ∑\\nj|βj| (3.88)\\nfor ﬁxed λ >0.\\n(a) Setting βj=β+\\nj−β−\\njwithβ+\\nj,β−\\nj≥0, expression (3.88) becomes\\nL(β) +λ∑\\nj(β+\\nj+β−\\nj). Show that the Lagrange dual function is\\nL(β) +λ∑\\nj(β+\\nj+β−\\nj)−∑\\njλ+\\njβ+\\nj−∑\\njλ−\\njβ−\\nj (3.89)\\nand the Karush–Kuhn–Tucker optimality conditions are\\n∇L(β)j+λ−λ+\\nj= 0\\n−∇L(β)j+λ−λ−\\nj= 0\\nλ+\\njβ+\\nj= 0\\nλ−\\njβ−\\nj= 0,\\nalong with the non-negativity constraints on the parameters and all\\nthe Lagrange multipliers.\\n(b) Show that |∇L(β)j| ≤λ∀j,and that the KKT conditions imply one\\nof the following three scenarios:\\nλ= 0⇒ ∇ L(β)j= 0∀j\\nβ+\\nj>0, λ > 0⇒λ+\\nj= 0,∇L(β)j=−λ <0, β−\\nj= 0\\nβ−\\nj>0, λ > 0⇒λ−\\nj= 0,∇L(β)j=λ >0, β+\\nj= 0.\\nHence show that for any “active” predictor having βj̸= 0, we must\\nhave∇L(β)j=−λifβj>0, and ∇L(β)j=λifβj<0. Assuming\\nthe predictors are standardized, relate λto the correlation between\\nthejth predictor and the current residuals.\\n(c) Suppose that the set of active predictors is unchanged for λ0≥λ≥λ1.\\nShow that there is a vector γ0such that\\nˆβ(λ) =ˆβ(λ0)−(λ−λ0)γ0 (3.90)\\nThus the lasso solution path is linear as λranges from λ0toλ1(Efron\\net al., 2004; Rosset and Zhu, 2007).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26ad088d-2a29-4111-8ff5-e2fc52012da4', embedding=None, metadata={'page_label': '118', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 99\\nEx. 3.28 Suppose for a given tin (3.51), the ﬁtted lasso coeﬃcient for\\nvariable Xjisˆβj=a. Suppose we augment our set of variables with an\\nidentical copy X∗\\nj=Xj. Characterize the eﬀect of this exact collinearity\\nby describing the set of solutions for ˆβjandˆβ∗\\nj, using the same value of t.\\nEx. 3.29 Suppose we run a ridge regression with parameter λon a single\\nvariable X, and get coeﬃcient a. We now include an exact copy X∗=X,\\nand reﬁt our ridge regression. Show that both coeﬃcients are identical, and\\nderive their value. Show in general that if mcopies of a variable Xjare\\nincluded in a ridge regression, their coeﬃcients are all the same.\\nEx. 3.30 Consider the elastic-net optimization problem:\\nmin\\nβ||y−Xβ||2+λ[\\nα||β||2\\n2+ (1−α)||β||1]\\n. (3.91)\\nShow how one can turn this into a lasso problem, using an augmented\\nversion of Xandy.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56e076f7-6e5d-45e2-b681-fe84be47dcc4', embedding=None, metadata={'page_label': '119', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='100 3. Linear Methods for Regression', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='809bb3e4-7df7-4c74-a935-0e39cb4fa39a', embedding=None, metadata={'page_label': '120', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 101\\nPrinter: Opaque this\\n4\\nLinear Methods for Classiﬁcation\\n4.1 Introduction\\nIn this chapter we revisit the classiﬁcation problem and focus on linear\\nmethods for classiﬁcation. Since our predictor G(x) takes values in a dis-\\ncrete set G, we can always divide the input space into a collection of regions\\nlabeled according to the classiﬁcation. We saw in Chapter 2 that the bound-\\naries of these regions can be rough or smooth, depending on the prediction\\nfunction. For an important class of procedures, these decision boundaries\\nare linear; this is what we will mean by linear methods for classiﬁcation.\\nThere are several diﬀerent ways in which linear decision boundaries can\\nbe found. In Chapter 2 we ﬁt linear regression models to the class indicator\\nvariables, and classify to the largest ﬁt. Suppose there are Kclasses, for\\nconvenience labeled 1 ,2,... ,K , and the ﬁtted linear model for the kth\\nindicator response variable is ˆfk(x) =ˆβk0+ˆβT\\nkx. The decision boundary\\nbetween class kandℓis that set of points for which ˆfk(x) =ˆfℓ(x), that is,\\nthe set {x: (ˆβk0−ˆβℓ0) + (ˆβk−ˆβℓ)Tx= 0}, an aﬃne set or hyperplane1\\nSince the same is true for any pair of classes, the input space is divided\\ninto regions of constant classiﬁcation, with piecewise hyperplanar decision\\nboundaries. This regression approach is a member of a class of methods\\nthat model discriminant functions δk(x) for each class, and then classify x\\nto the class with the largest value for its discriminant function. Methods\\n1Strictly speaking, a hyperplane passes through the origin, while an aﬃne set need\\nnot. We sometimes ignore the distinction and refer in genera l to hyperplanes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9026d9b6-6fe1-4b3a-8e3f-e3a17814a723', embedding=None, metadata={'page_label': '121', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='102 4. Linear Methods for Classiﬁcation\\nthat model the posterior probabilities Pr( G=k|X=x) are also in this\\nclass. Clearly, if either the δk(x) or Pr( G=k|X=x) are linear in x, then\\nthe decision boundaries will be linear.\\nActually, all we require is that some monotone transformation of δkor\\nPr(G=k|X=x) be linear for the decision boundaries to be linear. For\\nexample, if there are two classes, a popular model for the posterior proba-\\nbilities is\\nPr(G= 1|X=x) =exp(β0+βTx)\\n1 + exp( β0+βTx),\\nPr(G= 2|X=x) =1\\n1 + exp( β0+βTx).(4.1)\\nHere the monotone transformation is the logittransformation: log[ p/(1−p)],\\nand in fact we see that\\nlogPr(G= 1|X=x)\\nPr(G= 2|X=x)=β0+βTx. (4.2)\\nThe decision boundary is the set of points for which the log-odds are zero,\\nand this is a hyperplane deﬁned by{\\nx|β0+βTx= 0}\\n. We discuss two very\\npopular but diﬀerent methods that result in linear log-odds or logits: linear\\ndiscriminant analysis and linear logistic regression. Although they diﬀer in\\ntheir derivation, the essential diﬀerence between them is in the way the\\nlinear function is ﬁt to the training data.\\nA more direct approach is to explicitly model the boundaries between\\nthe classes as linear. For a two-class problem in a p-dimensional input\\nspace, this amounts to modeling the decision boundary as a hyperplane—in\\nother words, a normal vector and a cut-point. We will look at two methods\\nthat explicitly look for “separating hyperplanes.” The ﬁrst is the well-\\nknown perceptron model of Rosenblatt (1958), with an algorithm that ﬁnds\\na separating hyperplane in the training data, if one exists. The second\\nmethod, due to Vapnik (1996), ﬁnds an optimally separating hyperplane if\\none exists, else ﬁnds a hyperplane that minimizes some measure of overlap\\nin the training data. We treat the separable case here, and defer treatment\\nof the nonseparable case to Chapter 12.\\nWhile this entire chapter is devoted to linear decision boundaries, there is\\nconsiderable scope for generalization. For example, we can expand our vari-\\nable set X1,... ,X pby including their squares and cross-products X2\\n1,X2\\n2,... ,\\nX1X2,..., thereby adding p(p+1)/2 additional variables. Linear functions\\nin the augmented space map down to quadratic functions in the original\\nspace—hence linear decision boundaries to quadratic decision boundaries.\\nFigure 4.1 illustrates the idea. The data are the same: the left plot uses\\nlinear decision boundaries in the two-dimensional space shown, while the\\nright plot uses linear decision boundaries in the augmented ﬁve-dimensional\\nspace described above. This approach can be used with any basis transfor-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d69fac22-7232-4cc7-a485-f27b7485c444', embedding=None, metadata={'page_label': '122', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2 Linear Regression of an Indicator Matrix 103\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\nFIGURE 4.1. The left plot shows some data from three classes, with linear\\ndecision boundaries found by linear discriminant analysis. The ri ght plot shows\\nquadratic decision boundaries. These were obtained by ﬁnding line ar boundaries\\nin the ﬁve-dimensional space X1, X2, X1X2, X2\\n1, X2\\n2. Linear inequalities in this\\nspace are quadratic inequalities in the original space.\\nmation h(X) where h: IRp↦→IRqwithq > p, and will be explored in later\\nchapters.\\n4.2 Linear Regression of an Indicator Matrix\\nHere each of the response categories are coded via an indicator variable.\\nThus if GhasKclasses, there will be Ksuch indicators Yk, k= 1,... ,K ,\\nwithYk= 1 if G=kelse 0. These are collected together in a vector\\nY= (Y1,... ,Y K), and the Ntraining instances of these form an N×K\\nindicator response matrix Y.Yis a matrix of 0’s and 1’s, with each row\\nhaving a single 1. We ﬁt a linear regression model to each of the columns\\nofYsimultaneously, and the ﬁt is given by\\nˆY=X(XTX)−1XTY. (4.3)\\nChapter 3 has more details on linear regression. Note that we have a coeﬃ-\\ncient vector for each response column yk, and hence a ( p+1)×Kcoeﬃcient\\nmatrix ˆB= (XTX)−1XTY. HereXis the model matrix with p+1 columns\\ncorresponding to the pinputs, and a leading column of 1’s for the intercept.\\nA new observation with input xis classiﬁed as follows:\\n•compute the ﬁtted output ˆf(x)T= (1,xT)ˆB, aKvector;\\n•identify the largest component and classify accordingly:\\nˆG(x) = argmaxk∈Gˆfk(x). (4.4)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e45b73d9-1461-4ace-9e63-4ed516afbe3f', embedding=None, metadata={'page_label': '123', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='104 4. Linear Methods for Classiﬁcation\\nWhat is the rationale for this approach? One rather formal justiﬁcation\\nis to view the regression as an estimate of conditional expectation. For the\\nrandom variable Yk,E(Yk|X=x) = Pr( G=k|X=x), so conditional\\nexpectation of each of the Ykseems a sensible goal. The real issue is: how\\ngood an approximation to conditional expectation is the rather rigid linear\\nregression model? Alternatively, are the ˆfk(x) reasonable estimates of the\\nposterior probabilities Pr( G=k|X=x), and more importantly, does this\\nmatter?\\nIt is quite straightforward to verify that∑\\nk∈Gˆfk(x) = 1 for any x, as\\nlong as there is an intercept in the model (column of 1’s in X). However,\\ntheˆfk(x) can be negative or greater than 1, and typically some are. This\\nis a consequence of the rigid nature of linear regression, especially if we\\nmake predictions outside the hull of the training data. These violations in\\nthemselves do not guarantee that this approach will not work, and in fact\\non many problems it gives similar results to more standard linear meth-\\nods for classiﬁcation. If we allow linear regression onto basis expansions\\nh(X) of the inputs, this approach can lead to consistent estimates of the\\nprobabilities. As the size of the training set Ngrows bigger, we adaptively\\ninclude more basis elements so that linear regression onto these basis func-\\ntions approaches conditional expectation. We discuss such approaches in\\nChapter 5.\\nA more simplistic viewpoint is to construct targets tkfor each class,\\nwhere tkis the kth column of the K×Kidentity matrix. Our prediction\\nproblem is to try and reproduce the appropriate target for an observation.\\nWith the same coding as before, the response vector yi(ith row of Y) for\\nobservation ihas the value yi=tkifgi=k. We might then ﬁt the linear\\nmodel by least squares:\\nmin\\nBN∑\\ni=1||yi−[(1,xT\\ni)B]T||2. (4.5)\\nThe criterion is a sum-of-squared Euclidean distances of the ﬁtted vectors\\nfrom their targets. A new observation is classiﬁed by computing its ﬁtted\\nvector ˆf(x) and classifying to the closest target:\\nˆG(x) = argmin\\nk||ˆf(x)−tk||2. (4.6)\\nThis is exactly the same as the previous approach:\\n•The sum-of-squared-norm criterion is exactly the criterion for multi-\\nple response linear regression, just viewed slightly diﬀerently. Since\\na squared norm is itself a sum of squares, the components decouple\\nand can be rearranged as a separate linear model for each element.\\nNote that this is only possible because there is nothing in the model\\nthat binds the diﬀerent responses together.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ddd906b5-0ad4-4852-bec1-ce685ae6f128', embedding=None, metadata={'page_label': '124', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2 Linear Regression of an Indicator Matrix 105\\nLinear Regression\\n1\\n11\\n1\\n1\\n11111\\n11\\n1\\n11111\\n111\\n11 111\\n111\\n1111\\n111\\n1\\n11111\\n111\\n11\\n1111\\n11\\n11\\n11111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n1\\n1\\n11\\n111\\n111111\\n11\\n111\\n11 1\\n111\\n1\\n1\\n11\\n1\\n11\\n11\\n11\\n111\\n111\\n11\\n11\\n11\\n1\\n11\\n1111\\n11\\n1\\n111\\n11111\\n1\\n1111\\n11\\n111\\n111\\n111\\n1\\n11\\n1111\\n11\\n1\\n111 11\\n11\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n11\\n1\\n111\\n111 111\\n11\\n11\\n11\\n111111\\n1\\n11\\n11\\n11\\n1111\\n1111\\n111\\n1\\n11 1\\n111\\n1111\\n11\\n111\\n111\\n11\\n1\\n11\\n111\\n111\\n111\\n11\\n11\\n11\\n111\\n11 1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n111\\n1\\n11111\\n11111\\n1\\n11\\n111\\n1\\n11\\n111 1\\n11\\n1\\n11 11\\n1\\n111\\n111\\n11\\n1\\n111\\n1\\n1\\n1111\\n1111\\n11\\n11\\n1111\\n11\\n1\\n11\\n11\\n11\\n1 1\\n111\\n1 11\\n1\\n11\\n11\\n11 1\\n11\\n1\\n1111111\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n111\\n1111\\n11\\n11\\n11\\n1 1\\n11\\n11111\\n1\\n11\\n11\\n111\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n111\\n111\\n11\\n111\\n11\\n111\\n1\\n11\\n1\\n11\\n11\\n1\\n111\\n1111\\n11\\n1\\n11\\n1\\n11\\n1 12\\n22\\n2\\n222\\n22\\n222\\n2\\n22\\n222\\n222\\n222\\n2222\\n2\\n22222\\n2\\n2\\n222\\n2\\n222\\n2\\n222\\n2222\\n2\\n22\\n22\\n22\\n22222\\n2222\\n2\\n222\\n222\\n22\\n2\\n22\\n22\\n222\\n2\\n2222222\\n22\\n22\\n222\\n222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n2 2\\n22\\n2222\\n2\\n2\\n222\\n2222\\n22\\n222\\n222\\n222\\n222\\n22\\n222\\n2\\n2\\n222\\n222\\n2\\n22\\n2222\\n222\\n2222\\n22\\n222\\n2\\n2222\\n222\\n22\\n2\\n222\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22222\\n2222\\n22\\n222\\n22\\n22\\n22\\n2222\\n2222\\n2\\n22\\n22\\n22\\n222\\n22222\\n22\\n22\\n2\\n22\\n2\\n2\\n22\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n22\\n2\\n222\\n22\\n2222\\n2222\\n222\\n222\\n222\\n2222\\n2\\n222\\n2\\n22\\n222\\n222\\n22\\n222 22\\n222\\n222\\n22222\\n22 222\\n222\\n2\\n22\\n22 2\\n2\\n2222\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n222 22\\n2\\n222\\n2222\\n222\\n2\\n2 2222 2\\n22\\n22\\n2222\\n22\\n222\\n2\\n22\\n22\\n2\\n22\\n22\\n222\\n22\\n22\\n2222\\n22\\n2222\\n2222\\n2\\n222\\n22 2\\n22\\n2\\n22\\n22222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n2\\n2 22 222\\n22\\n22\\n222\\n22 22\\n222\\n22\\n22\\n22 22\\n22\\n223\\n33\\n33\\n33\\n3\\n3\\n333\\n3\\n33\\n3\\n333\\n33\\n3\\n33\\n333\\n33\\n333\\n3\\n333\\n3333\\n3\\n33\\n33\\n333\\n33\\n33\\n33\\n3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3333\\n333\\n333\\n3\\n333\\n3\\n3\\n33333333333\\n33\\n333\\n3\\n33\\n333\\n33\\n33\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n333\\n3\\n3333\\n333\\n33\\n33\\n33\\n333\\n33\\n3\\n33\\n33\\n33\\n33\\n33333\\n33\\n33\\n33\\n33\\n3 33\\n333\\n3\\n33 33\\n3\\n33\\n333\\n333333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3333\\n3\\n33\\n3\\n3\\n33333\\n333\\n3333\\n3\\n33\\n333\\n3\\n333\\n3\\n333\\n33\\n3\\n33\\n33\\n3333\\n333\\n33\\n33\\n33 3\\n3\\n3333\\n333\\n3\\n3\\n3\\n333\\n33\\n3\\n333\\n33\\n3\\n3333\\n33\\n3\\n33\\n3333\\n333\\n33\\n333\\n33\\n3\\n3\\n33\\n333\\n3\\n3\\n3333333\\n3333\\n33\\n3\\n3\\n33\\n3\\n3333\\n3\\n33\\n33 3\\n33\\n3\\n3333\\n333\\n33\\n3\\n3\\n333\\n33\\n3\\n3333\\n3\\n33Linear Discriminant Analysis\\n1\\n11\\n1\\n1\\n11111\\n11\\n1\\n11111\\n111\\n11 111\\n111\\n1111\\n111\\n1\\n11111\\n111\\n11\\n1111\\n11\\n11\\n11111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n1\\n1\\n11\\n111\\n111111\\n11\\n111\\n11 1\\n111\\n1\\n1\\n11\\n1\\n11\\n11\\n11\\n111\\n111\\n11\\n11\\n11\\n1\\n11\\n1111\\n11\\n1\\n111\\n11111\\n1\\n1111\\n11\\n111\\n111\\n111\\n1\\n11\\n1111\\n11\\n1\\n111 11\\n11\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n11\\n1\\n111\\n111 111\\n11\\n11\\n11\\n111111\\n1\\n11\\n11\\n11\\n1111\\n1111\\n111\\n1\\n11 1\\n111\\n1111\\n11\\n111\\n111\\n11\\n1\\n11\\n111\\n111\\n111\\n11\\n11\\n11\\n111\\n11 1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n111\\n1\\n11111\\n11111\\n1\\n11\\n111\\n1\\n11\\n111 1\\n11\\n1\\n11 11\\n1\\n111\\n111\\n11\\n1\\n111\\n1\\n1\\n1111\\n1111\\n11\\n11\\n1111\\n11\\n1\\n11\\n11\\n11\\n1 1\\n111\\n1 11\\n1\\n11\\n11\\n11 1\\n11\\n1\\n1111111\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n111\\n1111\\n11\\n11\\n11\\n1 1\\n11\\n11111\\n1\\n11\\n11\\n111\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n111\\n111\\n11\\n111\\n11\\n111\\n1\\n11\\n1\\n11\\n11\\n1\\n111\\n1111\\n11\\n1\\n11\\n1\\n11\\n1 12\\n22\\n2\\n222\\n22\\n222\\n2\\n22\\n222\\n222\\n222\\n2222\\n2\\n22222\\n2\\n2\\n222\\n2\\n222\\n2\\n222\\n2222\\n2\\n22\\n22\\n22\\n22222\\n2222\\n2\\n222\\n222\\n22\\n2\\n22\\n22\\n222\\n2\\n2222222\\n22\\n22\\n222\\n222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n2 2\\n22\\n2222\\n2\\n2\\n222\\n2222\\n22\\n222\\n222\\n222\\n222\\n22\\n222\\n2\\n2\\n222\\n222\\n2\\n22\\n2222\\n222\\n2222\\n22\\n222\\n2\\n2222\\n222\\n22\\n2\\n222\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22222\\n2222\\n22\\n222\\n22\\n22\\n22\\n2222\\n2222\\n2\\n22\\n22\\n22\\n222\\n22222\\n22\\n22\\n2\\n22\\n2\\n2\\n22\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n22\\n2\\n222\\n22\\n2222\\n2222\\n222\\n222\\n222\\n2222\\n2\\n222\\n2\\n22\\n222\\n222\\n22\\n222 22\\n222\\n222\\n22222\\n22 222\\n222\\n2\\n22\\n22 2\\n2\\n2222\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n222 22\\n2\\n222\\n2222\\n222\\n2\\n2 2222 2\\n22\\n22\\n2222\\n22\\n222\\n2\\n22\\n22\\n2\\n22\\n22\\n222\\n22\\n22\\n2222\\n22\\n2222\\n2222\\n2\\n222\\n22 2\\n22\\n2\\n22\\n22222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n2\\n2 22 222\\n22\\n22\\n222\\n22 22\\n222\\n22\\n22\\n22 22\\n22\\n223\\n33\\n33\\n33\\n3\\n3\\n333\\n3\\n33\\n3\\n333\\n33\\n3\\n33\\n333\\n33\\n333\\n3\\n333\\n3333\\n3\\n33\\n33\\n333\\n33\\n33\\n33\\n3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3333\\n333\\n333\\n3\\n333\\n3\\n3\\n33333333333\\n33\\n333\\n3\\n33\\n333\\n33\\n33\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n333\\n3\\n3333\\n333\\n33\\n33\\n33\\n333\\n33\\n3\\n33\\n33\\n33\\n33\\n33333\\n33\\n33\\n33\\n33\\n3 33\\n333\\n3\\n33 33\\n3\\n33\\n333\\n333333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3333\\n3\\n33\\n3\\n3\\n33333\\n333\\n3333\\n3\\n33\\n333\\n3\\n333\\n3\\n333\\n33\\n3\\n33\\n33\\n3333\\n333\\n33\\n33\\n33 3\\n3\\n3333\\n333\\n3\\n3\\n3\\n333\\n33\\n3\\n333\\n33\\n3\\n3333\\n33\\n3\\n33\\n3333\\n333\\n33\\n333\\n33\\n3\\n3\\n33\\n333\\n3\\n3\\n3333333\\n3333\\n33\\n3\\n3\\n33\\n3\\n3333\\n3\\n33\\n33 3\\n33\\n3\\n3333\\n333\\n33\\n3\\n3\\n333\\n33\\n3\\n3333\\n3\\n33\\nX1 X1\\nX2X2\\nFIGURE 4.2. The data come from three classes in I R2and are easily separated\\nby linear decision boundaries. The right plot shows the boundar ies found by linear\\ndiscriminant analysis. The left plot shows the boundaries found b y linear regres-\\nsion of the indicator response variables. The middle class is c ompletely masked\\n(never dominates).\\n•The closest target classiﬁcation rule (4.6) is easily seen to be exactly\\nthe same as the maximum ﬁtted component criterion (4.4), but does\\nrequire that the ﬁtted values sum to 1.\\nThere is a serious problem with the regression approach when the number\\nof classes K≥3, especially prevalent when Kis large. Because of the rigid\\nnature of the regression model, classes can be masked by others. Figure 4.2\\nillustrates an extreme situation when K= 3. The three classes are perfectly\\nseparated by linear decision boundaries, yet linear regression misses the\\nmiddle class completely.\\nIn Figure 4.3 we have projected the data onto the line joining the three\\ncentroids (there is no information in the orthogonal direction in this case),\\nand we have included and coded the three response variables Y1,Y2and\\nY3. The three regression lines (left panel) are included, and we see that\\nthe line corresponding to the middle class is horizontal and its ﬁtted values\\nare never dominant! Thus, observations from class 2 are classiﬁed either\\nas class 1 or class 3. The right panel uses quadratic regression rather than\\nlinear regression. For this simple example a quadratic rather than linear\\nﬁt (for the middle class at least) would solve the problem. However, it\\ncan be seen that if there were four rather than three classes lined up like\\nthis, a quadratic would not come down fast enough, and a cubic would\\nbe needed as well. A loose but general rule is that if K≥3 classes are\\nlined up, polynomial terms up to degree K−1 might be needed to resolve\\nthem. Note also that these are polynomials along the derived direction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36b88b8f-4a65-489b-b838-071ded5d0eb8', embedding=None, metadata={'page_label': '125', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='106 4. Linear Methods for Classiﬁcation\\n111\\n111\\n1\\n1\\n1111\\n11\\n11\\n1\\n11\\n11111\\n1\\n11111\\n11\\n1\\n11\\n11\\n11\\n11111\\n111\\n111\\n1\\n1111\\n111\\n111\\n111\\n111111\\n11\\n1\\n11\\n111\\n1111\\n11\\n111\\n111\\n111\\n111\\n111\\n1\\n11\\n11\\n111\\n111\\n11\\n11\\n1111\\n11\\n1\\n111\\n1\\n11\\n1\\n11\\n1\\n11\\n11\\n111\\n1\\n1111\\n111\\n111\\n12222 2222222 2 2 222 2 2 222222 222222 2 2 2222 22 22222 2222222222222 22 22222 222 2222222 2 22 222 2 22222 22 22 222 222222 22222222 22 2 222222222 22222222 222 222 2 222222222222222 2\\n3\\n3\\n33\\n3\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3\\n33333\\n3\\n33\\n333\\n33\\n333333\\n333333\\n3\\n3333\\n3\\n33\\n3\\n33\\n3\\n33\\n333\\n33\\n33\\n33333\\n33\\n3\\n3333\\n33\\n333\\n33\\n3333\\n33333\\n3333\\n33333\\n33\\n3\\n33\\n33\\n33\\n33\\n33\\n333\\n3\\n333\\n333\\n333\\n3333\\n3\\n3333\\n333\\n3\\n33333\\n0.00.51.0\\n0.0 0.2 0.4 0.6 0.8 1.0111\\n111\\n1\\n1\\n1111\\n11\\n11\\n1\\n11\\n11111\\n1\\n11\\n111\\n11\\n1\\n11\\n11\\n11\\n11111\\n11\\n1\\n111\\n1\\n1111\\n111\\n111\\n111\\n111111\\n111\\n11111\\n111111\\n111\\n111\\n111\\n111\\n111\\n1\\n1111111 111\\n111111111111\\n1111 1111111 11111111111111112\\n2\\n22\\n2\\n2222\\n22\\n22\\n22\\n222\\n222\\n2\\n2\\n22222\\n2\\n22\\n222\\n22\\n222222\\n222222\\n2\\n22222\\n2\\n222\\n222222222222222\\n22\\n22222222222222222 22 22\\n22 22\\n22\\n22\\n222\\n222\\n22\\n22\\n2222\\n22\\n2\\n222\\n2\\n22\\n2\\n22\\n2\\n22\\n22\\n222\\n2\\n222\\n222\\n222\\n2333333\\n333333\\n3333\\n33333333\\n333333\\n33\\n33333\\n3 333333333333 3333\\n33\\n3\\n33\\n3\\n33\\n3333 333\\n33333\\n33\\n3\\n333333333\\n33\\n3333\\n33333\\n3333\\n33333\\n33\\n3\\n33\\n33\\n33\\n33\\n33\\n333\\n3\\n333\\n333\\n333\\n3333\\n3\\n333\\n3\\n33\\n3\\n33333\\n0.00.51.0\\n0.0 0.2 0.4 0.6 0.8 1.0Degree = 1; Error = 0.33 Degree = 2; Error = 0.04\\nFIGURE 4.3. The eﬀects of masking on linear regression in I Rfor a three-class\\nproblem. The rug plot at the base indicates the positions and class membership of\\neach observation. The three curves in each panel are the ﬁtted re gressions to the\\nthree-class indicator variables; for example, for the blue cl ass,yblueis1for the\\nblue observations, and 0for the green and orange. The ﬁts are linear and quadratic\\npolynomials. Above each plot is the training error rate. The Bay es error rate is\\n0.025for this problem, as is the LDA error rate.\\npassing through the centroids, which can have arbitrary orientation. So in\\np-dimensional input space, one would need general polynomial terms and\\ncross-products of total degree K−1,O(pK−1) terms in all, to resolve such\\nworst-case scenarios.\\nThe example is extreme, but for large Kand small psuch maskings\\nnaturally occur. As a more realistic illustration, Figure 4.4 is a project ion\\nof the training data for a vowel recognition problem onto an informative\\ntwo-dimensional subspace. There are K= 11 classes in p= 10 dimensions.\\nThis is a diﬃcult classiﬁcation problem, and the best methods achieve\\naround 40% errors on the test data. The main point here is summarized in\\nTable 4.1; linear regression has an error rate of 67%, while a close relat ive,\\nlinear discriminant analysis, has an error rate of 56%. It seems that mask ing\\nhas hurt in this case. While all the other methods in this chapter are based\\non linear functions of xas well, they use them in such a way that avoids\\nthis masking problem.\\n4.3 Linear Discriminant Analysis\\nDecision theory for classiﬁcation (Section 2.4) tells us that we need to know\\nthe class posteriors Pr( G|X) for optimal classiﬁcation. Suppose fk(x) is\\nthe class-conditional density of Xin class G=k, and let πkbe the prior\\nprobability of class k, with∑K\\nk=1πk= 1. A simple application of Bayes', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c8c7cefa-c348-4ddf-90d9-765ff04b5212', embedding=None, metadata={'page_label': '126', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Linear Discriminant Analysis 107\\nCoordinate 1 for Training DataCoordinate 2 for Training Data\\n-4 -2 0 2 4-6 -4 -2 0 2 4oooo oo\\nooooooo\\no\\noooo\\noo\\no\\no\\no\\no\\noooooooooooo\\no\\no\\no\\no\\no ooooooo\\nooooooo\\no\\noo\\no\\nooooo\\no\\nooooooo\\noo\\noooo\\no\\no\\nooooooooooooo\\no\\noooooo\\no\\nooo\\noooo\\noooo\\no\\nooooo\\noooooo o\\no\\no\\noooooooooo\\no\\noo\\no\\noo\\nooooooooooooo\\noooooooooooooooooo\\noooooooooooo\\no\\no\\nooooooo\\no\\no\\nooooooo\\nooooo\\nooooooo\\no\\no\\noooooooooooooooo\\no\\no\\no\\no\\noooooooo\\noooo\\noo\\noooooooooo\\no\\nooo\\no\\no\\no\\noooo\\no\\no\\nooooo\\no\\no o\\no\\no\\no\\no\\nooooooooo oooo\\no\\no\\noooooo\\no\\noooooooo\\no\\noo\\no\\noooooooo\\no\\no\\no\\no\\nooo\\nooooooooooooooo\\nooo\\noooooooo\\no\\nooo\\nooooooooooooo\\no\\noooo\\nooooo\\nooooo\\no\\noo\\no\\no\\no\\no\\nooo\\nooo\\no\\nooooo\\no\\noooo\\no\\nooooooooooooo\\no ooooooooooooooooo\\no\\no\\nooooo\\nooooooooooo\\no\\no\\nooo\\nooo\\noo oo\\noooooo\\noooooo\\nooooooooo\\no\\no\\nooooooo\\noooooooooooo\\noo\\no\\no\\noo\\n••••••••••••••\\n••\\n••\\n••••Linear Discriminant Analysis\\nFIGURE 4.4. A two-dimensional plot of the vowel training data. There are\\neleven classes with X∈I R10, and this is the best view in terms of a LDA model\\n(Section 4.3.3). The heavy circles are the projected mean vec tors for each class.\\nThe class overlap is considerable.\\nTABLE 4.1. Training and test error rates using a variety of linear techniques\\non the vowel data. There are eleven classes in ten dimensions, o f which three\\naccount for 90%of the variance (via a principal components analysis). We see\\nthat linear regression is hurt by masking, increasing the test and training error\\nby over 10%.\\nTechnique Error Rates\\nTraining Test\\nLinear regression 0.48 0.67\\nLinear discriminant analysis 0.32 0.56\\nQuadratic discriminant analysis 0.01 0.53\\nLogistic regression 0.22 0.51', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='830247dc-9652-466a-8bf8-7a1d36b52a21', embedding=None, metadata={'page_label': '127', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='108 4. Linear Methods for Classiﬁcation\\ntheorem gives us\\nPr(G=k|X=x) =fk(x)πk∑K\\nℓ=1fℓ(x)πℓ. (4.7)\\nWe see that in terms of ability to classify, having the fk(x) is almost equiv-\\nalent to having the quantity Pr( G=k|X=x).\\nMany techniques are based on models for the class densities:\\n•linear and quadratic discriminant analysis use Gaussian densities;\\n•more ﬂexible mixtures of Gaussians allow for nonlinear decision bound-\\naries (Section 6.8);\\n•general nonparametric density estimates for each class density allow\\nthe most ﬂexibility (Section 6.6.2);\\n•Naive Bayes models are a variant of the previous case, and assume\\nthat each of the class densities are products of marginal densities;\\nthat is, they assume that the inputs are conditionally independent in\\neach class (Section 6.6.3).\\nSuppose that we model each class density as multivariate Gaussian\\nfk(x) =1\\n(2π)p/2|Σk|1/2e−1\\n2(x−θk)TΣ−1\\nk(x−θk). (4.8)\\nLinear discriminant analysis (LDA) arises in the special case when we\\nassume that the classes have a common covariance matrix Σk=Σ∀k. In\\ncomparing two classes kandℓ, it is suﬃcient to look at the log-ratio, and\\nwe see that\\nlogPr(G=k|X=x)\\nPr(G=ℓ|X=x)= logfk(x)\\nfℓ(x)+ logπk\\nπℓ\\n= logπk\\nπℓ−1\\n2(θk+θℓ)TΣ−1(θk−θℓ)\\n+xTΣ−1(θk−θℓ),(4.9)\\nan equation linear in x. The equal covariance matrices cause the normal-\\nization factors to cancel, as well as the quadratic part in the exponents.\\nThis linear log-odds function implies that the decision boundary between\\nclasses kandℓ—the set where Pr( G=k|X=x) = Pr( G=ℓ|X=x)—is\\nlinear in x; inpdimensions a hyperplane. This is of course true for any pair\\nof classes, so all the decision boundaries are linear. If we divide IRpinto\\nregions that are classiﬁed as class 1, class 2, etc., these regions will be sep-\\narated by hyperplanes. Figure 4.5 (left panel) shows an idealized example\\nwith three classes and p= 2. Here the data do arise from three Gaus-\\nsian distributions with a common covariance matrix. We have included in', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f4b1444-b701-4537-b9b5-cdbaca8472f8', embedding=None, metadata={'page_label': '128', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Linear Discriminant Analysis 109\\n+++\\n3\\n21\\n11\\n233\\n3\\n123\\n32\\n11211\\n33\\n12 1\\n23\\n23\\n3\\n12\\n211\\n1\\n13\\n222\\n21 3\\n2 23\\n13\\n13\\n32\\n13\\n3\\n23\\n133\\n2133\\n22\\n3\\n22\\n21\\n11\\n11\\n2\\n133\\n1\\n13\\n32\\n222 3\\n12\\nFIGURE 4.5. The left panel shows three Gaussian distributions, with the sa me\\ncovariance and diﬀerent means. Included are the contours of constant density\\nenclosing 95% of the probability in each case. The Bayes decision boundari es\\nbetween each pair of classes are shown (broken straight lines ), and the Bayes\\ndecision boundaries separating all three classes are the thic ker solid lines (a subset\\nof the former). On the right we see a sample of 30drawn from each Gaussian\\ndistribution, and the ﬁtted LDA decision boundaries.\\nthe ﬁgure the contours corresponding to 95% highest probability density,\\nas well as the class centroids. Notice that the decision boundaries are not\\nthe perpendicular bisectors of the line segments joining the centroids. This\\nwould be the case if the covariance Σwere spherical σ2I, and the class\\npriors were equal. From (4.9) we see that the linear discriminant functions\\nδk(x) =xTΣ−1θk−1\\n2θT\\nkΣ−1θk+ logπk (4.10)\\nare an equivalent description of the decision rule, with G(x) = argmaxkδk(x).\\nIn practice we do not know the parameters of the Gaussian distributions,\\nand will need to estimate them using our training data:\\n•ˆπk=Nk/N, where Nkis the number of class- kobservations;\\n•ˆθk=∑\\ngi=kxi/Nk;\\n•ˆΣ=∑K\\nk=1∑\\ngi=k(xi−ˆθk)(xi−ˆθk)T/(N−K).\\nFigure 4.5 (right panel) shows the estimated decision boundaries based on\\na sample of size 30 each from three Gaussian distributions. Figure 4.1 on\\npage 103 is another example, but here the classes are not Gaussian.\\nWith two classes there is a simple correspondence between linear dis-\\ncriminant analysis and classiﬁcation by linear least squares, as in (4.5) .\\nThe LDA rule classiﬁes to class 2 if\\nxTˆΣ−1(ˆθ2−ˆθ1)>1\\n2ˆθT\\n2ˆΣ−1ˆθ2−1\\n2ˆθT\\n1ˆΣ−1ˆθ1+ log( N1/N)−log(N2/N)\\n(4.11)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='65c6a88e-67e4-47dd-8d7b-6d2e0761b46f', embedding=None, metadata={'page_label': '129', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='110 4. Linear Methods for Classiﬁcation\\nand class 1 otherwise. Suppose we code the targets in the two classes as +1\\nand−1, respectively. It is easy to show that the coeﬃcient vector from least\\nsquares is proportional to the LDA direction given in (4.11) (Exercise 4. 2).\\n[In fact, this correspondence occurs for any (distinct) coding of the targets;\\nsee Exercise 4.2]. However unless N1=N2the intercepts are diﬀerent and\\nhence the resulting decision rules are diﬀerent.\\nSince this derivation of the LDA direction via least squares does not use a\\nGaussian assumption for the features, its applicability extends beyond the\\nrealm of Gaussian data. However the derivation of the particular intercept\\nor cut-point given in (4.11) doesrequire Gaussian data. Thus it makes\\nsense to instead choose the cut-point that empirically minimizes training\\nerror for a given dataset. This is something we have found to work well in\\npractice, but have not seen it mentioned in the literature.\\nWith more than two classes, LDA is not the same as linear regression of\\nthe class indicator matrix, and it avoids the masking problems associated\\nwith that approach (Hastie et al., 1994). A correspondence between regres-\\nsion and LDA can be established through the notion of optimal scoring ,\\ndiscussed in Section 12.5.\\nGetting back to the general discriminant problem (4.8), if the Σkare\\nnot assumed to be equal, then the convenient cancellations in (4.9) do not\\noccur; in particular the pieces quadratic in xremain. We then get quadratic\\ndiscriminant functions (QDA),\\nδk(x) =−1\\n2log|Σk| −1\\n2(x−θk)TΣ−1\\nk(x−θk) + log πk. (4.12)\\nThe decision boundary between each pair of classes kandℓis described by\\na quadratic equation {x:δk(x) =δℓ(x)}.\\nFigure 4.6 shows an example (from Figure 4.1 on page 103) where the\\nthree classes are Gaussian mixtures (Section 6.8) and the decision bound-\\naries are approximated by quadratic equations in x. Here we illustrate\\ntwo popular ways of ﬁtting these quadratic boundaries. The right plot\\nuses QDA as described here, while the left plot uses LDA in the enlarged\\nﬁve-dimensional quadratic polynomial space. The diﬀerences are generally\\nsmall; QDA is the preferred approach, with the LDA method a convenient\\nsubstitute2.\\nThe estimates for QDA are similar to those for LDA, except that separate\\ncovariance matrices must be estimated for each class. When pis large this\\ncan mean a dramatic increase in parameters. Since the decision boundaries\\nare functions of the parameters of the densities, counting the number of\\nparameters must be done with care. For LDA, it seems there are ( K−\\n1)×(p+ 1) parameters, since we only need the diﬀerences δk(x)−δK(x)\\n2For this ﬁgure and many similar ﬁgures in the book we compute t he decision bound-\\naries by an exhaustive contouring method. We compute the dec ision rule on a ﬁne lattice\\nof points, and then use contouring algorithms to compute the boundaries.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7ae10cff-45f2-43bb-a7dc-fabab00788a0', embedding=None, metadata={'page_label': '130', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Linear Discriminant Analysis 111\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\nFIGURE 4.6. Two methods for ﬁtting quadratic boundaries. The left plot show s\\nthe quadratic decision boundaries for the data in Figure 4.1 ( obtained using LDA\\nin the ﬁve-dimensional space X1, X2, X1X2, X2\\n1, X2\\n2). The right plot shows the\\nquadratic decision boundaries found by QDA. The diﬀerences are small, as is\\nusually the case.\\nbetween the discriminant functions where Kis some pre-chosen class (here\\nwe have chosen the last), and each diﬀerence requires p+ 1 parameters3.\\nLikewise for QDA there will be ( K−1)× {p(p+ 3)/2 + 1}parameters.\\nBoth LDA and QDA perform well on an amazingly large and diverse set\\nof classiﬁcation tasks. For example, in the STATLOG project (Michie et\\nal., 1994) LDA was among the top three classiﬁers for 7 of the 22 datasets,\\nQDA among the top three for four datasets, and one of the pair were in the\\ntop three for 10 datasets. Both techniques are widely used, and entire books\\nare devoted to LDA. It seems that whatever exotic tools are the rage of the\\nday, we should always have available these two simple tools. The question\\narises why LDA and QDA have such a good track record. The reason is not\\nlikely to be that the data are approximately Gaussian, and in addition for\\nLDA that the covariances are approximately equal. More likely a reason is\\nthat the data can only support simple decision boundaries such as linear or\\nquadratic, and the estimates provided via the Gaussian models are stable.\\nThis is a bias variance tradeoﬀ—we can put up with the bias of a linear\\ndecision boundary because it can be estimated with much lower variance\\nthan more exotic alternatives. This argument is less believable for QDA,\\nsince it can have many parameters itself, although perhaps fewer than the\\nnon-parametric alternatives.\\n3Although we ﬁt the covariance matrix ˆΣto compute the LDA discriminant functions,\\na much reduced function of it is all that is required to estima te the O(p) parameters\\nneeded to compute the decision boundaries.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2331bf3e-f447-487f-8267-760075afe785', embedding=None, metadata={'page_label': '131', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='112 4. Linear Methods for Classiﬁcation\\nMisclassification Rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Regularized Discriminant Analysis on the Vowel Data\\nTest Data\\nTrain Data\\nα\\nFIGURE 4.7. Test and training errors for the vowel data, using regularized\\ndiscriminant analysis with a series of values of α∈[0,1]. The optimum for the\\ntest data occurs around α= 0.9, close to quadratic discriminant analysis.\\n4.3.1 Regularized Discriminant Analysis\\nFriedman (1989) proposed a compromise between LDA and QDA, which\\nallows one to shrink the separate covariances of QDA toward a common\\ncovariance as in LDA. These methods are very similar in ﬂavor to ridge\\nregression. The regularized covariance matrices have the form\\nˆΣk(α) =αˆΣk+ (1−α)ˆΣ, (4.13)\\nwhere ˆΣis the pooled covariance matrix as used in LDA. Here α∈[0,1]\\nallows a continuum of models between LDA and QDA, and needs to be\\nspeciﬁed. In practice αcan be chosen based on the performance of the\\nmodel on validation data, or by cross-validation.\\nFigure 4.7 shows the results of RDA applied to the vowel data. Both\\nthe training and test error improve with increasing α, although the test\\nerror increases sharply after α= 0.9. The large discrepancy between the\\ntraining and test error is partly due to the fact that there are many repeat\\nmeasurements on a small number of individuals, diﬀerent in the training\\nand test set.\\nSimilar modiﬁcations allow ˆΣitself to be shrunk toward the scalar\\ncovariance,\\nˆΣ(γ) =γˆΣ+ (1−γ)ˆσ2I (4.14)\\nforγ∈[0,1]. Replacing ˆΣin (4.13) by ˆΣ(γ) leads to a more general family\\nof covariances ˆΣ(α,γ) indexed by a pair of parameters.\\nIn Chapter 12, we discuss other regularized versions of LDA, which are\\nmore suitable when the data arise from digitized analog signals and images.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb71d48b-aa26-4cac-a475-608a97d2af90', embedding=None, metadata={'page_label': '132', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Linear Discriminant Analysis 113\\nIn these situations the features are high-dimensional and correlated, and the\\nLDA coeﬃcients can be regularized to be smooth or sparse in the original\\ndomain of the signal. This leads to better generalization and allows for\\neasier interpretation of the coeﬃcients. In Chapter 18 we also deal with\\nvery high-dimensional problems, where for example the features are gene-\\nexpression measurements in microarray studies. There the methods focus\\non the case γ= 0 in (4.14), and other severely regularized versions of LDA.\\n4.3.2 Computations for LDA\\nAs a lead-in to the next topic, we brieﬂy digress on the computations\\nrequired for LDA and especially QDA. Their computations are simpliﬁed\\nby diagonalizing ˆΣorˆΣk. For the latter, suppose we compute the eigen-\\ndecomposition for each ˆΣk=UkDkUT\\nk, where Ukisp×porthonormal,\\nandDka diagonal matrix of positive eigenvalues dkℓ. Then the ingredients\\nforδk(x) (4.12) are\\n•(x−ˆθk)TˆΣ−1\\nk(x−ˆθk) = [UT\\nk(x−ˆθk)]TD−1\\nk[UT\\nk(x−ˆθk)];\\n•log|ˆΣk|=∑\\nℓlogdkℓ.\\nIn light of the computational steps outlined above, the LDA classiﬁer\\ncan be implemented by the following pair of steps:\\n•Sphere the data with respect to the common covariance estimate ˆΣ:\\nX∗←D−1\\n2UTX, where ˆΣ=UDUT. The common covariance esti-\\nmate of X∗will now be the identity.\\n•Classify to the closest class centroid in the transformed space, modulo\\nthe eﬀect of the class prior probabilities πk.\\n4.3.3 Reduced-Rank Linear Discriminant Analysis\\nSo far we have discussed LDA as a restricted Gaussian classiﬁer. Part of\\nits popularity is due to an additional restriction that allows us to view\\ninformative low-dimensional projections of the data.\\nTheKcentroids in p-dimensional input space lie in an aﬃne subspace\\nof dimension ≤K−1, and if pis much larger than K, this will be a con-\\nsiderable drop in dimension. Moreover, in locating the closest centroid, we\\ncan ignore distances orthogonal to this subspace, since they will contribute\\nequally to each class. Thus we might just as well project the X∗onto this\\ncentroid-spanning subspace HK−1, and make distance comparisons there.\\nThus there is a fundamental dimension reduction in LDA, namely, that we\\nneed only consider the data in a subspace of dimension at most K−1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1b8fcab-9cab-4a8a-b286-05792ed1b381', embedding=None, metadata={'page_label': '133', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='114 4. Linear Methods for Classiﬁcation\\nIfK= 3, for instance, this could allow us to view the data in a two-\\ndimensional plot, color-coding the classes. In doing so we would not have\\nrelinquished any of the information needed for LDA classiﬁcation.\\nWhat if K >3? We might then ask for a L < K −1 dimensional subspace\\nHL⊆HK−1optimal for LDA in some sense. Fisher deﬁned optimal to\\nmean that the projected centroids were spread out as much as possible in\\nterms of variance. This amounts to ﬁnding principal component subspaces\\nof the centroids themselves (principal components are described brieﬂy in\\nSection 3.5.1, and in more detail in Section 14.5.1). Figure 4.4 shows such an\\noptimal two-dimensional subspace for the vowel data. Here there are eleven\\nclasses, each a diﬀerent vowel sound, in a ten-dimensional input space. The\\ncentroids require the full space in this case, since K−1 =p, but we have\\nshown an optimal two-dimensional subspace. The dimensions are ordered,\\nso we can compute additional dimensions in sequence. Figure 4.8 shows four\\nadditional pairs of coordinates, also known as canonical ordiscriminant\\nvariables. In summary then, ﬁnding the sequences of optimal subspaces\\nfor LDA involves the following steps:\\n•compute the K×pmatrix of class centroids Mand the common\\ncovariance matrix W(forwithin-class covariance);\\n•compute M∗=MW−1\\n2using the eigen-decomposition of W;\\n•compute B∗, the covariance matrix of M∗(Bforbetween-class covari-\\nance), and its eigen-decomposition B∗=V∗DBV∗T. The columns\\nv∗\\nℓofV∗in sequence from ﬁrst to last deﬁne the coordinates of the\\noptimal subspaces.\\nCombining all these operations the ℓthdiscriminant variable is given by\\nZℓ=vT\\nℓXwithvℓ=W−1\\n2v∗\\nℓ.\\nFisher arrived at this decomposition via a diﬀerent route, without refer-\\nring to Gaussian distributions at all. He posed the problem:\\nFind the linear combination Z=aTXsuch that the between-\\nclass variance is maximized relative to the within-class var iance.\\nAgain, the between class variance is the variance of the class means of\\nZ, and the within class variance is the pooled variance about the means.\\nFigure 4.9 shows why this criterion makes sense. Although the direction\\njoining the centroids separates the means as much as possible (i.e., max-\\nimizes the between-class variance), there is considerable overlap between\\nthe projected classes due to the nature of the covariances. By taking the\\ncovariance into account as well, a direction with minimum overlap can be\\nfound.\\nThe between-class variance of ZisaTBaand the within-class variance\\naTWa, where Wis deﬁned earlier, and Bis the covariance matrix of the\\nclass centroid matrix M. Note that B+W=T, where Tis the total\\ncovariance matrix of X, ignoring class information.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='636d96bd-ed99-4430-82b4-0a7a4a4cbd00', embedding=None, metadata={'page_label': '134', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Linear Discriminant Analysis 115\\nCoordinate 1 Coordinate 3 \\n-4 -2 0 2 4-2 0 2o\\nooooo\\no\\no\\nooo\\nooo\\nooooooo\\no\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\no\\nooo\\nooooo\\noooooo ooooooo\\no\\no\\nooo oo\\no\\no\\nooo\\no\\no\\noooooooooooo\\noooo\\noooooo\\noo\\nooo\\noooooooo\\nooo\\noooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo\\noooooooo\\noooo\\noooooooooo\\noo\\nooo\\no\\nooo\\noooooo\\nooo\\noooooo\\no\\no\\noooooo\\noooooooooooo\\no\\nooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooooo\\no\\nooooo\\no\\nooooooo\\noooo\\no\\noooo\\noo\\nooooo\\noooo\\no\\noo\\nooooo\\no\\no\\no\\nooo\\no\\nooooo\\nooooooo\\no\\noooo\\noooooo\\noooo\\no\\no\\noo ooo\\noo\\nooo\\no\\noooo\\no\\nooooooo\\no\\noo\\noo\\no\\nooo\\nooooo\\no\\nooooo\\no\\noo\\no\\noo\\no\\noooooo\\no\\noo\\nooooooooooo\\noo\\noooooo\\noo\\no\\no\\noooo\\no\\nooooo\\no\\no\\no\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\no\\no\\nooo\\nooooooooooo\\noooo\\nooo\\noooooo\\no\\no\\noooooooo\\nooo\\noooooo\\no\\nooooo\\nooo\\no\\nooo\\no\\noooooooooo oooo ooooooo\\nooooooo\\n••••••••\\n••••••••••••••\\nCoordinate 2 Coordinate 3 \\n-6 -4 -2 0 2 4-2 0 2o\\nooooo\\no\\no\\nooo\\nooo\\nooooooo\\no\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\no\\nooo\\nooooo\\nooooooooooooo\\no\\no\\nooo oo\\no\\no\\nooo\\no\\no\\noooooooooooo\\noooo\\noooooo\\noo\\nooo\\noooooooo\\nooo\\noooo\\nooooo\\noooo\\noooo\\noo\\noooo\\no\\noo\\noooooooo\\noooo\\noooooooooo\\noo\\nooo\\no\\nooo\\noooooo\\nooo\\noooooo\\no\\no\\noooooo\\noooooooooooo\\no\\nooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooooo\\no\\nooooo\\no\\nooooooo\\noooo\\no\\noooo\\noo\\nooooo\\noooo\\no\\noo\\nooooo\\no\\no\\no\\nooo\\no\\nooooo\\nooooooo\\no\\noooo\\noooooo\\noooo\\no\\no\\noo ooo\\noo\\nooo\\no\\noooo\\no\\nooooooo\\no\\noo\\noo\\no\\nooo\\nooooo\\no\\nooooo\\no\\noo\\no\\noo\\no\\noooooo\\no\\noo\\nooooooooooo\\noo\\noooooo\\noo\\no\\no\\noooo\\no\\nooooo\\no\\no\\no\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\no\\no\\nooo\\nooooooooooo\\noo oo\\nooo\\noooooo\\no\\no\\noooooooo\\nooo\\noooooo\\no\\nooooo\\nooo\\no\\nooo\\no\\noooooooooo ooooooooooo\\nooooooo\\n••••••••\\n••••••••••••••\\nCoordinate 1 Coordinate 7 \\n-4 -2 0 2 4-3 -2 -1 0 1 2 3ooo ooo\\no\\nooooooo\\noooo\\nooooooooooooooo\\nooo\\nooo\\no\\no\\nooooooooooooo\\noooo\\no\\nooooo\\no\\nooooooo\\no\\no\\nooooo\\nooooooooooo\\nooo\\no\\noooo\\noooo\\noooo\\noo\\noo\\no\\nooo\\noooooooooo\\nooo\\nooooo\\noooooooooooo\\noo oooo\\noooooo\\noooooo\\noooooooo\\no\\nooooooooo\\no\\nooooooooooo\\no\\nooooo\\noooooo\\noo\\noooo\\nooooooooooo\\nooo o\\nooo\\nooooooooo\\nooo\\noooooo\\noooo\\noooooooo\\no\\noooooo\\nooo\\nooo\\no\\noooo\\nooooooooooo\\no\\noooooo\\nooooo\\nooooooooooooo\\no\\nooooooo\\noooo\\noooooooo\\noooo\\noooooo\\no\\nooooo\\nooooo\\no\\noooooo\\noooooooooooo\\nooooo\\nooooooooooooo\\nooo\\no\\nooo\\noo\\no\\noooooooo\\nooooooo\\nooo\\noo\\noooooooooooo\\nooooooooo\\nooo\\noo\\nooooo\\no\\nooo\\no\\no\\nooo\\no\\noo\\no\\noooooo\\nooooo\\noo\\noo\\no\\noooooo\\nooo\\noo\\nooooooooooo\\no\\noooooo\\no\\nooooooooooo\\noooo\\no\\noo\\n••••••••••••••••••••••\\nCoordinate 9 Coordinate 10 \\n-2 -1 0 1 2 3-2 -1 0 1 2oo\\no\\no\\noooooooo\\nooooo\\no\\nooooooo\\nooooo\\nooooo\\no\\nooooooo\\no\\no\\no\\noo\\noo\\no\\nooo\\noooooooooo\\no\\no\\noooooooooooo o\\noo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\noooooo o\\noo\\noooo\\noooo\\nooooooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\noooooooooooooooooo\\nooo\\nooo\\nooo\\noo\\nooooooo\\no\\noooo\\nooooo\\no\\nooooooooooo\\nooooo\\nooooooooooooo\\no\\noooo\\nooooo\\no\\noo\\no\\no\\noooo\\nooooo\\no\\nooo\\no\\noooo\\noo\\no\\noo\\noo\\noooo\\noooo\\no\\nooooooooooo\\no\\nooo\\noo\\no\\nooo\\no\\no oo\\noooooooo\\nooo\\nooooo\\nooooo\\no\\no\\nooo\\noooooooo\\nooo\\noo\\noo\\noooooooooooo\\no\\no\\no\\noo\\nooooooooo\\noooo\\no\\noooooooooo\\nooo\\no\\no\\no\\no\\no\\noo\\nooooooo\\no\\no\\noo\\no\\no\\noooooooooo\\noooo\\nooo\\noooooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooooo\\nooooooooooooo\\nooooo\\no\\no\\nooo\\nooooooo\\nooo\\no\\noo\\noooo\\nooooo\\noo\\no\\no\\no\\noooooooooooooooo\\nooo\\no\\no ooooo\\noo\\noo\\no\\no\\no\\no••••••••••••••••••••••Linear Discriminant Analysis\\nFIGURE 4.8. Four projections onto pairs of canonical variates. Notice that a s\\nthe rank of the canonical variates increases, the centroids becom e less spread out.\\nIn the lower right panel they appear to be superimposed, and the classes most\\nconfused.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='df8f7e47-7947-4e89-a568-b8b2a55ac5c1', embedding=None, metadata={'page_label': '135', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='116 4. Linear Methods for Classiﬁcation\\n++\\n++\\nFIGURE 4.9. Although the line joining the centroids deﬁnes the direction of\\ngreatest centroid spread, the projected data overlap becaus e of the covariance\\n(left panel). The discriminant direction minimizes this overla p for Gaussian data\\n(right panel).\\nFisher’s problem therefore amounts to maximizing the Rayleigh quotient ,\\nmax\\naaTBa\\naTWa, (4.15)\\nor equivalently\\nmax\\naaTBasubject to aTWa= 1. (4.16)\\nThis is a generalized eigenvalue problem, with agiven by the largest\\neigenvalue of W−1B. It is not hard to show (Exercise 4.1) that the optimal\\na1is identical to v1deﬁned above. Similarly one can ﬁnd the next direction\\na2, orthogonal in Wtoa1, such that aT\\n2Ba2/aT\\n2Wa2is maximized; the\\nsolution is a2=v2, and so on. The aℓare referred to as discriminant\\ncoordinates , not to be confused with discriminant functions. They are also\\nreferred to as canonical variates , since an alternative derivation of these\\nresults is through a canonical correlation analysis of the indicator response\\nmatrix Yon the predictor matrix X. This line is pursued in Section 12.5.\\nTo summarize the developments so far:\\n•Gaussian classiﬁcation with common covariances leads to linear deci-\\nsion boundaries. Classiﬁcation can be achieved by sphering the data\\nwith respect to W, and classifying to the closest centroid (modulo\\nlogπk) in the sphered space.\\n•Since only the relative distances to the centroids count, one can con-\\nﬁne the data to the subspace spanned by the centroids in the sphered\\nspace.\\n•This subspace can be further decomposed into successively optimal\\nsubspaces in term of centroid separation. This decomposition is iden-\\ntical to the decomposition due to Fisher.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='406aa5e9-141d-4fa4-815b-862f09c60610', embedding=None, metadata={'page_label': '136', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Linear Discriminant Analysis 117\\nDimensionMisclassification Rate\\n2 4 6 8 100.3 0.4 0.5 0.6 0.7LDA and Dimension Reduction on the Vowel Data\\n•\\n••••• • ••••\\n•\\n• •\\n•\\n•••••Test Data\\nTrain Data\\nFIGURE 4.10. Training and test error rates for the vowel data, as a function\\nof the dimension of the discriminant subspace. In this case the b est error rate is\\nfor dimension 2. Figure 4.11 shows the decision boundaries in this space.\\nThe reduced subspaces have been motivated as a data reduction (for\\nviewing) tool. Can they also be used for classiﬁcation, and what is the\\nrationale? Clearly they can, as in our original derivation; we simply limit\\nthe distance-to-centroid calculations to the chosen subspace. One can show\\nthat this is a Gaussian classiﬁcation rule with the additional restriction\\nthat the centroids of the Gaussians lie in a L-dimensional subspace of IRp.\\nFitting such a model by maximum likelihood, and then constructing the\\nposterior probabilities using Bayes’ theorem amounts to the classiﬁcation\\nrule described above (Exercise 4.8).\\nGaussian classiﬁcation dictates the log πkcorrection factor in the dis-\\ntance calculation. The reason for this correction can be seen in Figure 4.9.\\nThe misclassiﬁcation rate is based on the area of overlap between the two\\ndensities. If the πkare equal (implicit in that ﬁgure), then the optimal\\ncut-point is midway between the projected means. If the πkare not equal,\\nmoving the cut-point toward the smaller class will improve the error rate.\\nAs mentioned earlier for two classes, one can derive the linear rule using\\nLDA (or any other method), and then choose the cut-point to minimize\\nmisclassiﬁcation error over the training data.\\nAs an example of the beneﬁt of the reduced-rank restriction, we return\\nto the vowel data. There are 11 classes and 10 variables, and hence 10\\npossible dimensions for the classiﬁer. We can compute the training and\\ntest error in each of these hierarchical subspaces; Figure 4.10 shows the\\nresults. Figure 4.11 shows the decision boundaries for the classiﬁer based\\non the two-dimensional LDA solution.\\nThere is a close connection between Fisher’s reduced rank discriminant\\nanalysis and regression of an indicator response matrix. It turns out that', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2db955ee-1c0a-404a-afa2-87e21a2786ec', embedding=None, metadata={'page_label': '137', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='118 4. Linear Methods for Classiﬁcation\\noooo\\noo o\\noo\\noo\\no\\noo o\\noo\\noooo\\no\\noo\\nooooo\\noo\\nooo\\no\\nooo\\noooo\\noo\\noooo\\no\\nooo\\no\\noo\\nooo\\no\\no\\no\\nooo\\no\\noo\\noo oo\\noo\\nooo\\no\\noo\\noo\\noo\\no\\noo\\no\\noooo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\nooo\\no\\nooo\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\noo\\no\\nooo\\no\\no\\nooo\\no\\nooo\\noo\\noo\\noooo\\noo\\noo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\no o\\nooo\\no\\no\\no\\noo\\noooo\\noo\\noo\\no\\noo\\nooo\\no\\noo\\noo\\noo\\no oo\\nooo\\no\\noo\\noo\\noooo\\noo\\noo\\noo\\no\\noo\\no\\noo\\noooo\\no\\nooo\\noooooo\\nooo\\noo\\noo\\nooooo\\nooo\\no\\nooo\\no oo\\noo\\nooo\\no\\noooo\\noo\\no\\nooo\\no\\nooo\\nooooo\\no\\noo\\noo\\no oo\\noo\\noo\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\noooo\\no\\no\\noooo\\no\\noo\\noo\\no\\noo\\noo\\no\\noo\\nooo\\noooo\\noo\\noo\\nooo\\noo\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noo\\no\\noooo\\noooo\\no\\noo\\noo\\no\\nooo\\nooooo\\nooo\\noo\\noo\\noo\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\nooo\\no\\no o\\noo\\no o\\noo\\no\\noo\\no\\nooo\\noo\\nooo\\no\\noo\\no\\noo o\\nooo\\noo\\noo\\no\\noo\\noo\\noo\\noo\\nooo\\nooo\\no\\noo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\nooo\\noo\\noo\\no\\noo\\no\\no\\no\\nCanonical Coordinate 1Canonical Coordinate 2Classification in Reduced Subspace\\n••••••••••••••\\n••\\n••\\n••••\\nFIGURE 4.11. Decision boundaries for the vowel training data, in the two-di-\\nmensional subspace spanned by the ﬁrst two canonical variates. Note that in\\nany higher-dimensional subspace, the decision boundaries are h igher-dimensional\\naﬃne planes, and could not be represented as lines.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca866325-d9c0-477a-a76b-d7a87aa29913', embedding=None, metadata={'page_label': '138', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4 Logistic Regression 119\\nLDA amounts to the regression followed by an eigen-decomposition of\\nˆYTY. In the case of two classes, there is a single discriminant variable\\nthat is identical up to a scalar multiplication to either of the columns of ˆY.\\nThese connections are developed in Chapter 12. A related fact is that if one\\ntransforms the original predictors XtoˆY, then LDA using ˆYis identical\\nto LDA in the original space (Exercise 4.3).\\n4.4 Logistic Regression\\nThe logistic regression model arises from the desire to model the posterior\\nprobabilities of the Kclasses via linear functions in x, while at the same\\ntime ensuring that they sum to one and remain in [0 ,1]. The model has\\nthe form\\nlogPr(G= 1|X=x)\\nPr(G=K|X=x)=β10+βT\\n1x\\nlogPr(G= 2|X=x)\\nPr(G=K|X=x)=β20+βT\\n2x\\n...\\nlogPr(G=K−1|X=x)\\nPr(G=K|X=x)=β(K−1)0+βT\\nK−1x.(4.17)\\nThe model is speciﬁed in terms of K−1 log-odds or logit transformations\\n(reﬂecting the constraint that the probabilities sum to one). Although the\\nmodel uses the last class as the denominator in the odds-ratios, the choice\\nof denominator is arbitrary in that the estimates are equivariant under this\\nchoice. A simple calculation shows that\\nPr(G=k|X=x) =exp(βk0+βT\\nkx)\\n1 +∑K−1\\nℓ=1exp(βℓ0+βT\\nℓx), k= 1,... ,K −1,\\nPr(G=K|X=x) =1\\n1 +∑K−1\\nℓ=1exp(βℓ0+βT\\nℓx), (4.18)\\nand they clearly sum to one. To emphasize the dependence on the entire pa-\\nrameter set θ={β10,βT\\n1,... ,β (K−1)0,βT\\nK−1}, we denote the probabilities\\nPr(G=k|X=x) =pk(x;θ).\\nWhen K= 2, this model is especially simple, since there is only a single\\nlinear function. It is widely used in biostatistical applications where binary\\nresponses (two classes) occur quite frequently. For example, patients survive\\nor die, have heart disease or not, or a condition is present or absent.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ad5e6195-a2f5-4fb7-ae08-3f05646bf3d9', embedding=None, metadata={'page_label': '139', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='120 4. Linear Methods for Classiﬁcation\\n4.4.1 Fitting Logistic Regression Models\\nLogistic regression models are usually ﬁt by maximum likelihood, using the\\nconditional likelihood of Ggiven X. Since Pr( G|X) completely speciﬁes the\\nconditional distribution, the multinomial distribution is appropriate. The\\nlog-likelihood for Nobservations is\\nℓ(θ) =N∑\\ni=1logpgi(xi;θ), (4.19)\\nwhere pk(xi;θ) = Pr( G=k|X=xi;θ).\\nWe discuss in detail the two-class case, since the algorithms simplify\\nconsiderably. It is convenient to code the two-class givia a 0 /1 response yi,\\nwhere yi= 1 when gi= 1, and yi= 0 when gi= 2. Let p1(x;θ) =p(x;θ),\\nandp2(x;θ) = 1−p(x;θ). The log-likelihood can be written\\nℓ(β) =N∑\\ni=1{\\nyilogp(xi;β) + (1 −yi)log(1 −p(xi;β))}\\n=N∑\\ni=1{\\nyiβTxi−log(1 + eβTxi)}\\n. (4.20)\\nHereβ={β10,β1}, and we assume that the vector of inputs xiincludes\\nthe constant term 1 to accommodate the intercept.\\nTo maximize the log-likelihood, we set its derivatives to zero. These score\\nequations are\\n∂ℓ(β)\\n∂β=N∑\\ni=1xi(yi−p(xi;β)) = 0 , (4.21)\\nwhich are p+1 equations nonlinear inβ. Notice that since the ﬁrst compo-\\nnent of xiis 1, the ﬁrst score equation speciﬁes that∑N\\ni=1yi=∑N\\ni=1p(xi;β);\\ntheexpected number of class ones matches the observed number (and hence\\nalso class twos.)\\nTo solve the score equations (4.21), we use the Newton–Raphson algo-\\nrithm, which requires the second-derivative or Hessian matrix\\n∂2ℓ(β)\\n∂β∂βT=−N∑\\ni=1xixiTp(xi;β)(1−p(xi;β)). (4.22)\\nStarting with βold, a single Newton update is\\nβnew=βold−(∂2ℓ(β)\\n∂β∂βT)−1∂ℓ(β)\\n∂β, (4.23)\\nwhere the derivatives are evaluated at βold.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1627a1be-e3a0-4fa2-8444-d54616763b7a', embedding=None, metadata={'page_label': '140', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4 Logistic Regression 121\\nIt is convenient to write the score and Hessian in matrix notation. Let\\nydenote the vector of yivalues, XtheN×(p+ 1) matrix of xivalues,\\npthe vector of ﬁtted probabilities with ith element p(xi;βold) andWa\\nN×Ndiagonal matrix of weights with ith diagonal element p(xi;βold)(1−\\np(xi;βold)). Then we have\\n∂ℓ(β)\\n∂β=XT(y−p) (4.24)\\n∂2ℓ(β)\\n∂β∂βT=−XTWX (4.25)\\nThe Newton step is thus\\nβnew=βold+ (XTWX)−1XT(y−p)\\n= (XTWX)−1XTW(\\nXβold+W−1(y−p))\\n= (XTWX)−1XTWz. (4.26)\\nIn the second and third line we have re-expressed the Newton step as a\\nweighted least squares step, with the response\\nz=Xβold+W−1(y−p), (4.27)\\nsometimes known as the adjusted response . These equations get solved re-\\npeatedly, since at each iteration pchanges, and hence so does Wandz.\\nThis algorithm is referred to as iteratively reweighted least squares or IRLS,\\nsince each iteration solves the weighted least squares problem:\\nβnew←arg min\\nβ(z−Xβ)TW(z−Xβ). (4.28)\\nIt seems that β= 0 is a good starting value for the iterative procedure,\\nalthough convergence is never guaranteed. Typically the algorithm does\\nconverge, since the log-likelihood is concave, but overshooting can occur.\\nIn the rare cases that the log-likelihood decreases, step size halving will\\nguarantee convergence.\\nFor the multiclass case ( K≥3) the Newton algorithm can also be ex-\\npressed as an iteratively reweighted least squares algorithm, but with a\\nvector ofK−1 responses and a nondiagonal weight matrix per observation.\\nThe latter precludes any simpliﬁed algorithms, and in this case it is numer-\\nically more convenient to work with the expanded vector θdirectly (Ex-\\nercise 4.4). Alternatively coordinate-descent methods (Section 3.8.6) can\\nbe used to maximize the log-likelihood eﬃciently. The Rpackageglmnet\\n(Friedman et al., 2010) can ﬁt very large logistic regression problems ef-\\nﬁciently, both in Nandp. Although designed to ﬁt regularized models,\\noptions allow for unregularized ﬁts.\\nLogistic regression models are used mostly as a data analysis and infer-\\nence tool, where the goal is to understand the role of the input variables', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='493cb481-e431-41dd-b086-77803aa09fa4', embedding=None, metadata={'page_label': '141', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='122 4. Linear Methods for Classiﬁcation\\nTABLE 4.2. Results from a logistic regression ﬁt to the South African hear t\\ndisease data.\\nCoeﬃcient Std. Error ZScore\\n(Intercept) −4.130 0 .964 −4.285\\nsbp 0.006 0 .006 1 .023\\ntobacco 0.080 0 .026 3 .034\\nldl 0.185 0 .057 3 .219\\nfamhist 0.939 0 .225 4 .178\\nobesity -0.035 0 .029 −1.187\\nalcohol 0.001 0 .004 0 .136\\nage 0.043 0 .010 4 .184\\ninexplaining the outcome. Typically many models are ﬁt in a search for a\\nparsimonious model involving a subset of the variables, possibly with some\\ninteractions terms. The following example illustrates some of the issues\\ninvolved.\\n4.4.2 Example: South African Heart Disease\\nHere we present an analysis of binary data to illustrate the traditional\\nstatistical use of the logistic regression model. The data in Figure 4.1 2 are a\\nsubset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried\\nout in three rural areas of the Western Cape, South Africa (Rousseauw et\\nal., 1983). The aim of the study was to establish the intensity of ischemic\\nheart disease risk factors in that high-incidence region. The data represent\\nwhite males between 15 and 64, and the response variable is the presence or\\nabsence of myocardial infarction (MI) at the time of the survey (the overall\\nprevalence of MI was 5.1% in this region). There are 160 cases in our data\\nset, and a sample of 302 controls. These data are described in more detail\\nin Hastie and Tibshirani (1987).\\nWe ﬁt a logistic-regression model by maximum likelihood, giving the\\nresults shown in Table 4.2. This summary includes Zscores for each of the\\ncoeﬃcients in the model (coeﬃcients divided by their standard errors); a\\nnonsigniﬁcant Zscore suggests a coeﬃcient can be dropped from the model.\\nEach of these correspond formally to a test of the null hypothesis that the\\ncoeﬃcient in question is zero, while all the others are not (also known as\\nthe Wald test). A Zscore greater than approximately 2 in absolute value\\nis signiﬁcant at the 5% level.\\nThere are some surprises in this table of coeﬃcients, which must be in-\\nterpreted with caution. Systolic blood pressure ( sbp) is not signiﬁcant! Nor\\nisobesity , and its sign is negative. This confusion is a result of the corre-\\nlation between the set of predictors. On their own, both sbpandobesity\\nare signiﬁcant, and with positive sign. However, in the presence of many', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e47b5450-68ea-41d2-9f00-db6622bba980', embedding=None, metadata={'page_label': '142', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4 Logistic Regression 123\\nsbp0 10 20 30\\no\\no\\noo\\nooo\\noooo\\no\\noo\\nooooo\\no\\nooo\\noo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooooo\\noooooooooooo\\nooooo\\nooooooooo\\noo\\nooooo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noooooo\\noooo\\noooooo\\nooo\\noo\\nooo\\noo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\noooo\\no\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\nooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noo\\nooo\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\no oo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\nooooooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\noooooooooooo\\nooo\\noooo\\noooo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooooooooooooo\\nooooo\\nooooooooo\\noo\\nooooo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noooooo\\noooooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\nooooo\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooo oo\\noooooo\\nooo\\noooooo\\noo\\noooo\\nooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\noooo ooo\\nooooo\\noo\\noooo\\noooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooo oo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0.0 0.4 0.8\\no\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooo\\noo\\nooo o ooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\nooo ooo\\nooo\\noo\\nooo oo\\noooo\\noooooo o ooo oo\\noooo o\\no ooo\\no oooo\\noo\\no oo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noo\\no ooo\\noooooooo oo\\no oo\\noo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\nooo\\nooo\\noooo\\no\\nooo o\\noo oo\\noo ooooo\\noo o ooooooo\\nooo\\noo\\noo oo\\noo oo\\noooooooo\\nooooo\\no\\no ooo o\\noo\\noo\\nooo\\noo oo\\noooooooo\\noooo oo\\nooo\\nooo\\nooo\\noo\\nooooooooo\\noooooo\\no oo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\no oo\\noo\\nooo\\nooooo\\nooo\\noo oo\\nooo\\no\\no oo\\nooo\\nooo oo oo\\no oooo\\noo\\nooo o\\no o oo\\no\\no o oo\\nooo\\nooo\\no ooo\\noooo\\noooo\\no oo ooo\\no\\nooo\\noooooo\\noooo\\no ooo\\noo\\noooo\\nooooo\\nooo o ooo\\nooo\\noo o ooooo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooooooooooooooooo\\nooooo\\nooooooooo\\noo\\nooo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooo\\noooooo\\nooo\\noo\\nooooo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\noooo\\no\\noooooooo\\nooooooo\\noooo\\noooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooooo\\noooooo\\nooo\\noooooo\\noo\\nooo\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\noooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\noooo\\noooo\\noo\\no\\nooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0 50 100\\no\\no\\noo\\nooo\\noooo\\no\\noo\\nooooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\noooooooo\\nooooooooooooo\\nooooo\\noooo\\nooooo\\noo\\nooooo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooooo\\nooo ooo\\noo\\noooo\\noooooooooo\\nooo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\no\\nooo\\nooooo\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooo\\noo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooooo\\noooooo\\nooo\\noooooo\\noo\\noooo\\nooooo\\noooooo\\nooo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooooooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo\\n100 160 220o\\no\\noo\\nooo\\no ooo\\no\\noo\\nooooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooo o\\noooooooooo ooo\\nooooo\\noooo\\nooooo\\noo\\nooo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\no ooooo\\noooooo\\noooo\\noooo oo\\nooo\\noo\\nooooo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\nooooo\\nooooooo\\no\\nooo oooo\\noo ooooo ooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooo\\noo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noo ooo\\nooo\\noooooo\\nooo\\noooooo\\noo\\noooooo\\nooo\\noooo oo\\nooo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\no o\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooo oooo\\nooooo\\noooooo\\noo oo\\nooooo\\nooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\nooo\\nooo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0 10 20 30o\\noooo\\nooooooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooo\\noo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooo oo\\nooooooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\no oooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooo\\noo\\noo\\nooooooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooo oo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\noo\\noooooo\\nooo\\noo\\noooooooo\\nooo\\no\\noooo\\nooooo\\noooo\\noo\\noo o\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooo oooooooo\\nooo\\noooooooooooooooo oo\\notobaccoo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooooo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\noo\\nooooooo\\nooooo\\noooooooo oo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\nooooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\nooo\\nooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooooo\\noo\\noooo\\nooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\noo ooo\\nooo\\nooo\\nooooooo\\noo\\nooooooooo\\noo\\noooo oo\\noo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooooooooooo\\nooo\\noooooooooo\\noooooooo\\noo\\no ooo\\noo o\\noooo\\nooo\\nooo\\noo\\noo\\no oo\\nooo\\no\\nooooooooo\\noo\\no o ooooo\\nooo\\noooo\\noo\\noo\\no o o oo\\noooo\\noo\\noooo\\no ooo oo\\nooooo\\no oo\\noo ooooooo oooo\\no\\no o ooooo\\noooo\\nooo\\noooo\\noooooo o\\no oooo\\nooooooo o oo\\no oooo oooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\noooooo o oooooo\\no oooo\\noo\\noo\\no o oo oo o oo oooo ooo ooooo\\noooooo\\noooo\\no ooo oo oo\\noooo\\noooo\\no o\\nooo oooo\\no ooooo\\nooooo\\nooo\\noo oooo\\nooo\\noo\\noo oo\\noo\\noo\\nooooooo\\noooooooooooooo\\no\\nooo o o oo\\no\\noooo\\nooo\\noooo\\no\\noo oo\\noo o oo\\nooooo\\nooo o\\nooooo\\noo oo o\\nooo\\nooo\\nooooooo\\noooo oo oo\\nooo\\noo\\no ooo oooo\\no oo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\nooo\\noo\\noo\\noo\\noooo\\noo\\noooo oo\\nooooo o o o ooo\\no oo\\noo oo oooooo\\no ooo oooo\\noo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooooooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooo\\noo\\nooo\\nooooo\\noooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooooo\\nooooooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooooo\\noooooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooooo\\noo\\nooooooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooooooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\noooooooo\\nooo\\noo\\noooooooo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooooooo oooo\\nooo\\noooooooooo\\noooooooo\\noo\\noooo\\nooo\\noooo\\no oo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooooooo\\noo\\nooooooo\\nooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooooo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooooo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\no oooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noo oo\\noo\\nooooooo\\nooooo o\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooo\\noo\\noo\\noooo\\nooo\\noooooooooooooo\\noooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\nooooooooooo\\noo\\no ooo oo\\noo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\no ooooooo ooo\\nooo\\noooooooooooooooooo\\noo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\no o ooo\\noooo\\noo\\noooo\\noo oooo\\nooooo\\no oo\\nooo oooooo oooo\\no\\nooooooo\\noooo\\nooo\\noo\\noooo ooooo\\nooooo\\no ooooooo oo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooooooo\\noooo oo\\noooo\\no ooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo\\nooo ooo\\nooo\\noo\\noooo\\noo\\noo\\noooo\\nooo\\nooooooooo\\nooooo\\nooooo ooo\\no\\noooo\\nooo\\noooo\\nooooo\\nooo oo\\no oooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\nooooooooooo\\noo\\noooooooo\\nooo\\nooo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\noooooooooooooo\\noooooooooo\\nooo o oooo\\no\\noooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\no\\nooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\nooooooooo\\noooooo\\noooo\\nooo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooooo\\no\\nooooooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\no\\nooooooo\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\nooooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\nooooo ooooo\\noo\\noooo\\nooo\\noooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\no oo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\noooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooooooooo\\noooooo\\noooo\\nooo\\nooo\\no oooooo\\nooo\\noo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\noo\\noo\\nooooo\\no\\nooo\\noooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooo oooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooooo\\noooooo\\noooo\\nooo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\noooo\\noooooo\\noo\\no\\nooo\\nooo\\noooo\\no\\nooo\\no\\noo\\noldl\\noo oo\\noo\\noooo\\noo\\noo o oo\\nooo\\nooo\\nooo\\no\\no oo\\noo ooo\\noo\\noooo\\no\\noo oooo\\nooo\\noo\\no oo\\noooo\\nooo o\\no oo\\noo\\noooooo\\noo oo oo\\noo\\noooo oo\\nooo\\nooo\\noooo\\noo\\noo oo\\nooo o\\noo o\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\no o ooooooo\\nooo ooo\\nooooo ooooo\\nooooooo\\nooo\\noo\\no oo\\no oo\\nooo\\no\\noo\\nooooo\\noooo\\nooooo\\no\\nooo\\no oo ooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\no\\noooooo\\noo ooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\no oooo\\nooooo\\no ooo\\noooo\\noooooooooooo\\noo\\noo\\nooooo o oo\\noo\\no oo\\noo oo\\no ooo\\nooooo ooooooo\\nooo\\no oo\\nooooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooo\\noooo\\no o o\\nooo ooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\nooo ooo\\no\\nooooo\\noo\\noo\\noo\\noo oooooooo\\noo oo\\noooo\\no ooooo\\noo\\noooo\\nooo\\noo oo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noo o ooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\nooooooooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\noo ooooooo\\noooooo\\noo\\noo\\nooo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo oo\\no\\nooooooooooo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\nooooooooo\\nooooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooo\\noooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\noooooooooo\\noo\\noooo\\nooo\\noooo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooo\\noo\\nooo oo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\noooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\nooooooo\\noo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\nooooooooo\\noooooo\\noooo\\no oo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\noooo\\nooooo\\no\\nooo\\noooooo\\noo\\no\\no\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noo ooooo\\nooooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\no oo\\nooo\\noo\\nooo\\no\\nooo\\no\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooo\\nooooo oo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooo oooooo\\noooo\\noooo\\noooooo\\noo\\noooo\\nooooooo\\no\\noooo\\noo\\no\\n2 6 10 14oooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noo o ooo\\nooo\\noo\\nooo\\noooo\\noooo\\no oooo\\noooooo\\noooooo\\noo\\noooooo\\noooooo\\no ooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noo oo\\nooo\\nooo\\noo\\no oo\\noo\\nooo oooooo\\noooooo\\no oooo oo\\nooo\\no ooo ooo\\nooo\\noo\\no oo\\nooo\\noooo\\noo\\nooooo\\noo\\noo\\nooooo\\no\\nooooooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\nooooooo\\no oooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\noo ooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooo\\noooo\\noooo\\nooo ooooooooo\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooooooo\\noo o\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\noooo\\noooooo\\noo\\noooo\\nooooooo\\no\\noooo\\noo\\no0.0 0.4 0.8o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooooo ooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooo o ooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\nooooooo o oo\\noo\\nooo\\nooo o\\nooooooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\nooooo o\\noo\\noooo\\noooooo oo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooooo oo\\nooo\\no oooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\noo ooo o\\nooooo\\noooo\\noo\\noooo\\nooo o\\nooo\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\no oooo\\noooo\\noo\\noo\\nooo\\noo ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooo o ooooooooo\\noo oooo\\nooooooooo\\no ooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\noooo\\noooo ooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\no ooo\\noo\\noooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noo oo\\noo o ooooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooooo o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooooooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\noooo\\nooo oooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\noo ooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooo oooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooo oo\\nfamhisto\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\no ooo\\nooo\\nooo\\noooooo\\noo o\\noooo\\noo\\noo ooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooo oooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooooooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\noooo\\nooooooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\noo o\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooooooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooooo o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noo oooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\noo o oo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\no ooooooo ooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\nooo o\\nooo o ooo\\noooo\\nooo\\nooo\\noo\\no ooooooo\\noo\\nooooo\\no ooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\nooo o\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\noo o\\noo o\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\no ooooooo\\nooo\\noo ooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\nooo o\\noooo\\nooo\\nooooo o\\nooooo\\noo oo\\noo\\no ooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooo oo\\nooo\\no ooo\\no oo\\nooo\\noooooo\\nooo\\noooo\\noo\\noo ooo\\noooo\\nooo\\noo\\noo\\noooo\\no o o ooooooo\\noo\\noo\\noo ooo\\noooo\\noo\\noo\\nooo\\no o ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\no oooo o ooo oooooo\\noo oooo\\noo o oooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\no oo\\noo o\\noo\\noo\\no oo oooo o oo\\noo\\nooo\\noooo\\noo ooooo\\noo oo\\nooo\\nooo\\noo\\no o oooooo\\noo\\noo ooo\\noo oooo\\noo\\noooo\\noooooooo\\nooooooo oo\\noooo ooo\\nooo\\nooo\\noooo\\noo\\no ooo\\noo\\nooo\\noo\\nooo\\nooo ooo\\noooo\\noo\\noooo\\no oo o\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\noo o\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noo oooooo\\nooo\\noo ooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooooo\\noooo\\noo\\nooooo\\nooooooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\noooooo\\nooooooo\\noooooo\\no\\noooooo\\nooooo\\nooooooooo\\no\\nooooo\\nooooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooo oo\\nooooo\\noo\\noooo\\nooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooooo\\noo\\noooo\\no\\nooooooooo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\noooo\\nooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\nooooooooo\\nooooooooo\\noo\\noo\\noooo\\noo\\noo\\noooooo\\nooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noo oo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\noooooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noooooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\nooooooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooooo\\noo\\noooooooooo oo\\noo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooo oooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\noooo\\nooooo\\nooooooo\\noo\\noooo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\noooo\\nooooo\\no\\noooooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\nooooooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooo oo\\noo\\nooooo\\nooo oo oooo\\nooooooo\\noo o\\nooooo\\nooo\\nooo\\noooo\\nooo\\noo ooo\\noooooo o ooo o\\nooo\\noo\\noooo\\noo\\nooooo\\noo\\no o\\noo\\noo\\noo\\nooo\\noo\\noooooooo ooo\\noo\\nooo o\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooo o ooo\\nooo\\noo oo o\\nooooooo oo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooo oooo\\noo\\no ooo\\nooo oo\\no oooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooo o ooooo\\nooooo\\noooo oo\\no\\no oo oo o\\noooooo\\no oo\\noooo\\no\\nooo\\noooo\\no o\\noooo\\nooooooo\\nooooo\\nooooo\\no\\noooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\no oooo oo oo\\nooo\\nooo\\nooo\\noo ooooo\\nooooooo\\noooooo\\nooooo\\no oooooo oo\\no\\noo\\noooooooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\noo ooo\\nooo\\nooo\\noooo\\no\\noo\\no oo\\nooooo\\nooooo\\noo\\nooo oooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\noobesity\\noooo\\noo\\noooo\\noooooooooo\\nooooo\\noo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooooo\\noooo\\nooooooooo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noo\\nooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\noooo\\nooooo\\nooooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\no\\nooo\\noooo\\nooooo\\no\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noo\\nooooo\\nooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\noooooo\\noooo\\no\\noo\\nooo\\nooo oo\\nooooo\\noo\\nooooooooo oo\\noooo\\noo\\nooo\\noooo\\no\\noo\\noo\\noo\\no\\n15 25 35 45oooo\\noo\\nooooo\\nooo oooooo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\noooooo ooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noooo\\nooooo\\nooo\\nooooo\\noooo\\nooo oo\\noooo\\nooo ooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\no\\nooo\\noooo\\noo\\nooo\\no\\nooooooo\\nooooo\\nooooo\\no\\noooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noo\\noooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\noooo\\nooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\no0 50 100o\\noooo\\noooooo\\noooooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooooooo\\nooo\\noooo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooooooooo\\no\\noooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooooooooo\\no\\noo\\noo\\noo\\nooooooooo oooooooooooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooooo\\noooooooooooooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooooo\\nooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooo oooooooooo\\nooo\\no\\noo\\nooo\\noooooo\\noo\\nooo\\nooooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooo\\noo\\nooooo\\no ooo\\noooo\\noo\\noooo\\no\\noooooo ooooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noo oooo\\noo\\nooo\\no ooooo\\nooooooooo\\nooo\\noooo\\nooooo\\nooo\\noooo\\noooooooooo\\nooooooo\\nooo\\noooo\\noooooo\\nooooooooooo\\nooo\\noooooo\\nooooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooooooooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooo\\noo\\noo\\noooooooooooooooo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\noooooooooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\nooooooooo\\nooooo\\nooo\\no\\noo\\nooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo ooooo oooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noo\\noo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooo\\noooooo\\nooooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\noooooooooooooooo\\no\\nooo\\nooo\\noo\\noo\\noooooooooooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\noooooooooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooooooooooooo\\nooo\\no\\noo\\nooo\\noooooo\\noo\\nooo\\nooooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\nooooooooo\\noooo\\noo\\noooo\\no\\noooooooooo ooooo\\noo\\nooooo\\no\\noo\\nooo\\noooo o\\noo\\no ooo\\noo oooo\\no oo oo o\\noo\\nooo\\no o o o oo\\no oooooooo\\nooo\\no ooooo ooo\\no oo\\noooo\\nooo o o ooooo\\nooooooo\\no ooo ooo\\no o oooo\\no oo oooooooo\\noooo o oooo\\noo ooo o o oo\\noo\\noo\\noooooo\\no ooo\\nooo\\nooo\\no oo\\noooo\\noo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooo ooo ooooooooooo\\no oooo o ooooooo ooo\\no\\nooo\\no oo\\noo\\nooooooooooo ooo oo\\noo\\nooo\\noooo\\noooooo\\noo\\no oo oo ooo\\noo oooo\\nooo\\noooo\\no ooooo\\no o ooo ooooooo\\nooo\\noo ooo\\noooo\\noo\\nooo\\noo oo o oo\\noooo\\noo\\nooo\\no\\noo\\nooo\\noo\\no oo o oooooooooo\\nooo\\no\\noo\\no oo\\noo o ooooo\\nooo\\noo\\noo ooo\\noo\\nooo o oo\\no\\noo\\noo\\noo\\nooo\\no oo\\noo ooo\\noooo\\noo oo\\no\\nooooo o ooo\\no oooooooo\\noooo\\noo\\no ooo\\no\\no ooooo o oooooo oo\\noo\\no oooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\nooooooooooooooo\\nooo\\nooooo oooo\\nooo\\noooo\\nooooooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooooooooo\\no\\noooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noo oooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\noooooo oooooooooo\\no\\nooo\\nooo\\noo\\nooooooooooo ooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooooo\\nooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooooooooooooo\\nooo\\no\\noo\\nooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\nooooooooo\\noooo\\noo\\noooo\\no\\nooooooooooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noalcoholo\\noooo\\noooooo\\nooo ooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noooo\\nooo o ooo\\nooo\\nooooooo\\nooooooo\\noooooo\\noooo o oooooo\\nooooooooo\\nooooo oooo\\noo\\noo\\noo oooo\\no ooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooooooooooo o ooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooooo\\noooo o oooooo ooooo\\noo\\nooo\\noo oo\\nooo o\\noo\\noo\\noooooooo\\no oooo\\no\\nooo\\noooo\\no oooo o\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooo oooooo\\noo\\nooo\\no\\noo\\nooo\\noo\\noooo ooooo\\nooooo\\nooo\\nooo\\nooo\\noooooooo\\nooo\\noooo ooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\no ooooo ooooooooo\\noo\\nooooo\\no\\noo\\nooo\\no oooo\\no\\n100 160 220oo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\no o\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\no\\noooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\noooooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooooo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\nooooo\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\noo o\\no\\nooo\\noo\\nooo\\noo\\nooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\no\\nooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooo\\noo\\noo\\noo\\nooo\\noo\\no\\no\\nooooo\\nooooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\noooooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\noooo o\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooo\\noo\\noo\\n2 6 10 14oo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooooo\\no\\no\\noooo o\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\nooo\\no\\nooooo\\no\\nooo\\noooooooo\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\nooo\\noo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\no oo o\\nooo\\noooo o o\\noooo o\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\noo o\\no\\nooo\\noo\\nooo\\noo\\noooo oo\\noo\\noo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooo oo\\no\\no\\noo o o o\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\noooo oooo\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\noo o\\noooo o\\noo\\nooo\\noo\\no\\nooo\\noo\\noo o\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\nooooo o\\noo o\\no\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\no oo\\no o\\nooooo ooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\noo o\\noo\\noo\\n15 25 35 45oo\\noo\\no\\no\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooo oo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noooo\\noooo\\noo\\noo\\noo\\nooooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\noooooo\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\nooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\nooooooooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooooo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\no oooo\\no\\nooo\\nooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\nooo\\noo\\noo\\n20 40 60\\n20 40 60age\\nFIGURE 4.12. A scatterplot matrix of the South African heart disease data.\\nEach plot shows a pair of risk factors, and the cases and controls are color coded\\n(red is a case). The variable family history of heart disease ( famhist )is binary\\n(yes or no).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de92213d-b44b-4597-bd40-00939ef83674', embedding=None, metadata={'page_label': '143', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='124 4. Linear Methods for Classiﬁcation\\nTABLE 4.3. Results from stepwise logistic regression ﬁt to South African heart\\ndisease data.\\nCoeﬃcient Std. Error Zscore\\n(Intercept) −4.204 0 .498 −8.45\\ntobacco 0.081 0 .026 3 .16\\nldl 0.168 0 .054 3 .09\\nfamhist 0.924 0 .223 4 .14\\nage 0.044 0 .010 4 .52\\nother correlated variables, they are no longer needed (and can even get a\\nnegative sign).\\nAt this stage the analyst might do some model selection; ﬁnd a subset\\nof the variables that are suﬃcient for explaining their joint eﬀect on the\\nprevalence of chd. One way to proceed by is to drop the least signiﬁcant co-\\neﬃcient, and reﬁt the model. This is done repeatedly until no further terms\\ncan be dropped from the model. This gave the model shown in Table 4.3.\\nA better but more time-consuming strategy is to reﬁt each of the models\\nwith one variable removed, and then perform an analysis of deviance to\\ndecide which variable to exclude. The residual deviance of a ﬁtted model\\nis minus twice its log-likelihood, and the deviance between two models is\\nthe diﬀerence of their individual residual deviances (in analogy to sums-of-\\nsquares). This strategy gave the same ﬁnal model as above.\\nHow does one interpret a coeﬃcient of 0 .081 (Std. Error = 0 .026) for\\ntobacco , for example? Tobacco is measured in total lifetime usage in kilo-\\ngrams, with a median of 1 .0kg for the controls and 4 .1kg for the cases. Thus\\nan increase of 1kg in lifetime tobacco usage accounts for an increase in the\\nodds of coronary heart disease of exp(0 .081) = 1 .084 or 8 .4%. Incorporat-\\ning the standard error we get an approximate 95% conﬁdence interval of\\nexp(0.081±2×0.026) = (1 .03,1.14).\\nWe return to these data in Chapter 5, where we see that some of the\\nvariables have nonlinear eﬀects, and when modeled appropriately, are not\\nexcluded from the model.\\n4.4.3 Quadratic Approximations and Inference\\nThe maximum-likelihood parameter estimates ˆβsatisfy a self-consistency\\nrelationship: they are the coeﬃcients of a weighted least squares ﬁt, where\\nthe responses are\\nzi=xT\\niˆβ+(yi−ˆpi)\\nˆpi(1−ˆpi), (4.29)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ee09cab-67ea-49b0-83b6-3aacf04bbcf6', embedding=None, metadata={'page_label': '144', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4 Logistic Regression 125\\nand the weights are wi= ˆpi(1−ˆpi), both depending on ˆβitself. Apart from\\nproviding a convenient algorithm, this connection with least squares has\\nmore to oﬀer:\\n•The weighted residual sum-of-squares is the familiar Pearson chi-\\nsquare statistic\\nN∑\\ni=1(yi−ˆpi)2\\nˆpi(1−ˆpi), (4.30)\\na quadratic approximation to the deviance.\\n•Asymptotic likelihood theory says that if the model is correct, then\\nˆβis consistent (i.e., converges to the trueβ).\\n•A central limit theorem then shows that the distribution of ˆβcon-\\nverges to N(β,(XTWX)−1). This and other asymptotics can be de-\\nrived directly from the weighted least squares ﬁt by mimicking normal\\ntheory inference.\\n•Model building can be costly for logistic regression models, because\\neach model ﬁtted requires iteration. Popular shortcuts are the Rao\\nscore test which tests for inclusion of a term, and the Wald test which\\ncan be used to test for exclusion of a term. Neither of these require\\niterative ﬁtting, and are based on the maximum-likelihood ﬁt of the\\ncurrent model. It turns out that both of these amount to adding\\nor dropping a term from the weighted least squares ﬁt, using the\\nsame weights. Such computations can be done eﬃciently, without\\nrecomputing the entire weighted least squares ﬁt.\\nSoftware implementations can take advantage of these connections. For\\nexample, the generalized linear modeling software in R (which includes lo-\\ngistic regression as part of the binomial family of models) exploits them\\nfully. GLM (generalized linear model) objects can be treated as linear model\\nobjects, and all the tools available for linear models can be applied auto-\\nmatically.\\n4.4.4 L1Regularized Logistic Regression\\nTheL1penalty used in the lasso (Section 3.4.2) can be used for variable\\nselection and shrinkage with any linear regression model. For logistic re-\\ngression, we would maximize a penalized version of (4.20):\\nmax\\nβ0,β\\uf8f1\\n\\uf8f2\\n\\uf8f3N∑\\ni=1[\\nyi(β0+βTxi)−log(1 + eβ0+βTxi)]\\n−λp∑\\nj=1|βj|\\uf8fc\\n\\uf8fd\\n\\uf8fe.(4.31)\\nAs with the lasso, we typically do not penalize the intercept term, and stan-\\ndardize the predictors for the penalty to be meaningful. Criterion (4.31) is', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d995cc18-bec6-4e29-a9c7-121ad4ed510d', embedding=None, metadata={'page_label': '145', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='126 4. Linear Methods for Classiﬁcation\\nconcave, and a solution can be found using nonlinear programming meth-\\nods (Koh et al., 2007, for example). Alternatively, using the same quadratic\\napproximations that were used in the Newton algorithm in Section 4.4.1,\\nwe can solve (4.31) by repeated application of a weighted lasso algorit hm.\\nInterestingly, the score equations [see (4.24)] for the variables with non-zer o\\ncoeﬃcients have the form\\nxT\\nj(y−p) =λ≤sign(βj), (4.32)\\nwhich generalizes (3.58) in Section 3.4.4; the active variables are tied in\\ntheirgeneralized correlation with the residuals.\\nPath algorithms such as LAR for lasso are more diﬃcult, because the\\ncoeﬃcient proﬁles are piecewise smooth rather than linear. Nevertheless,\\nprogress can be made using quadratic approximations.\\n******************************* ************** ****** *************************************************** ****************************************** **************************************************************************** *****************\\n0.0 0.5 1.0 1.5 2.00.0 0.2 0.4 0.6******************************* * ******************************************************************************************************************************************************************************************** *****************\\n******************************* ************** * ****************************************************************************************************************************************************************************** *****************\\n****************************** ********************************************************************************************************************************************************************************************** *****************\\n******************************* ************** ****** *************************************************** *************************************************** ************************ ******************************************* ************************************************ ************** ****** *************************************************** *************************************************** ************************ *************************** *************** * ********************************************************************************************************************************************************************************************************************************************* *****************\\nobesityalcoholsbptobaccoldlfamhistage1 2 4 5 6 7Coeﬃcients βj(λ)\\n||β(λ)||1\\nFIGURE 4.13. L1regularized logistic regression coeﬃcients for the South\\nAfrican heart disease data, plotted as a function of the L1norm. The variables\\nwere all standardized to have unit variance. The proﬁles are comp uted exactly at\\neach of the plotted points.\\nFigure 4.13 shows the L1regularization path for the South African\\nheart disease data of Section 4.4.2. This was produced using the Rpackage\\nglmpath (Park and Hastie, 2007), which uses predictor–corrector methods\\nof convex optimization to identify the exact values of λat which the active\\nset of non-zero coeﬃcients changes (vertical lines in the ﬁgure). Here the\\nproﬁles look almost linear; in other examples the curvature will be more\\nvisible.\\nCoordinate descent methods (Section 3.8.6) are very eﬃcient for comput-\\ning the coeﬃcient proﬁles on a grid of values for λ. TheRpackageglmnet', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ae8b6605-2405-4023-8737-98dd3426eeaa', embedding=None, metadata={'page_label': '146', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4 Logistic Regression 127\\n(Friedman et al., 2010) can ﬁt coeﬃcient paths for very large logistic re-\\ngression problems eﬃciently (large in Norp). Their algorithms can exploit\\nsparsity in the predictor matrix X, which allows for even larger problems.\\nSee Section 18.4 for more details, and a discussion of L1-regularized multi-\\nnomial models.\\n4.4.5 Logistic Regression or LDA?\\nIn Section 4.3 we ﬁnd that the log-posterior odds between class kandK\\nare linear functions of x(4.9):\\nlogPr(G=k|X=x)\\nPr(G=K|X=x)= logπk\\nπK−1\\n2(θk+θK)TΣ−1(θk−θK)\\n+xTΣ−1(θk−θK)\\n=αk0+αT\\nkx. (4.33)\\nThis linearity is a consequence of the Gaussian assumption for the class\\ndensities, as well as the assumption of a common covariance matrix. The\\nlinear logistic model (4.17) by construction has linear logits:\\nlogPr(G=k|X=x)\\nPr(G=K|X=x)=βk0+βT\\nkx. (4.34)\\nIt seems that the models are the same. Although they have exactly the same\\nform, the diﬀerence lies in the way the linear coeﬃcients are estimated. The\\nlogistic regression model is more general, in that it makes less assumptio ns.\\nWe can write the joint density ofXandGas\\nPr(X,G=k) = Pr( X)Pr(G=k|X), (4.35)\\nwhere Pr( X) denotes the marginal density of the inputs X. For both LDA\\nand logistic regression, the second term on the right has the logit-linear\\nform\\nPr(G=k|X=x) =eβk0+βT\\nkx\\n1 +∑K−1\\nℓ=1eβℓ0+βT\\nℓx, (4.36)\\nwhere we have again arbitrarily chosen the last class as the reference.\\nThe logistic regression model leaves the marginal density of Xas an arbi-\\ntrary density function Pr( X), and ﬁts the parameters of Pr( G|X) by max-\\nimizing the conditional likelihood —the multinomial likelihood with proba-\\nbilities the Pr( G=k|X). Although Pr( X) is totally ignored, we can think\\nof this marginal density as being estimated in a fully nonparametric and\\nunrestricted fashion, using the empirical distribution function which places\\nmass 1 /Nat each observation.\\nWith LDA we ﬁt the parameters by maximizing the full log-likelihood,\\nbased on the joint density\\nPr(X,G=k) =φ(X;θk,Σ)πk, (4.37)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0a78d0e9-f63a-4af1-866e-57b4b58de87a', embedding=None, metadata={'page_label': '147', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='128 4. Linear Methods for Classiﬁcation\\nwhere φis the Gaussian density function. Standard normal theory leads\\neasily to the estimates ˆ θk,ˆΣ, and ˆ πkgiven in Section 4.3. Since the linear\\nparameters of the logistic form (4.33) are functions of the Gaussian para m-\\neters, we get their maximum-likelihood estimates by plugging in the corre-\\nsponding estimates. However, unlike in the conditional case, the marginal\\ndensity Pr( X) does play a role here. It is a mixture density\\nPr(X) =K∑\\nk=1πkφ(X;θk,Σ), (4.38)\\nwhich also involves the parameters.\\nWhat role can this additional component/restriction play? By relying\\non the additional model assumptions, we have more information about the\\nparameters, and hence can estimate them more eﬃciently (lower variance).\\nIf in fact the true fk(x) are Gaussian, then in the worst case ignoring this\\nmarginal part of the likelihood constitutes a loss of eﬃciency of about 30%\\nasymptotically in the error rate (Efron, 1975). Paraphrasing: with 3 0%\\nmore data, the conditional likelihood will do as well.\\nFor example, observations far from the decision boundary (which are\\ndown-weighted by logistic regression) play a role in estimating the common\\ncovariance matrix. This is not all good news, because it also means that\\nLDA is not robust to gross outliers.\\nFrom the mixture formulation, it is clear that even observations without\\nclass labels have information about the parameters. Often it is expensive\\nto generate class labels, but unclassiﬁed observations come cheaply. By\\nrelying on strong model assumptions, such as here, we can use both types\\nof information.\\nThe marginal likelihood can be thought of as a regularizer, requiring\\nin some sense that class densities be visible from this marginal view. For\\nexample, if the data in a two-class logistic regression model can be per-\\nfectly separated by a hyperplane, the maximum likelihood estimates of the\\nparameters are undeﬁned (i.e., inﬁnite; see Exercise 4.5). The LDA coeﬃ-\\ncients for the same data will be well deﬁned, since the marginal likelihood\\nwill not permit these degeneracies.\\nIn practice these assumptions are never correct, and often some of the\\ncomponents of Xare qualitative variables. It is generally felt that logistic\\nregression is a safer, more robust bet than the LDA model, relying on fewer\\nassumptions. It is our experience that the models give very similar results,\\neven when LDA is used inappropriately, such as with qualitative predictors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='18d401ad-fb92-4bc3-8102-b1d373a133d0', embedding=None, metadata={'page_label': '148', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Separating Hyperplanes 129\\nFIGURE 4.14. A toy example with two classes separable by a hyperplane. The\\norange line is the least squares solution, which misclassiﬁes one of the training\\npoints. Also shown are two blue separating hyperplanes found by t heperceptron\\nlearning algorithm with diﬀerent random starts.\\n4.5 Separating Hyperplanes\\nWe have seen that linear discriminant analysis and logistic regression bot h\\nestimate linear decision boundaries in similar but slightly diﬀerent ways.\\nFor the rest of this chapter we describe separating hyperplane classiﬁers.\\nThese procedures construct linear decision boundaries that explicitly try\\nto separate the data into diﬀerent classes as well as possible. They provide\\nthe basis for support vector classiﬁers, discussed in Chapter 12. The math-\\nematical level of this section is somewhat higher than that of the previous\\nsections.\\nFigure 4.14 shows 20 data points in two classes in IR2. These data can be\\nseparated by a linear boundary. Included in the ﬁgure (blue lines) are two\\nof the inﬁnitely many possible separating hyperplanes . The orange line is\\nthe least squares solution to the problem, obtained by regressing the −1/1\\nresponse YonX(with intercept); the line is given by\\n{x:ˆβ0+ˆβ1x1+ˆβ2x2= 0}. (4.39)\\nThis least squares solution does not do a perfect job in separating the\\npoints, and makes one error. This is the same boundary found by LDA,\\nin light of its equivalence with linear regression in the two-class case (Sec-\\ntion 4.3 and Exercise 4.2).\\nClassiﬁers such as (4.39), that compute a linear combination of the input\\nfeatures and return the sign, were called perceptrons in the engineering liter-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ffca710-5b87-4937-aa14-098123b3826b', embedding=None, metadata={'page_label': '149', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='130 4. Linear Methods for Classiﬁcation\\nx0x\\nβ∗β0+βTx= 0\\nFIGURE 4.15. The linear algebra of a hyperplane (aﬃne set).\\nature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations\\nfor the neural network models of the 1980s and 1990s.\\nBefore we continue, let us digress slightly and review some vector algebra.\\nFigure 4.15 depicts a hyperplane or aﬃne set Ldeﬁned by the equation\\nf(x) =β0+βTx= 0; since we are in IR2this is a line.\\nHere we list some properties:\\n1. For any two points x1andx2lying in L,βT(x1−x2) = 0, and hence\\nβ∗=β/||β||is the vector normal to the surface of L.\\n2. For any point x0inL,βTx0=−β0.\\n3. The signed distance of any point xtoLis given by\\nβ∗T(x−x0) =1\\n∥β∥(βTx+β0)\\n=1\\n||f′(x)||f(x). (4.40)\\nHence f(x) is proportional to the signed distance from xto the hyperplane\\ndeﬁned by f(x) = 0.\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm\\nTheperceptron learning algorithm tries to ﬁnd a separating hyperplane by\\nminimizing the distance of misclassiﬁed points to the decision boundary. If', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aa103215-bb31-4ec8-a218-c1269d6842bf', embedding=None, metadata={'page_label': '150', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Separating Hyperplanes 131\\na response yi= 1 is misclassiﬁed, then xT\\niβ+β0<0, and the opposite for\\na misclassiﬁed response with yi=−1. The goal is to minimize\\nD(β,β0) =−∑\\ni∈Myi(xT\\niβ+β0), (4.41)\\nwhere Mindexes the set of misclassiﬁed points. The quantity is non-\\nnegative and proportional to the distance of the misclassiﬁed points to\\nthe decision boundary deﬁned by βTx+β0= 0. The gradient (assuming\\nMis ﬁxed) is given by\\n∂D(β,β0)\\n∂β=−∑\\ni∈Myixi, (4.42)\\n∂D(β,β0)\\n∂β0=−∑\\ni∈Myi. (4.43)\\nThe algorithm in fact uses stochastic gradient descent to minimize this\\npiecewise linear criterion. This means that rather than computing the sum\\nof the gradient contributions of each observation followed by a step in the\\nnegative gradient direction, a step is taken after each observation is visit ed.\\nHence the misclassiﬁed observations are visited in some sequence, and the\\nparameters βare updated via\\n(\\nβ\\nβ0)\\n←(\\nβ\\nβ0)\\n+ρ(\\nyixi\\nyi)\\n. (4.44)\\nHereρis the learning rate, which in this case can be taken to be 1 without\\nloss in generality. If the classes are linearly separable, it can be shown that\\nthe algorithm converges to a separating hyperplane in a ﬁnite number of\\nsteps (Exercise 4.6). Figure 4.14 shows two solutions to a toy problem, eac h\\nstarted at a diﬀerent random guess.\\nThere are a number of problems with this algorithm, summarized in\\nRipley (1996):\\n•When the data are separable, there are many solutions, and which\\none is found depends on the starting values.\\n•The “ﬁnite” number of steps can be very large. The smaller the gap,\\nthe longer the time to ﬁnd it.\\n•When the data are not separable, the algorithm will not converge,\\nand cycles develop. The cycles can be long and therefore hard to\\ndetect.\\nThe second problem can often be eliminated by seeking a hyperplane not\\nin the original space, but in a much enlarged space obtained by creating', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9121f3ca-ce7e-4258-ab91-824ec4c040ed', embedding=None, metadata={'page_label': '151', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='132 4. Linear Methods for Classiﬁcation\\nmany basis-function transformations of the original variables. This is a nal-\\nogous to driving the residuals in a polynomial regression problem down\\nto zero by making the degree suﬃciently large. Perfect separation cannot\\nalways be achieved: for example, if observations from two diﬀerent classes\\nshare the same input. It may not be desirable either, since the resulting\\nmodel is likely to be overﬁt and will not generalize well. We return to this\\npoint at the end of the next section.\\nA rather elegant solution to the ﬁrst problem is to add additional con-\\nstraints to the separating hyperplane.\\n4.5.2 Optimal Separating Hyperplanes\\nTheoptimal separating hyperplane separates the two classes and maximizes\\nthe distance to the closest point from either class (Vapnik, 1996). Not only\\ndoes this provide a unique solution to the separating hyperplane problem,\\nbut by maximizing the margin between the two classes on the training data,\\nthis leads to better classiﬁcation performance on test data.\\nWe need to generalize criterion (4.41). Consider the optimization problem\\nmax\\nβ,β0,||β||=1M\\nsubject to yi(xT\\niβ+β0)≥M, i= 1,... ,N.(4.45)\\nThe set of conditions ensure that all the points are at least a signed\\ndistance Mfrom the decision boundary deﬁned by βandβ0, and we seek\\nthe largest such Mand associated parameters. We can get rid of the ||β||=\\n1 constraint by replacing the conditions with\\n1\\n||β||yi(xT\\niβ+β0)≥M, (4.46)\\n(which redeﬁnes β0) or equivalently\\nyi(xT\\niβ+β0)≥M||β||. (4.47)\\nSince for any βandβ0satisfying these inequalities, any positively scaled\\nmultiple satisﬁes them too, we can arbitrarily set ||β||= 1/M. Thus (4.45)\\nis equivalent to\\nmin\\nβ,β01\\n2||β||2\\nsubject to yi(xT\\niβ+β0)≥1, i= 1,... ,N.(4.48)\\nIn light of (4.40), the constraints deﬁne an empty slab or margin around the\\nlinear decision boundary of thickness 1 /||β||. Hence we choose βandβ0to\\nmaximize its thickness. This is a convex optimization problem (quadratic', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27bf50a3-8c82-4558-b0ac-68b4f61b0e53', embedding=None, metadata={'page_label': '152', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Separating Hyperplanes 133\\ncriterion with linear inequality constraints). The Lagrange (primal) func-\\ntion, to be minimized w.r.t. βandβ0, is\\nLP=1\\n2||β||2−N∑\\ni=1αi[yi(xT\\niβ+β0)−1]. (4.49)\\nSetting the derivatives to zero, we obtain:\\nβ=N∑\\ni=1αiyixi, (4.50)\\n0 =N∑\\ni=1αiyi, (4.51)\\nand substituting these in (4.49) we obtain the so-called Wolfe dual\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\nk=1αiαkyiykxT\\nixk\\nsubject to αi≥0. (4.52)\\nThe solution is obtained by maximizing LDin the positive orthant, a sim-\\npler convex optimization problem, for which standard software can be used.\\nIn addition the solution must satisfy the Karush–Kuhn–Tucker conditions,\\nwhich include (4.50), (4.51), (4.52) and\\nαi[yi(xT\\niβ+β0)−1] = 0 ∀i. (4.53)\\nFrom these we can see that\\n•ifαi>0, then yi(xT\\niβ+β0) = 1, or in other words, xiis on the\\nboundary of the slab;\\n•ifyi(xT\\niβ+β0)>1,xiis not on the boundary of the slab, and αi= 0.\\nFrom (4.50) we see that the solution vector βis deﬁned in terms of a linear\\ncombination of the support points xi—those points deﬁned to be on the\\nboundary of the slab via αi>0. Figure 4.16 shows the optimal separating\\nhyperplane for our toy example; there are three support points. Likewise,\\nβ0is obtained by solving (4.53) for any of the support points.\\nThe optimal separating hyperplane produces a function ˆf(x) =xTˆβ+ˆβ0\\nfor classifying new observations:\\nˆG(x) = sign ˆf(x). (4.54)\\nAlthough none of the training observations fall in the margin (by con-\\nstruction), this will not necessarily be the case for test observations. The', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='92427c24-5bde-4c71-9a06-5d96b5eddb66', embedding=None, metadata={'page_label': '153', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='134 4. Linear Methods for Classiﬁcation\\nFIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates\\nthe maximum margin separating the two classes. There are three support points\\nindicated, which lie on the boundary of the margin, and the optima l separating\\nhyperplane (blue line) bisects the slab. Included in the ﬁgure is the boundary found\\nusing logistic regression (red line), which is very close to th e optimal separating\\nhyperplane (see Section 12.3.3).\\nintuition is that a large margin on the training data will lead to good\\nseparation on the test data.\\nThe description of the solution in terms of support points seems to sug-\\ngest that the optimal hyperplane focuses more on the points that count,\\nand is more robust to model misspeciﬁcation. The LDA solution, on the\\nother hand, depends on all of the data, even points far away from the de-\\ncision boundary. Note, however, that the identiﬁcation of these support\\npoints required the use of all the data. Of course, if the classes are really\\nGaussian, then LDA is optimal, and separating hyperplanes will pay a price\\nfor focusing on the (noisier) data at the boundaries of the classes.\\nIncluded in Figure 4.16 is the logistic regression solution to this prob-\\nlem, ﬁt by maximum likelihood. Both solutions are similar in this case.\\nWhen a separating hyperplane exists, logistic regression will always ﬁnd\\nit, since the log-likelihood can be driven to 0 in this case (Exercise 4.5).\\nThe logistic regression solution shares some other qualitative features with\\nthe separating hyperplane solution. The coeﬃcient vector is deﬁned by a\\nweighted least squares ﬁt of a zero-mean linearized response on the input\\nfeatures, and the weights are larger for points near the decision boundary\\nthan for those further away.\\nWhen the data are not separable, there will be no feasible solution to\\nthis problem, and an alternative formulation is needed. Again one can en-\\nlarge the space using basis transformations, but this can lead to artiﬁcial', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0fac05e-0f18-4f1d-b03a-db30dc51e24a', embedding=None, metadata={'page_label': '154', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 135\\nseparation through over-ﬁtting. In Chapter 12 we discuss a more attractive\\nalternative known as the support vector machine , which allows for overlap,\\nbut minimizes a measure of the extent of this overlap.\\nBibliographic Notes\\nGood general texts on classiﬁcation include Duda et al. (2000), Hand\\n(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1979) have\\na concise discussion of linear discriminant analysis. Michie et al. (1994)\\ncompare a large number of popular classiﬁers on benchmark datasets. Lin-\\near separating hyperplanes are discussed in Vapnik (1996). Our account of\\nthe perceptron learning algorithm follows Ripley (1996).\\nExercises\\nEx. 4.1 Show how to solve the generalized eigenvalue problem max aTBa\\nsubject to aTWa= 1 by transforming to a standard eigenvalue problem.\\nEx. 4.2 Suppose we have features x∈IRp, a two-class response, with class\\nsizesN1,N2, and the target coded as −N/N 1,N/N 2.\\n(a) Show that the LDA rule classiﬁes to class 2 if\\nxTˆΣ−1(ˆθ2−ˆθ1)>1\\n2ˆθT\\n2ˆΣ−1ˆθ2−1\\n2ˆθT\\n1ˆΣ−1ˆθ1+ log(N1\\nN)\\n−log(N2\\nN)\\n,\\nand class 1 otherwise.\\n(b) Consider minimization of the least squares criterion\\nN∑\\ni=1(yi−β0−βTxi)2. (4.55)\\nShow that the solution ˆβsatisﬁes\\n[\\n(N−2)ˆΣ+N1N2\\nNˆΣB]\\nβ=N(ˆθ2−ˆθ1) (4.56)\\n(after simpliﬁcation),where ˆΣB= (ˆθ2−ˆθ1)(ˆθ2−ˆθ1)T.\\n(c) Hence show that ˆΣBβis in the direction (ˆ θ2−ˆθ1) and thus\\nˆβ∝ˆΣ−1(ˆθ2−ˆθ1). (4.57)\\nTherefore the least squares regression coeﬃcient is identical to the\\nLDA coeﬃcient, up to a scalar multiple.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c8fb5397-91a2-4307-982e-44cfd2c4c2bc', embedding=None, metadata={'page_label': '155', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='136 4. Linear Methods for Classiﬁcation\\n(d) Show that this result holds for any (distinct) coding of the two classes.\\n(e) Find the solution ˆβ0, and hence the predicted values ˆf=ˆβ0+ˆβTx.\\nConsider the following rule: classify to class 2 if ˆ yi>0 and class\\n1 otherwise. Show this is not the same as the LDA rule unless the\\nclasses have equal numbers of observations.\\n(Fisher, 1936; Ripley, 1996)\\nEx. 4.3 Suppose we transform the original predictors XtoˆYvia linear\\nregression. In detail, let ˆY=X(XTX)−1XTY=XˆB, where Yis the\\nindicator response matrix. Similarly for any input x∈IRp, we get a trans-\\nformed vector ˆ y=ˆBTx∈IRK. Show that LDA using ˆYis identical to\\nLDA in the original space.\\nEx. 4.4 Consider the multilogit model with Kclasses (4.17). Let βbe the\\n(p+ 1)(K−1)-vector consisting of all the coeﬃcients. Deﬁne a suitably\\nenlarged version of the input vector xto accommodate this vectorized co-\\neﬃcient matrix. Derive the Newton-Raphson algorithm for maximizing the\\nmultinomial log-likelihood, and describe how you would implement this\\nalgorithm.\\nEx. 4.5 Consider a two-class logistic regression problem with x∈IR. Char-\\nacterize the maximum-likelihood estimates of the slope and intercept pa-\\nrameter if the sample xifor the two classes are separated by a point x0∈IR.\\nGeneralize this result to (a) x∈IRp(see Figure 4.16), and (b) more than\\ntwo classes.\\nEx. 4.6 Suppose we have Npoints xiin IRpin general position, with class\\nlabels yi∈ {−1,1}. Prove that the perceptron learning algorithm converges\\nto a separating hyperplane in a ﬁnite number of steps:\\n(a) Denote a hyperplane by f(x) =βT\\n1x+β0= 0, or in more compact\\nnotation βTx∗= 0, where x∗= (x,1) and β= (β1,β0). Let zi=\\nx∗\\ni/||x∗\\ni||. Show that separability implies the existence of a βsepsuch\\nthatyiβT\\nsepzi≥1∀i\\n(b) Given a current βold, the perceptron algorithm identiﬁes a point zithat\\nis misclassiﬁed, and produces the update βnew←βold+yizi. Show\\nthat||βnew−βsep||2≤ ||βold−βsep||2−1, and hence that the algorithm\\nconverges to a separating hyperplane in no more than ||βstart−βsep||2\\nsteps (Ripley, 1996).\\nEx. 4.7 Consider the criterion\\nD∗(β,β0) =−N∑\\ni=1yi(xT\\niβ+β0), (4.58)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60dd5d57-1980-4918-9ace-19df4ab86967', embedding=None, metadata={'page_label': '156', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 137\\na generalization of (4.41) where we sum over all the observations. Consider\\nminimizing D∗subject to ||β||= 1. Describe this criterion in words. Does\\nit solve the optimal separating hyperplane problem?\\nEx. 4.8 Consider the multivariate Gaussian model X|G=k∼N(θk,Σ),\\nwith the additional restriction that rank {θk}K\\n1=L < max(K−1,p).\\nDerive the constrained MLEs for the θkandΣ. Show that the Bayes clas-\\nsiﬁcation rule is equivalent to classifying in the reduced subspace computed\\nby LDA (Hastie and Tibshirani, 1996b).\\nEx. 4.9 Write a computer program to perform a quadratic discriminant\\nanalysis by ﬁtting a separate Gaussian model per class. Try it out on the\\nvowel data, and compute the misclassiﬁcation error for the test data. The\\ndata can be found in the book website www-stat.stanford.edu/ElemStatLearn .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c6565b67-2e0a-45f9-a593-10fd14dbae95', embedding=None, metadata={'page_label': '157', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='138 4. Linear Methods for Classiﬁcation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='884ce855-82e8-4677-9b91-caf144811626', embedding=None, metadata={'page_label': '158', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 139\\nPrinter: Opaque this\\n5\\nBasis Expansions and Regularization\\n5.1 Introduction\\nWe have already made use of models linear in the input features, both for\\nregression and classiﬁcation. Linear regression, linear discriminant analysi s,\\nlogistic regression and separating hyperplanes all rely on a linear model.\\nIt is extremely unlikely that the true function f(X) is actually linear in\\nX. In regression problems, f(X) = E( Y|X) will typically be nonlinear and\\nnonadditive in X, and representing f(X) by a linear model is usually a con-\\nvenient, and sometimes a necessary, approximation. Convenient because a\\nlinear model is easy to interpret, and is the ﬁrst-order Taylor approxima-\\ntion to f(X). Sometimes necessary, because with Nsmall and/or plarge,\\na linear model might be all we are able to ﬁt to the data without overﬁt-\\nting. Likewise in classiﬁcation, a linear, Bayes-optimal decision boundary\\nimplies that some monotone transformation of Pr( Y= 1|X) is linear in X.\\nThis is inevitably an approximation.\\nIn this chapter and the next we discuss popular methods for moving\\nbeyond linearity. The core idea in this chapter is to augment/replace the\\nvector of inputs Xwith additional variables, which are transformations of\\nX, and then use linear models in this new space of derived input features.\\nDenote by hm(X) : IRp↦→IR the mth transformation of X,m=\\n1,... ,M . We then model\\nf(X) =M∑\\nm=1βmhm(X), (5.1)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5c115aa-dea3-4caf-8536-5956e45cef65', embedding=None, metadata={'page_label': '159', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='140 5. Basis Expansions and Regularization\\nalinear basis expansion inX. The beauty of this approach is that once the\\nbasis functions hmhave been determined, the models are linear in these\\nnew variables, and the ﬁtting proceeds as before.\\nSome simple and widely used examples of the hmare the following:\\n•hm(X) =Xm, m= 1,... ,p recovers the original linear model.\\n•hm(X) =X2\\njorhm(X) =XjXkallows us to augment the inputs with\\npolynomial terms to achieve higher-order Taylor expansions. Note,\\nhowever, that the number of variables grows exponentially in the de-\\ngree of the polynomial. A full quadratic model in pvariables requires\\nO(p2) square and cross-product terms, or more generally O(pd) for a\\ndegree- dpolynomial.\\n•hm(X) = log( Xj),√\\nXj,...permits other nonlinear transformations\\nof single inputs. More generally one can use similar functions involv-\\ning several inputs, such as hm(X) =||X||.\\n•hm(X) =I(Lm≤Xk< Um), an indicator for a region of Xk. By\\nbreaking the range of Xkup into Mksuch nonoverlapping regions\\nresults in a model with a piecewise constant contribution for Xk.\\nSometimes the problem at hand will call for particular basis functions hm,\\nsuch as logarithms or power functions. More often, however, we use the basis\\nexpansions as a device to achieve more ﬂexible representations for f(X).\\nPolynomials are an example of the latter, although they are limited by\\ntheir global nature—tweaking the coeﬃcients to achieve a functional form\\nin one region can cause the function to ﬂap about madly in remote regions.\\nIn this chapter we consider more useful families of piecewise-polynomials\\nandsplines that allow for local polynomial representations. We also discuss\\nthewavelet bases, especially useful for modeling signals and images. These\\nmethods produce a dictionary Dconsisting of typically a very large number\\n|D|of basis functions, far more than we can aﬀord to ﬁt to our data. Along\\nwith the dictionary we require a method for controlling the complexity\\nof our model, using basis functions from the dictionary. There are three\\ncommon approaches:\\n•Restriction methods, where we decide before-hand to limit the class\\nof functions. Additivity is an example, where we assume that our\\nmodel has the form\\nf(X) =p∑\\nj=1fj(Xj)\\n=p∑\\nj=1Mj∑\\nm=1βjmhjm(Xj). (5.2)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='30f0555f-12bf-43b9-a6be-a83864a66403', embedding=None, metadata={'page_label': '160', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.2 Piecewise Polynomials and Splines 141\\nThe size of the model is limited by the number of basis functions Mj\\nused for each component function fj.\\n•Selection methods, which adaptively scan the dictionary and include\\nonly those basis functions hmthat contribute signiﬁcantly to the ﬁt of\\nthe model. Here the variable selection techniques discussed in Chap-\\nter 3 are useful. The stagewise greedy approaches such as CART,\\nMARS and boosting fall into this category as well.\\n•Regularization methods where we use the entire dictionary but re-\\nstrict the coeﬃcients. Ridge regression is a simple example of a regu-\\nlarization approach, while the lasso is both a regularization and selec-\\ntion method. Here we discuss these and more sophisticated methods\\nfor regularization.\\n5.2 Piecewise Polynomials and Splines\\nWe assume until Section 5.7 that Xis one-dimensional. A piecewise poly-\\nnomial function f(X) is obtained by dividing the domain of Xinto contigu-\\nous intervals, and representing fby a separate polynomial in each interval.\\nFigure 5.1 shows two simple piecewise polynomials. The ﬁrst is piecewise\\nconstant, with three basis functions:\\nh1(X) =I(X < ξ 1), h 2(X) =I(ξ1≤X < ξ 2), h 3(X) =I(ξ2≤X).\\nSince these are positive over disjoint regions, the least squares estimate o f\\nthe model f(X) =∑3\\nm=1βmhm(X) amounts to ˆβm=¯Ym, the mean of Y\\nin the mth region.\\nThe top right panel shows a piecewise linear ﬁt. Three additional basis\\nfunctions are needed: hm+3=hm(X)X, m = 1,... ,3. Except in special\\ncases, we would typically prefer the third panel, which is also piecewise\\nlinear, but restricted to be continuous at the two knots. These continu-\\nity restrictions lead to linear constraints on the parameters; for example,\\nf(ξ−\\n1) =f(ξ+\\n1) implies that β1+ξ1β4=β2+ξ1β5. In this case, since there\\nare two restrictions, we expect to get back two parameters, leaving four free\\nparameters.\\nA more direct way to proceed in this case is to use a basis that incorpo-\\nrates the constraints:\\nh1(X) = 1, h 2(X) =X, h 3(X) = (X−ξ1)+, h 4(X) = (X−ξ2)+,\\nwhere t+denotes the positive part. The function h3is shown in the lower\\nright panel of Figure 5.1. We often prefer smoother functions, and these\\ncan be achieved by increasing the order of the local polynomial. Figure 5.2\\nshows a series of piecewise-cubic polynomials ﬁt to the same data, with', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e968cc76-7b7f-458c-ba40-8737f4c0a71e', embedding=None, metadata={'page_label': '161', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='142 5. Basis Expansions and Regularization\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOPiecewise Constant\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOPiecewise Linear\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOContinuous Piecewise Linear Piecewise-linear Basis Function\\n•\\n•••\\n••••\\n••\\n•• • ••\\n••••\\n••\\n•••\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n•••••\\n•••\\n•\\n••\\n••\\nξ1 ξ1ξ1 ξ1\\nξ2 ξ2ξ2 ξ2\\n(X−ξ1)+\\nFIGURE 5.1. The top left panel shows a piecewise constant function ﬁt to some\\nartiﬁcial data. The broken vertical lines indicate the positio ns of the two knots\\nξ1andξ2. The blue curve represents the true function, from which the dat a were\\ngenerated with Gaussian noise. The remaining two panels show piec ewise lin-\\near functions ﬁt to the same data—the top right unrestricted, and t he lower left\\nrestricted to be continuous at the knots. The lower right panel sh ows a piecewise–\\nlinear basis function, h3(X) = ( X−ξ1)+, continuous at ξ1. The black points\\nindicate the sample evaluations h3(xi), i= 1, . . . , N .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d9c5f26f-8a1b-429a-a46d-5c15a7c7a34e', embedding=None, metadata={'page_label': '162', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.2 Piecewise Polynomials and Splines 143\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOODiscontinuous\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous First Derivative\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous Second DerivativePiecewise Cubic Polynomials\\nξ1 ξ1ξ1 ξ1\\nξ2 ξ2ξ2 ξ2\\nFIGURE 5.2. A series of piecewise-cubic polynomials, with increasing orde rs of\\ncontinuity.\\nincreasing orders of continuity at the knots. The function in the lower\\nright panel is continuous, and has continuous ﬁrst and second derivatives\\nat the knots. It is known as a cubic spline . Enforcing one more order of\\ncontinuity would lead to a global cubic polynomial. It is not hard to show\\n(Exercise 5.1) that the following basis represents a cubic spline with knots\\natξ1andξ2:\\nh1(X) = 1, h 3(X) =X2, h5(X) = (X−ξ1)3\\n+,\\nh2(X) =X, h 4(X) =X3, h6(X) = (X−ξ2)3\\n+.(5.3)\\nThere are six basis functions corresponding to a six-dimensional linear space\\nof functions. A quick check conﬁrms the parameter count: (3 regions) ×(4\\nparameters per region) −(2 knots) ×(3 constraints per knot)= 6.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db4b34fd-c109-4068-bf2e-6b121f4fedad', embedding=None, metadata={'page_label': '163', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='144 5. Basis Expansions and Regularization\\nMore generally, an order- Mspline with knots ξj, j= 1,... ,K is a\\npiecewise-polynomial of order M, and has continuous derivatives up to\\norder M−2. A cubic spline has M= 4. In fact the piecewise-constant\\nfunction in Figure 5.1 is an order-1 spline, while the continuous piece-\\nwise linear function is an order-2 spline. Likewise the general form for the\\ntruncated-power basis set would be\\nhj(X) = Xj−1, j= 1,... ,M,\\nhM+ℓ(X) = ( X−ξℓ)M−1\\n+, ℓ= 1,... ,K.\\nIt is claimed that cubic splines are the lowest-order spline for which the\\nknot-discontinuity is not visible to the human eye. There is seldom any\\ngood reason to go beyond cubic-splines, unless one is interested in smooth\\nderivatives. In practice the most widely used orders are M= 1,2 and 4.\\nThese ﬁxed-knot splines are also known as regression splines . One needs\\nto select the order of the spline, the number of knots and their placement.\\nOne simple approach is to parameterize a family of splines by the number\\nof basis functions or degrees of freedom, and have the observations xide-\\ntermine the positions of the knots. For example, the expression bs(x,df=7)\\ninRgenerates a basis matrix of cubic-spline functions evaluated at the N\\nobservations in x, with the 7 −3 = 41interior knots at the appropriate per-\\ncentiles of x(20,40,60 and 80th.) One can be more explicit, however; bs(x,\\ndegree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,\\nwith three interior knots, and returns an N×4 matrix.\\nSince the space of spline functions of a particular order and knot sequence\\nis a vector space, there are many equivalent bases for representing them\\n(just as there are for ordinary polynomials.) While the truncated power\\nbasis is conceptually simple, it is not too attractive numerically: powers of\\nlarge numbers can lead to severe rounding problems. The B-spline basis,\\ndescribed in the Appendix to this chapter, allows for eﬃcient computations\\neven when the number of knots Kis large.\\n5.2.1 Natural Cubic Splines\\nWe know that the behavior of polynomials ﬁt to data tends to be erratic\\nnear the boundaries, and extrapolation can be dangerous. These problems\\nare exacerbated with splines. The polynomials ﬁt beyond the boundary\\nknots behave even more wildly than the corresponding global polynomials\\nin that region. This can be conveniently summarized in terms of the point-\\nwise variance of spline functions ﬁt by least squares (see the example in the\\nnext section for details on these variance calculations). Figure 5.3 compares\\n1A cubic spline with four knots is eight-dimensional. The bs() function omits by\\ndefault the constant term in the basis, since terms like this are typically included with\\nother terms in the model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b0ea1123-75d5-4dfb-b783-0c67cd3ffed2', embedding=None, metadata={'page_label': '164', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.2 Piecewise Polynomials and Splines 145\\nXPointwise Variances\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6•\\n••\\n•\\n••\\n•••••••••••••• •••• ••••••••••••••••••••••••••\\n•••••\\n•••••••••••••••••••••••••••••••••••••••••••\\n•\\n••\\n•\\n••••••••••••••••••••••••••\\n•\\n••••••••••••••••••\\n•••••••••••••••••••• ••••••••••••••••••••••••• ••••Global Linear\\nGlobal Cubic Polynomial\\nCubic Spline - 2 knots\\nNatural Cubic Spline - 6 knots\\nFIGURE 5.3. Pointwise variance curves for four diﬀerent models, with Xcon-\\nsisting of 50points drawn at random from U[0,1], and an assumed error model\\nwith constant variance. The linear and cubic polynomial ﬁts have two and four\\ndegrees of freedom, respectively, while the cubic spline and na tural cubic spline\\neach have six degrees of freedom. The cubic spline has two knots at0.33and0.66,\\nwhile the natural spline has boundary knots at 0.1and0.9, and four interior knots\\nuniformly spaced between them.\\nthe pointwise variances for a variety of diﬀerent models. The explosion of\\nthe variance near the boundaries is clear, and inevitably is worst for cubic\\nsplines.\\nAnatural cubic spline adds additional constraints, namely that the func-\\ntion is linear beyond the boundary knots. This frees up four degrees of\\nfreedom (two constraints each in both boundary regions), which can be\\nspent more proﬁtably by sprinkling more knots in the interior region. This\\ntradeoﬀ is illustrated in terms of variance in Figure 5.3. There will be a\\nprice paid in bias near the boundaries, but assuming the function is lin-\\near near the boundaries (where we have less information anyway) is often\\nconsidered reasonable.\\nA natural cubic spline with Kknots is represented by Kbasis functions.\\nOne can start from a basis for cubic splines, and derive the reduced ba-\\nsis by imposing the boundary constraints. For example, starting from the\\ntruncated power series basis described in Section 5.2, we arrive at (Exer-\\ncise 5.4):\\nN1(X) = 1, N 2(X) =X, N k+2(X) =dk(X)−dK−1(X),(5.4)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb513135-09c8-4410-8554-e9abdc2a0b9b', embedding=None, metadata={'page_label': '165', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='146 5. Basis Expansions and Regularization\\nwhere\\ndk(X) =(X−ξk)3\\n+−(X−ξK)3\\n+\\nξK−ξk. (5.5)\\nEach of these basis functions can be seen to have zero second and third\\nderivative for X≥ξK.\\n5.2.2 Example: South African Heart Disease (Continued)\\nIn Section 4.4.2 we ﬁt linear logistic regression models to the South African\\nheart disease data. Here we explore nonlinearities in the functions using\\nnatural splines. The functional form of the model is\\nlogit[Pr(chd|X)] =θ0+h1(X1)Tθ1+h2(X2)Tθ2+≤≤≤+hp(Xp)Tθp,(5.6)\\nwhere each of the θjare vectors of coeﬃcients multiplying their associated\\nvector of natural spline basis functions hj.\\nWe use four natural spline bases for each term in the model. For example,\\nwithX1representing sbp,h1(X1) is a basis consisting of four basis func-\\ntions. This actually implies three rather than two interior knots (chosen at\\nuniform quantiles of sbp), plus two boundary knots at the extremes of the\\ndata, since we exclude the constant term from each of the hj.\\nSincefamhist is a two-level factor, it is coded by a simple binary or\\ndummy variable, and is associated with a single coeﬃcient in the ﬁt of the\\nmodel.\\nMore compactly we can combine all pvectors of basis functions (and\\nthe constant term) into one big vector h(X), and then the model is simply\\nh(X)Tθ, with total number of parameters df = 1 +∑p\\nj=1dfj, the sum of\\nthe parameters in each component term. Each basis function is evaluated\\nat each of the Nsamples, resulting in a N×df basis matrix H. At this\\npoint the model is like any other linear logistic model, and the algorithms\\ndescribed in Section 4.4.1 apply.\\nWe carried out a backward stepwise deletion process, dropping terms\\nfrom this model while preserving the group structure of each term, rather\\nthan dropping one coeﬃcient at a time. The AIC statistic (Section 7.5) was\\nused to drop terms, and all the terms remaining in the ﬁnal model would\\ncause AIC to increase if deleted from the model (see Table 5.1). Figure 5.4\\nshows a plot of the ﬁnal model selected by the stepwise regression. The\\nfunctions displayed are ˆfj(Xj) =hj(Xj)Tˆθjfor each variable Xj. The\\ncovariance matrix Cov( ˆθ) =Σis estimated by ˆΣ= (HTWH)−1, where W\\nis the diagonal weight matrix from the logistic regression. Hence vj(Xj) =\\nVar[ˆfj(Xj)] =hj(Xj)TˆΣjjhj(Xj) is the pointwise variance function of ˆfj,\\nwhere Cov( ˆθj) =ˆΣjjis the appropriate sub-matrix of ˆΣ. The shaded region\\nin each panel is deﬁned by ˆfj(Xj)±2√\\nvj(Xj).\\nThe AIC statistic is slightly more generous than the likelihood-ratio tes t\\n(deviance test). Both sbpandobesity are included in this model, while', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09324dd8-b0a8-4b38-b1a5-2f98de5b8070', embedding=None, metadata={'page_label': '166', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.2 Piecewise Polynomials and Splines 147\\n100 120 140 160 180 200 220-2 0 2 4\\n0 5 10 15 20 25 300 2 4 6 8\\n2 4 6 8 10 12 14-4 -2 0 2 4\\n-4 -2 0 2 4\\nAbsent Present\\n15 20 25 30 35 40 45-2 0 2 4 6\\n20 30 40 50 60-6 -4 -2 0 2ˆf(sbp)\\nsbp\\nˆf(tobacco )\\ntobaccoˆf(ldl)\\nldlˆf(obesity )\\nobesity\\nˆf(age)\\nageˆf(famhist )\\nfamhist\\nFIGURE 5.4. Fitted natural-spline functions for each of the terms in the ﬁnal\\nmodel selected by the stepwise procedure. Included are pointw ise standard-error\\nbands. The rug plot at the base of each ﬁgure indicates the location of each of the\\nsample values for that variable (jittered to break ties).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4b8cb841-5f19-4dad-ae3c-2bac31a20ca4', embedding=None, metadata={'page_label': '167', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='148 5. Basis Expansions and Regularization\\nTABLE 5.1. Final logistic regression model, after stepwise deletion of natural\\nsplines terms. The column labeled “LRT” is the likelihood-ra tio test statistic when\\nthat term is deleted from the model, and is the change in deviance from the full\\nmodel (labeled “none”).\\nTerms Df Deviance AIC LRT P-value\\nnone 458.09 502.09\\nsbp 4 467.16 503.16 9.076 0.059\\ntobacco 4 470.48 506.48 12.387 0.015\\nldl 4 472.39 508.39 14.307 0.006\\nfamhist 1 479.44 521.44 21.356 0.000\\nobesity 4 466.24 502.24 8.147 0.086\\nage 4 481.86 517.86 23.768 0.000\\nthey were not in the linear model. The ﬁgure explains why, since their\\ncontributions are inherently nonlinear. These eﬀects at ﬁrst may come as\\na surprise, but an explanation lies in the nature of the retrospective data.\\nThese measurements were made sometime after the patients suﬀered a\\nheart attack, and in many cases they had already beneﬁted from a healthier\\ndiet and lifestyle, hence the apparent increase in risk at low values for\\nobesity andsbp. Table 5.1 shows a summary of the selected model.\\n5.2.3 Example: Phoneme Recognition\\nIn this example we use splines to reduce ﬂexibility rather than increase it;\\nthe application comes under the general heading of functional modeling. In\\nthe top panel of Figure 5.5 are displayed a sample of 15 log-periodograms\\nfor each of the two phonemes “aa” and “ao” measured at 256 frequencies.\\nThe goal is to use such data to classify a spoken phoneme. These two\\nphonemes were chosen because they are diﬃcult to separate.\\nThe input feature is a vector xof length 256, which we can think of as\\na vector of evaluations of a function X(f) over a grid of frequencies f. In\\nreality there is a continuous analog signal which is a function of frequency,\\nand we have a sampled version of it.\\nThe gray lines in the lower panel of Figure 5.5 show the coeﬃcients of\\na linear logistic regression model ﬁt by maximum likelihood to a training\\nsample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s. The\\ncoeﬃcients are also plotted as a function of frequency, and in fact we can\\nthink of the model in terms of its continuous counterpart\\nlogPr(aa|X)\\nPr(ao|X)=∫\\nX(f)β(f)df, (5.7)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='41434a11-8244-43f2-9485-8c38ae905239', embedding=None, metadata={'page_label': '168', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.2 Piecewise Polynomials and Splines 149\\nFrequencyLog-periodogram\\n0 50 100 150 200 2500 5 10 15 20 25Phoneme Examples\\naa\\nao\\nFrequencyLogistic Regression Coefficients\\n0 50 100 150 200 250-0.4 -0.2 0.0 0.2 0.4Phoneme Classification: Raw and Restricted Logistic Regression\\nFIGURE 5.5. The top panel displays the log-periodogram as a function of fre -\\nquency for 15examples each of the phonemes “aa” and “ao” sampled from a total\\nof695“aa”s and 1022“ao”s. Each log-periodogram is measured at 256uniformly\\nspaced frequencies. The lower panel shows the coeﬃcients (as a f unction of fre-\\nquency) of a logistic regression ﬁt to the data by maximum likeli hood, using the\\n256log-periodogram values as inputs. The coeﬃcients are restric ted to be smooth\\nin the red curve, and are unrestricted in the jagged gray curve.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ddd1c8f-0652-409f-ad3b-c865d4569e10', embedding=None, metadata={'page_label': '169', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='150 5. Basis Expansions and Regularization\\nwhich we approximate by\\n256∑\\nj=1X(fj)β(fj) =256∑\\nj=1xjβj. (5.8)\\nThe coeﬃcients compute a contrast functional, and will have appreciable\\nvalues in regions of frequency where the log-periodograms diﬀer between\\nthe two classes.\\nThe gray curves are very rough. Since the input signals have fairly strong\\npositive autocorrelation, this results in negative autocorrelation in t he co-\\neﬃcients. In addition the sample size eﬀectively provides only four obser-\\nvations per coeﬃcient.\\nApplications such as this permit a natural regularization. We force the\\ncoeﬃcients to vary smoothly as a function of frequency. The red curve in the\\nlower panel of Figure 5.5 shows such a smooth coeﬃcient curve ﬁt to these\\ndata. We see that the lower frequencies oﬀer the most discriminatory power.\\nNot only does the smoothing allow easier interpretation of the contrast, it\\nalso produces a more accurate classiﬁer:\\nRaw Regularized\\nTraining error 0.080 0.185\\nTest error 0.255 0.158\\nThe smooth red curve was obtained through a very simple use of natural\\ncubic splines. We can represent the coeﬃcient function as an expansion of\\nsplines β(f) =∑M\\nm=1hm(f)θm. In practice this means that β=Hθwhere,\\nHis ap×Mbasis matrix of natural cubic splines, deﬁned on the set of\\nfrequencies. Here we used M= 12 basis functions, with knots uniformly\\nplaced over the integers 1 ,2,... ,256 representing the frequencies. Since\\nxTβ=xTHθ, we can simply replace the input features xby their ﬁltered\\nversions x∗=HTx, and ﬁt θby linear logistic regression on the x∗. The\\nred curve is thus ˆβ(f) =h(f)Tˆθ.\\n5.3 Filtering and Feature Extraction\\nIn the previous example, we constructed a p×Mbasis matrix H, and then\\ntransformed our features xinto new features x∗=HTx. These ﬁltered\\nversions of the features were then used as inputs into a learning procedure:\\nin the previous example, this was linear logistic regression.\\nPreprocessing of high-dimensional features is a very general and pow-\\nerful method for improving the performance of a learning algorithm. The\\npreprocessing need not be linear as it was above, but can be a general', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='875e679e-9ff0-4642-a222-dd6841cc46d9', embedding=None, metadata={'page_label': '170', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.4 Smoothing Splines 151\\n(nonlinear) function of the form x∗=g(x). The derived features x∗can\\nthen be used as inputs into any (linear or nonlinear) learning procedure.\\nFor example, for signal or image recognition a popular approach is to ﬁrst\\ntransform the raw features via a wavelet transform x∗=HTx(Section 5.9)\\nand then use the features x∗as inputs into a neural network (Chapter 11).\\nWavelets are eﬀective in capturing discrete jumps or edges, and the neural\\nnetwork is a powerful tool for constructing nonlinear functions of these\\nfeatures for predicting the target variable. By using domain knowledge\\nto construct appropriate features, one can often improve upon a learning\\nmethod that has only the raw features xat its disposal.\\n5.4 Smoothing Splines\\nHere we discuss a spline basis method that avoids the knot selection prob-\\nlem completely by using a maximal set of knots. The complexity of the ﬁt\\nis controlled by regularization. Consider the following problem: among all\\nfunctions f(x) with two continuous derivatives, ﬁnd one that minimizes the\\npenalized residual sum of squares\\nRSS(f,λ) =N∑\\ni=1{yi−f(xi)}2+λ∫\\n{f′′(t)}2dt, (5.9)\\nwhere λis a ﬁxed smoothing parameter . The ﬁrst term measures closeness\\nto the data, while the second term penalizes curvature in the function, and\\nλestablishes a tradeoﬀ between the two. Two special cases are:\\nλ= 0 : fcan be any function that interpolates the data.\\nλ=∞:the simple least squares line ﬁt, since no second derivative can\\nbe tolerated.\\nThese vary from very rough to very smooth, and the hope is that λ∈(0,∞)\\nindexes an interesting class of functions in between.\\nThe criterion (5.9) is deﬁned on an inﬁnite-dimensional function space—\\nin fact, a Sobolev space of functions for which the second term is deﬁned.\\nRemarkably, it can be shown that (5.9) has an explicit, ﬁnite-dimensional,\\nunique minimizer which is a natural cubic spline with knots at the unique\\nvalues of the xi, i= 1,... ,N (Exercise 5.7). At face value it seems that\\nthe family is still over-parametrized, since there are as many as Nknots,\\nwhich implies Ndegrees of freedom. However, the penalty term translates\\nto a penalty on the spline coeﬃcients, which are shrunk some of the way\\ntoward the linear ﬁt.\\nSince the solution is a natural spline, we can write it as\\nf(x) =N∑\\nj=1Nj(x)θj, (5.10)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59876055-0aeb-4990-8f1a-8db4016db244', embedding=None, metadata={'page_label': '171', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='152 5. Basis Expansions and Regularization\\nAgeRelative Change in Spinal BMD\\n10 15 20 25-0.05 0.0 0.05 0.10 0.15 0.20••\\n••\\n••\\n••\\n•\\n••\\n••\\n••\\n••\\n•\\n•\\n•\\n••••\\n••\\n•\\n••\\n•\\n•\\n••\\n•••\\n••\\n•••\\n•••\\n•\\n•\\n•\\n••\\n•\\n••••\\n•••••\\n••\\n•••••\\n•\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••\\n••••\\n•\\n•••\\n••\\n•••••\\n•\\n••\\n••\\n•\\n•\\n••••\\n• ••• ••••\\n••\\n••\\n•••\\n•\\n••\\n•••••\\n•\\n•••••\\n••\\n•••\\n•\\n• •••\\n•••\\n••••\\n••\\n••••\\n• •\\n••••\\n••• •\\n••••\\n•\\n•••\\n•\\n••••••\\n••\\n•\\n••\\n•\\n•\\n••\\n••\\n•\\n••\\n•\\n• •••\\n•\\n•\\n••••••\\n•\\n••\\n••\\n••••\\n•\\n••\\n••\\n•\\n••••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n•\\n• •••\\n• ••\\n•••••\\n•\\n••\\n•••\\n•\\n••••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n••\\n•••••\\n••\\n•\\n••••••\\n•\\n•\\n•••\\n• ••\\n•••••••\\n•\\n•••\\n••\\n•\\n•••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•• ••\\n•\\n••••\\n•\\n•\\n•••\\n••\\n• •\\n•••\\n•\\n•••\\n• •\\n••\\n•\\n•••\\n••••\\n•\\n••••\\n••\\n•\\n••\\n••••\\n••••\\n•\\n••\\n•\\n•\\n••••\\n•\\n••\\n•\\n••••\\n• ••\\n•\\n••••\\n•\\n••\\n•••\\n•\\n••\\n•••\\n•\\n•\\n••\\n••\\n•\\n•\\n••\\n••\\n••\\n•••\\n•••\\n•\\n••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n••\\n••\\n•\\n•••\\n•••\\n• ••••\\n••\\n••\\n••\\n•\\n•Male\\nFemale\\nFIGURE 5.6. The response is the relative change in bone mineral density mea-\\nsured at the spine in adolescents, as a function of age. A separat e smoothing spline\\nwas ﬁt to the males and females, with λ≈0.00022. This choice corresponds to\\nabout 12degrees of freedom.\\nwhere the Nj(x) are an N-dimensional set of basis functions for repre-\\nsenting this family of natural splines (Section 5.2.1 and Exercise 5.4). The\\ncriterion thus reduces to\\nRSS(θ,λ) = (y−Nθ)T(y−Nθ) +λθTΩNθ, (5.11)\\nwhere {N}ij=Nj(xi) and {ΩN}jk=∫\\nN′′\\nj(t)N′′\\nk(t)dt. The solution is\\neasily seen to be\\nˆθ= (NTN+λΩN)−1NTy, (5.12)\\na generalized ridge regression. The ﬁtted smoothing spline is given by\\nˆf(x) =N∑\\nj=1Nj(x)ˆθj. (5.13)\\nEﬃcient computational techniques for smoothing splines are discussed in\\nthe Appendix to this chapter.\\nFigure 5.6 shows a smoothing spline ﬁt to some data on bone mineral\\ndensity (BMD) in adolescents. The response is relative change in spinal\\nBMD over two consecutive visits, typically about one year apart. The data\\nare color coded by gender, and two separate curves were ﬁt. This simple', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='21acee15-9d65-42bb-975a-529390bd360a', embedding=None, metadata={'page_label': '172', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.4 Smoothing Splines 153\\nsummary reinforces the evidence in the data that the growth spurt for\\nfemales precedes that for males by about two years. In both cases the\\nsmoothing parameter λwas approximately 0 .00022; this choice is discussed\\nin the next section.\\n5.4.1 Degrees of Freedom and Smoother Matrices\\nWe have not yet indicated how λis chosen for the smoothing spline. Later\\nin this chapter we describe automatic methods using techniques such as\\ncross-validation. In this section we discuss intuitive ways of prespecifying\\nthe amount of smoothing.\\nA smoothing spline with prechosen λis an example of a linear smoother\\n(as in linear operator). This is because the estimated parameters in (5.12)\\nare a linear combination of the yi. Denote by ˆftheN-vector of ﬁtted values\\nˆf(xi) at the training predictors xi. Then\\nˆf=N(NTN+λΩN)−1NTy\\n=Sλy. (5.14)\\nAgain the ﬁt is linear in y, and the ﬁnite linear operator Sλis known as\\nthesmoother matrix . One consequence of this linearity is that the recipe\\nfor producing ˆffromydoes not depend on yitself;Sλdepends only on\\nthexiandλ.\\nLinear operators are familiar in more traditional least squares ﬁtting as\\nwell. Suppose Bξis aN×Mmatrix of Mcubic-spline basis functions\\nevaluated at the Ntraining points xi, with knot sequence ξ, and M≪N.\\nThen the vector of ﬁtted spline values is given by\\nˆf=Bξ(BT\\nξBξ)−1BT\\nξy\\n=Hξy. (5.15)\\nHere the linear operator Hξis a projection operator, also known as the hat\\nmatrix in statistics. There are some important similarities and di ﬀerences\\nbetween HξandSλ:\\n•Both are symmetric, positive semideﬁnite matrices.\\n•HξHξ=Hξ(idempotent), while SλSλ⪯Sλ, meaning that the right-\\nhand side exceeds the left-hand side by a positive semideﬁnite matrix.\\nThis is a consequence of the shrinking nature of Sλ, which we discuss\\nfurther below.\\n•Hξhas rank M, while Sλhas rank N.\\nThe expression M= trace( Hξ) gives the dimension of the projection space,\\nwhich is also the number of basis functions, and hence the number of pa-\\nrameters involved in the ﬁt. By analogy we deﬁne the eﬀective degrees of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab33545a-5b00-446e-a2be-eaca3cac5780', embedding=None, metadata={'page_label': '173', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='154 5. Basis Expansions and Regularization\\nfreedom of a smoothing spline to be\\ndfλ= trace( Sλ), (5.16)\\nthe sum of the diagonal elements of Sλ. This very useful deﬁnition allows\\nus a more intuitive way to parameterize the smoothing spline, and indeed\\nmany other smoothers as well, in a consistent fashion. For example, in Fig-\\nure 5.6 we speciﬁed df λ= 12 for each of the curves, and the corresponding\\nλ≈0.00022 was derived numerically by solving trace( Sλ) = 12. There are\\nmany arguments supporting this deﬁnition of degrees of freedom, and we\\ncover some of them here.\\nSinceSλis symmetric (and positive semideﬁnite), it has a real eigen-\\ndecomposition. Before we proceed, it is convenient to rewrite Sλin the\\nReinsch form\\nSλ= (I+λK)−1, (5.17)\\nwhere Kdoes not depend on λ(Exercise 5.9). Since ˆf=Sλysolves\\nmin\\nf(y−f)T(y−f) +λfTKf, (5.18)\\nKis known as the penalty matrix , and indeed a quadratic form in Khas\\na representation in terms of a weighted sum of squared (divided) second\\ndiﬀerences. The eigen-decomposition of Sλis\\nSλ=N∑\\nk=1ρk(λ)ukuT\\nk (5.19)\\nwith\\nρk(λ) =1\\n1 +λdk, (5.20)\\nanddkthe corresponding eigenvalue of K. Figure 5.7 (top) shows the re-\\nsults of applying a cubic smoothing spline to some air pollution data (128\\nobservations). Two ﬁts are given: a smoother ﬁt corresponding to a larger\\npenalty λand a rougher ﬁt for a smaller penalty. The lower panels repre-\\nsent the eigenvalues (lower left) and some eigenvectors (lower right) of the\\ncorresponding smoother matrices. Some of the highlights of the eigenrep-\\nresentation are the following:\\n•The eigenvectors are not aﬀected by changes in λ, and hence the whole\\nfamily of smoothing splines (for a particular sequence x) indexed by\\nλhave the same eigenvectors.\\n•Sλy=∑N\\nk=1ukρk(λ)⟨uk,y⟩, and hence the smoothing spline oper-\\nates by decomposing yw.r.t. the (complete) basis {uk}, and diﬀer-\\nentially shrinking the contributions using ρk(λ). This is to be con-\\ntrasted with a basis-regression method, where the components are', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5520c51-11c0-4e5a-87e3-f65c1f929c0a', embedding=None, metadata={'page_label': '174', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.4 Smoothing Splines 155\\nDaggot Pressure GradientOzone Concentration\\n-50 0 50 1000 10 20 30•••\\n••••••\\n••\\n••••••\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n•\\n••\\n•\\n••\\n•••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•••••\\n•\\n•••\\n•••\\n••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n••\\n•••\\n•\\n••••\\n••\\n••\\n••\\n•••\\n•••\\n•••\\n••\\n•\\n••\\n••••\\n••\\n•••\\n•\\n•\\n•\\n•\\nOrderEigenvalues\\n5 10 15 20 25-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2•••\\n•\\n•\\n•\\n••••••••••• •• • • •• • ••••••••\\n•\\n•\\n•\\n•\\n•\\n•••••••••••••df=5\\ndf=11\\n-50 0 50 100 -50 0 50 100\\nFIGURE 5.7. (Top:) Smoothing spline ﬁt of ozone concentration versus Daggot\\npressure gradient. The two ﬁts correspond to diﬀerent values of t he smoothing\\nparameter, chosen to achieve ﬁve and eleven eﬀective degrees o f freedom, deﬁned\\nby dfλ=trace(Sλ). (Lower left:) First 25eigenvalues for the two smoothing-spline\\nmatrices. The ﬁrst two are exactly 1, and all are ≥0. (Lower right:) Third to\\nsixth eigenvectors of the spline smoother matrices. In each cas e,ukis plotted\\nagainst x, and as such is viewed as a function of x. The rugat the base of the\\nplots indicate the occurrence of data points. The damped functio ns represent the\\nsmoothed versions of these functions (using the 5df smoother).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c336ff16-8f97-40db-ac91-155aa557642a', embedding=None, metadata={'page_label': '175', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='156 5. Basis Expansions and Regularization\\neither left alone, or shrunk to zero—that is, a projection matrix such\\nasHξabove has Meigenvalues equal to 1, and the rest are 0. For\\nthis reason smoothing splines are referred to as shrinking smoothers,\\nwhile regression splines are projection smoothers (see Figure 3.17 on\\npage 80).\\n•The sequence of uk, ordered by decreasing ρk(λ), appear to increase\\nin complexity. Indeed, they have the zero-crossing behavior of polyno-\\nmials of increasing degree. Since Sλuk=ρk(λ)uk, we see how each of\\nthe eigenvectors themselves are shrunk by the smoothing spline: the\\nhigher the complexity, the more they are shrunk. If the domain of X\\nis periodic, then the ukare sines and cosines at diﬀerent frequencies.\\n•The ﬁrst two eigenvalues are always one, and they correspond to the\\ntwo-dimensional eigenspace of functions linear in x(Exercise 5.11),\\nwhich are never shrunk.\\n•The eigenvalues ρk(λ) = 1/(1 +λdk) are an inverse function of the\\neigenvalues dkof the penalty matrix K, moderated by λ;λcontrols\\nthe rate at which the ρk(λ) decrease to zero. d1=d2= 0 and again\\nlinear functions are not penalized.\\n•One can reparametrize the smoothing spline using the basis vectors\\nuk(theDemmler–Reinsch basis). In this case the smoothing spline\\nsolves\\nmin\\nθ∥y−Uθ∥2+λθTDθ, (5.21)\\nwhere Uhas columns ukandDis a diagonal matrix with elements\\ndk.\\n•dfλ= trace( Sλ) =∑N\\nk=1ρk(λ). For projection smoothers, all the\\neigenvalues are 1, each one corresponding to a dimension of the pro-\\njection subspace.\\nFigure 5.8 depicts a smoothing spline matrix, with the rows ordered with\\nx. The banded nature of this representation suggests that a smoothing\\nspline is a local ﬁtting method, much like the locally weighted regression\\nprocedures in Chapter 6. The right panel shows in detail selected rows of\\nS, which we call the equivalent kernels . Asλ→0, df λ→N, andSλ→I,\\ntheN-dimensional identity matrix. As λ→ ∞, dfλ→2, and Sλ→H, the\\nhat matrix for linear regression on x.\\n5.5 Automatic Selection of the Smoothing\\nParameters\\nThe smoothing parameters for regression splines encompass the degree of\\nthe splines, and the number and placement of the knots. For smoothing', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e065b5b7-57e8-404e-afe0-ae343c412339', embedding=None, metadata={'page_label': '176', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.5 Automatic Selection of the Smoothing Parameters 157\\n11510075502512Smoother Matrix\\n••••• • •••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•Row 115••••• ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•Row 100••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Row 75•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 50•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 25•••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 12Equivalent Kernels\\nFIGURE 5.8. The smoother matrix for a smoothing spline is nearly banded,\\nindicating an equivalent kernel with local support. The left pane l represents the\\nelements of Sas an image. The right panel shows the equivalent kernel or weigh t-\\ning function in detail for the indicated rows.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa5d426c-d227-44bc-b750-4a55ba6d843d', embedding=None, metadata={'page_label': '177', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='158 5. Basis Expansions and Regularization\\nsplines, we have only the penalty parameter λto select, since the knots are\\nat all the unique training X’s, and cubic degree is almost always used in\\npractice.\\nSelecting the placement and number of knots for regression splines can be\\na combinatorially complex task, unless some simpliﬁcations are enforced.\\nThe MARS procedure in Chapter 9 uses a greedy algorithm with some\\nadditional approximations to achieve a practical compromise. We will not\\ndiscuss this further here.\\n5.5.1 Fixing the Degrees of Freedom\\nSince df λ= trace( Sλ) is monotone in λfor smoothing splines, we can in-\\nvert the relationship and specify λby ﬁxing df. In practice this can be\\nachieved by simple numerical methods. So, for example, in Rone can use\\nsmooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-\\nages a more traditional mode of model selection, where we might try a cou-\\nple of diﬀerent values of df, and select one based on approximate F-tests,\\nresidual plots and other more subjective criteria. Using df in this way pro-\\nvides a uniform approach to compare many diﬀerent smoothing methods.\\nIt is particularly useful in generalized additive models (Chapter 9), where\\nseveral smoothing methods can be simultaneously used in one model.\\n5.5.2 The Bias–Variance Tradeoﬀ\\nFigure 5.9 shows the eﬀect of the choice of df λwhen using a smoothing\\nspline on a simple example:\\nY=f(X) +ε,\\nf(X) =sin(12( X+ 0.2))\\nX+ 0.2,(5.22)\\nwithX∼U[0,1] and ε∼N(0,1). Our training sample consists of N= 100\\npairsxi,yidrawn independently from this model.\\nThe ﬁtted splines for three diﬀerent values of df λare shown. The yellow\\nshaded region in the ﬁgure represents the pointwise standard error of ˆfλ,\\nthat is, we have shaded the region between ˆfλ(x)±2≤se(ˆfλ(x)). Since\\nˆf=Sλy,\\nCov(ˆf) = SλCov(y)ST\\nλ\\n=SλST\\nλ. (5.23)\\nThe diagonal contains the pointwise variances at the training xi. The bias\\nis given by\\nBias(ˆf) = f−E(ˆf)\\n=f−Sλf, (5.24)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='edc6afce-daec-47e0-b973-2edf03eb4ff0', embedding=None, metadata={'page_label': '178', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.5 Automatic Selection of the Smoothing Parameters 159\\n6 8 10 12 140.9 1.0 1.1 1.2•••••••• ••••••••••\\n••••••••••••••••••\\ny\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\\nO\\nOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOO\\nOOOOOOOO\\nOOO\\nOOOO\\nO\\nO\\nO\\nOOOO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOy\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\\nO\\nOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOO\\nOOOOOOOO\\nOOO\\nOOOO\\nO\\nO\\nO\\nOOOO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOO\\nO\\nO\\ny\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\\nO\\nOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOO\\nOOOOOOOO\\nOOO\\nOOOO\\nO\\nO\\nO\\nOOOO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOEPECV\\nX XXdfλ= 5\\ndfλ= 9 dfλ= 15dfλCross-ValidationEPE( λ) and CV( λ)\\nFIGURE 5.9. The top left panel shows the EPE( λ)andCV(λ)curves for a\\nrealization from a nonlinear additive error model (5.22). The r emaining panels\\nshow the data, the true functions (in purple), and the ﬁtted curve s (in green) with\\nyellow shaded ±2×standard error bands, for three diﬀerent values of dfλ.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='973ea607-9dc7-4efd-a3bf-c265f7c2dadc', embedding=None, metadata={'page_label': '179', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='160 5. Basis Expansions and Regularization\\nwherefis the (unknown) vector of evaluations of the true fat the training\\nX’s. The expectations and variances are with respect to repeated draws\\nof samples of size N= 100 from the model (5.22). In a similar fashion\\nVar(ˆfλ(x0)) and Bias( ˆfλ(x0)) can be computed at any point x0(Exer-\\ncise 5.10). The three ﬁts displayed in the ﬁgure give a visual demonstration\\nof the bias-variance tradeoﬀ associated with selecting the smoothing\\nparameter.\\ndfλ= 5:The spline under ﬁts, and clearly trims down the hills and ﬁlls in\\nthe valleys . This leads to a bias that is most dramatic in regions of\\nhigh curvature. The standard error band is very narrow, so we esti-\\nmate a badly biased version of the true function with great reliability!\\ndfλ= 9:Here the ﬁtted function is close to the true function, although a\\nslight amount of bias seems evident. The variance has not increased\\nappreciably.\\ndfλ= 15: The ﬁtted function is somewhat wiggly, but close to the true\\nfunction. The wiggliness also accounts for the increased width of the\\nstandard error bands—the curve is starting to follow some individual\\npoints too closely.\\nNote that in these ﬁgures we are seeing a single realization of data and\\nhence ﬁtted spline ˆfin each case, while the bias involves an expectation\\nE(ˆf). We leave it as an exercise (5.10) to compute similar ﬁgures where the\\nbias is shown as well. The middle curve seems “just right,” in that it has\\nachieved a good compromise between bias and variance.\\nThe integrated squared prediction error (EPE) combines both bias and\\nvariance in a single summary:\\nEPE( ˆfλ) = E( Y−ˆfλ(X))2\\n= Var( Y) + E[\\nBias2(ˆfλ(X)) + Var( ˆfλ(X))]\\n=σ2+ MSE( ˆfλ). (5.25)\\nNote that this is averaged both over the training sample (giving rise to ˆfλ),\\nand the values of the (independently chosen) prediction points ( X,Y). EPE\\nis a natural quantity of interest, and does create a tradeoﬀ between bias\\nand variance. The blue points in the top left panel of Figure 5.9 suggest\\nthat df λ= 9 is spot on!\\nSince we don’t know the true function, we do not have access to EPE, and\\nneed an estimate. This topic is discussed in some detail in Chapter 7, and\\ntechniques such as K-fold cross-validation, GCV and Cpare all in common\\nuse. In Figure 5.9 we include the N-fold (leave-one-out) cross-validation\\ncurve:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6480613a-85cc-4b19-97a4-5d5fbc2addb6', embedding=None, metadata={'page_label': '180', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6 Nonparametric Logistic Regression 161\\nCV(ˆfλ) =1\\nNN∑\\ni=1(yi−ˆf(−i)\\nλ(xi))2(5.26)\\n=1\\nNN∑\\ni=1(\\nyi−ˆfλ(xi)\\n1−Sλ(i,i))2\\n, (5.27)\\nwhich can (remarkably) be computed for each value of λfrom the original\\nﬁtted values and the diagonal elements Sλ(i,i) ofSλ(Exercise 5.13).\\nThe EPE and CV curves have a similar shape, but the entire CV curve\\nis above the EPE curve. For some realizations this is reversed, and overall\\nthe CV curve is approximately unbiased as an estimate of the EPE curve.\\n5.6 Nonparametric Logistic Regression\\nThe smoothing spline problem (5.9) in Section 5.4 is posed in a regression\\nsetting. It is typically straightforward to transfer this technology t o other\\ndomains. Here we consider logistic regression with a single quantitativ e\\ninput X. The model is\\nlogPr(Y= 1|X=x)\\nPr(Y= 0|X=x)=f(x), (5.28)\\nwhich implies\\nPr(Y= 1|X=x) =ef(x)\\n1 +ef(x). (5.29)\\nFitting f(x) in a smooth fashion leads to a smooth estimate of the condi-\\ntional probability Pr( Y= 1|x), which can be used for classiﬁcation or risk\\nscoring.\\nWe construct the penalized log-likelihood criterion\\nℓ(f;λ) =N∑\\ni=1[yilogp(xi) + (1 −yi)log(1 −p(xi))]−1\\n2λ∫\\n{f′′(t)}2dt\\n=N∑\\ni=1[\\nyif(xi)−log(1 + ef(xi))]\\n−1\\n2λ∫\\n{f′′(t)}2dt, (5.30)\\nwhere we have abbreviated p(x) = Pr( Y= 1|x). The ﬁrst term in this ex-\\npression is the log-likelihood based on the binomial distribution (c.f. Chap-\\nter 4, page 120). Arguments similar to those used in Section 5.4 show that\\nthe optimal fis a ﬁnite-dimensional natural spline with knots at the unique', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='48321ba1-0ae6-4f6f-be2f-cc6ffbc7ecda', embedding=None, metadata={'page_label': '181', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='162 5. Basis Expansions and Regularization\\nvalues of x. This means that we can represent f(x) =∑N\\nj=1Nj(x)θj. We\\ncompute the ﬁrst and second derivatives\\n∂ℓ(θ)\\n∂θ=NT(y−p)−λΩθ, (5.31)\\n∂2ℓ(θ)\\n∂θ∂θT=−NTWN−λΩ, (5.32)\\nwhere pis the N-vector with elements p(xi), and Wis a diagonal matrix\\nof weights p(xi)(1−p(xi)). The ﬁrst derivative (5.31) is nonlinear in θ, so\\nwe need to use an iterative algorithm as in Section 4.4.1. Using Newton–\\nRaphson as in (4.23) and (4.26) for linear logistic regression, the updat e\\nequation can be written\\nθnew= (NTWN+λΩ)−1NTW(\\nNθold+W−1(y−p))\\n= (NTWN+λΩ)−1NTWz. (5.33)\\nWe can also express this update in terms of the ﬁtted values\\nfnew=N(NTWN+λΩ)−1NTW(\\nfold+W−1(y−p))\\n=Sλ,wz. (5.34)\\nReferring back to (5.12) and (5.14), we see that the update ﬁts a weighted\\nsmoothing spline to the working response z(Exercise 5.12).\\nThe form of (5.34) is suggestive. It is tempting to replace Sλ,wby any\\nnonparametric (weighted) regression operator, and obtain general fami-\\nlies of nonparametric logistic regression models. Although here xis one-\\ndimensional, this procedure generalizes naturally to higher-dimensional x.\\nThese extensions are at the heart of generalized additive models , which we\\npursue in Chapter 9.\\n5.7 Multidimensional Splines\\nSo far we have focused on one-dimensional spline models. Each of the ap-\\nproaches have multidimensional analogs. Suppose X∈IR2, and we have\\na basis of functions h1k(X1), k= 1,... ,M 1for representing functions of\\ncoordinate X1, and likewise a set of M2functions h2k(X2) for coordinate\\nX2. Then the M1×M2dimensional tensor product basis deﬁned by\\ngjk(X) =h1j(X1)h2k(X2), j= 1,... ,M 1, k= 1,... ,M 2 (5.35)\\ncan be used for representing a two-dimensional function:\\ng(X) =M1∑\\nj=1M2∑\\nk=1θjkgjk(X). (5.36)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1e676a6d-d5c2-4d60-b1f9-037c3f2bf759', embedding=None, metadata={'page_label': '182', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.7 Multidimensional Splines 163\\nFIGURE 5.10. A tensor product basis of B-splines, showing some selected pair s.\\nEach two-dimensional function is the tensor product of the corre sponding one\\ndimensional marginals.\\nFigure 5.10 illustrates a tensor product basis using B-splines. The coeﬃ-\\ncients can be ﬁt by least squares, as before. This can be generalized to d\\ndimensions, but note that the dimension of the basis grows exponentially\\nfast—yet another manifestation of the curse of dimensionality. The MARS\\nprocedure discussed in Chapter 9 is a greedy forward algorithm for includ-\\ning only those tensor products that are deemed necessary by least squares.\\nFigure 5.11 illustrates the diﬀerence between additive and tensor product\\n(natural) splines on the simulated classiﬁcation example from Chapter 2.\\nA logistic regression model logit[Pr( T|x)] =h(x)Tθis ﬁt to the binary re-\\nsponse, and the estimated decision boundary is the contour h(x)Tˆθ= 0.\\nThe tensor product basis can achieve more ﬂexibility at the decision bound-\\nary, but introduces some spurious structure along the way.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d011a183-4e82-4ea6-80fc-d1ff0849d334', embedding=None, metadata={'page_label': '183', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='164 5. Basis Expansions and Regularization\\nAdditive Natural Cubic Splines - 4 df each\\n.. .. . .. . . .. . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . . .. . . .. . . .. . . .. . .. . .. . .. .. .. ...\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.23\\nTest Error:       0.28\\nBayes Error:    0.21\\nNatural Cubic Splines - Tensor Product - 4 df each\\n. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . ... . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.230\\nTest Error:       0.282\\nBayes Error:    0.210\\nFIGURE 5.11. The simulation example of Figure 2.1. The upper panel shows the\\ndecision boundary of an additive logistic regression model, using natural splines\\nin each of the two coordinates (total df = 1 + (4 −1) + (4 −1) = 7 ). The lower\\npanel shows the results of using a tensor product of natural spline bases in each\\ncoordinate (total df = 4×4 = 16) . The broken purple boundary is the Bayes\\ndecision boundary for this problem.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5d65f5bb-1fad-4add-a231-38525ae43479', embedding=None, metadata={'page_label': '184', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.7 Multidimensional Splines 165\\nOne-dimensional smoothing splines (via regularization) generalize to high-\\ner dimensions as well. Suppose we have pairs yi,xiwithxi∈IRd, and we\\nseek a d-dimensional regression function f(x). The idea is to set up the\\nproblem\\nmin\\nfN∑\\ni=1{yi−f(xi)}2+λJ[f], (5.37)\\nwhere Jis an appropriate penalty functional for stabilizing a function fin\\nIRd. For example, a natural generalization of the one-dimensional roughness\\npenalty (5.9) for functions on IR2is\\nJ[f] =∫ ∫\\nI R2[(∂2f(x)\\n∂x2\\n1)2\\n+2(∂2f(x)\\n∂x1∂x2)2\\n+(∂2f(x)\\n∂x2\\n2)2]\\ndx1dx2.(5.38)\\nOptimizing (5.37) with this penalty leads to a smooth two-dimensional\\nsurface, known as a thin-plate spline. It shares many properties with the\\none-dimensional cubic smoothing spline:\\n•asλ→0, the solution approaches an interpolating function [the one\\nwith smallest penalty (5.38)];\\n•asλ→ ∞, the solution approaches the least squares plane;\\n•for intermediate values of λ, the solution can be represented as a\\nlinear expansion of basis functions, whose coeﬃcients are obtained\\nby a form of generalized ridge regression.\\nThe solution has the form\\nf(x) =β0+βTx+N∑\\nj=1αjhj(x), (5.39)\\nwhere hj(x) =||x−xj||2log||x−xj||. These hjare examples of radial\\nbasis functions , which are discussed in more detail in the next section. The\\ncoeﬃcients are found by plugging (5.39) into (5.37), which reduces to a\\nﬁnite-dimensional penalized least squares problem. For the penalty to be\\nﬁnite, the coeﬃcients αjhave to satisfy a set of linear constraints; see\\nExercise 5.14.\\nThin-plate splines are deﬁned more generally for arbitrary dimension d,\\nfor which an appropriately more general Jis used.\\nThere are a number of hybrid approaches that are popular in practice,\\nboth for computational and conceptual simplicity. Unlike one-dimensional\\nsmoothing splines, the computational complexity for thin-plate splines is\\nO(N3), since there is not in general any sparse structure that can be ex-\\nploited. However, as with univariate smoothing splines, we can get away\\nwith substantially less than the Nknots prescribed by the solution (5.39).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e7d174d-cc55-4a35-9623-77101b502bac', embedding=None, metadata={'page_label': '185', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='166 5. Basis Expansions and Regularization\\n125\\n130135140145150155\\n15202530354045\\n20 30 40 50 60\\nAgeObesitySystolic Blood Pressure\\n120125130135140145150155160\\n•• ••\\n••\\n••••\\n•\\n•\\n•• ••••\\n••\\n••\\n••\\n•\\n••\\n••\\n•\\n•••••\\n•••\\n•••\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••\\n••\\n••••••\\n•••\\n••\\n•\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n•\\n••\\n••••\\n••••\\n•\\n•\\n•\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n•••••\\n•••\\n•\\n• •••\\n••••\\n•••\\n••\\n••••\\n•\\n••••\\n• ••\\n•\\n•••\\n•••\\n•••••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•\\n••••\\n••\\n•••\\n••••\\n•\\n•••\\n•••\\n•\\n•••••\\n•\\n•\\n•••\\n••\\n•••\\n••\\n•\\n••\\n•\\n•••\\n••••\\n• •\\n•••\\n•\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••••\\n•\\n••••\\n•••\\n•\\n•••\\n•••\\n••\\n••\\n•\\n••••\\n••\\n• ••\\n•••\\n•••\\n•••\\n••• •••\\n•\\n•\\n•\\n••\\n••\\n•\\n••\\n••\\n••\\n•••••\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•\\n••\\n••\\n•\\n••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n•\\n•••\\n•\\n••\\n•••\\n••\\n•••\\n••••\\n•\\n••\\n••••\\n•••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••\\n••\\n••\\n•• • • • • •• • • • • • •• • • • • • •• • • • • •• • • •• •\\n• • • • • • • •• •••• •• • • •• • • • • •• • • • • • • •\\nFIGURE 5.12. A thin-plate spline ﬁt to the heart disease data, displayed as a\\ncontour plot. The response is systolic blood pressure , modeled as a function\\nofageandobesity . The data points are indicated, as well as the lattice of points\\nused as knots. Care should be taken to use knots from the lattice inside the convex\\nhull of the data (red), and ignore those outside (green).\\nIn practice, it is usually suﬃcient to work with a lattice of knots covering\\nthe domain. The penalty is computed for the reduced expansion just as\\nbefore. Using Kknots reduces the computations to O(NK2+K3). Fig-\\nure 5.12 shows the result of ﬁtting a thin-plate spline to some heart disease\\nrisk factors, representing the surface as a contour plot. Indicated are the\\nlocation of the input features, as well as the knots used in the ﬁt. Note that\\nλwas speciﬁed via df λ= trace( Sλ) = 15.\\nMore generally one can represent f∈IRdas an expansion in any arbi-\\ntrarily large collection of basis functions, and control the complexity by a p-\\nplying a regularizer such as (5.38). For example, we could construct a basis\\nby forming the tensor products of all pairs of univariate smoothing-spline\\nbasis functions as in (5.35), using, for example, the univariate B-splines\\nrecommended in Section 5.9.2 as ingredients. This leads to an exponential', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56d56d60-2f69-4df2-b9c7-96fa89f8d723', embedding=None, metadata={'page_label': '186', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.8 Regularization and Reproducing Kernel Hilbert Spaces 167\\ngrowth in basis functions as the dimension increases, and typically we have\\nto reduce the number of functions per coordinate accordingly.\\nThe additive spline models discussed in Chapter 9 are a restricted class\\nof multidimensional splines. They can be represented in this general formu-\\nlation as well; that is, there exists a penalty J[f] that guarantees that the\\nsolution has the form f(X) =α+f1(X1) +≤≤≤+fd(Xd) and that each of\\nthe functions fjare univariate splines. In this case the penalty is somewhat\\ndegenerate, and it is more natural to assume thatfis additive, and then\\nsimply impose an additional penalty on each of the component functions:\\nJ[f] = J(f1+f2+≤≤≤+fd)\\n=d∑\\nj=1∫\\nf′′\\nj(tj)2dtj. (5.40)\\nThese are naturally extended to ANOVA spline decompositions,\\nf(X) =α+∑\\njfj(Xj) +∑\\nj<kfjk(Xj,Xk) +≤≤≤, (5.41)\\nwhere each of the components are splines of the required dimension. There\\nare many choices to be made:\\n•The maximum order of interaction—we have shown up to order 2\\nabove.\\n•Which terms to include—not all main eﬀects and interactions are\\nnecessarily needed.\\n•What representation to use—some choices are:\\n–regression splines with a relatively small number of basis func-\\ntions per coordinate, and their tensor products for interactions;\\n–a complete basis as in smoothing splines, and include appropri-\\nate regularizers for each term in the expansion.\\nIn many cases when the number of potential dimensions (features) is large,\\nautomatic methods are more desirable. The MARS and MART procedures\\n(Chapters 9 and 10, respectively), both fall into this category.\\n5.8 Regularization and Reproducing Kernel\\nHilbert Spaces\\nIn this section we cast splines into the larger context of regularization meth-\\nods and reproducing kernel Hilbert spaces. This section is quite technical\\nand can be skipped by the disinterested or intimidated reader.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ef201b36-f770-4c1d-a34b-1c2f868c75b8', embedding=None, metadata={'page_label': '187', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='168 5. Basis Expansions and Regularization\\nA general class of regularization problems has the form\\nmin\\nf∈H[N∑\\ni=1L(yi,f(xi)) +λJ(f)]\\n(5.42)\\nwhere L(y,f(x)) is a loss function, J(f) is a penalty functional, and His\\na space of functions on which J(f) is deﬁned. Girosi et al. (1995) describe\\nquite general penalty functionals of the form\\nJ(f) =∫\\nI Rd|˜f(s)|2\\n˜G(s)ds, (5.43)\\nwhere ˜fdenotes the Fourier transform of f, and ˜Gis some positive function\\nthat falls oﬀ to zero as ||s|| → ∞ . The idea is that 1 /˜Gincreases the penalty\\nfor high-frequency components of f. Under some additional assumptions\\nthey show that the solutions have the form\\nf(X) =K∑\\nk=1αkφk(X) +N∑\\ni=1θiG(X−xi), (5.44)\\nwhere the φkspan the null space of the penalty functional J, and Gis the\\ninverse Fourier transform of ˜G. Smoothing splines and thin-plate splines\\nfall into this framework. The remarkable feature of this solution is tha t\\nwhile the criterion (5.42) is deﬁned over an inﬁnite-dimensional space, the\\nsolution is ﬁnite-dimensional. In the next sections we look at some speciﬁc\\nexamples.\\n5.8.1 Spaces of Functions Generated by Kernels\\nAn important subclass of problems of the form (5.42) are generated by\\na positive deﬁnite kernel K(x,y), and the corresponding space of func-\\ntionsHKis called a reproducing kernel Hilbert space (RKHS). The penalty\\nfunctional Jis deﬁned in terms of the kernel as well. We give a brief and\\nsimpliﬁed introduction to this class of models, adapted from Wahba (1990)\\nand Girosi et al. (1995), and nicely summarized in Evgeniou et al. (2000).\\nLetx,y∈IRp. We consider the space of functions generated by the linear\\nspan of {K(≤,y), y∈IRp)}; i.e arbitrary linear combinations of the form\\nf(x) =∑\\nmαmK(x,ym), where each kernel term is viewed as a function\\nof the ﬁrst argument, and indexed by the second. Suppose that Khas an\\neigen-expansion\\nK(x,y) =∞∑\\ni=1γiφi(x)φi(y) (5.45)\\nwithγi≥0,∑∞\\ni=1γ2\\ni<∞. Elements of HKhave an expansion in terms of\\nthese eigen-functions,\\nf(x) =∞∑\\ni=1ciφi(x), (5.46)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='749e8c45-f9ef-45d3-9377-c4ec886a56f5', embedding=None, metadata={'page_label': '188', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.8 Regularization and Reproducing Kernel Hilbert Spaces 169\\nwith the constraint that\\n||f||2\\nHKdef=∞∑\\ni=1c2\\ni/γi<∞, (5.47)\\nwhere ||f||HKis the norm induced by K. The penalty functional in (5.42)\\nfor the space HKis deﬁned to be the squared norm J(f) =||f||2\\nHK. The\\nquantity J(f) can be interpreted as a generalized ridge penalty, where\\nfunctions with large eigenvalues in the expansion (5.45) get penalized less,\\nand vice versa.\\nRewriting (5.42) we have\\nmin\\nf∈HK[N∑\\ni=1L(yi,f(xi)) +λ||f||2\\nHK]\\n(5.48)\\nor equivalently\\nmin\\n{cj}∞\\n1\\uf8ee\\n\\uf8f0N∑\\ni=1L(yi,∞∑\\nj=1cjφj(xi)) +λ∞∑\\nj=1c2\\nj/γj\\uf8f9\\n\\uf8fb. (5.49)\\nIt can be shown (Wahba, 1990, see also Exercise 5.15) that the solution\\nto (5.48) is ﬁnite-dimensional, and has the form\\nf(x) =N∑\\ni=1αiK(x,xi). (5.50)\\nThe basis function hi(x) =K(x,xi) (as a function of the ﬁrst argument) is\\nknown as the representer of evaluation atxiinHK, since for f∈ H K, it is\\neasily seen that ⟨K(≤,xi),f⟩HK=f(xi). Similarly ⟨K(≤,xi),K(≤,xj)⟩HK=\\nK(xi,xj) (the reproducing property of HK), and hence\\nJ(f) =N∑\\ni=1N∑\\nj=1K(xi,xj)αiαj (5.51)\\nforf(x) =∑N\\ni=1αiK(x,xi).\\nIn light of (5.50) and (5.51), (5.48) reduces to a ﬁnite-dimensional crite-\\nrion\\nmin\\nαL(y,Kα) +λαTKα. (5.52)\\nWe are using a vector notation, in which Kis the N×Nmatrix with ijth\\nentry K(xi,xj) and so on. Simple numerical algorithms can be used to\\noptimize (5.52). This phenomenon, whereby the inﬁnite-dimensional prob-\\nlem (5.48) or (5.49) reduces to a ﬁnite dimensional optimization problem,\\nhas been dubbed the kernel property in the literature on support-vector\\nmachines (see Chapter 12).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7c1920d0-b0b2-4011-a0fa-58710e84bf64', embedding=None, metadata={'page_label': '189', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='170 5. Basis Expansions and Regularization\\nThere is a Bayesian interpretation of this class of models, in which f\\nis interpreted as a realization of a zero-mean stationary Gaussian process,\\nwith prior covariance function K. The eigen-decomposition produces a se-\\nries of orthogonal eigen-functions φj(x) with associated variances γj. The\\ntypical scenario is that “smooth” functions φjhave large prior variance,\\nwhile “rough” φjhave small prior variances. The penalty in (5.48) is the\\ncontribution of the prior to the joint likelihood, and penalizes more those\\ncomponents with smaller prior variance (compare with (5.43)).\\nFor simplicity we have dealt with the case here where all members of H\\nare penalized, as in (5.48). More generally, there may be some components\\ninHthat we wish to leave alone, such as the linear functions for cubic\\nsmoothing splines in Section 5.4. The multidimensional thin-plate splines\\nof Section 5.7 and tensor product splines fall into this category as well.\\nIn these cases there is a more convenient representation H=H0⊕ H1,\\nwith the null space H0consisting of, for example, low degree polynomi-\\nals in xthat do not get penalized. The penalty becomes J(f) =∥P1f∥,\\nwhere P1is the orthogonal projection of fontoH1. The solution has the\\nformf(x) =∑M\\nj=1βjhj(x) +∑N\\ni=1αiK(x,xi), where the ﬁrst term repre-\\nsents an expansion in H0. From a Bayesian perspective, the coeﬃcients of\\ncomponents in H0have improper priors, with inﬁnite variance.\\n5.8.2 Examples of RKHS\\nThe machinery above is driven by the choice of the kernel Kand the loss\\nfunction L. We consider ﬁrst regression using squared-error loss. In this\\ncase (5.48) specializes to penalized least squares, and the solution can be\\ncharacterized in two equivalent ways corresponding to (5.49) or (5.52):\\nmin\\n{cj}∞\\n1N∑\\ni=1\\uf8eb\\n\\uf8edyi−∞∑\\nj=1cjφj(xi)\\uf8f6\\n\\uf8f82\\n+λ∞∑\\nj=1c2\\nj\\nγj(5.53)\\nan inﬁnite-dimensional, generalized ridge regression problem, or\\nmin\\nα(y−Kα)T(y−Kα) +λαTKα. (5.54)\\nThe solution for αis obtained simply as\\nˆα= (K+λI)−1y, (5.55)\\nand\\nˆf(x) =N∑\\nj=1ˆαjK(x,xj). (5.56)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='029d6ab2-208b-4d52-a1d5-ea993001ca4d', embedding=None, metadata={'page_label': '190', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.8 Regularization and Reproducing Kernel Hilbert Spaces 171\\nThe vector of Nﬁtted values is given by\\nˆf=Kˆα\\n=K(K+λI)−1y (5.57)\\n= (I+λK−1)−1y. (5.58)\\nThe estimate (5.57) also arises as the kriging estimate of a Gaussian ran-\\ndom ﬁeld in spatial statistics (Cressie, 1993). Compare also (5.58) w ith the\\nsmoothing spline ﬁt (5.17) on page 154.\\nPenalized Polynomial Regression\\nThe kernel K(x,y) = (⟨x,y⟩+ 1)d(Vapnik, 1996), for x,y∈IRp, has\\nM=(p+d\\nd)\\neigen-functions that span the space of polynomials in IRpof\\ntotal degree d. For example, with p= 2 and d= 2,M= 6 and\\nK(x,y) = 1 + 2 x1y1+ 2x2y2+x2\\n1y2\\n1+x2\\n2y2\\n2+ 2x1x2y1y2(5.59)\\n=M∑\\nm=1hm(x)hm(y) (5.60)\\nwith\\nh(x)T= (1,√\\n2x1,√\\n2x2,x2\\n1,x2\\n2,√\\n2x1x2). (5.61)\\nOne can represent hin terms of the Morthogonal eigen-functions and\\neigenvalues of K,\\nh(x) =VD1\\n2γφ(x), (5.62)\\nwhere Dγ= diag( γ1,γ2,... ,γ M), and VisM×Mand orthogonal.\\nSuppose we wish to solve the penalized polynomial regression problem\\nmin\\n{βm}M\\n1N∑\\ni=1(\\nyi−M∑\\nm=1βmhm(xi))2\\n+λM∑\\nm=1β2\\nm. (5.63)\\nSubstituting (5.62) into (5.63), we get an expression of the form (5.53) to\\noptimize (Exercise 5.16).\\nThe number of basis functions M=(p+d\\nd)\\ncan be very large, often much\\nlarger than N. Equation (5.55) tells us that if we use the kernel represen-\\ntation for the solution function, we have only to evaluate the kernel N2\\ntimes, and can compute the solution in O(N3) operations.\\nThis simplicity is not without implications. Each of the polynomials hm\\nin (5.61) inherits a scaling factor from the particular form of K, which has\\na bearing on the impact of the penalty in (5.63). We elaborate on this in\\nthe next section.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5fce260a-97bb-4810-8cb6-eee6a107bb2b', embedding=None, metadata={'page_label': '191', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='172 5. Basis Expansions and Regularization\\n−2 −1 0 1 2 3 40.0 0.4 0.8\\nXRadial Kernel in I R1K(≤, xm)\\nFIGURE 5.13. Radial kernels kk(x)for the mixture data, with scale parameter\\nν= 1. The kernels are centered at ﬁve points xmchosen at random from the 200.\\nGaussian Radial Basis Functions\\nIn the preceding example, the kernel is chosen because it represents an\\nexpansion of polynomials and can conveniently compute high-dimensional\\ninner products. In this example the kernel is chosen because of its functional\\nform in the representation (5.50).\\nThe Gaussian kernel K(x,y) =e−ν||x−y||2along with squared-error loss,\\nfor example, leads to a regression model that is an expansion in Gaussian\\nradial basis functions,\\nkm(x) =e−ν||x−xm||2, m= 1,... ,N, (5.64)\\neach one centered at one of the training feature vectors xm. The coeﬃcients\\nare estimated using (5.54).\\nFigure 5.13 illustrates radial kernels in IR1using the ﬁrst coordinate of\\nthe mixture example from Chapter 2. We show ﬁve of the 200 kernel basis\\nfunctions km(x) =K(x,xm).\\nFigure 5.14 illustrates the implicit feature space for the radial kernel\\nwithx∈IR1. We computed the 200 ×200 kernel matrix K, and its eigen-\\ndecomposition ΦDγΦT. We can think of the columns of Φand the corre-\\nsponding eigenvalues in Dγas empirical estimates of the eigen expansion\\n(5.45)2. Although the eigenvectors are discrete, we can represent them as\\nfunctions on IR1(Exercise 5.17). Figure 5.15 shows the largest 50 eigenval-\\nues ofK. The leading eigenfunctions are smooth, and they are successively\\nmore wiggly as the order increases. This brings to life the penalty in (5.49) ,\\nwhere we see the coeﬃcients of higher-order functions get penalized more\\nthan lower-order ones. The right panel in Figure 5.14 shows the correspond-\\n2Theℓth column of Φis an estimate of φℓ, evaluated at each of the Nobservations.\\nAlternatively, the ith row of Φis the estimated vector of basis functions φ(xi), evaluated\\nat the point xi. Although in principle, there can be inﬁnitely many element s inφ, our\\nestimate has at most Nelements.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='84a541e5-ea61-417e-bd2f-7ac3ec03ab9d', embedding=None, metadata={'page_label': '192', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.8 Regularization and Reproducing Kernel Hilbert Spaces 173\\n*\\n******* ** ** ******\\n******\\n*******\\n******\\n**********\\n****\\n** *******\\n*****\\n**\\n***\\n**********\\n*****\\n*****\\n***\\n**\\n***\\n*****\\n*************\\n******\\n***\\n**\\n***\\n**\\n***********\\n**\\n****\\n**** *\\n** ******* * **\\n**************\\n********* ***\\n**\\n*******\\n***\\n**\\n****\\n***\\n***\\n**\\n***\\n*****\\n*******\\n**\\n**\\n**\\n**\\n***\\n**\\n*****\\n**\\n******\\n*\\n***\\n**\\n**\\n*********\\n**\\n**\\n****\\n**\\n**\\n**\\n****\\n****\\n***\\n**\\n**\\n*******\\n**\\n***\\n**\\n*\\n****\\n**\\n****\\n**\\n**\\n****\\n****\\n**\\n**\\n*****\\n****\\n**\\n****\\n*\\n**\\n***\\n**\\n**\\n****\\n****\\n**\\n**\\n*\\n***\\n**\\n*********\\n******\\n*\\n***\\n** **\\n****\\n*\\n***\\n***\\n***\\n***\\n*******\\n**\\n**\\n**\\n*\\n*******\\n**\\n***\\n***\\n******\\n**\\n***\\n**\\n******\\n**\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n**\\n**\\n****\\n*** **\\n*********\\n****\\n**\\n****\\n** **\\n** ****\\n****\\n***\\n***\\n**\\n** ***\\n**\\n**\\n*\\n***\\n***\\n* * * **\\n****\\n**\\n**\\n****\\n*****\\n****\\n* **** ***\\n**\\n*** **** * *\\n**\\n* *\\n**\\n**\\n******\\n***\\n***\\n****\\n***\\n***\\n*\\n**\\n**\\n**\\n**\\n***\\n***\\n***\\n****\\n***\\n********\\n*******\\n** ** **\\n**\\n***\\n***\\n**\\n*\\n*********\\n**********\\n*\\n****\\n***\\n***\\n**\\n**\\n***\\n*****\\n****\\n**\\n**\\n****\\n***\\n***\\n*\\n**\\n**\\n***\\n**\\n**\\n******\\n*******\\n***\\n***\\n*** *\\n****\\n***\\n****\\n* ***\\n* * ** *\\n* ***\\n**\\n*\\n**\\n***\\n***\\n**\\n***\\n**\\n*******\\n**\\n**\\n*******\\n**\\n*\\n* *********\\n***\\n******\\n******\\n** ***\\n*\\n**\\n*\\n**\\n**\\n***\\n* **\\n***\\n*\\n***\\n**\\n***********\\n**\\n**\\n**\\n***\\n**\\n***\\n**\\n**\\n*********\\n***\\n***\\n**\\n**\\n*\\n**\\n*\\n**\\n***\\n* * **\\n*****\\n**\\n**\\n*****\\n**\\n**\\n** ******\\n**\\n**\\n***** ****\\n***\\n****\\n* *****\\n****\\n****\\n***\\n**\\n**\\n**\\n* *******\\n****\\n****\\n***\\n**\\n****\\n*****\\n******\\n*****\\n*\\n*****\\n***\\n*\\n***\\n******\\n*****\\n***\\n***********\\n***\\n***** **\\n* *\\n***\\n*****\\n*******\\n*****\\n**\\n****\\n**\\n*\\n**\\n*\\n*****\\n***\\n*******\\n******* *\\n*****\\n*\\n**\\n***\\n*\\n*****\\n**\\n*****\\n**\\n*** *\\n***\\n***\\n******\\n**\\n***\\n********\\n***\\n**\\n**\\n***\\n***** ****\\n***\\n**\\n****\\n********\\n*******\\n***\\n**\\n***\\n**\\n***\\n**\\n*********\\n*\\n*******\\n*****\\n*\\n**\\n**\\n***\\n***\\n*\\n**\\n*\\n**\\n**\\n******\\n**\\n*\\n*****\\n*\\n***\\n** **\\n*\\n***\\n** ***\\n****\\n**\\n*******\\n****\\n**\\n****\\n*****\\n******\\n**\\n****\\n**\\n**\\n** ***\\n*****\\n*\\n**\\n**\\n***\\n**\\n****\\n**\\n*****\\n***\\n**\\n****\\n***\\n**\\n**\\n*\\n**\\n***\\n*\\n*****\\n***\\n******* ***\\n**\\n****\\n* ***\\n******\\n*****\\n*****\\n***\\n**\\n* ** ***\\n***\\n***\\n*\\n***\\n***\\n****\\n****\\n***\\n*****\\n*****\\n***\\n****\\n**\\n**\\n**\\n*\\n*****\\n**\\n******\\n********* *\\n**\\n*\\n****\\n**\\n*\\n* *\\n***\\n**\\n****\\n***\\n****\\n*** ****\\n****\\n**\\n*****\\n***\\n**\\n**\\n***\\n*****\\n*\\n***\\n*** *\\n******\\n*\\n***********\\n**\\n******\\n**\\n*******\\n**\\n****\\n*\\n**********\\n*\\n**** *\\n**\\n******\\n*\\n* **\\n***\\n*\\n**\\n*****\\n*\\n**\\n*\\n****\\n**\\n*\\n***\\n** *\\n****\\n******\\n***\\n**\\n*\\n**\\n**** ****\\n**\\n***\\n**\\n***\\n***\\n*****\\n**** * *\\n**\\n**\\n******\\n*****\\n*****\\n***\\n** *\\n****\\n*\\n****\\n** *\\n*****\\n****\\n**** *\\n*****\\n**\\n****\\n*****\\n** **\\n***\\n****\\n****\\n**\\n** *****\\n**\\n*****\\n***\\n**\\n****\\n*\\n****\\n****\\n**\\n*****\\n***\\n********\\n*\\n****\\n***\\n**\\n***\\n**\\n*****\\n**\\n***\\n*****\\n********\\n****\\n* *******\\n******* * *\\n***\\n****\\n**** *\\n**\\n**\\n***\\n****\\n**\\n**\\n**\\n******\\n**\\n**\\n**\\n****\\n*****\\n***\\n****\\n*****\\n*\\n***********\\n**\\n**\\n**\\n**\\n**\\n*******\\n******\\n***********\\n***\\n***\\n*****\\n****\\n***\\n* ***\\n*****\\n**\\n*********\\n*\\n**\\n***\\n** ****\\n** * *\\n**\\n***\\n***\\n**\\n*****\\n***\\n*****\\n****\\n**\\n**\\n***\\n*****\\n* * **\\n**\\n**\\n* **\\n***\\n**\\n*****\\n***\\n***\\n***\\n*******\\n**\\n*** *\\n****\\n*\\n* ***\\n****\\n***\\n***\\n***\\n***\\n*****\\n* ***\\n****\\n****\\n***\\n****\\n*****\\n*******\\n***\\n**\\n***\\n****\\n*\\n***\\n***\\n*\\n**\\n*\\n*****\\n**\\n***\\n**\\n**\\n***\\n**\\n*** *\\n******\\n**\\n* **\\n****\\n**\\n***\\n**\\n***\\n*******\\n**\\n***\\n**\\n****\\n***\\n****\\n* * *\\n****\\n* ***\\n***\\n**\\n**\\n******\\n***\\n**\\n*****\\n**\\n**\\n*\\n***\\n***\\n**\\n** *\\n******\\n*****\\n***\\n*\\n***\\n****\\n**\\n* **\\n*******\\n****\\n*******\\n**\\n***\\n***\\n*\\n*****\\n*\\n********\\n***\\n*\\n***\\n***\\n**\\n*******\\n**\\n*\\n*****\\n***\\n**\\n***\\n***\\n****\\n* ***\\n******\\n***\\n***\\n*****\\n*** **\\n**\\n**\\n***\\n***\\n********\\n**\\n****\\n**** *****\\n****\\n****\\n**\\n***\\n**\\n****\\n** *****\\n**\\n**\\n*****\\n**\\n**\\n**\\n***\\n****\\n*****\\n*** *\\n*******\\n****\\n********\\n**\\n** **\\n*****\\n**\\n***\\n*\\n******\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n**\\n******\\n***\\n***\\n*\\n**\\n*******\\n**\\n****\\n****\\n*\\n*******\\n**** ******\\n*\\n****\\n****\\n***\\n*\\n**\\n****\\n** **\\n** *\\n*** *\\n**\\n****\\n***************\\n***\\n***\\n*****\\n*****\\n* *****\\n***\\n****\\n*****\\n**\\n***\\n**\\n****\\n**\\n***\\n*\\n*****\\n***\\n**\\n****\\n***\\n**\\n**\\n*******\\n*****\\n***\\n*\\n***\\n*******\\n*\\n**\\n**** ***\\n****\\n*\\n****\\n****\\n*****\\n*****\\n***\\n***\\n*****\\n**\\n***\\n**\\n*\\n**\\n**\\n*****\\n**\\n**\\n*\\n**\\n**\\n**\\n****\\n**** *\\n*\\n**\\n******\\n****\\n**\\n****\\n***\\n*\\n*\\n****\\n****\\n****\\n***\\n***\\n******\\n***\\n****\\n*\\n***\\n**\\n*****\\n*\\n* ****\\n****\\n**\\n*\\n**\\n*\\n***\\n****\\n**\\n**\\n***\\n****\\n***\\n**\\n*\\n**\\n**\\n********* *\\n**\\n******\\n**\\n***\\n**\\n**\\n****\\n***\\n***\\n**\\n**\\n*******\\n***\\n**\\n**\\n***\\n**\\n*\\n**\\n*\\n**\\n**Orthonormal Basis Φ\\n*\\n*****\\n** *\\n* ***\\n*\\n****\\n***\\n*\\n**\\n*\\n***\\n***\\n******\\n**\\n**\\n***\\n***\\n****\\n**\\n*\\n**\\n****\\n*****\\n**\\n***\\n**\\n******\\n**\\n**\\n*\\n**\\n**\\n***\\n***\\n**\\n***\\n*****\\n*\\n***\\n*\\n***\\n*****\\n***\\n***\\n***\\n**\\n***\\n**\\n*\\n**\\n********\\n**\\n****\\n*\\n***\\n*\\n** *\\n*\\n***\\n** ***\\n**\\n**\\n**\\n**\\n***\\n***\\n***\\n***\\n***\\n***\\n**\\n***\\n****\\n***\\n**\\n****\\n***\\n***\\n**\\n***\\n****\\n*\\n**\\n**\\n***\\n**\\n**\\n**\\n**\\n**\\n*\\n**\\n****\\n*\\n**\\n******\\n*\\n***\\n**\\n**\\n*\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n****\\n*\\n*\\n**\\n**\\n**\\n**\\n****\\n***\\n**\\n**\\n***\\n***\\n*\\n**\\n***\\n**\\n*\\n****\\n**\\n*\\n***\\n**\\n**\\n****\\n****\\n**\\n**\\n****\\n*\\n****\\n**\\n****\\n*\\n**\\n***\\n**\\n**\\n***\\n*\\n****\\n**\\n**\\n*\\n***\\n**\\n***\\n**\\n*\\n**\\n*\\n******\\n*\\n**\\n*\\n***\\n*\\n***\\n*\\n*\\n***\\n***\\n***\\n*\\n**\\n****\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n***\\n*\\n**\\n***\\n**\\n*\\n****\\n**\\n**\\n***\\n**\\n*****\\n*\\n**\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n**\\n**\\n***\\n*\\n*****\\n*****\\n****\\n****\\n**\\n*\\n***\\n** **\\n***\\n***\\n***\\n*\\n**\\n*\\n***\\n**\\n** ***\\n**\\n**\\n*\\n***\\n***\\n** ***\\n****\\n**\\n**\\n***\\n*\\n**\\n***\\n*\\n***\\n*******\\n*\\n**\\n*\\n***\\n*** **\\n**\\n* *\\n**\\n**\\n***\\n***\\n***\\n***\\n****\\n***\\n***\\n*\\n**\\n**\\n**\\n*\\n*\\n***\\n**\\n*\\n*\\n**\\n**\\n**\\n***\\n***\\n*****\\n*\\n******\\n** ****\\n**\\n***\\n**\\n*\\n**\\n*\\n****\\n*\\n****\\n**\\n********\\n*\\n****\\n**\\n*\\n***\\n**\\n**\\n***\\n*****\\n****\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n**\\n***\\n**\\n**\\n******\\n*****\\n**\\n***\\n**\\n*\\n****\\n****\\n***\\n*\\n**\\n*\\n* ***\\n* *** *\\n* ***\\n**\\n*\\n**\\n***\\n***\\n**\\n*\\n**\\n**\\n*******\\n**\\n**\\n**\\n*****\\n**\\n*\\n* ****\\n***\\n**\\n***\\n*\\n*****\\n******\\n** ***\\n*\\n**\\n*\\n**\\n**\\n*\\n**\\n* **\\n***\\n*\\n***\\n**\\n*****\\n***\\n***\\n**\\n**\\n**\\n***\\n**\\n***\\n**\\n**\\n**\\n*******\\n*\\n**\\n***\\n**\\n**\\n*\\n**\\n*\\n**\\n***\\n* ***\\n*****\\n**\\n**\\n*****\\n**\\n**\\n** ***\\n*\\n**\\n**\\n**\\n***** ****\\n***\\n****\\n******\\n****\\n****\\n***\\n**\\n**\\n**\\n* *******\\n**\\n**\\n****\\n*\\n**\\n**\\n****\\n*****\\n******\\n*****\\n*\\n*****\\n***\\n*\\n***\\n******\\n*****\\n***\\n****\\n*****\\n**\\n***\\n***** **\\n* *\\n***\\n*****\\n*******\\n****\\n*\\n**\\n****\\n**\\n*\\n**\\n*\\n***\\n**\\n***\\n***\\n****\\n******* *\\n*****\\n*\\n**\\n***\\n*\\n*****\\n**\\n*****\\n**\\n*** *\\n***\\n***\\n******\\n**\\n***\\n********\\n***\\n**\\n**\\n***\\n***** ****\\n***\\n**\\n****\\n********\\n*******\\n***\\n**\\n***\\n**\\n***\\n**\\n*********\\n*\\n*******\\n*****\\n*\\n**\\n**\\n***\\n***\\n*\\n**\\n*\\n**\\n**\\n******\\n**\\n*\\n*****\\n*\\n***\\n** **\\n*\\n***\\n** ***\\n****\\n**\\n*******\\n****\\n**\\n****\\n*****\\n******\\n**\\n****\\n****\\n** ********\\n***\\n*****\\n**\\n****\\n**\\n**********\\n****\\n***\\n**\\n***\\n**\\n****\\n*****\\n***\\n******* ***\\n**\\n****\\n* ***\\n******\\n*****\\n*****\\n*****\\n* ** ***\\n***\\n***\\n*\\n***\\n***\\n****\\n*******\\n*****\\n*****\\n***\\n* *****\\n**\\n**\\n*\\n*****\\n**\\n******\\n********* ****\\n****\\n***\\n* ****\\n**\\n****\\n***\\n******* ** ********\\n********\\n**\\n**\\n********\\n****\\n*** *\\n***** ***************\\n******\\n**\\n*** ****\\n******************** ** ****** ***\\n** **\\n***\\n*\\n**********\\n*****\\n******** *\\n****\\n** * ******\\n********* * ***\\n**\\n********\\n******* ***** * *\\n****** ***** *** *********** ************ ** ***** ******* **\\n**** ************** *** ********* **** * ******************* ********** ***************** *******\\n*****\\n***\\n******** *************** ******* ** * ****\\n******* * ******* ** ** * ***** *** ** ************* ******** ** ***** ** ***** **\\n**** * ************************* ******************************** ** * ** *** *********** * ************** * ****** * *************** ***************\\n****** * **** * ** ** *** ***** ***** ************* ******** ** ***** ** ************ ************* * ********** * **** ********* ********************* ** * *** *********** ** ********** ** * * ****** * **************** ****** *** ***********\\n* **** * ** ** ** * **** * *** ** ************* ******** ********* ***** ****** * ************* * ** ******** * **** ********* *********** ** ****** *** ** *** *********** * * ***** * **** ** * * ****** * * ******* ******* * ****** ** * *********** * **** * ** ** ** * **** * *** ** ************* ******** ** ***** ** ***** ** **** * **** ********* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** ** ****** * * ************** * ****** ** * **** * ****** * ** ** * ** ** ** * **** * *** ** ************* ******** ** **** *** ***** ** **** * ************* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** * * ****** * * ************** * ****** ** * **** * ****** * **** * ** ** ** * **** * *** ** **** ***** **** ******** ** ***** ** ***** ** **** * ************* * ** ******** * **** ********* *** * ******* ** ****** ** * * * *** *********** ** ***** * **** ** * * * * **** * * ************** * ****** ** * **** * ******Feature Space H\\nFIGURE 5.14. (Left panel) The ﬁrst 16normalized eigenvectors of K, the\\n200×200kernel matrix for the ﬁrst coordinate of the mixture data. These a re\\nviewed as estimates ˆφℓof the eigenfunctions in (5.45), and are represented as\\nfunctions in I R1with the observed values superimposed in color. They are arr anged\\nin rows, starting at the top left. (Right panel) Rescaled versi onshℓ=√ˆγℓˆφℓof\\nthe functions in the left panel, for which the kernel computes the “inner product.”\\n0 10 20 30 40 501e−15 1e−11 1e−07 1e−03 1e+01Eigenvalue\\nFIGURE 5.15. The largest 50eigenvalues of K; all those beyond the 30th are\\neﬀectively zero.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98471717-b4bc-4dd5-8783-10ba390593be', embedding=None, metadata={'page_label': '193', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='174 5. Basis Expansions and Regularization\\ningfeature space representation of the eigenfunctions\\nhℓ(x) =√\\nˆγℓˆφℓ(x), ℓ= 1,... ,N. (5.65)\\nNote that ⟨h(xi),h(xi′)⟩=K(xi,xi′). The scaling by the eigenvalues quickly\\nshrinks most of the functions down to zero, leaving an eﬀective dimension\\nof about 12 in this case. The corresponding optimization problem is a stan-\\ndard ridge regression, as in (5.63). So although in principle the implicit\\nfeature space is inﬁnite dimensional, the eﬀective dimension is dramat-\\nically lower because of the relative amounts of shrinkage applied to each\\nbasis function. The kernel scale parameter νplays a role here as well; larger\\nνimplies more local kmfunctions, and increases the eﬀective dimension of\\nthe feature space. See Hastie and Zhu (2006) for more details.\\nIt is also known (Girosi et al., 1995) that a thin-plate spline (Section 5.7 )\\nis an expansion in radial basis functions, generated by the kernel\\nK(x,y) =∥x−y∥2log(∥x−y∥). (5.66)\\nRadial basis functions are discussed in more detail in Section 6.7.\\nSupport Vector Classiﬁers\\nThe support vector machines of Chapter 12 for a two-class classiﬁcation\\nproblem have the form f(x) =α0+∑N\\ni=1αiK(x,xi), where the parameters\\nare chosen to minimize\\nmin\\nα0,α{N∑\\ni=1[1−yif(xi)]++λ\\n2αTKα}\\n, (5.67)\\nwhere yi∈ {−1,1}, and [ z]+denotes the positive part of z. This can be\\nviewed as a quadratic optimization problem with linear constraints, and\\nrequires a quadratic programming algorithm for its solution. The name\\nsupport vector arises from the fact that typically many of the ˆ αi= 0 [due\\nto the piecewise-zero nature of the loss function in (5.67)], and so ˆfis an\\nexpansion in a subset of the K(≤,xi). See Section 12.3.3 for more details.\\n5.9 Wavelet Smoothing\\nWe have seen two diﬀerent modes of operation with dictionaries of basis\\nfunctions. With regression splines, we select a subset of the bases, using\\neither subject-matter knowledge, or else automatically. The more adaptive\\nprocedures such as MARS (Chapter 9) can capture both smooth and non-\\nsmooth behavior. With smoothing splines, we use a complete basis, but\\nthen shrink the coeﬃcients toward smoothness.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2d51b05-d863-4f35-a06f-ee19eee7dcd2', embedding=None, metadata={'page_label': '194', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.9 Wavelet Smoothing 175\\nTime0.0 0.2 0.4 0.6 0.8 1.0Haar Wavelets\\nTime0.0 0.2 0.4 0.6 0.8 1.0Symmlet-8 Wavelets\\nψ1,0ψ2,1ψ2,3ψ3,2ψ3,5ψ4,4ψ4,9ψ5,1ψ5,15ψ6,15ψ6,35\\nFIGURE 5.16. Some selected wavelets at diﬀerent translations and dilations\\nfor the Haar and symmlet families. The functions have been scale d to suit the\\ndisplay.\\nWavelets typically use a complete orthonormal basis to represent func-\\ntions, but then shrink and select the coeﬃcients toward a sparse represen-\\ntation. Just as a smooth function can be represented by a few spline basis\\nfunctions, a mostly ﬂat function with a few isolated bumps can be repre-\\nsented with a few (bumpy) basis functions. Wavelets bases are very popular\\nin signal processing and compression, since they are able to represent both\\nsmooth and/or locally bumpy functions in an eﬃcient way—a phenomenon\\ndubbed time and frequency localization . In contrast, the traditional Fourier\\nbasis allows only frequency localization.\\nBefore we give details, let’s look at the Haar wavelets in the left panel\\nof Figure 5.16 to get an intuitive idea of how wavelet smoothing works.\\nThe vertical axis indicates the scale (frequency) of the wavelets, from low\\nscale at the bottom to high scale at the top. At each scale the wavelets are\\n“packed in” side-by-side to completely ﬁll the time axis: we have only shown', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='12bdf6dd-2a7e-4c4f-9a72-767c2e357eb7', embedding=None, metadata={'page_label': '195', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='176 5. Basis Expansions and Regularization\\na selected subset. Wavelet smoothing ﬁts the coeﬃcients for this basis by\\nleast squares, and then thresholds (discards, ﬁlters) the smaller coeﬃcients.\\nSince there are many basis functions at each scale, it can use bases where\\nit needs them and discard the ones it does not need, to achieve time and\\nfrequency localization. The Haar wavelets are simple to understand, but not\\nsmooth enough for most purposes. The symmlet wavelets in the right panel\\nof Figure 5.16 have the same orthonormal properties, but are smoother.\\nFigure 5.17 displays an NMR (nuclear magnetic resonance) signal, which\\nappears to be composed of smooth components and isolated spikes, plus\\nsome noise. The wavelet transform, using a symmlet basis, is shown in the\\nlower left panel. The wavelet coeﬃcients are arranged in rows, from lowest\\nscale at the bottom, to highest scale at the top. The length of each line\\nsegment indicates the size of the coeﬃcient. The bottom right panel shows\\nthe wavelet coeﬃcients after they have been thresholded. The threshold\\nprocedure, given below in equation (5.69), is the same soft-thresholding\\nrule that arises in the lasso procedure for linear regression (Section 3.4.2).\\nNotice that many of the smaller coeﬃcients have been set to zero. The\\ngreen curve in the top panel shows the back-transform of the thresholded\\ncoeﬃcients: this is the smoothed version of the original signal. In the next\\nsection we give the details of this process, including the construction of\\nwavelets and the thresholding rule.\\n5.9.1 Wavelet Bases and the Wavelet Transform\\nIn this section we give details on the construction and ﬁltering of wavelets.\\nWavelet bases are generated by translations and dilations of a single scal-\\ning function φ(x) (also known as the father ). The red curves in Figure 5.18\\nare the Haar andsymmlet-8 scaling functions. The Haar basis is particu-\\nlarly easy to understand, especially for anyone with experience in analysis\\nof variance or trees, since it produces a piecewise-constant representation.\\nThus if φ(x) =I(x∈[0,1]), then φ0,k(x) =φ(x−k),kan integer, generates\\nan orthonormal basis for functions with jumps at the integers. Call this ref-\\nerence space V0. The dilations φ1,k(x) =√\\n2φ(2x−k) form an orthonormal\\nbasis for a space V1⊃V0of functions piecewise constant on intervals of\\nlength1\\n2. In fact, more generally we have ≤≤≤ ⊃ V1⊃V0⊃V−1⊃ ≤≤≤ where\\neachVjis spanned by φj,k= 2j/2φ(2jx−k).\\nNow to the deﬁnition of wavelets. In analysis of variance, we often rep-\\nresent a pair of means θ1andθ2by their grand mean θ=1\\n2(θ1+θ2), and\\nthen a contrast α=1\\n2(θ1−θ2). A simpliﬁcation occurs if the contrast αis\\nvery small, because then we can set it to zero. In a similar manner we might\\nrepresent a function in Vj+1by a component in Vjplus the component in\\nthe orthogonal complement WjofVjtoVj+1, written as Vj+1=Vj⊕Wj.\\nThe component in Wjrepresents detail, and we might wish to set some ele-\\nments of this component to zero. It is easy to see that the functions ψ(x−k)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34a9fef6-d6c8-4382-8884-92c881039edc', embedding=None, metadata={'page_label': '196', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.9 Wavelet Smoothing 177\\nNMR Signal\\n0 200 400 600 800 10000 20 40 60\\n0 200 400 600 800 1000Wavelet Transform - Original Signal\\n0 200 400 600 800 1000Wavelet Transform - WaveShrunk Signal\\nSignal Signal\\nW9 W9\\nW8 W8\\nW7 W7\\nW6 W6\\nW5 W5\\nW4 W4\\nV4 V4\\nFIGURE 5.17. The top panel shows an NMR signal, with the wavelet-shrunk\\nversion superimposed in green. The lower left panel represents the wavelet trans-\\nform of the original signal, down to V4, using the symmlet-8 basis. Each coeﬃ-\\ncient is represented by the height (positive or negative) of the vertical bar. The\\nlower right panel represents the wavelet coeﬃcients after being shrunken using\\nthewaveshrink function in S-PLUS, which implements the SureShrink method\\nof wavelet adaptation of Donoho and Johnstone.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b50e5721-0e17-4349-aecf-36f500435d47', embedding=None, metadata={'page_label': '197', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='178 5. Basis Expansions and Regularization\\nHaar Basis Symmlet Basis\\nφ(x) φ(x)\\nψ(x) ψ(x)\\nFIGURE 5.18. TheHaarandsymmlet father (scaling) wavelet φ(x)and mother\\nwavelet ψ(x).\\ngenerated by the mother wavelet ψ(x) =φ(2x)−φ(2x−1) form an orthonor-\\nmal basis for W0for the Haar family. Likewise ψj,k= 2j/2ψ(2jx−k) form\\na basis for Wj.\\nNowVj+1=Vj⊕Wj=Vj−1⊕Wj−1⊕Wj, so besides representing a\\nfunction by its level- jdetail and level- jrough components, the latter can\\nbe broken down to level-( j−1) detail and rough, and so on. Finally we get\\na representation of the form VJ=V0⊕W0⊕W1≤≤≤ ⊕WJ−1. Figure 5.16\\non page 175 shows particular wavelets ψj,k(x).\\nNotice that since these spaces are orthogonal, all the basis functions are\\northonormal. In fact, if the domain is discrete with N= 2J(time) points,\\nthis is as far as we can go. There are 2jbasis elements at level j, and\\nadding up, we have a total of 2J−1 elements in the Wj, and one in V0.\\nThis structured orthonormal basis allows for a multiresolution analysis ,\\nwhich we illustrate in the next section.\\nWhile helpful for understanding the construction above, the Haar basis\\nis often too coarse for practical purposes. Fortunately, many clever wavelet\\nbases have been invented. Figures 5.16 and 5.18 include the Daubechies\\nsymmlet-8 basis. This basis has smoother elements than the corresponding\\nHaar basis, but there is a tradeoﬀ:\\n•Each wavelet has a support covering 15 consecutive time intervals,\\nrather than one for the Haar basis. More generally, the symmlet- p\\nfamily has a support of 2 p−1 consecutive intervals. The wider the\\nsupport, the more time the wavelet has to die to zero, and so it can', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b208b4c-ca52-4845-9f95-7ced1cdebc97', embedding=None, metadata={'page_label': '198', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.9 Wavelet Smoothing 179\\nachieve this more smoothly. Note that the eﬀective support seems to\\nbe much narrower.\\n•The symmlet- pwavelet ψ(x) has pvanishing moments; that is,\\n∫\\nψ(x)xjdx= 0, j= 0,... ,p −1.\\nOne implication is that any order- ppolynomial over the N= 2Jtimes\\npoints is reproduced exactly in V0(Exercise 5.18). In this sense V0\\nis equivalent to the null space of the smoothing-spline penalty. The\\nHaar wavelets have one vanishing moment, and V0can reproduce any\\nconstant function.\\nThe symmlet- pscaling functions are one of many families of wavelet\\ngenerators. The operations are similar to those for the Haar basis:\\n•IfV0is spanned by φ(x−k), then V1⊃V0is spanned by φ1,k(x) =√\\n2φ(2x−k) and φ(x) =∑\\nk∈Zh(k)φ1,k(x), for some ﬁlter coeﬃcients\\nh(k).\\n•W0is spanned by ψ(x) =∑\\nk∈Zg(k)φ1,k(x), with ﬁlter coeﬃcients\\ng(k) = (−1)1−kh(1−k).\\n5.9.2 Adaptive Wavelet Filtering\\nWavelets are particularly useful when the data are measured on a uniform\\nlattice, such as a discretized signal, image, or a time series. We will focus o n\\nthe one-dimensional case, and having N= 2Jlattice-points is convenient.\\nSuppose yis the response vector, and Wis the N×Northonormal wavelet\\nbasis matrix evaluated at the Nuniformly spaced observations. Then y∗=\\nWTyis called the wavelet transform ofy(and is the full least squares\\nregression coeﬃcient). A popular method for adaptive wavelet ﬁtting is\\nknown as SURE shrinkage (Stein Unbiased Risk Estimation, Donoho and\\nJohnstone (1994)). We start with the criterion\\nmin\\nθ||y−Wθ||2\\n2+ 2λ||θ||1, (5.68)\\nwhich is the same as the lasso criterion in Chapter 3. Because Wis or-\\nthonormal, this leads to the simple solution:\\nˆθj= sign( y∗\\nj)(|y∗\\nj| −λ)+. (5.69)\\nThe least squares coeﬃcients are translated toward zero, and truncated\\nat zero. The ﬁtted function (vector) is then given by the inverse wavelet\\ntransform ˆf=Wˆθ.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16b5fb81-4278-4625-b039-b022d22ad39b', embedding=None, metadata={'page_label': '199', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='180 5. Basis Expansions and Regularization\\nA simple choice for λisλ=σ√2logN, where σis an estimate of the\\nstandard deviation of the noise. We can give some motivation for this cho ice.\\nSinceWis an orthonormal transformation, if the elements of yare white\\nnoise (independent Gaussian variates with mean 0 and variance σ2), then\\nso arey∗. Furthermore if random variables Z1,Z2,... ,Z Nare white noise,\\nthe expected maximum of |Zj|,j= 1,... ,N is approximately σ√2logN.\\nHence all coeﬃcients below σ√2logNare likely to be noise and are set to\\nzero.\\nThe space Wcould be any basis of orthonormal functions: polynomials,\\nnatural splines or cosinusoids. What makes wavelets special is the particular\\nform of basis functions used, which allows for a representation localized in\\ntime and in frequency .\\nLet’s look again at the NMR signal of Figure 5.17. The wavelet transfor m\\nwas computed using a symmlet −8 basis. Notice that the coeﬃcients do not\\ndescend all the way to V0, but stop at V4which has 16 basis functions.\\nAs we ascend to each level of detail, the coeﬃcients get smaller, except in\\nlocations where spiky behavior is present. The wavelet coeﬃcients represent\\ncharacteristics of the signal localized in time (the basis functions at each\\nlevel are translations of each other) and localized in frequency. Each dilation\\nincreases the detail by a factor of two, and in this sense corresponds to\\ndoubling the frequency in a traditional Fourier representation. In fact, a\\nmore mathematical understanding of wavelets reveals that the wavelets at\\na particular scale have a Fourier transform that is restricted to a limited\\nrange or octave of frequencies.\\nThe shrinking/truncation in the right panel was achieved using the SURE\\napproach described in the introduction to this section. The orthonormal\\nN×Nbasis matrix Whas columns which are the wavelet basis functions\\nevaluated at the Ntime points. In particular, in this case there will be 16\\ncolumns corresponding to the φ4,k(x), and the remainder devoted to the\\nψj,k(x), j= 4,... ,11. In practice λdepends on the noise variance, and has\\nto be estimated from the data (such as the variance of the coeﬃcients at\\nthe highest level).\\nNotice the similarity between the SURE criterion (5.68) on page 179,\\nand the smoothing spline criterion (5.21) on page 156:\\n•Both are hierarchically structured from coarse to ﬁne detail, although\\nwavelets are also localized in time within each resolution level.\\n•The splines build in a bias toward smooth functions by imposing\\ndiﬀerential shrinking constants dk. Early versions of SURE shrinkage\\ntreated all scales equally. The S+wavelets function waveshrink() has\\nmany options, some of which allow for diﬀerential shrinkage.\\n•The spline L2penalty cause pure shrinkage, while the SURE L1\\npenalty does shrinkage and selection.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c9d49d78-a29a-43c9-b706-b29e55c5809a', embedding=None, metadata={'page_label': '200', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 181\\nMore generally smoothing splines achieve compression of the original signal\\nby imposing smoothness, while wavelets impose sparsity. Figure 5.19 co m-\\npares a wavelet ﬁt (using SURE shrinkage) to a smoothing spline ﬁt (using\\ncross-validation) on two examples diﬀerent in nature. For the NMR data in\\nthe upper panel, the smoothing spline introduces detail everywhere in order\\nto capture the detail in the isolated spikes; the wavelet ﬁt nicely localizes\\nthe spikes. In the lower panel, the true function is smooth, and the noise is\\nrelatively high. The wavelet ﬁt has let in some additional and unnecessary\\nwiggles—a price it pays in variance for the additional adaptivity.\\nThe wavelet transform is not performed by matrix multiplication as in\\ny∗=WTy. In fact, using clever pyramidal schemes y∗can be obtained\\ninO(N) computations, which is even faster than the Nlog(N) of the fast\\nFourier transform (FFT). While the general construction is beyond the\\nscope of this book, it is easy to see for the Haar basis (Exercise 5.19).\\nLikewise, the inverse wavelet transform Wˆθis also O(N).\\nThis has been a very brief glimpse of this vast and growing ﬁeld. There is\\na very large mathematical and computational base built on wavelets. Mod-\\nern image compression is often performed using two-dimensional wavelet\\nrepresentations.\\nBibliographic Notes\\nSplines and B-splines are discussed in detail in de Boor (1978). Green\\nand Silverman (1994) and Wahba (1990) give a thorough treatment of\\nsmoothing splines and thin-plate splines; the latter also covers reproducing\\nkernel Hilbert spaces. See also Girosi et al. (1995) and Evgeniou et al.\\n(2000) for connections between many nonparametric regression techniques\\nusing RKHS approaches. Modeling functional data, as in Section 5.2.3, is\\ncovered in detail in Ramsay and Silverman (1997).\\nDaubechies (1992) is a classic and mathematical treatment of wavelets.\\nOther useful sources are Chui (1992) and Wickerhauser (1994). Donoho and\\nJohnstone (1994) developed the SURE shrinkage and selection technology\\nfrom a statistical estimation framework; see also Vidakovic (199 9). Bruce\\nand Gao (1996) is a useful applied introduction, which also describes the\\nwavelet software in S-PLUS.\\nExercises\\nEx. 5.1 Show that the truncated power basis functions in (5.3) represent a\\nbasis for a cubic spline with the two knots as indicated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='81ab7160-6875-47b2-91ff-866a5ed1d74c', embedding=None, metadata={'page_label': '201', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='182 5. Basis Expansions and Regularization\\nNMR Signal0 200 400 600 800 10000 20 40 60spline\\nwavelet\\nSmooth Function (Simulated)n\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2 4spline\\nwavelet\\ntrue•\\n••\\n•\\n•••\\n•\\n•\\n•\\n••••\\n•••\\n••••\\n••\\n••\\n••\\n••••••\\n•••\\n•\\n••\\n••\\n•••••\\n•••••••\\n••••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n••••\\n••\\n••\\n•••\\n•••\\n•••\\n••\\n•\\n•••\\n•••\\n•\\n••••••\\n••••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\nFIGURE 5.19. Wavelet smoothing compared with smoothing splines on two\\nexamples. Each panel compares the SURE-shrunk wavelet ﬁt to the cro ss-validated\\nsmoothing spline ﬁt.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='920658fe-093c-4bc4-a787-6f6a6a02098f', embedding=None, metadata={'page_label': '202', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 183\\nEx. 5.2 Suppose that Bi,M(x) is an order- M B-spline deﬁned in the Ap-\\npendix on page 186 through the sequence (5.77)–(5.78).\\n(a) Show by induction that Bi,M(x) = 0 for x̸∈[τi,τi+M]. This shows, for\\nexample, that the support of cubic B-splines is at most 5 knots.\\n(b) Show by induction that Bi,M(x)>0 forx∈(τi,τi+M). The B-splines\\nare positive in the interior of their support.\\n(c) Show by induction that∑K+M\\ni=1Bi,M(x) = 1∀x∈[ξ0,ξK+1].\\n(d) Show that Bi,Mis a piecewise polynomial of order M(degree M−1)\\non [ξ0,ξK+1], with breaks only at the knots ξ1,... ,ξ K.\\n(e) Show that an order- M B-spline basis function is the density function\\nof a convolution of Muniform random variables.\\nEx. 5.3 Write a program to reproduce Figure 5.3 on page 145.\\nEx. 5.4 Consider the truncated power series representation for cubic splines\\nwithKinterior knots. Let\\nf(X) =3∑\\nj=0βjXj+K∑\\nk=1θk(X−ξk)3\\n+. (5.70)\\nProve that the natural boundary conditions for natural cubic splines (Sec-\\ntion 5.2.1) imply the following linear constraints on the coeﬃcients:\\nβ2= 0,∑K\\nk=1θk= 0,\\nβ3= 0,∑K\\nk=1ξkθk= 0.(5.71)\\nHence derive the basis (5.4) and (5.5).\\nEx. 5.5 Write a program to classify the phoneme data using a quadratic dis-\\ncriminant analysis (Section 4.3). Since there are many correlated features,\\nyou should ﬁlter them using a smooth basis of natural cubic splines (Sec-\\ntion 5.2.3). Decide beforehand on a series of ﬁve diﬀerent choices for the\\nnumber and position of the knots, and use tenfold cross-validation to make\\nthe ﬁnal selection. The phoneme data are available from the book website\\nwww-stat.stanford.edu/ElemStatLearn .\\nEx. 5.6 Suppose you wish to ﬁt a periodic function, with a known period T.\\nDescribe how you could modify the truncated power series basis to achieve\\nthis goal.\\nEx. 5.7 Derivation of smoothing splines (Green and Silverman, 1994). Sup-\\npose that N≥2, and that gis the natural cubic spline interpolant to the\\npairs{xi,zi}N\\n1, with a < x 1<≤≤≤< x N< b. This is a natural spline', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5211b02c-3037-4efd-893a-29152808d281', embedding=None, metadata={'page_label': '203', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='184 5. Basis Expansions and Regularization\\nwith a knot at every xi; being an N-dimensional space of functions, we can\\ndetermine the coeﬃcients such that it interpolates the sequence ziexactly.\\nLet ˜gbe any other diﬀerentiable function on [ a,b] that interpolates the N\\npairs.\\n(a) Let h(x) = ˜g(x)−g(x). Use integration by parts and the fact that gis\\na natural cubic spline to show that\\n∫b\\nag′′(x)h′′(x)dx=−N−1∑\\nj=1g′′′(x+\\nj){h(xj+1)−h(xj)}(5.72)\\n= 0.\\n(b) Hence show that∫b\\na˜g′′(t)2dt≥∫b\\nag′′(t)2dt,\\nand that equality can only hold if his identically zero in [ a,b].\\n(c) Consider the penalized least squares problem\\nmin\\nf[N∑\\ni=1(yi−f(xi))2+λ∫b\\naf′′(t)2dt]\\n.\\nUse (b) to argue that the minimizer must be a cubic spline with knots\\nat each of the xi.\\nEx. 5.8 In the appendix to this chapter we show how the smoothing spline\\ncomputations could be more eﬃciently carried out using a ( N+ 4) dimen-\\nsional basis of B-splines. Describe a slightly simpler scheme using a ( N+2)\\ndimensional B-spline basis deﬁned on the N−2 interior knots.\\nEx. 5.9 Derive the Reinsch form Sλ= (I+λK)−1for the smoothing spline.\\nEx. 5.10 Derive an expression for Var( ˆfλ(x0)) and bias( ˆfλ(x0)). Using the\\nexample (5.22), create a version of Figure 5.9 where the mean and several\\n(pointwise) quantiles of ˆfλ(x) are shown.\\nEx. 5.11 Prove that for a smoothing spline the null space of Kis spanned\\nby functions linear in X.\\nEx. 5.12 Characterize the solution to the following problem,\\nmin\\nfRSS(f,λ) =N∑\\ni=1wi{yi−f(xi)}2+λ∫\\n{f′′(t)}2dt, (5.73)\\nwhere the wi≥0 are observation weights.\\nCharacterize the solution to the smoothing spline problem (5.9) when\\nthe training data have ties in X.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d02a2704-69a9-45a1-83c8-f47328105770', embedding=None, metadata={'page_label': '204', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 185\\nEx. 5.13 You have ﬁtted a smoothing spline ˆfλto a sample of Npairs\\n(xi,yi). Suppose you augment your original sample with the pair x0,ˆfλ(x0),\\nand reﬁt; describe the result. Use this to derive the N-fold cross-validation\\nformula (5.26).\\nEx. 5.14 Derive the constraints on the αjin the thin-plate spline expan-\\nsion (5.39) to guarantee that the penalty J(f) is ﬁnite. How else could one\\nensure that the penalty was ﬁnite?\\nEx. 5.15 This exercise derives some of the results quoted in Section 5.8.1.\\nSuppose K(x,y) satisfying the conditions (5.45) and let f(x)∈ H K. Show\\nthat\\n(a)⟨K(≤,xi),f⟩HK=f(xi).\\n(b)⟨K(≤,xi),K(≤,xj)⟩HK=K(xi,xj).\\n(c) If g(x) =∑N\\ni=1αiK(x,xi), then\\nJ(g) =N∑\\ni=1N∑\\nj=1K(xi,xj)αiαj.\\nSuppose that ˜ g(x) =g(x) +ρ(x), with ρ(x)∈ H K, and orthogonal in HK\\nto each of K(x,xi), i= 1,... ,N . Show that\\n(d)\\nN∑\\ni=1L(yi,˜g(xi)) +λJ(˜g)≥N∑\\ni=1L(yi,g(xi)) +λJ(g) (5.74)\\nwith equality iﬀ ρ(x) = 0.\\nEx. 5.16 Consider the ridge regression problem (5.53), and assume M≥N.\\nAssume you have a kernel Kthat computes the inner product K(x,y) =∑M\\nm=1hm(x)hm(y).\\n(a) Derive (5.62) on page 171 in the text. How would you compute the\\nmatrices VandDγ, given K? Hence show that (5.63) is equivalent\\nto (5.53).\\n(b) Show that\\nˆf=Hˆβ\\n=K(K+λI)−1y, (5.75)\\nwhereHis the N×Mmatrix of evaluations hm(xi), and K=HHT\\ntheN×Nmatrix of inner-products h(xi)Th(xj).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe64d69c-924e-47f2-90c6-412c1a10dd0e', embedding=None, metadata={'page_label': '205', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='186 5. Basis Expansions and Regularization\\n(c) Show that\\nˆf(x) = h(x)Tˆβ\\n=N∑\\ni=1K(x,xi)ˆαi (5.76)\\nandˆα= (K+λI)−1y.\\n(d) How would you modify your solution if M < N ?\\nEx. 5.17 Show how to convert the discrete eigen-decomposition of Kin\\nSection 5.8.2 to estimates of the eigenfunctions of K.\\nEx. 5.18 The wavelet function ψ(x) of the symmlet- pwavelet basis has\\nvanishing moments up to order p. Show that this implies that polynomials\\nof order pare represented exactly in V0, deﬁned on page 176.\\nEx. 5.19 Show that the Haar wavelet transform of a signal of length N= 2J\\ncan be computed in O(N) computations.\\nAppendix: Computations for Splines\\nIn this Appendix, we describe the B-spline basis for representing polyno-\\nmial splines. We also discuss their use in the computations of smoothing\\nsplines.\\nB-splines\\nBefore we can get started, we need to augment the knot sequence deﬁned\\nin Section 5.2. Let ξ0< ξ1andξK< ξK+1be two boundary knots, which\\ntypically deﬁne the domain over which we wish to evaluate our spline. We\\nnow deﬁne the augmented knot sequence τsuch that\\n•τ1≤τ2≤ ≤≤≤ ≤ τM≤ξ0;\\n•τj+M=ξj, j= 1,≤≤≤,K;\\n•ξK+1≤τK+M+1≤τK+M+2≤ ≤≤≤ ≤ τK+2M.\\nThe actual values of these additional knots beyond the boundary are arbi-\\ntrary, and it is customary to make them all the same and equal to ξ0and\\nξK+1, respectively.\\nDenote by Bi,m(x) the ithB-spline basis function of order mfor the\\nknot-sequence τ,m≤M. They are deﬁned recursively in terms of divided', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98b77ee0-b4a0-4b29-8b7f-d035d1daf958', embedding=None, metadata={'page_label': '206', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appendix: Computations for Splines 187\\ndiﬀerences as follows:\\nBi,1(x) ={\\n1 ifτi≤x < τ i+1\\n0 otherwise(5.77)\\nfori= 1,... ,K + 2M−1. These are also known as Haar basis functions.\\nBi,m(x) =x−τi\\nτi+m−1−τiBi,m−1(x) +τi+m−x\\nτi+m−τi+1Bi+1,m−1(x)\\nfori= 1,... ,K + 2M−m.\\n(5.78)\\nThus with M= 4,Bi,4, i= 1,≤≤≤,K+ 4 are the K+ 4 cubic B-spline\\nbasis functions for the knot sequence ξ. This recursion can be contin-\\nued and will generate the B-spline basis for any order spline. Figure 5.20\\nshows the sequence of B-splines up to order four with knots at the points\\n0.0,0.1,... ,1.0. Since we have created some duplicate knots, some care\\nhas to be taken to avoid division by zero. If we adopt the convention\\nthatBi,1= 0 if τi=τi+1, then by induction Bi,m= 0 if τi=τi+1=\\n...=τi+m. Note also that in the construction above, only the subset\\nBi,m, i=M−m+ 1,... ,M +Kare required for the B-spline basis\\nof order m < M with knots ξ.\\nTo fully understand the properties of these functions, and to show that\\nthey do indeed span the space of cubic splines for the knot sequence, re-\\nquires additional mathematical machinery, including the properties of di-\\nvided diﬀerences. Exercise 5.2 explores these issues.\\nThe scope of B-splines is in fact bigger than advertised here, and has to\\ndo with knot duplication. If we duplicate an interior knot in the construc-\\ntion of the τsequence above, and then generate the B-spline sequence as\\nbefore, the resulting basis spans the space of piecewise polynomials with\\none less continuous derivative at the duplicated knot. In general, if in ad-\\ndition to the repeated boundary knots, we include the interior knot ξj\\n1≤rj≤Mtimes, then the lowest-order derivative to be discontinuous\\natx=ξjwill be order M−rj. Thus for cubic splines with no repeats,\\nrj= 1, j= 1,... ,K , and at each interior knot the third derivatives (4 −1)\\nare discontinuous. Repeating the jth knot three times leads to a discontin-\\nuous 1st derivative; repeating it four times leads to a discontinuous zeroth\\nderivative, i.e., the function is discontinuous at x=ξj. This is exactly what\\nhappens at the boundary knots; we repeat the knots Mtimes, so the spline\\nbecomes discontinuous at the boundary knots (i.e., undeﬁned beyond the\\nboundary).\\nThe local support of B-splines has important computational implica-\\ntions, especially when the number of knots Kis large. Least squares com-\\nputations with Nobservations and K+Mvariables (basis functions) take\\nO(N(K+M)2+ (K+M)3) ﬂops (ﬂoating point operations.) If Kis some\\nappreciable fraction of N, this leads to O(N3) algorithms which becomes', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='199cbbea-3935-49d1-a50c-b1b01ab1ed1e', embedding=None, metadata={'page_label': '207', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='188 5. Basis Expansions and Regularization\\nB-splines of Order 1\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 2\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 3\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 4\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nFIGURE 5.20. The sequence of B-splines up to order four with ten knots evenly\\nspaced from 0to1. The B-splines have local support ; they are nonzero on an\\ninterval spanned by M+ 1knots.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9871e4c3-c26a-43f8-86c8-56660a80e411', embedding=None, metadata={'page_label': '208', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appendix: Computations for Splines 189\\nunacceptable for large N. If the Nobservations are sorted, the N×(K+M)\\nregression matrix consisting of the K+M B-spline basis functions evalu-\\nated at the Npoints has many zeros, which can be exploited to reduce the\\ncomputational complexity back to O(N). We take this up further in the\\nnext section.\\nComputations for Smoothing Splines\\nAlthough natural splines (Section 5.2.1) provide a basis for smoothing\\nsplines, it is computationally more convenient to operate in the larger space\\nof unconstrained B-splines. We write f(x) =∑N+4\\n1γjBj(x), where γjare\\ncoeﬃcients and the Bjare the cubic B-spline basis functions. The solution\\nlooks the same as before,\\nˆγ= (BTB+λΩB)−1BTy, (5.79)\\nexcept now the N×Nmatrix Nis replaced by the N×(N+ 4) matrix\\nB, and similarly the ( N+ 4)×(N+ 4) penalty matrix ΩBreplaces the\\nN×Ndimensional ΩN. Although at face value it seems that there are\\nno boundary derivative constraints, it turns out that the penalty term\\nautomatically imposes them by giving eﬀectively inﬁnite weight to any non\\nzero derivative beyond the boundary. In practice, ˆ γis restricted to a linear\\nsubspace for which the penalty is always ﬁnite.\\nSince the columns of Bare the evaluated B-splines, in order from left\\nto right and evaluated at the sorted values of X, and the cubic B-splines\\nhave local support, Bis lower 4-banded. Consequently the matrix M=\\n(BTB+λΩ) is 4-banded and hence its Cholesky decomposition M=LLT\\ncan be computed easily. One then solves LLTγ=BTyby back-substitution\\nto give γand hence the solution ˆfinO(N) operations.\\nIn practice, when Nis large, it is unnecessary to use all Ninterior knots,\\nand any reasonable thinning strategy will save in computations and have\\nnegligible eﬀect on the ﬁt. For example, the smooth.spline function in S-\\nPLUS uses an approximately logarithmic strategy: if N <50 all knots are\\nincluded, but even at N= 5,000 only 204 knots are used.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89343934-c4fd-4236-bd7e-6c3b8967849a', embedding=None, metadata={'page_label': '209', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='190 5. Basis Expansions and Regularization', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='028afa39-f6b5-413e-8a7e-5d2c9f8d0132', embedding=None, metadata={'page_label': '210', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 191\\nPrinter: Opaque this\\n6\\nKernel Smoothing Methods\\nIn this chapter we describe a class of regression techniques that achieve\\nﬂexibility in estimating the regression function f(X) over the domain IRp\\nby ﬁtting a diﬀerent but simple model separately at each query point x0.\\nThis is done by using only those observations close to the target point x0to\\nﬁt the simple model, and in such a way that the resulting estimated function\\nˆf(X) issmooth in IRp. This localization is achieved via a weighting function\\norkernel Kλ(x0,xi), which assigns a weight to xibased on its distance from\\nx0. The kernels Kλare typically indexed by a parameter λthat dictates\\nthe width of the neighborhood. These memory-based methods require in\\nprinciple little or no training; all the work gets done at evaluation time.\\nThe only parameter that needs to be determined from the training data is\\nλ. The model, however, is the entire training data set.\\nWe also discuss more general classes of kernel-based techniques , which\\ntie in with structured methods in other chapters, and are useful for density\\nestimation and classiﬁcation.\\nThe techniques in this chapter should not be confused with those asso-\\nciated with the more recent usage of the phrase “kernel methods”. In this\\nchapter kernels are mostly used as a device for localization. We discuss ker-\\nnel methods in Sections 5.8, 14.5.4, 18.5 and Chapter 12; in those contexts\\nthe kernel computes an inner product in a high-dimensional (implicit) fea-\\nture space, and is used for regularized nonlinear modeling. We make some\\nconnections to the methodology in this chapter at the end of Section 6.7.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ef29f59f-ecd5-44fe-a081-f88d13139f37', embedding=None, metadata={'page_label': '211', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='192 6. Kernel Smoothing Methods\\nNearest-Neighbor Kernel\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOOOO\\nOO\\nOO\\nOOOO\\nOO\\nOO\\nOOOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nO\\nOO\\nOO\\nO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO•\\nx0ˆf(x0)Epanechnikov Kernel\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOOOO\\nOO\\nOO\\nOOOO\\nOO\\nOO\\nOOOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO•\\nx0ˆf(x0)\\nFIGURE 6.1. In each panel 100pairs xi, yiare generated at random from the\\nblue curve with Gaussian errors: Y= sin(4 X)+ε,X∼U[0,1],ε∼N(0,1/3). In\\nthe left panel the green curve is the result of a 30-nearest-neighbor running-mean\\nsmoother. The red point is the ﬁtted constant ˆf(x0), and the red circles indicate\\nthose observations contributing to the ﬁt at x0. The solid yellow region indicates\\nthe weights assigned to observations. In the right panel, the gr een curve is the\\nkernel-weighted average, using an Epanechnikov kernel with (hal f) window width\\nλ= 0.2.\\n6.1 One-Dimensional Kernel Smoothers\\nIn Chapter 2, we motivated the k–nearest-neighbor average\\nˆf(x) = Ave( yi|xi∈Nk(x)) (6.1)\\nas an estimate of the regression function E( Y|X=x). Here Nk(x) is the set\\nofkpoints nearest to xin squared distance, and Ave denotes the average\\n(mean). The idea is to relax the deﬁnition of conditional expectation, as\\nillustrated in the left panel of Figure 6.1, and compute an average in a\\nneighborhood of the target point. In this case we have used the 30-nearest\\nneighborhood—the ﬁt at x0is the average of the 30 pairs whose xivalues\\nare closest to x0. The green curve is traced out as we apply this deﬁnition\\nat diﬀerent values x0. The green curve is bumpy, since ˆf(x) is discontinuous\\ninx. As we move x0from left to right, the k-nearest neighborhood remains\\nconstant, until a point xito the right of x0becomes closer than the furthest\\npoint xi′in the neighborhood to the left of x0, at which time xireplaces xi′.\\nThe average in (6.1) changes in a discrete way, leading to a discontinuous\\nˆf(x).\\nThis discontinuity is ugly and unnecessary. Rather than give all the\\npoints in the neighborhood equal weight, we can assign weights that die\\noﬀ smoothly with distance from the target point. The right panel shows\\nan example of this, using the so-called Nadaraya–Watson kernel-weighted', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='554c6bbf-8e6d-45a5-a510-508bb98664fa', embedding=None, metadata={'page_label': '212', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.1 One-Dimensional Kernel Smoothers 193\\naverage\\nˆf(x0) =∑N\\ni=1Kλ(x0,xi)yi∑N\\ni=1Kλ(x0,xi), (6.2)\\nwith the Epanechnikov quadratic kernel\\nKλ(x0,x) =D(|x−x0|\\nλ)\\n, (6.3)\\nwith\\nD(t) ={3\\n4(1−t2) if|t| ≤1;\\n0 otherwise .(6.4)\\nThe ﬁtted function is now continuous, and quite smooth in the right panel\\nof Figure 6.1. As we move the target from left to right, points enter t he\\nneighborhood initially with weight zero, and then their contribution slowly\\nincreases (see Exercise 6.1).\\nIn the right panel we used a metric window size λ= 0.2 for the kernel\\nﬁt, which does not change as we move the target point x0, while the size\\nof the 30-nearest-neighbor smoothing window adapts to the local density\\nof the xi. One can, however, also use such adaptive neighborhoods with\\nkernels, but we need to use a more general notation. Let hλ(x0) be a width\\nfunction (indexed by λ) that determines the width of the neighborhood at\\nx0. Then more generally we have\\nKλ(x0,x) =D(|x−x0|\\nhλ(x0))\\n. (6.5)\\nIn (6.3), hλ(x0) =λis constant. For k-nearest neighborhoods, the neigh-\\nborhood size kreplaces λ, and we have hk(x0) =|x0−x[k]|where x[k]is\\nthekth closest xitox0.\\nThere are a number of details that one has to attend to in practice:\\n•The smoothing parameter λ, which determines the width of the local\\nneighborhood, has to be determined. Large λimplies lower variance\\n(averages over more observations) but higher bias (we essentially as-\\nsume the true function is constant within the window).\\n•Metric window widths (constant hλ(x)) tend to keep the bias of the\\nestimate constant, but the variance is inversely proportional to the\\nlocal density. Nearest-neighbor window widths exhibit the opposite\\nbehavior; the variance stays constant and the absolute bias varies\\ninversely with local density.\\n•Issues arise with nearest-neighbors when there are ties in the xi. With\\nmost smoothing techniques one can simply reduce the data set by\\naveraging the yiat tied values of X, and supplementing these new\\nobservations at the unique values of xiwith an additional weight wi\\n(which multiples the kernel weight).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c7fda56f-6adb-4bfb-bfae-3c42c8a0eb82', embedding=None, metadata={'page_label': '213', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='194 6. Kernel Smoothing Methods\\n-3 -2 -1 0 1 2 30.0 0.4 0.8Epanechnikov\\nTri-cube\\nGaussianKλ(x0, x)\\nFIGURE 6.2. A comparison of three popular kernels for local smoothing. Eac h\\nhas been calibrated to integrate to 1. The tri-cube kernel is compact and has two\\ncontinuous derivatives at the boundary of its support, while th e Epanechnikov ker-\\nnel has none. The Gaussian kernel is continuously diﬀerentiable, bu t has inﬁnite\\nsupport.\\n•This leaves a more general problem to deal with: observation weights\\nwi. Operationally we simply multiply them by the kernel weights be-\\nfore computing the weighted average. With nearest neighborhoods, it\\nis now natural to insist on neighborhoods with a total weight content\\nk(relative to∑wi). In the event of overﬂow (the last observation\\nneeded in a neighborhood has a weight wjwhich causes the sum of\\nweights to exceed the budget k), then fractional parts can be used.\\n•Boundary issues arise. The metric neighborhoods tend to contain less\\npoints on the boundaries, while the nearest-neighborhoods get wider.\\n•The Epanechnikov kernel has compact support (needed when used\\nwith nearest-neighbor window size). Another popular compact kernel\\nis based on the tri-cube function\\nD(t) ={\\n(1− |t|3)3if|t| ≤1;\\n0 otherwise(6.6)\\nThis is ﬂatter on the top (like the nearest-neighbor box) and is diﬀer-\\nentiable at the boundary of its support. The Gaussian density func-\\ntionD(t) =φ(t) is a popular noncompact kernel, with the standard-\\ndeviation playing the role of the window size. Figure 6.2 compares\\nthe three.\\n6.1.1 Local Linear Regression\\nWe have progressed from the raw moving average to a smoothly varying\\nlocally weighted average by using kernel weighting. The smooth kernel ﬁt\\nstill has problems, however, as exhibited in Figure 6.3 (left panel). Locally-\\nweighted averages can be badly biased on the boundaries of the domain,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='daeba50c-65a4-45a3-a39b-f1c509712bff', embedding=None, metadata={'page_label': '214', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.1 One-Dimensional Kernel Smoothers 195\\nN-W Kernel at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nO\\nO\\nOO\\nOOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\n•\\nx0ˆf(x0)Local Linear Regression at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nO\\nO\\nOO\\nOOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\n•\\nx0ˆf(x0)\\nFIGURE 6.3. The locally weighted average has bias problems at or near the\\nboundaries of the domain. The true function is approximately line ar here, but\\nmost of the observations in the neighborhood have a higher mean than the target\\npoint, so despite weighting, their mean will be biased upwards . By ﬁtting a locally\\nweighted linear regression (right panel), this bias is remove d to ﬁrst order\\nbecause of the asymmetry of the kernel in that region. By ﬁtting straight\\nlines rather than constants locally, we can remove this bias exactly to ﬁrst\\norder; see Figure 6.3 (right panel). Actually, this bias can be present in the\\ninterior of the domain as well, if the Xvalues are not equally spaced (for\\nthe same reasons, but usually less severe). Again locally weighted linear\\nregression will make a ﬁrst-order correction.\\nLocally weighted regression solves a separate weighted least squares prob-\\nlem at each target point x0:\\nmin\\nα(x0),β(x0)N∑\\ni=1Kλ(x0,xi)[yi−α(x0)−β(x0)xi]2. (6.7)\\nThe estimate is then ˆf(x0) = ˆα(x0) +ˆβ(x0)x0. Notice that although we ﬁt\\nan entire linear model to the data in the region, we only use it to evaluate\\nthe ﬁt at the single point x0.\\nDeﬁne the vector-valued function b(x)T= (1,x). Let Bbe the N×2\\nregression matrix with ith row b(xi)T, andW(x0) the N×Ndiagonal\\nmatrix with ith diagonal element Kλ(x0,xi). Then\\nˆf(x0) = b(x0)T(BTW(x0)B)−1BTW(x0)y (6.8)\\n=N∑\\ni=1li(x0)yi. (6.9)\\nEquation (6.8) gives an explicit expression for the local linear regression\\nestimate, and (6.9) highlights the fact that the estimate is linear in the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='398a75e7-96cd-452b-ab8d-252995411568', embedding=None, metadata={'page_label': '215', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='196 6. Kernel Smoothing Methods\\nLocal Linear Equivalent Kernel at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO\\nOOOO\\nOO\\nO\\nOOO\\nOOOOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOO\\n•••••••••\\n•\\n••\\n••\\n•••••••••••••• • ••••• •• • • • ••• • ••• • •• ••• • •• ••••• ••• • ••• •••• •• • •• • • •• • •• •••• ••• • • • ••••••\\nx0ˆf(x0)Local Linear Equivalent Kernel in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO\\nOOOO\\nOO\\nO\\nOOO\\nOOOOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO•\\n•• • ••• •••• •••• • •••••••• ••• ••••••••••••••••••••••••••••••••••••••\\n•••\\n••\\n• •• • ••• • •• • •• •••• ••• • • • ••••••\\nx0ˆf(x0)\\nFIGURE 6.4. The green points show the equivalent kernel li(x0)for local re-\\ngression. These are the weights in ˆf(x0) =PN\\ni=1li(x0)yi, plotted against their\\ncorresponding xi. For display purposes, these have been rescaled, since in fac t\\nthey sum to 1. Since the yellow shaded region is the (rescaled) equivalent ke rnel\\nfor the Nadaraya–Watson local average, we see how local regr ession automati-\\ncally modiﬁes the weighting kernel to correct for biases due to a symmetry in the\\nsmoothing window.\\nyi(theli(x0) do not involve y). These weights li(x0) combine the weight-\\ning kernel Kλ(x0,≤) and the least squares operations, and are sometimes\\nreferred to as the equivalent kernel . Figure 6.4 illustrates the eﬀect of lo-\\ncal linear regression on the equivalent kernel. Historically, the bias in the\\nNadaraya–Watson and other local average kernel methods were corrected\\nby modifying the kernel. These modiﬁcations were based on theoretical\\nasymptotic mean-square-error considerations, and besides being tedious to\\nimplement, are only approximate for ﬁnite sample sizes. Local linear re-\\ngression automatically modiﬁes the kernel to correct the bias exactly to\\nﬁrst order, a phenomenon dubbed as automatic kernel carpentry . Consider\\nthe following expansion for E ˆf(x0), using the linearity of local regression\\nand a series expansion of the true function faround x0,\\nEˆf(x0) =N∑\\ni=1li(x0)f(xi)\\n=f(x0)N∑\\ni=1li(x0) +f′(x0)N∑\\ni=1(xi−x0)li(x0)\\n+f′′(x0)\\n2N∑\\ni=1(xi−x0)2li(x0) +R, (6.10)\\nwhere the remainder term Rinvolves third- and higher-order derivatives of\\nf, and is typically small under suitable smoothness assumptions. It can be', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cf8efe02-ae59-4750-a2f7-eb410fefcfcf', embedding=None, metadata={'page_label': '216', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.1 One-Dimensional Kernel Smoothers 197\\nLocal Linear in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOOOO\\nOOOO\\nOOOOO\\nOO\\nOO\\nOO\\nO\\nOO\\nO\\nO\\nOO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nO•ˆf(x0)Local Quadratic in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOOOO\\nOOOO\\nOOOOO\\nOO\\nOO\\nOO\\nO\\nOO\\nO\\nO\\nOO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nO•ˆf(x0)\\nFIGURE 6.5. Local linear ﬁts exhibit bias in regions of curvature of the true\\nfunction. Local quadratic ﬁts tend to eliminate this bias.\\nshown (Exercise 6.2) that for local linear regression,∑N\\ni=1li(x0) = 1 and∑N\\ni=1(xi−x0)li(x0) = 0. Hence the middle term equals f(x0), and since\\nthe bias is E ˆf(x0)−f(x0), we see that it depends only on quadratic and\\nhigher–order terms in the expansion of f.\\n6.1.2 Local Polynomial Regression\\nWhy stop at local linear ﬁts? We can ﬁt local polynomial ﬁts of any de-\\ngreed,\\nmin\\nα(x0),βj(x0), j=1,...,dN∑\\ni=1Kλ(x0,xi)\\uf8ee\\n\\uf8f0yi−α(x0)−d∑\\nj=1βj(x0)xj\\ni\\uf8f9\\n\\uf8fb2\\n(6.11)\\nwith solution ˆf(x0) = ˆα(x0)+∑d\\nj=1ˆβj(x0)xj\\n0. In fact, an expansion such as\\n(6.10) will tell us that the bias will only have components of degree d+1 and\\nhigher (Exercise 6.2). Figure 6.5 illustrates local quadratic regression. Local\\nlinear ﬁts tend to be biased in regions of curvature of the true function, a\\nphenomenon referred to as trimming the hills andﬁlling the valleys . Local\\nquadratic regression is generally able to correct this bias.\\nThere is of course a price to be paid for this bias reduction, and that is\\nincreased variance. The ﬁt in the right panel of Figure 6.5 is slightly more\\nwiggly, especially in the tails. Assuming the model yi=f(xi) +εi, with\\nεiindependent and identically distributed with mean zero and variance\\nσ2, Var( ˆf(x0)) =σ2||l(x0)||2, where l(x0) is the vector of equivalent kernel\\nweights at x0. It can be shown (Exercise 6.3) that ||l(x0)||increases with d,\\nand so there is a bias–variance tradeoﬀ in selecting the polynomial degree.\\nFigure 6.6 illustrates these variance curves for degree zero, one and two', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ab13536-f840-430a-ac61-c512de4f18db', embedding=None, metadata={'page_label': '217', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='198 6. Kernel Smoothing Methods\\nVariance\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5Constant\\nLinear\\nQuadratic\\nFIGURE 6.6. The variances functions ||l(x)||2for local constant, linear and\\nquadratic regression, for a metric bandwidth ( λ= 0.2) tri-cube kernel.\\nlocal polynomials. To summarize some collected wisdom on this issue:\\n•Local linear ﬁts can help bias dramatically at the boundaries at a\\nmodest cost in variance. Local quadratic ﬁts do little at the bound-\\naries for bias, but increase the variance a lot.\\n•Local quadratic ﬁts tend to be most helpful in reducing bias due to\\ncurvature in the interior of the domain.\\n•Asymptotic analysis suggest that local polynomials of odd degree\\ndominate those of even degree. This is largely due to the fact that\\nasymptotically the MSE is dominated by boundary eﬀects.\\nWhile it may be helpful to tinker, and move from local linear ﬁts at the\\nboundary to local quadratic ﬁts in the interior, we do not recommend such\\nstrategies. Usually the application will dictate the degree of the ﬁt. For\\nexample, if we are interested in extrapolation, then the boundary is of\\nmore interest, and local linear ﬁts are probably more reliable.\\n6.2 Selecting the Width of the Kernel\\nIn each of the kernels Kλ,λis a parameter that controls its width:\\n•For the Epanechnikov or tri-cube kernel with metric width, λis the\\nradius of the support region.\\n•For the Gaussian kernel, λis the standard deviation.\\n•λis the number kof nearest neighbors in k-nearest neighborhoods,\\noften expressed as a fraction or spank/Nof the total training sample.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='23a93983-c208-4122-b2c2-da8d91bd25e6', embedding=None, metadata={'page_label': '218', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.2 Selecting the Width of the Kernel 199\\n•••\\n••••••\\n•\\n••\\n••\\n•••••\\n••••• ••••••• •••• •• ••• ••••• • • ••••• •••• ••• • ••••• •••••••• • • • •• •• •••• • ••• • ••••••••••••••••\\n••\\n••\\n••••••\\n••\\n•• ••••••• •••• •• ••• •••••• • ••••• ••••••• • ••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• •• ••••• •• •• • •••• •• •• ••••••••••••••••••••• •• ••••• •••••\\n•\\n••••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• ••••••• •• •• • •••••• ••••••••••••••••••••••• • • •••••••••••\\n•••••••••••••••• •• •• •• •••• •••• • ••••••\\nFIGURE 6.7. Equivalent kernels for a local linear regression smoother (tri -cube\\nkernel; orange) and a smoothing spline (blue), with matching degre es of freedom.\\nThe vertical spikes indicates the target points.\\nThere is a natural bias–variance tradeoﬀ as we change the width of the\\naveraging window, which is most explicit for local averages:\\n•If the window is narrow, ˆf(x0) is an average of a small number of yi\\nclose to x0, and its variance will be relatively large—close to that of\\nan individual yi. The bias will tend to be small, again because each\\nof the E(yi) =f(xi) should be close to f(x0).\\n•If the window is wide, the variance of ˆf(x0) will be small relative to\\nthe variance of any yi, because of the eﬀects of averaging. The bias\\nwill be higher, because we are now using observations xifurther from\\nx0, and there is no guarantee that f(xi) will be close to f(x0).\\nSimilar arguments apply to local regression estimates, say local linear: a s\\nthe width goes to zero, the estimates approach a piecewise-linear function\\nthat interpolates the training data1; as the width gets inﬁnitely large, the\\nﬁt approaches the global linear least-squares ﬁt to the data.\\nThe discussion in Chapter 5 on selecting the regularization parameter for\\nsmoothing splines applies here, and will not be repeated. Local regression\\nsmoothers are linear estimators; the smoother matrix in ˆf=Sλyis built up\\nfrom the equivalent kernels (6.8), and has ijth entry {Sλ}ij=li(xj). Leave-\\none-out cross-validation is particularly simple (Exercise 6.7), as is genera l-\\nized cross-validation, Cp(Exercise 6.10), and k-fold cross-validation. The\\neﬀective degrees of freedom is again deﬁned as trace( Sλ), and can be used\\nto calibrate the amount of smoothing. Figure 6.7 compares the equivalent\\nkernels for a smoothing spline and local linear regression. The local regres-\\nsion smoother has a span of 40%, which results in df = trace( Sλ) = 5.86.\\nThe smoothing spline was calibrated to have the same df, and their equiv-\\nalent kernels are qualitatively quite similar.\\n1With uniformly spaced xi; with irregularly spaced xi, the behavior can deteriorate.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7243ac19-19c3-4598-b831-9603e7ed1b3e', embedding=None, metadata={'page_label': '219', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='200 6. Kernel Smoothing Methods\\n6.3 Local Regression in I Rp\\nKernel smoothing and local regression generalize very naturally to two or\\nmore dimensions. The Nadaraya–Watson kernel smoother ﬁts a constant\\nlocally with weights supplied by a p-dimensional kernel. Local linear re-\\ngression will ﬁt a hyperplane locally in X, by weighted least squares, with\\nweights supplied by a p-dimensional kernel. It is simple to implement and\\nis generally preferred to the local constant ﬁt for its superior performance\\non the boundaries.\\nLetb(X) be a vector of polynomial terms in Xof maximum degree d.\\nFor example, with d= 1 and p= 2 we get b(X) = (1 ,X1,X2); with d= 2\\nwe get b(X) = (1 ,X1,X2,X2\\n1,X2\\n2,X1X2); and trivially with d= 0 we get\\nb(X) = 1. At each x0∈IRpsolve\\nmin\\nβ(x0)N∑\\ni=1Kλ(x0,xi)(yi−b(xi)Tβ(x0))2(6.12)\\nto produce the ﬁt ˆf(x0) =b(x0)Tˆβ(x0). Typically the kernel will be a radial\\nfunction, such as the radial Epanechnikov or tri-cube kernel\\nKλ(x0,x) =D(||x−x0||\\nλ)\\n, (6.13)\\nwhere ||≤||is the Euclidean norm. Since the Euclidean norm depends on the\\nunits in each coordinate, it makes most sense to standardize each predictor,\\nfor example, to unit standard deviation, prior to smoothing.\\nWhile boundary eﬀects are a problem in one-dimensional smoothing,\\nthey are a much bigger problem in two or higher dimensions, since the\\nfraction of points on the boundary is larger. In fact, one of the manifesta -\\ntions of the curse of dimensionality is that the fraction of points close to the\\nboundary increases to one as the dimension grows. Directly modifying the\\nkernel to accommodate two-dimensional boundaries becomes very messy,\\nespecially for irregular boundaries. Local polynomial regression seamless ly\\nperforms boundary correction to the desired order in any dimensions. Fig-\\nure 6.8 illustrates local linear regression on some measurements from an\\nastronomical study with an unusual predictor design (star-shaped). Here\\nthe boundary is extremely irregular, and the ﬁtted surface must also inter-\\npolate over regions of increasing data sparsity as we approach the boundary.\\nLocal regression becomes less useful in dimensions much higher than two\\nor three. We have discussed in some detail the problems of dimensional-\\nity, for example, in Chapter 2. It is impossible to simultaneously main-\\ntain localness ( ⇒low bias) and a sizable sample in the neighborhood ( ⇒\\nlow variance) as the dimension increases, without the total sample size in-\\ncreasing exponentially in p. Visualization of ˆf(X) also becomes diﬃcult in\\nhigher dimensions, and this is often one of the primary goals of smoothing.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ca69fbc-f4ca-4643-afdd-92f31e94e83a', embedding=None, metadata={'page_label': '220', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4 Structured Local Regression Models in I Rp201\\nEast-WestSouth-NorthVelocity\\nEast-WestSouth-NorthVelocity\\nFIGURE 6.8. The left panel shows three-dimensional data, where the response\\nis the velocity measurements on a galaxy, and the two predictors record positions\\non the celestial sphere. The unusual “star”-shaped design ind icates the way the\\nmeasurements were made, and results in an extremely irregular b oundary. The\\nright panel shows the results of local linear regression smoot hing in I R2, using a\\nnearest-neighbor window with 15%of the data.\\nAlthough the scatter-cloud and wire-frame pictures in Figure 6.8 look at-\\ntractive, it is quite diﬃcult to interpret the results except at a gross level.\\nFrom a data analysis perspective, conditional plots are far more useful.\\nFigure 6.9 shows an analysis of some environmental data with three pre-\\ndictors. The trellis display here shows ozone as a function of radiation,\\nconditioned on the other two variables, temperature and wind speed. How-\\never, conditioning on the value of a variable really implies local to that\\nvalue (as in local regression). Above each of the panels in Figure 6.9 is an\\nindication of the range of values present in that panel for each of the condi-\\ntioning values. In the panel itself the data subsets are displayed (response\\nversus remaining variable), and a one-dimensional local linear regression is\\nﬁt to the data. Although this is not quite the same as looking at slices of\\na ﬁtted three-dimensional surface, it is probably more useful in terms of\\nunderstanding the joint behavior of the data.\\n6.4 Structured Local Regression Models in I Rp\\nWhen the dimension to sample-size ratio is unfavorable, local regression\\ndoes not help us much, unless we are willing to make some structural as-\\nsumptions about the model. Much of this book is about structured regres-\\nsion and classiﬁcation models. Here we focus on some approaches directly\\nrelated to kernel methods.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7a0c939-a05e-48e1-8a8e-fd5cf1ae37d4', embedding=None, metadata={'page_label': '221', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='202 6. Kernel Smoothing Methods\\n12345TempWind\\n0 50 150 250TempWind\\nTempWind\\n0 50 150 250TempWindTempWind\\nTempWind\\nTempWind\\n12345TempWind12345TempWind\\nTempWind\\nTempWind\\nTempWindTempWind\\nTempWind0 50 150 250\\nTempWind\\n12345TempWind0 50 150 250\\nSolar Radiation (langleys)Cube Root Ozone (cube root ppb)\\nFIGURE 6.9. Three-dimensional smoothing example. The response is (cube-roo t\\nof) ozone concentration, and the three predictors are temperatur e, wind speed and\\nradiation. The trellis display shows ozone as a function of radiation, conditioned\\non intervals of temperature and wind speed (indicated by darker g reen or orange\\nshaded bars). Each panel contains about 40%of the range of each of the condi-\\ntioned variables. The curve in each panel is a univariate local l inear regression,\\nﬁt to the data in the panel.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='609a3bc7-d1a9-4ee6-aad3-aae2859c683a', embedding=None, metadata={'page_label': '222', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4 Structured Local Regression Models in I Rp203\\n6.4.1 Structured Kernels\\nOne line of approach is to modify the kernel. The default spherical ker-\\nnel (6.13) gives equal weight to each coordinate, and so a natural default\\nstrategy is to standardize each variable to unit standard deviation. A more\\ngeneral approach is to use a positive semideﬁnite matrix Ato weigh the\\ndiﬀerent coordinates:\\nKλ,A(x0,x) =D((x−x0)TA(x−x0)\\nλ)\\n. (6.14)\\nEntire coordinates or directions can be downgraded or omitted by imposing\\nappropriate restrictions on A. For example, if Ais diagonal, then we can\\nincrease or decrease the inﬂuence of individual predictors Xjby increasing\\nor decreasing Ajj. Often the predictors are many and highly correlated,\\nsuch as those arising from digitized analog signals or images. The covari ance\\nfunction of the predictors can be used to tailor a metric Athat focuses less,\\nsay, on high-frequency contrasts (Exercise 6.4). Proposals have been made\\nfor learning the parameters for multidimensional kernels. For example, the\\nprojection-pursuit regression model discussed in Chapter 11 is of this ﬂavor,\\nwhere low-rank versions of Aimply ridge functions for ˆf(X). More general\\nmodels for Aare cumbersome, and we favor instead the structured forms\\nfor the regression function discussed next.\\n6.4.2 Structured Regression Functions\\nWe are trying to ﬁt a regression function E(Y|X) =f(X1,X2,... ,X p) in\\nIRp, in which every level of interaction is potentially present. It is natural\\nto consider analysis-of-variance (ANOVA) decompositions of the form\\nf(X1,X2,... ,X p) =α+∑\\njgj(Xj) +∑\\nk<ℓgkℓ(Xk,Xℓ) +≤≤≤ (6.15)\\nand then introduce structure by eliminating some of the higher-order terms.\\nAdditive models assume only main eﬀect terms: f(X) =α+∑p\\nj=1gj(Xj);\\nsecond-order models will have terms with interactions of order at most\\ntwo, and so on. In Chapter 9, we describe iterative backﬁtting algorithms\\nfor ﬁtting such low-order interaction models. In the additive model, for\\nexample, if all but the kth term is assumed known, then we can estimate gk\\nby local regression of Y−∑\\nj̸=kgj(Xj) onXk. This is done for each function\\nin turn, repeatedly, until convergence. The important detail is that at any\\nstage, one-dimensional local regression is all that is needed. The same ideas\\ncan be used to ﬁt low-dimensional ANOVA decompositions.\\nAn important special case of these structured models are the class of\\nvarying coeﬃcient models . Suppose, for example, that we divide the ppre-\\ndictors in Xinto a set ( X1,X2,... ,X q) with q < p, and the remainder of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9db8b2d5-4dc4-4068-baad-edaf99a6a7e2', embedding=None, metadata={'page_label': '223', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='204 6. Kernel Smoothing Methods\\n1012141618202224DepthFemale\\n20 30 40 50 60DepthFemale\\nDepthFemale\\n20 30 40 50 60DepthFemale\\nDepthFemale\\n20 30 40 50 60DepthFemaleDepthMale\\nDepthMale20 30 40 50 60\\nDepthMale\\nDepthMale20 30 40 50 60\\nDepthMale\\n1012141618202224DepthMale20 30 40 50 60\\nAgeDiameterAortic Diameter vs Age\\nFIGURE 6.10. In each panel the aorta diameter is modeled as a linear func-\\ntion ofage. The coeﬃcients of this model vary with gender anddepth down\\ntheaorta (left is near the top, right is low down). There is a clear trend in the\\ncoeﬃcients of the linear model.\\nthe variables we collect in the vector Z. We then assume the conditionally\\nlinear model\\nf(X) =α(Z) +β1(Z)X1+≤≤≤+βq(Z)Xq. (6.16)\\nFor given Z, this is a linear model, but each of the coeﬃcients can vary\\nwithZ. It is natural to ﬁt such a model by locally weighted least squares:\\nmin\\nα(z0),β(z0)N∑\\ni=1Kλ(z0,zi)(yi−α(z0)−x1iβ1(z0)− ≤≤≤ − xqiβq(z0))2.\\n(6.17)\\nFigure 6.10 illustrates the idea on measurements of the human aorta.\\nA longstanding claim has been that the aorta thickens with age. Here we\\nmodel the diameter of the aorta as a linear function of age, but allow the\\ncoeﬃcients to vary with gender anddepth down the aorta. We used a local\\nregression model separately for males and females. While the aorta clearly\\ndoes thicken with age at the higher regions of the aorta, the relationship\\nfades with distance down the aorta. Figure 6.11 shows the intercept and\\nslope as a function of depth.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbed6451-0c61-4b15-a788-37551dbc9d6f', embedding=None, metadata={'page_label': '224', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.5 Local Likelihood and Other Models 205\\nMaleAge Intercept\\nDistance Down AortaAge Slope\\n0.0 0.2 0.4 0.6 0.8 1.0Female\\n14 16 18 20\\nDistance Down Aorta\\n0.0 0.4 0.8 1.2\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.11. The intercept and slope of ageas a function of distance down\\nthe aorta, separately for males and females. The yellow bands i ndicate one stan-\\ndard error.\\n6.5 Local Likelihood and Other Models\\nThe concept of local regression and varying coeﬃcient models is extremely\\nbroad: any parametric model can be made local if the ﬁtting method ac-\\ncommodates observation weights. Here are some examples:\\n•Associated with each observation yiis a parameter θi=θ(xi) =xT\\niβ\\nlinear in the covariate(s) xi, and inference for βis based on the log-\\nlikelihood l(β) =∑N\\ni=1l(yi,xT\\niβ). We can model θ(X) more ﬂexibly\\nby using the likelihood local to x0for inference of θ(x0) =xT\\n0β(x0):\\nl(β(x0)) =N∑\\ni=1Kλ(x0,xi)l(yi,xT\\niβ(x0)).\\nMany likelihood models, in particular the family of generalized linear\\nmodels including logistic and log-linear models, involve the covariates\\nin a linear fashion. Local likelihood allows a relaxation from a global ly\\nlinear model to one that is locally linear.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5fba003b-2d7d-4799-b7b0-5730a9f193de', embedding=None, metadata={'page_label': '225', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='206 6. Kernel Smoothing Methods\\n•As above, except diﬀerent variables are associated with θfrom those\\nused for deﬁning the local likelihood:\\nl(θ(z0)) =N∑\\ni=1Kλ(z0,zi)l(yi,η(xi,θ(z0))).\\nFor example, η(x,θ) =xTθcould be a linear model in x. This will ﬁt\\na varying coeﬃcient model θ(z) by maximizing the local likelihood.\\n•Autoregressive time series models of order khave the form yt=\\nβ0+β1yt−1+β2yt−2+≤≤≤+βkyt−k+εt. Denoting the lag set by\\nzt= (yt−1,yt−2,... ,y t−k), the model looks like a standard linear\\nmodel yt=zT\\ntβ+εt, and is typically ﬁt by least squares. Fitting\\nby local least squares with a kernel K(z0,zt) allows the model to\\nvary according to the short-term history of the series. This is to be\\ndistinguished from the more traditional dynamic linear models that\\nvary by windowing time.\\nAs an illustration of local likelihood, we consider the local version of the\\nmulticlass linear logistic regression model (4.36) of Chapter 4. The data\\nconsist of features xiand an associated categorical response gi∈ {1,2,... ,J },\\nand the linear model has the form\\nPr(G=j|X=x) =eβj0+βT\\njx\\n1 +∑J−1\\nk=1eβk0+βT\\nkx. (6.18)\\nThe local log-likelihood for this Jclass model can be written\\nN∑\\ni=1Kλ(x0,xi){\\nβgi0(x0) +βgi(x0)T(xi−x0)\\n−log[\\n1 +J−1∑\\nk=1exp(\\nβk0(x0) +βk(x0)T(xi−x0))]}\\n.\\n(6.19)\\nNotice that\\n•we have used gias a subscript in the ﬁrst line to pick out the appro-\\npriate numerator;\\n•βJ0= 0 and βJ= 0 by the deﬁnition of the model;\\n•we have centered the local regressions at x0, so that the ﬁtted poste-\\nrior probabilities at x0are simply\\nˆPr(G=j|X=x0) =eˆβj0(x0)\\n1 +∑J−1\\nk=1eˆβk0(x0). (6.20)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6c571b4-f7f0-4d03-82dd-06dc2d153f5b', embedding=None, metadata={'page_label': '226', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.5 Local Likelihood and Other Models 207\\nSystolic Blood PressurePrevalence CHD\\n100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\\nObesityPrevalence CHD\\n15 25 35 450.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.12. Each plot shows the binary response CHD (coronary heart dis-\\nease) as a function of a risk factor for the South African heart d isease data.\\nFor each plot we have computed the ﬁtted prevalence of CHD using a local linear\\nlogistic regression model. The unexpected increase in the prev alence of CHD at\\nthe lower ends of the ranges is because these are retrospective data, and some of\\nthe subjects had already undergone treatment to reduce their bl ood pressure and\\nweight. The shaded region in the plot indicates an estimated p ointwise standard\\nerror band.\\nThis model can be used for ﬂexible multiclass classiﬁcation in moderately\\nlow dimensions, although successes have been reported with the high-\\ndimensional ZIP-code classiﬁcation problem. Generalized additive models\\n(Chapter 9) using kernel smoothing methods are closely related, and avoid\\ndimensionality problems by assuming an additive structure for the regres-\\nsion function.\\nAs a simple illustration we ﬁt a two-class local linear logistic model to\\nthe heart disease data of Chapter 4. Figure 6.12 shows the univariate local\\nlogistic models ﬁt to two of the risk factors (separately). This is a useful\\nscreening device for detecting nonlinearities, when the data themselves have\\nlittle visual information to oﬀer. In this case an unexpected anomaly is\\nuncovered in the data, which may have gone unnoticed with traditional\\nmethods.\\nSinceCHDis a binary indicator, we could estimate the conditional preva-\\nlence Pr( G=j|x0) by simply smoothing this binary response directly with-\\nout resorting to a likelihood formulation. This amounts to ﬁtting a locall y\\nconstant logistic regression model (Exercise 6.5). In order to enjoy the bia s-\\ncorrection of local-linear smoothing, it is more natural to operate on the\\nunrestricted logit scale.\\nTypically with logistic regression, we compute parameter estimates as\\nwell as their standard errors. This can be done locally as well, and so', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b90e0203-c243-4068-9ebb-e159aeb5c3e9', embedding=None, metadata={'page_label': '227', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='208 6. Kernel Smoothing Methods\\nSystolic Blood Pressure (for CHD group)Density Estimate\\n100 120 140 160 180 200 2200.0 0.005 0.010 0.015 0.020\\nFIGURE 6.13. A kernel density estimate for systolic blood pressure (for the\\nCHD group). The density estimate at each point is the average co ntribution from\\neach of the kernels at that point. We have scaled the kernels down by a factor of\\n10 to make the graph readable.\\nwe can produce, as shown in the plot, estimated pointwise standard-error\\nbands about our ﬁtted prevalence.\\n6.6 Kernel Density Estimation and Classiﬁcation\\nKernel density estimation is an unsupervised learning procedure, which\\nhistorically precedes kernel regression. It also leads naturally to a simple\\nfamily of procedures for nonparametric classiﬁcation.\\n6.6.1 Kernel Density Estimation\\nSuppose we have a random sample x1,... ,x Ndrawn from a probability\\ndensity fX(x), and we wish to estimate fXat a point x0. For simplicity we\\nassume for now that X∈IR. Arguing as before, a natural local estimate\\nhas the form\\nˆfX(x0) =#xi∈ N(x0)\\nNλ, (6.21)\\nwhere N(x0) is a small metric neighborhood around x0of width λ. This\\nestimate is bumpy, and the smooth Parzen estimate is preferred\\nˆfX(x0) =1\\nNλN∑\\ni=1Kλ(x0,xi), (6.22)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f244dc39-a86e-4e4f-b103-6f825dc1fccd', embedding=None, metadata={'page_label': '228', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.6 Kernel Density Estimation and Classiﬁcation 209\\nSystolic Blood PressureDensity Estimates\\n100 140 180 2200.0 0.010 0.020CHD\\nno CHD\\nSystolic Blood PressurePosterior Estimate\\n100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.14. The left panel shows the two separate density estimates for\\nsystolic blood pressure in the CHD versus no-CHD groups, using a Gaussian\\nkernel density estimate in each. The right panel shows the estim ated posterior\\nprobabilities for CHD, using (6.25).\\nbecause it counts observations close to x0with weights that decrease with\\ndistance from x0. In this case a popular choice for Kλis the Gaussian kernel\\nKλ(x0,x) =φ(|x−x0|/λ). Figure 6.13 shows a Gaussian kernel density ﬁt\\nto the sample values for systolic blood pressure for theCHDgroup. Letting\\nφλdenote the Gaussian density with mean zero and standard-deviation λ,\\nthen (6.22) has the form\\nˆfX(x) =1\\nNN∑\\ni=1φλ(x−xi)\\n= (ˆF ⋆ φ λ)(x), (6.23)\\nthe convolution of the sample empirical distribution ˆFwithφλ. The dis-\\ntribution ˆF(x) puts mass 1 /Nat each of the observed xi, and is jumpy; in\\nˆfX(x) we have smoothed ˆFby adding independent Gaussian noise to each\\nobservation xi.\\nThe Parzen density estimate is the equivalent of the local average, and\\nimprovements have been proposed along the lines of local regression [on the\\nlog scale for densities; see Loader (1999)]. We will not pursue these here.\\nIn IRpthe natural generalization of the Gaussian density estimate amounts\\nto using the Gaussian product kernel in (6.23),\\nˆfX(x0) =1\\nN(2λ2π)p\\n2N∑\\ni=1e−1\\n2(||xi−x0||/λ)2. (6.24)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d382f4f8-8aa0-4229-8355-002b7472dcc4', embedding=None, metadata={'page_label': '229', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='210 6. Kernel Smoothing Methods\\n0.00.51.0\\nFIGURE 6.15. The population class densities may have interesting structure\\n(left) that disappears when the posterior probabilities ar e formed (right).\\n6.6.2 Kernel Density Classiﬁcation\\nOne can use nonparametric density estimates for classiﬁcation in a straight-\\nforward fashion using Bayes’ theorem. Suppose for a Jclass problem we ﬁt\\nnonparametric density estimates ˆfj(X), j= 1,... ,J separately in each of\\nthe classes, and we also have estimates of the class priors ˆ πj(usually the\\nsample proportions). Then\\nˆPr(G=j|X=x0) =ˆπjˆfj(x0)∑J\\nk=1ˆπkˆfk(x0). (6.25)\\nFigure 6.14 uses this method to estimate the prevalence of CHD for the\\nheart risk factor study, and should be compared with the left panel of Fig-\\nure 6.12. The main diﬀerence occurs in the region of high SBP in the right\\npanel of Figure 6.14. In this region the data are sparse for both classes, a nd\\nsince the Gaussian kernel density estimates use metric kernels, the density\\nestimates are low and of poor quality (high variance) in these regions. The\\nlocal logistic regression method (6.20) uses the tri-cube kernel with k-NN\\nbandwidth; this eﬀectively widens the kernel in this region, and makes use\\nof the local linear assumption to smooth out the estimate (on the logit\\nscale).\\nIf classiﬁcation is the ultimate goal, then learning the separate class den-\\nsities well may be unnecessary, and can in fact be misleading. Figure 6.15\\nshows an example where the densities are both multimodal, but the pos-\\nterior ratio is quite smooth. In learning the separate densities from data,\\none might decide to settle for a rougher, high-variance ﬁt to capture these\\nfeatures, which are irrelevant for the purposes of estimating the posterior\\nprobabilities. In fact, if classiﬁcation is the ultimate goal, then we need onl y\\nto estimate the posterior well near the decision boundary (for two classes,\\nthis is the set {x|Pr(G= 1|X=x) =1\\n2}).\\n6.6.3 The Naive Bayes Classiﬁer\\nThis is a technique that has remained popular over the years, despite its\\nname (also known as “Idiot’s Bayes”!) It is especially appropriate when', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f8418a8-f031-41bc-b2c7-052ea8daa423', embedding=None, metadata={'page_label': '230', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.6 Kernel Density Estimation and Classiﬁcation 211\\nthe dimension pof the feature space is high, making density estimation\\nunattractive. The naive Bayes model assumes that given a class G=j, the\\nfeatures Xkare independent:\\nfj(X) =p∏\\nk=1fjk(Xk). (6.26)\\nWhile this assumption is generally not true, it does simplify the estimation\\ndramatically:\\n•The individual class-conditional marginal densities fjkcan each be\\nestimated separately using one-dimensional kernel density estimates.\\nThis is in fact a generalization of the original naive Bayes procedures,\\nwhich used univariate Gaussians to represent these marginals.\\n•If a component XjofXis discrete, then an appropriate histogram\\nestimate can be used. This provides a seamless way of mixing variable\\ntypes in a feature vector.\\nDespite these rather optimistic assumptions, naive Bayes classiﬁers often\\noutperform far more sophisticated alternatives. The reasons are related to\\nFigure 6.15: although the individual class density estimates may be biased,\\nthis bias might not hurt the posterior probabilities as much, especially\\nnear the decision regions. In fact, the problem may be able to withstand\\nconsiderable bias for the savings in variance such a “naive” assumption\\nearns.\\nStarting from (6.26) we can derive the logit-transform (using class Jas\\nthe base):\\nlogPr(G=ℓ|X)\\nPr(G=J|X)= logπℓfℓ(X)\\nπJfJ(X)\\n= logπℓ∏p\\nk=1fℓk(Xk)\\nπJ∏p\\nk=1fJk(Xk)\\n= logπℓ\\nπJ+p∑\\nk=1logfℓk(Xk)\\nfJk(Xk)\\n=αℓ+p∑\\nk=1gℓk(Xk).(6.27)\\nThis has the form of a generalized additive model , which is described in more\\ndetail in Chapter 9. The models are ﬁt in quite diﬀerent ways though; their\\ndiﬀerences are explored in Exercise 6.9. The relationship between naive\\nBayes and generalized additive models is analogous to that between linear\\ndiscriminant analysis and logistic regression (Section 4.4.5).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='87fadb40-fbe2-4c9d-95d3-0453d84164a8', embedding=None, metadata={'page_label': '231', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='212 6. Kernel Smoothing Methods\\n6.7 Radial Basis Functions and Kernels\\nIn Chapter 5, functions are represented as expansions in basis functions:\\nf(x) =∑M\\nj=1βjhj(x). The art of ﬂexible modeling using basis expansions\\nconsists of picking an appropriate family of basis functions, and then con-\\ntrolling the complexity of the representation by selection, regularization, or\\nboth. Some of the families of basis functions have elements that are deﬁned\\nlocally; for example, B-splines are deﬁned locally in IR. If more ﬂexibility\\nis desired in a particular region, then that region needs to be represented\\nby more basis functions (which in the case of B-splines translates to more\\nknots). Tensor products of IR-local basis functions deliver basis functions\\nlocal in IRp. Not all basis functions are local—for example, the truncated\\npower bases for splines, or the sigmoidal basis functions σ(α0+αx) used\\nin neural-networks (see Chapter 11). The composed function f(x) can nev-\\nertheless show local behavior, because of the particular signs and values\\nof the coeﬃcients causing cancellations of global eﬀects. For example, the\\ntruncated power basis has an equivalent B-spline basis for the same space\\nof functions; the cancellation is exact in this case.\\nKernel methods achieve ﬂexibility by ﬁtting simple models in a region\\nlocal to the target point x0. Localization is achieved via a weighting kernel\\nKλ, and individual observations receive weights Kλ(x0,xi).\\nRadial basis functions combine these ideas, by treating the kernel func-\\ntionsKλ(ξ,x) as basis functions. This leads to the model\\nf(x) =M∑\\nj=1Kλj(ξj,x)βj\\n=M∑\\nj=1D(||x−ξj||\\nλj)\\nβj, (6.28)\\nwhere each basis element is indexed by a location or prototype parameter ξj\\nand a scale parameter λj. A popular choice for Dis the standard Gaussian\\ndensity function. There are several approaches to learning the parameters\\n{λj,ξj,βj}, j= 1,... ,M . For simplicity we will focus on least squares\\nmethods for regression, and use the Gaussian kernel.\\n•Optimize the sum-of-squares with respect to all the parameters:\\nmin\\n{λj,ξj,βj}M\\n1N∑\\ni=1\\uf8eb\\n\\uf8edyi−β0−M∑\\nj=1βjexp{\\n−(xi−ξj)T(xi−ξj)\\nλ2\\nj}\\uf8f6\\n\\uf8f82\\n.\\n(6.29)\\nThis model is commonly referred to as an RBF network, an alterna-\\ntive to the sigmoidal neural network discussed in Chapter 11; the ξj\\nandλjplaying the role of the weights. This criterion is nonconvex', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='39b625b9-a0fd-4406-9bcb-f4d7df35e353', embedding=None, metadata={'page_label': '232', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.7 Radial Basis Functions and Kernels 2130 2 4 6 8 0.0 0.4 0.8 1.2\\nFIGURE 6.16. Gaussian radial basis functions in I Rwith ﬁxed width can leave\\nholes (top panel). Renormalized Gaussian radial basis functio ns avoid this prob-\\nlem, and produce basis functions similar in some respects to B-splines.\\nwith multiple local minima, and the algorithms for optimization are\\nsimilar to those used for neural networks.\\n•Estimate the {λj,ξj}separately from the βj. Given the former, the\\nestimation of the latter is a simple least squares problem. Often the\\nkernel parameters λjandξjare chosen in an unsupervised way using\\ntheXdistribution alone. One of the methods is to ﬁt a Gaussian\\nmixture density model to the training xi, which provides both the\\ncenters ξjand the scales λj. Other even more adhoc approaches use\\nclustering methods to locate the prototypes ξj, and treat λj=λ\\nas a hyper-parameter. The obvious drawback of these approaches is\\nthat the conditional distribution Pr( Y|X) and in particular E(Y|X)\\nis having no say in where the action is concentrated. On the positive\\nside, they are much simpler to implement.\\nWhile it would seem attractive to reduce the parameter set and assume\\na constant value for λj=λ, this can have an undesirable side eﬀect of\\ncreating holes—regions of IRpwhere none of the kernels has appreciable\\nsupport, as illustrated in Figure 6.16 (upper panel). Renormalized radial\\nbasis functions,\\nhj(x) =D(||x−ξj||/λ)∑M\\nk=1D(||x−ξk||/λ), (6.30)\\navoid this problem (lower panel).\\nThe Nadaraya–Watson kernel regression estimator (6.2) in IRpcan be\\nviewed as an expansion in renormalized radial basis functions,\\nˆf(x0) =∑N\\ni=1yiKλ(x0,xi)PN\\ni=1Kλ(x0,xi)\\n=∑N\\ni=1yihi(x0) (6.31)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c602e3e-dd01-43e5-ba60-4a6dbe07b37a', embedding=None, metadata={'page_label': '233', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='214 6. Kernel Smoothing Methods\\nwith a basis function hilocated at every observation and coeﬃcients yi;\\nthat is, ξi=xi,ˆβi=yi, i= 1,... ,N .\\nNote the similarity between the expansion (6.31) and the solution (5.50)\\non page 169 to the regularization problem induced by the kernel K. Radial\\nbasis functions form the bridge between the modern “kernel methods” and\\nlocal ﬁtting technology.\\n6.8 Mixture Models for Density Estimation and\\nClassiﬁcation\\nThe mixture model is a useful tool for density estimation, and can be viewed\\nas a kind of kernel method. The Gaussian mixture model has the form\\nf(x) =M∑\\nm=1αmφ(x;θm,Σm) (6.32)\\nwith mixing proportions αm,∑\\nmαm= 1, and each Gaussian density has\\na mean θmand covariance matrix Σm. In general, mixture models can use\\nany component densities in place of the Gaussian in (6.32): the Gaussian\\nmixture model is by far the most popular.\\nThe parameters are usually ﬁt by maximum likelihood, using the EM\\nalgorithm as described in Chapter 8. Some special cases arise:\\n•If the covariance matrices are constrained to be scalar: Σm=σmI,\\nthen (6.32) has the form of a radial basis expansion.\\n•If in addition σm=σ >0 is ﬁxed, and M↑N, then the max-\\nimum likelihood estimate for (6.32) approaches the kernel density\\nestimate (6.22) where ˆ αm= 1/Nand ˆθm=xm.\\nUsing Bayes’ theorem, separate mixture densities in each class lead to ﬂex-\\nible models for Pr( G|X); this is taken up in some detail in Chapter 12.\\nFigure 6.17 shows an application of mixtures to the heart disease risk-\\nfactor study. In the top row are histograms of Agefor theno CHD andCHD\\ngroups separately, and then combined on the right. Using the combined\\ndata, we ﬁt a two-component mixture of the form (6.32) with the (scalars)\\nΣ1andΣ2not constrained to be equal. Fitting was done via the EM\\nalgorithm (Chapter 8): note that the procedure does not use knowledge of\\ntheCHDlabels. The resulting estimates were\\nˆθ1= 36.4, ˆΣ1= 157 .7, ˆα1= 0.7,\\nˆθ2= 58.0, ˆΣ2= 15.6, ˆα2= 0.3.\\nThe component densities φ(ˆθ1,ˆΣ1) and φ(ˆθ2,ˆΣ2) are shown in the lower-\\nleft and middle panels. The lower-right panel shows these component den-\\nsities (orange and blue) along with the estimated mixture density (green).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a509c614-66ec-4ef1-bb46-4f29f36a1fe1', embedding=None, metadata={'page_label': '234', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.8 Mixture Models for Density Estimation and Classiﬁcation 215\\nNo CHD\\nAgeCount\\n20 30 40 50 600 5 10 15 20CHD\\nAgeCount\\n20 30 40 50 600 5 10 15Combined\\nAgeCount\\n20 30 40 50 600 5 10 15 20 25 30\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\nFIGURE 6.17. Application of mixtures to the heart disease risk-factor stu dy.\\n(Top row:) Histograms of Agefor theno CHD andCHDgroups separately, and\\ncombined. (Bottom row:) estimated component densities from a Ga ussian mix-\\nture model, (bottom left, bottom middle); (bottom right:) E stimated component\\ndensities (blue and orange) along with the estimated mixture densi ty (green). The\\norange density has a very large standard deviation, and approximat es a uniform\\ndensity.\\nThe mixture model also provides an estimate of the probability that\\nobservation ibelongs to component m,\\nˆrim=ˆαmφ(xi; ˆθm,ˆΣm)∑M\\nk=1ˆαkφ(xi; ˆθk,ˆΣk), (6.33)\\nwhere xiisAgein our example. Suppose we threshold each value ˆ ri2and\\nhence deﬁne ˆδi=I(ˆri2>0.5). Then we can compare the classiﬁcation of\\neach observation by CHDand the mixture model:\\nMixture model\\nˆδ= 0 ˆδ= 1\\nCHD No 232 70\\nYes 76 84\\nAlthough the mixture model did not use the CHDlabels, it has done a fair\\njob in discovering the two CHDsubpopulations. Linear logistic regression,\\nusing the CHDas a response, achieves the same error rate (32%) when ﬁt to\\nthese data using maximum-likelihood (Section 4.4).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0178e268-9429-4779-9cd8-027b07c6fdec', embedding=None, metadata={'page_label': '235', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='216 6. Kernel Smoothing Methods\\n6.9 Computational Considerations\\nKernel and local regression and density estimation are memory-based meth-\\nods: the model is the entire training data set, and the ﬁtting is done at\\nevaluation or prediction time. For many real-time applications, this can\\nmake this class of methods infeasible.\\nThe computational cost to ﬁt at a single observation x0isO(N) ﬂops,\\nexcept in oversimpliﬁed cases (such as square kernels). By comparison,\\nan expansion in Mbasis functions costs O(M) for one evaluation, and\\ntypically M∼O(logN). Basis function methods have an initial cost of at\\nleastO(NM2+M3).\\nThe smoothing parameter(s) λfor kernel methods are typically deter-\\nmined oﬀ-line, for example using cross-validation, at a cost of O(N2) ﬂops.\\nPopular implementations of local regression, such as the loess function in\\nS-PLUS and Rand the locfit procedure (Loader, 1999), use triangulation\\nschemes to reduce the computations. They compute the ﬁt exactly at M\\ncarefully chosen locations ( O(NM)), and then use blending techniques to\\ninterpolate the ﬁt elsewhere ( O(M) per evaluation).\\nBibliographic Notes\\nThere is a vast literature on kernel methods which we will not attempt to\\nsummarize. Rather we will point to a few good references that themselves\\nhave extensive bibliographies. Loader (1999) gives excellent coverage of lo-\\ncal regression and likelihood, and also describes state-of-the-art software\\nfor ﬁtting these models. Fan and Gijbels (1996) cover these models from\\na more theoretical aspect. Hastie and Tibshirani (1990) discuss local re-\\ngression in the context of additive modeling. Silverman (1986) gives a goo d\\noverview of density estimation, as does Scott (1992).\\nExercises\\nEx. 6.1 Show that the Nadaraya–Watson kernel smooth with ﬁxed metric\\nbandwidth λand a Gaussian kernel is diﬀerentiable. What can be said for\\nthe Epanechnikov kernel? What can be said for the Epanechnikov kernel\\nwith adaptive nearest-neighbor bandwidth λ(x0)?\\nEx. 6.2 Show that∑N\\ni=1(xi−x0)li(x0) = 0 for local linear regression. Deﬁne\\nbj(x0) =∑N\\ni=1(xi−x0)jli(x0). Show that b0(x0) = 1 for local polynomial\\nregression of any degree (including local constants). Show that bj(x0) = 0\\nfor all j∈ {1,2,... ,k }for local polynomial regression of degree k. What\\nare the implications of this on the bias?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='015fffe8-051f-4947-899b-df25c450940c', embedding=None, metadata={'page_label': '236', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 217\\nEx. 6.3 Show that ||l(x)||(Section 6.1.2) increases with the degree of the\\nlocal polynomial.\\nEx. 6.4 Suppose that the ppredictors Xarise from sampling relatively\\nsmooth analog curves at puniformly spaced abscissa values. Denote by\\nCov(X|Y) =Σthe conditional covariance matrix of the predictors, and\\nassume this does not change much with Y. Discuss the nature of Maha-\\nlanobis choice A=Σ−1for the metric in (6.14). How does this compare\\nwithA=I? How might you construct a kernel Athat (a) downweights\\nhigh-frequency components in the distance metric; (b) ignores them\\ncompletely?\\nEx. 6.5 Show that ﬁtting a locally constant multinomial logit model of\\nthe form (6.19) amounts to smoothing the binary response indicators for\\neach class separately using a Nadaraya–Watson kernel smoother with kernel\\nweights Kλ(x0,xi).\\nEx. 6.6 Suppose that all you have is software for ﬁtting local regression,\\nbut you can specify exactly which monomials are included in the ﬁt. How\\ncould you use this software to ﬁt a varying-coeﬃcient model in some of the\\nvariables?\\nEx. 6.7 Derive an expression for the leave-one-out cross-validated residual\\nsum-of-squares for local polynomial regression.\\nEx. 6.8 Suppose that for continuous response Yand predictor X, we model\\nthe joint density of X,Yusing a multivariate Gaussian kernel estimator.\\nNote that the kernel in this case would be the product kernel φλ(X)φλ(Y).\\nShow that the conditional mean E(Y|X) derived from this estimate is a\\nNadaraya–Watson estimator. Extend this result to classiﬁcation by pro-\\nviding a suitable kernel for the estimation of the joint distribution of a\\ncontinuous Xand discrete Y.\\nEx. 6.9 Explore the diﬀerences between the naive Bayes model (6.27) and\\na generalized additive logistic regression model, in terms of (a) model as-\\nsumptions and (b) estimation. If all the variables Xkare discrete, what can\\nyou say about the corresponding GAM?\\nEx. 6.10 Suppose we have Nsamples generated from the model yi=f(xi)+\\nεi, with εiindependent and identically distributed with mean zero and\\nvariance σ2, thexiassumed ﬁxed (non random). We estimate fusing a\\nlinear smoother (local regression, smoothing spline, etc.) with smoothing\\nparameter λ. Thus the vector of ﬁtted values is given by ˆf=Sλy. Consider\\nthein-sample prediction error\\nPE(λ) = E1\\nNN∑\\ni=1(y∗\\ni−ˆfλ(xi))2(6.34)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='33b22506-571b-4100-9f03-29f727d6b101', embedding=None, metadata={'page_label': '237', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='218 6. Kernel Smoothing Methods\\nfor predicting new responses at the Ninput values. Show that the aver-\\nage squared residual on the training data, ASR( λ), is a biased estimate\\n(optimistic) for PE( λ), while\\nCλ= ASR( λ) +2σ2\\nNtrace(Sλ) (6.35)\\nis unbiased.\\nEx. 6.11 Show that for the Gaussian mixture model (6.32) the likelihood\\nis maximized at + ∞, and describe how.\\nEx. 6.12 Write a computer program to perform a local linear discrimi-\\nnant analysis. At each query point x0, the training data receive weights\\nKλ(x0,xi) from a weighting kernel, and the ingredients for the linear deci-\\nsion boundaries (see Section 4.3) are computed by weighted averages. Try\\nout your program on the zipcode data, and show the training and test er-\\nrors for a series of ﬁve pre-chosen values of λ. Thezipcode data are available\\nfrom the book website www-stat.stanford.edu/ElemStatLearn .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d29b8363-470f-483e-89ce-7599db6c3982', embedding=None, metadata={'page_label': '238', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 219\\nPrinter: Opaque this\\n7\\nModel Assessment and Selection\\n7.1 Introduction\\nThegeneralization performance of a learning method relates to its predic-\\ntion capability on independent test data. Assessment of this performance\\nis extremely important in practice, since it guides the choice of learning\\nmethod or model, and gives us a measure of the quality of the ultimately\\nchosen model.\\nIn this chapter we describe and illustrate the key methods for perfor-\\nmance assessment, and show how they are used to select models. We begin\\nthe chapter with a discussion of the interplay between bias, variance and\\nmodel complexity.\\n7.2 Bias, Variance and Model Complexity\\nFigure 7.1 illustrates the important issue in assessing the ability of a learn-\\ning method to generalize. Consider ﬁrst the case of a quantitative or interval\\nscale response. We have a target variable Y, a vector of inputs X, and a\\nprediction model ˆf(X) that has been estimated from a training set T.\\nThe loss function for measuring errors between Yandˆf(X) is denoted by\\nL(Y,ˆf(X)). Typical choices are\\nL(Y,ˆf(X)) ={\\n(Y−ˆf(X))2squared error\\n|Y−ˆf(X)| absolute error .(7.1)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20638b59-851c-4386-b3d1-fb96daf003e2', embedding=None, metadata={'page_label': '239', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='220 7. Model Assessment and Selection\\n0 5 10 15 20 25 30 350.0 0.2 0.4 0.6 0.8 1.0 1.2\\nModel Complexity (df)Prediction ErrorHigh Bias Low Bias\\nHigh Variance Low Variance\\nFIGURE 7.1. Behavior of test sample and training sample error as the model\\ncomplexity is varied. The light blue curves show the training er rorerr, while the\\nlight red curves show the conditional test error ErrTfor100training sets of size\\n50each, as the model complexity is increased. The solid curves sh ow the expected\\ntest error Errand the expected training error E[err].\\nTest error , also referred to as generalization error , is the prediction error\\nover an independent test sample\\nErrT= E[L(Y,ˆf(X))|T] (7.2)\\nwhere both XandYare drawn randomly from their joint distribution\\n(population). Here the training set Tis ﬁxed, and test error refers to the\\nerror for this speciﬁc training set. A related quantity is the expected pre-\\ndiction error (or expected test error)\\nErr = E[ L(Y,ˆf(X))] = E[Err T]. (7.3)\\nNote that this expectation averages over everything that is random, includ-\\ning the randomness in the training set that produced ˆf.\\nFigure 7.1 shows the prediction error (light red curves) Err Tfor 100\\nsimulated training sets each of size 50. The lasso (Section 3.4.2) was used\\nto produce the sequence of ﬁts. The solid red curve is the average, and\\nhence an estimate of Err.\\nEstimation of Err Twill be our goal, although we will see that Err is\\nmore amenable to statistical analysis, and most methods eﬀectively esti-\\nmate the expected error. It does not seem possible to estimate conditional', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c5d6bf3a-9db0-4075-a9a5-9ba68cdb00de', embedding=None, metadata={'page_label': '240', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.2 Bias, Variance and Model Complexity 221\\nerror eﬀectively, given only the information in the same training set. Some\\ndiscussion of this point is given in Section 7.12.\\nTraining error is the average loss over the training sample\\nerr =1\\nNN∑\\ni=1L(yi,ˆf(xi)). (7.4)\\nWe would like to know the expected test error of our estimated model\\nˆf. As the model becomes more and more complex, it uses the training\\ndata more and is able to adapt to more complicated underlying structures.\\nHence there is a decrease in bias but an increase in variance. There is some\\nintermediate model complexity that gives minimum expected test error.\\nUnfortunately training error is not a good estimate of the test error,\\nas seen in Figure 7.1. Training error consistently decreases with model\\ncomplexity, typically dropping to zero if we increase the model complexity\\nenough. However, a model with zero training error is overﬁt to the training\\ndata and will typically generalize poorly.\\nThe story is similar for a qualitative or categorical response Gtaking\\none of Kvalues in a set G, labeled for convenience as 1 ,2,... ,K . Typically\\nwe model the probabilities pk(X) = Pr( G=k|X) (or some monotone\\ntransformations fk(X)), and then ˆG(X) = arg max kˆpk(X). In some cases,\\nsuch as 1-nearest neighbor classiﬁcation (Chapters 2 and 13) we produce\\nˆG(X) directly. Typical loss functions are\\nL(G,ˆG(X)) = I(G̸=ˆG(X)) (0–1 loss) , (7.5)\\nL(G,ˆp(X)) = −2K∑\\nk=1I(G=k)log ˆpk(X)\\n=−2log ˆpG(X) (−2×log-likelihood) .(7.6)\\nThe quantity −2×the log-likelihood is sometimes referred to as the\\ndeviance .\\nAgain, test error here is Err T= E[L(G,ˆG(X))|T], the population mis-\\nclassiﬁcation error of the classiﬁer trained on T, and Err is the expected\\nmisclassiﬁcation error.\\nTraining error is the sample analogue, for example,\\nerr =−2\\nNN∑\\ni=1log ˆpgi(xi), (7.7)\\nthe sample log-likelihood for the model.\\nThe log-likelihood can be used as a loss-function for general response\\ndensities, such as the Poisson, gamma, exponential, log-normal and others.\\nIf Pr θ(X)(Y) is the density of Y, indexed by a parameter θ(X) that depends\\non the predictor X, then\\nL(Y,θ(X)) =−2≤log Pr θ(X)(Y). (7.8)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5d191963-68b4-40b8-8589-7bf7041369de', embedding=None, metadata={'page_label': '241', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='222 7. Model Assessment and Selection\\nThe “−2” in the deﬁnition makes the log-likelihood loss for the Gaussian\\ndistribution match squared-error loss.\\nFor ease of exposition, for the remainder of this chapter we will use Yand\\nf(X) to represent all of the above situations, since we focus mainly on the\\nquantitative response (squared-error loss) setting. For the other situati ons,\\nthe appropriate translations are obvious.\\nIn this chapter we describe a number of methods for estimating the\\nexpected test error for a model. Typically our model will have a tuning\\nparameter or parameters αand so we can write our predictions as ˆfα(x).\\nThe tuning parameter varies the complexity of our model, and we wish to\\nﬁnd the value of αthat minimizes error, that is, produces the minimum of\\nthe average test error curve in Figure 7.1. Having said this, for brevity w e\\nwill often suppress the dependence of ˆf(x) onα.\\nIt is important to note that there are in fact two separate goals that we\\nmight have in mind:\\nModel selection: estimating the performance of diﬀerent models in order\\nto choose the best one.\\nModel assessment: having chosen a ﬁnal model, estimating its predic-\\ntion error (generalization error) on new data.\\nIf we are in a data-rich situation, the best approach for both problems is\\nto randomly divide the dataset into three parts: a training set, a validation\\nset, and a test set. The training set is used to ﬁt the models; the validation\\nset is used to estimate prediction error for model selection; the test set is\\nused for assessment of the generalization error of the ﬁnal chosen model.\\nIdeally, the test set should be kept in a “vault,” and be brought out only\\nat the end of the data analysis. Suppose instead that we use the test-set\\nrepeatedly, choosing the model with smallest test-set error. Then the test\\nset error of the ﬁnal chosen model will underestimate the true test error,\\nsometimes substantially.\\nIt is diﬃcult to give a general rule on how to choose the number of\\nobservations in each of the three parts, as this depends on the signal-to-\\nnoise ratio in the data and the training sample size. A typical split might\\nbe 50% for training, and 25% each for validation and testing:\\nTest Train Validation TestTrain Validation Test Validation Train Validation TestTrain\\nThe methods in this chapter are designed for situations where there is\\ninsuﬃcient data to split it into three parts. Again it is too diﬃcult to give\\na general rule on how much training data is enough; among other things,\\nthis depends on the signal-to-noise ratio of the underlying function, and\\nthe complexity of the models being ﬁt to the data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b611f42c-8f11-4028-86bd-6e077cfe0a47', embedding=None, metadata={'page_label': '242', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 The Bias–Variance Decomposition 223\\nThe methods of this chapter approximate the validation step either an-\\nalytically (AIC, BIC, MDL, SRM) or by eﬃcient sample re-use (cross-\\nvalidation and the bootstrap). Besides their use in model selection, we also\\nexamine to what extent each method provides a reliable estimate of test\\nerror of the ﬁnal chosen model.\\nBefore jumping into these topics, we ﬁrst explore in more detail the\\nnature of test error and the bias–variance tradeoﬀ.\\n7.3 The Bias–Variance Decomposition\\nAs in Chapter 2, if we assume that Y=f(X) +εwhere E( ε) = 0 and\\nVar(ε) =σ2\\nε, we can derive an expression for the expected prediction error\\nof a regression ﬁt ˆf(X) at an input point X=x0, using squared-error loss:\\nErr(x0) = E[(Y−ˆf(x0))2|X=x0]\\n=σ2\\nε+ [Eˆf(x0)−f(x0)]2+E[ˆf(x0)−Eˆf(x0)]2\\n=σ2\\nε+ Bias2(ˆf(x0)) + Var( ˆf(x0))\\n= Irreducible Error + Bias2+ Variance . (7.9)\\nThe ﬁrst term is the variance of the target around its true mean f(x0), and\\ncannot be avoided no matter how well we estimate f(x0), unless σ2\\nε= 0.\\nThe second term is the squared bias, the amount by which the average of\\nour estimate diﬀers from the true mean; the last term is the variance; the\\nexpected squared deviation of ˆf(x0) around its mean. Typically the more\\ncomplex we make the model ˆf, the lower the (squared) bias but the higher\\nthe variance.\\nFor the k-nearest-neighbor regression ﬁt, these expressions have the sim-\\nple form\\nErr(x0) = E[(Y−ˆfk(x0))2|X=x0]\\n=σ2\\nε+[\\nf(x0)−1\\nkk∑\\nℓ=1f(x(ℓ))]2\\n+σ2\\nε\\nk. (7.10)\\nHere we assume for simplicity that training inputs xiare ﬁxed, and the ran-\\ndomness arises from the yi. The number of neighbors kis inversely related\\nto the model complexity. For small k, the estimate ˆfk(x) can potentially\\nadapt itself better to the underlying f(x). As we increase k, the bias—the\\nsquared diﬀerence between f(x0) and the average of f(x) at the k-nearest\\nneighbors—will typically increase, while the variance decreases.\\nFor a linear model ﬁt ˆfp(x) =xTˆβ, where the parameter vector βwith\\npcomponents is ﬁt by least squares, we have\\nErr(x0) = E[(Y−ˆfp(x0))2|X=x0]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ef54cb91-f288-4af3-bddf-376ebe153eee', embedding=None, metadata={'page_label': '243', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='224 7. Model Assessment and Selection\\n=σ2\\nε+ [f(x0)−Eˆfp(x0)]2+||h(x0)||2σ2\\nε.(7.11)\\nHereh(x0) =X(XTX)−1x0, theN-vector of linear weights that produce\\nthe ﬁt ˆfp(x0) =x0T(XTX)−1XTy, and hence Var[ ˆfp(x0)] =||h(x0)||2σ2\\nε.\\nWhile this variance changes with x0, its average (with x0taken to be each\\nof the sample values xi) is (p/N)σ2\\nε, and hence\\n1\\nNN∑\\ni=1Err(xi) =σ2\\nε+1\\nNN∑\\ni=1[f(xi)−Eˆf(xi)]2+p\\nNσ2\\nε, (7.12)\\nthein-sample error. Here model complexity is directly related to the num-\\nber of parameters p.\\nThe test error Err( x0) for a ridge regression ﬁt ˆfα(x0) is identical in\\nform to (7.11), except the linear weights in the variance term are diﬀerent:\\nh(x0) =X(XTX+αI)Tx0. The bias term will also be diﬀerent.\\nFor a linear model family such as ridge regression, we can break down\\nthe bias more ﬁnely. Let β∗denote the parameters of the best-ﬁtting linear\\napproximation to f:\\nβ∗= arg min\\nβE(\\nf(X)−XTβ)2. (7.13)\\nHere the expectation is taken with respect to the distribution of the input\\nvariables X. Then we can write the average squared bias as\\nEx0[\\nf(x0)−Eˆfα(x0)]2\\n= E x0[\\nf(x0)−xT\\n0β∗]2+ Ex0[\\nxT\\n0β∗−ExT\\n0ˆβα]2\\n= Ave[Model Bias]2+ Ave[Estimation Bias]2\\n(7.14)\\nThe ﬁrst term on the right-hand side is the average squared model bias , the\\nerror between the best-ﬁtting linear approximation and the true function.\\nThe second term is the average squared estimation bias , the error between\\nthe average estimate E( xT\\n0ˆβ) and the best-ﬁtting linear approximation.\\nFor linear models ﬁt by ordinary least squares, the estimation bias is zero.\\nFor restricted ﬁts, such as ridge regression, it is positive, and we trade i t oﬀ\\nwith the beneﬁts of a reduced variance. The model bias can only be reduced\\nby enlarging the class of linear models to a richer collection of models, by\\nincluding interactions and transformations of the variables in the model.\\nFigure 7.2 shows the bias–variance tradeoﬀ schematically. In the case\\nof linear models, the model space is the set of all linear predictions from\\npinputs and the black dot labeled “closest ﬁt” is xTβ∗. The blue-shaded\\nregion indicates the error σεwith which we see the truth in the training\\nsample.\\nAlso shown is the variance of the least squares ﬁt, indicated by the large\\nyellow circle centered at the black dot labeled “closest ﬁt in population,’', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09414817-c5d8-4a10-bc6f-de6f7c12ae4f', embedding=None, metadata={'page_label': '244', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 The Bias–Variance Decomposition 225\\nRealizationClosest fit in population\\nEstimation BiasSPACE\\nVarianceEstimationClosest fit\\nTruth\\nModel bias\\nRESTRICTEDShrunken fit\\nMODELSPACEMODEL\\nFIGURE 7.2. Schematic of the behavior of bias and variance. The model space\\nis the set of all possible predictions from the model, with the “closest ﬁt” labeled\\nwith a black dot. The model bias from the truth is shown, along wi th the variance,\\nindicated by the large yellow circle centered at the black dot l abeled “closest ﬁt\\nin population.” A shrunken or regularized ﬁt is also shown, having additional\\nestimation bias, but smaller prediction error due to its dec reased variance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9dac2d82-d4c5-4bb3-af4d-97d4217ebb71', embedding=None, metadata={'page_label': '245', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='226 7. Model Assessment and Selection\\nNow if we were to ﬁt a model with fewer predictors, or regularize the coef-\\nﬁcients by shrinking them toward zero (say), we would get the “shrunken\\nﬁt” shown in the ﬁgure. This ﬁt has an additional estimation bias, due to\\nthe fact that it is not the closest ﬁt in the model space. On the other hand,\\nit has smaller variance. If the decrease in variance exceeds the increase in\\n(squared) bias, then this is worthwhile.\\n7.3.1 Example: Bias–Variance Tradeoﬀ\\nFigure 7.3 shows the bias–variance tradeoﬀ for two simulated examples.\\nThere are 80 observations and 20 predictors, uniformly distributed in the\\nhypercube [0 ,1]20. The situations are as follows:\\nLeft panels: Yis 0 if X1≤1/2 and 1 if X1>1/2, and we apply k-nearest\\nneighbors.\\nRight panels: Yis 1 if∑10\\nj=1Xjis greater than 5 and 0 otherwise, and we\\nuse best subset linear regression of size p.\\nThe top row is regression with squared error loss; the bottom row is cla ssi-\\nﬁcation with 0–1 loss. The ﬁgures show the prediction error (red), squared\\nbias (green) and variance (blue), all computed for a large test sample.\\nIn the regression problems, bias and variance add to produce the predic-\\ntion error curves, with minima at about k= 5 for k-nearest neighbors, and\\np≥10 for the linear model. For classiﬁcation loss (bottom ﬁgures), some\\ninteresting phenomena can be seen. The bias and variance curves are the\\nsame as in the top ﬁgures, and prediction error now refers to misclassiﬁ-\\ncation rate. We see that prediction error is no longer the sum of squared\\nbias and variance. For the k-nearest neighbor classiﬁer, prediction error\\ndecreases or stays the same as the number of neighbors is increased to 20,\\ndespite the fact that the squared bias is rising. For the linear model classi-\\nﬁer the minimum occurs for p≥10 as in regression, but the improvement\\nover the p= 1 model is more dramatic. We see that bias and variance seem\\nto interact in determining prediction error.\\nWhy does this happen? There is a simple explanation for the ﬁrst phe-\\nnomenon. Suppose at a given input point, the true probability of class 1 is\\n0.9 while the expected value of our estimate is 0 .6. Then the squared bias—\\n(0.6−0.9)2—is considerable, but the prediction error is zero since we make\\nthe correct decision. In other words, estimation errors that leave us on the\\nright side of the decision boundary don’t hurt. Exercise 7.2 demonstrates\\nthis phenomenon analytically, and also shows the interaction eﬀect between\\nbias and variance.\\nThe overall point is that the bias–variance tradeoﬀ behaves diﬀerently\\nfor 0–1 loss than it does for squared error loss. This in turn means that\\nthe best choices of tuning parameters may diﬀer substantially in the two', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='40f902f6-93b3-492a-9bc4-674c9a0cd04f', embedding=None, metadata={'page_label': '246', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 The Bias–Variance Decomposition 2270.0 0.1 0.2 0.3 0.4\\nNumber of Neighbors k50 40 30 20 10 0k−NN − Regression\\n5 10 15 200.0 0.1 0.2 0.3 0.4\\nSubset Size pLinear Model − Regression0.0 0.1 0.2 0.3 0.4\\nNumber of Neighbors k50 40 30 20 10 0k−NN − Classification\\n5 10 15 200.0 0.1 0.2 0.3 0.4\\nSubset Size pLinear Model − Classification\\nFIGURE 7.3. Expected prediction error (orange), squared bias (green) and va ri-\\nance (blue) for a simulated example. The top row is regression w ith squared error\\nloss; the bottom row is classiﬁcation with 0–1loss. The models are k-nearest\\nneighbors (left) and best subset regression of size p(right). The variance and bias\\ncurves are the same in regression and classiﬁcation, but the pre diction error curve\\nis diﬀerent.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2159faec-3b33-496e-8913-3a62ea23ffee', embedding=None, metadata={'page_label': '247', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='228 7. Model Assessment and Selection\\nsettings. One should base the choice of tuning parameter on an estimate of\\nprediction error, as described in the following sections.\\n7.4 Optimism of the Training Error Rate\\nDiscussions of error rate estimation can be confusing, because we have\\nto make clear which quantities are ﬁxed and which are random1. Before\\nwe continue, we need a few deﬁnitions, elaborating on the material of Sec-\\ntion 7.2. Given a training set T={(x1,y1),(x2,y2),...(xN,yN)}the gen-\\neralization error of a model ˆfis\\nErrT= EX0,Y0[L(Y0,ˆf(X0))|T]; (7.15)\\nNote that the training set Tis ﬁxed in expression (7.15). The point ( X0,Y0)\\nis a new test data point, drawn from F, the joint distribution of the data.\\nAveraging over training sets Tyields the expected error\\nErr = E TEX0,Y0[L(Y0,ˆf(X0))|T], (7.16)\\nwhich is more amenable to statistical analysis. As mentioned earlier, it\\nturns out that most methods eﬀectively estimate the expected error rather\\nthan E T; see Section 7.12 for more on this point.\\nNow typically, the training error\\nerr =1\\nNN∑\\ni=1L(yi,ˆf(xi)) (7.17)\\nwill be less than the true error Err T, because the same data is being used\\nto ﬁt the method and assess its error (see Exercise 2.9). A ﬁtting method\\ntypically adapts to the training data, and hence the apparent or training\\nerror err will be an overly optimistic estimate of the generalization error\\nErrT.\\nPart of the discrepancy is due to where the evaluation points occur. The\\nquantity Err Tcan be thought of as extra-sample error, since the test input\\nvectors don’t need to coincide with the training input vectors. The nature\\nof the optimism in err is easiest to understand when we focus instead on\\nthein-sample error\\nErrin=1\\nNN∑\\ni=1EY0[L(Y0\\ni,ˆf(xi))|T] (7.18)\\nTheY0notation indicates that we observe Nnew response values at\\neach of the training points xi, i= 1,2,... ,N . We deﬁne the optimism as\\n1Indeed, in the ﬁrst edition of our book, this section wasn’t s uﬃciently clear.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7de4c7b6-0f2c-4baf-b38d-bbc159f6859b', embedding=None, metadata={'page_label': '248', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.4 Optimism of the Training Error Rate 229\\nthe diﬀerence between Err inand the training error err:\\nop≡Errin−err. (7.19)\\nThis is typically positive since err is usually biased downward as an estimate\\nof prediction error. Finally, the average optimism is the expectation of the\\noptimism over training sets\\nω≡Ey(op). (7.20)\\nHere the predictors in the training set are ﬁxed, and the expectation is\\nover the training set outcome values; hence we have used the notation E y\\ninstead of E T. We can usually estimate only the expected error ωrather\\nthan op, in the same way that we can estimate the expected error Err\\nrather than the conditional error Err T.\\nFor squared error, 0–1, and other loss functions, one can show quite\\ngenerally that\\nω=2\\nNN∑\\ni=1Cov(ˆyi,yi), (7.21)\\nwhere Cov indicates covariance. Thus the amount by which err underesti-\\nmates the true error depends on how strongly yiaﬀects its own prediction.\\nThe harder we ﬁt the data, the greater Cov(ˆ yi,yi) will be, thereby increas-\\ning the optimism. Exercise 7.4 proves this result for squared error loss where\\nˆyiis the ﬁtted value from the regression. For 0–1 loss, ˆ yi∈ {0,1}is the\\nclassiﬁcation at xi, and for entropy loss, ˆ yi∈[0,1] is the ﬁtted probability\\nof class 1 at xi.\\nIn summary, we have the important relation\\nEy(Errin) = E y(err) +2\\nNN∑\\ni=1Cov(ˆyi,yi). (7.22)\\nThis expression simpliﬁes if ˆ yiis obtained by a linear ﬁt with dinputs\\nor basis functions. For example,\\nN∑\\ni=1Cov(ˆyi,yi) =dσ2\\nε (7.23)\\nfor the additive error model Y=f(X) +ε, and so\\nEy(Errin) = E y(err) + 2 ≤d\\nNσ2\\nε. (7.24)\\nExpression (7.23) is the basis for the deﬁnition of the eﬀective number of\\nparameters discussed in Section 7.6 The optimism increases linearly with', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b0176450-dfb6-4acb-a7c5-5ce27e5525a1', embedding=None, metadata={'page_label': '249', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='230 7. Model Assessment and Selection\\nthe number dof inputs or basis functions we use, but decreases as the\\ntraining sample size increases. Versions of (7.24) hold approximately for\\nother error models, such as binary data and entropy loss.\\nAn obvious way to estimate prediction error is to estimate the optimism\\nand then add it to the training error err. The methods described in the\\nnext section— Cp, AIC, BIC and others—work in this way, for a special\\nclass of estimates that are linear in their parameters.\\nIn contrast, cross-validation and bootstrap methods, described later in\\nthe chapter, are direct estimates of the extra-sample error Err. These gen-\\neral tools can be used with any loss function, and with nonlinear, adaptive\\nﬁtting techniques.\\nIn-sample error is not usually of direct interest since future values of the\\nfeatures are not likely to coincide with their training set values. But for\\ncomparison between models, in-sample error is convenient and often leads\\nto eﬀective model selection. The reason is that the relative (rather than\\nabsolute) size of the error is what matters.\\n7.5 Estimates of In-Sample Prediction Error\\nThe general form of the in-sample estimates is\\nˆErrin=err + ˆω, (7.25)\\nwhere ˆ ωis an estimate of the average optimism.\\nUsing expression (7.24), applicable when dparameters are ﬁt under\\nsquared error loss, leads to a version of the so-called Cpstatistic,\\nCp=err + 2 ≤d\\nNˆσε2. (7.26)\\nHere ˆσε2is an estimate of the noise variance, obtained from the mean-\\nsquared error of a low-bias model. Using this criterion we adjust the training\\nerror by a factor proportional to the number of basis functions used.\\nTheAkaike information criterion is a similar but more generally appli-\\ncable estimate of Err inwhen a log-likelihood loss function is used. It relies\\non a relationship similar to (7.24) that holds asymptotically as N→ ∞:\\n−2≤E[log Pr ˆθ(Y)]≈ −2\\nN≤E[loglik] + 2 ≤d\\nN. (7.27)\\nHere Pr θ(Y) is a family of densities for Y(containing the “true” density),\\nˆθis the maximum-likelihood estimate of θ, and “loglik” is the maximized\\nlog-likelihood:\\nloglik =N∑\\ni=1log Pr ˆθ(yi). (7.28)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dce989fc-8cbf-44c7-a37d-1cde239505d5', embedding=None, metadata={'page_label': '250', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.5 Estimates of In-Sample Prediction Error 231\\nFor example, for the logistic regression model, using the binomial log-\\nlikelihood, we have\\nAIC = −2\\nN≤loglik + 2 ≤d\\nN. (7.29)\\nFor the Gaussian model (with variance σ2\\nε= ˆσε2assumed known), the AIC\\nstatistic is equivalent to Cp, and so we refer to them collectively as AIC.\\nTo use AIC for model selection, we simply choose the model giving small-\\nest AIC over the set of models considered. For nonlinear and other complex\\nmodels, we need to replace dby some measure of model complexity. We\\ndiscuss this in Section 7.6.\\nGiven a set of models fα(x) indexed by a tuning parameter α, denote\\nbyerr(α) and d(α) the training error and number of parameters for each\\nmodel. Then for this set of models we deﬁne\\nAIC(α) =err(α) + 2≤d(α)\\nNˆσε2. (7.30)\\nThe function AIC( α) provides an estimate of the test error curve, and we\\nﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model\\nisfˆα(x). Note that if the basis functions are chosen adaptively, (7.23) no\\nlonger holds. For example, if we have a total of pinputs, and we choose\\nthe best-ﬁtting linear model with d < p inputs, the optimism will exceed\\n(2d/N)σ2\\nε. Put another way, by choosing the best-ﬁtting model with d\\ninputs, the eﬀective number of parameters ﬁt is more than d.\\nFigure 7.4 shows AIC in action for the phoneme recognition example\\nof Section 5.2.3 on page 148. The input vector is the log-periodogram of\\nthe spoken vowel, quantized to 256 uniformly spaced frequencies. A lin-\\near logistic regression model is used to predict the phoneme class, with\\ncoeﬃcient function β(f) =∑M\\nm=1hm(f)θm, an expansion in Mspline ba-\\nsis functions. For any given M, a basis of natural cubic splines is used\\nfor the hm, with knots chosen uniformly over the range of frequencies (so\\nd(α) =d(M) =M). Using AIC to select the number of basis functions will\\napproximately minimize Err( M) for both entropy and 0–1 loss.\\nThe simple formula\\n(2/N)N∑\\ni=1Cov(ˆyi,yi) = (2 d/N)σ2\\nε\\nholds exactly for linear models with additive errors and squared error loss,\\nand approximately for linear models and log-likelihoods. In particular, the\\nformula does not hold in general for 0–1 loss (Efron, 1986), although many\\nauthors nevertheless use it in that context (right panel of Figure 7.4).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='957a3126-6991-4658-8eef-27ac5eb98d6d', embedding=None, metadata={'page_label': '251', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='232 7. Model Assessment and Selection\\nNumber of Basis FunctionsLog-likelihood\\n0.5 1.0 1.5 2.0 2.5Log-likelihood Loss\\n2 4 8 16 32 64 128O\\nO\\nO\\nOOO\\nO\\nOO\\nO\\nO\\nOOOOO\\nO\\nO\\nO\\nOOOOOTrain\\nTest\\nAIC\\nNumber of Basis FunctionsMisclassification Error\\n0.10 0.15 0.20 0.25 0.30 0.350-1 Loss\\n2 4 8 16 32 64 128O\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nOOOOOO\\nO\\nO\\nOOOOO\\nFIGURE 7.4. AIC used for model selection for the phoneme recogni-\\ntion example of Section 5.2.3. The logistic regression coeﬃci ent function\\nβ(f) =PM\\nm=1hm(f)θmis modeled as an expansion in Mspline basis functions.\\nIn the left panel we see the AIC statistic used to estimate Errinusing log-likeli-\\nhood loss. Included is an estimate of Errbased on an independent test sample. It\\ndoes well except for the extremely over-parametrized case ( M= 256 parameters\\nforN= 1000 observations). In the right panel the same is done for 0–1 loss.\\nAlthough the AIC formula does not strictly apply here, it does a reasonable job in\\nthis case.\\n7.6 The Eﬀective Number of Parameters\\nThe concept of “number of parameters” can be generalized, especially to\\nmodels where regularization is used in the ﬁtting. Suppose we stack the\\noutcomes y1,y2,... ,y Ninto a vector y, and similarly for the predictions\\nˆy. Then a linear ﬁtting method is one for which we can write\\nˆy=Sy, (7.31)\\nwhereSis anN×Nmatrix depending on the input vectors xibut not on\\ntheyi. Linear ﬁtting methods include linear regression on the original fea-\\ntures or on a derived basis set, and smoothing methods that use quadratic\\nshrinkage, such as ridge regression and cubic smoothing splines. Then the\\neﬀective number of parameters is deﬁned as\\ndf(S) = trace( S), (7.32)\\nthe sum of the diagonal elements of S(also known as the eﬀective degrees-\\nof-freedom ). Note that if Sis an orthogonal-projection matrix onto a basis', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a5bf93a-8024-4487-aa35-ee4d946bd275', embedding=None, metadata={'page_label': '252', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.7 The Bayesian Approach and BIC 233\\nset spanned by Mfeatures, then trace( S) =M. It turns out that trace( S) is\\nexactly the correct quantity to replace das the number of parameters in the\\nCpstatistic (7.26). If yarises from an additive-error model Y=f(X) +ε\\nwith Var( ε) =σ2\\nε, then one can show that∑N\\ni=1Cov(ˆyi,yi) = trace( S)σ2\\nε,\\nwhich motivates the more general deﬁnition\\ndf(ˆy) =∑N\\ni=1Cov(ˆyi,yi)\\nσ2ε(7.33)\\n(Exercises 7.4 and 7.5). Section 5.4.1 on page 153 gives some more intuit ion\\nfor the deﬁnition df = trace( S) in the context of smoothing splines.\\nFor models like neural networks, in which we minimize an error function\\nR(w) with weight decay penalty (regularization) α∑\\nmw2\\nm, the eﬀective\\nnumber of parameters has the form\\ndf(α) =M∑\\nm=1θm\\nθm+α, (7.34)\\nwhere the θmare the eigenvalues of the Hessian matrix ∂2R(w)/∂w∂wT.\\nExpression (7.34) follows from (7.32) if we make a quadratic approx imation\\nto the error function at the solution (Bishop, 1995).\\n7.7 The Bayesian Approach and BIC\\nThe Bayesian information criterion (BIC), like AIC, is applicable in s ettings\\nwhere the ﬁtting is carried out by maximization of a log-likelihood. The\\ngeneric form of BIC is\\nBIC = −2≤loglik + (log N)≤d. (7.35)\\nThe BIC statistic (times 1/2) is also known as the Schwarz criterion ( Schwarz,\\n1978).\\nUnder the Gaussian model, assuming the variance σ2\\nεis known, −2≤loglik\\nequals (up to a constant)∑\\ni(yi−ˆf(xi))2/σ2\\nε, which is N≤err/σ2\\nεfor squared\\nerror loss. Hence we can write\\nBIC =N\\nσ2ε[\\nerr + (log N)≤d\\nNσ2\\nε]\\n. (7.36)\\nTherefore BIC is proportional to AIC ( Cp), with the factor 2 replaced\\nby log N. Assuming N > e2≈7.4, BIC tends to penalize complex models\\nmore heavily, giving preference to simpler models in selection. As with AIC,\\nσ2\\nεis typically estimated by the mean squared error of a low-bias model.\\nFor classiﬁcation problems, use of the multinomial log-likelihood leads to a\\nsimilar relationship with the AIC, using cross-entropy as the error measur e.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='21a6f4d9-179f-4192-b9f5-ef092faaa1d5', embedding=None, metadata={'page_label': '253', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='234 7. Model Assessment and Selection\\nNote however that the misclassiﬁcation error measure does not arise in the\\nBIC context, since it does not correspond to the log-likelihood of the data\\nunder any probability model.\\nDespite its similarity with AIC, BIC is motivated in quite a diﬀerent\\nway. It arises in the Bayesian approach to model selection, which we now\\ndescribe.\\nSuppose we have a set of candidate models Mm,m= 1,... ,M and\\ncorresponding model parameters θm, and we wish to choose a best model\\nfrom among them. Assuming we have a prior distribution Pr( θm|Mm) for\\nthe parameters of each model Mm, the posterior probability of a given\\nmodel is\\nPr(Mm|Z)∝Pr(Mm)≤Pr(Z|Mm) (7.37)\\n∝Pr(Mm)≤∫\\nPr(Z|θm,Mm)Pr(θm|Mm)dθm,\\nwhere Zrepresents the training data {xi,yi}N\\n1. To compare two models\\nMmandMℓ, we form the posterior odds\\nPr(Mm|Z)\\nPr(Mℓ|Z)=Pr(Mm)\\nPr(Mℓ)≤Pr(Z|Mm)\\nPr(Z|Mℓ). (7.38)\\nIf the odds are greater than one we choose model m, otherwise we choose\\nmodel ℓ. The rightmost quantity\\nBF(Z) =Pr(Z|Mm)\\nPr(Z|Mℓ)(7.39)\\nis called the Bayes factor , the contribution of the data toward the posterior\\nodds.\\nTypically we assume that the prior over models is uniform, so that\\nPr(Mm) is constant. We need some way of approximating Pr( Z|Mm).\\nA so-called Laplace approximation to the integral followed by some other\\nsimpliﬁcations (Ripley, 1996, page 64) to (7.37) gives\\nlog Pr( Z|Mm) = log Pr( Z|ˆθm,Mm)−dm\\n2≤logN+O(1).(7.40)\\nHere ˆθmis a maximum likelihood estimate and dmis the number of free\\nparameters in model Mm. If we deﬁne our loss function to be\\n−2log Pr( Z|ˆθm,Mm),\\nthis is equivalent to the BIC criterion of equation (7.35).\\nTherefore, choosing the model with minimum BIC is equivalent to choos-\\ning the model with largest (approximate) posterior probability. But this\\nframework gives us more. If we compute the BIC criterion for a set of M,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97316ba2-0afe-4488-a522-5970dfde6ed5', embedding=None, metadata={'page_label': '254', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.8 Minimum Description Length 235\\nmodels, giving BIC m,m= 1,2,... ,M , then we can estimate the posterior\\nprobability of each model Mmas\\ne−1\\n2≤BIC m\\n∑M\\nℓ=1e−1\\n2≤BIC ℓ. (7.41)\\nThus we can estimate not only the best model, but also assess the relative\\nmerits of the models considered.\\nFor model selection purposes, there is no clear choice between AIC and\\nBIC. BIC is asymptotically consistent as a selection criterion. What this\\nmeans is that given a family of models, including the true model, the prob-\\nability that BIC will select the correct model approaches one as the sample\\nsizeN→ ∞. This is not the case for AIC, which tends to choose models\\nwhich are too complex as N→ ∞. On the other hand, for ﬁnite samples,\\nBIC often chooses models that are too simple, because of its heavy penalty\\non complexity.\\n7.8 Minimum Description Length\\nThe minimum description length (MDL) approach gives a selection cri-\\nterion formally identical to the BIC approach, but is motivated from an\\noptimal coding viewpoint. We ﬁrst review the theory of coding for data\\ncompression, and then apply it to model selection.\\nWe think of our datum zas a message that we want to encode and\\nsend to someone else (the “receiver”). We think of our model as a way of\\nencoding the datum, and will choose the most parsimonious model, that is\\nthe shortest code, for the transmission.\\nSuppose ﬁrst that the possible messages we might want to transmit are\\nz1,z2,... ,z m. Our code uses a ﬁnite alphabet of length A: for example, we\\nmight use a binary code {0,1}of length A= 2. Here is an example with\\nfour possible messages and a binary coding:\\nMessage z1z2z3z4\\nCode 010110 111(7.42)\\nThis code is known as an instantaneous preﬁx code: no code is the pre-\\nﬁx of any other, and the receiver (who knows all of the possible codes),\\nknows exactly when the message has been completely sent. We restrict our\\ndiscussion to such instantaneous preﬁx codes.\\nOne could use the coding in (7.42) or we could permute the codes, for\\nexample use codes 110 ,10,111,0 forz1,z2,z3,z4. How do we decide which\\nto use? It depends on how often we will be sending each of the messages.\\nIf, for example, we will be sending z1most often, it makes sense to use the\\nshortest code 0 for z1. Using this kind of strategy—shorter codes for more\\nfrequent messages—the average message length will be shorter.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2e1d0af-4906-4f30-afe9-6c77530e6374', embedding=None, metadata={'page_label': '255', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='236 7. Model Assessment and Selection\\nIn general, if messages are sent with probabilities Pr( zi),i= 1,2,... ,4,\\na famous theorem due to Shannon says we should use code lengths li=\\n−log2Pr(zi) and the average message length satisﬁes\\nE(length) ≥ −∑\\nPr(zi)log2(Pr(zi)). (7.43)\\nThe right-hand side above is also called the entropy of the distribution\\nPr(zi). The inequality is an equality when the probabilities satisfy pi=\\nA−li. In our example, if Pr( zi) = 1/2,1/4,1/8,1/8,respectively, then the\\ncoding shown in (7.42) is optimal and achieves the entropy lower bound.\\nIn general the lower bound cannot be achieved, but procedures like the\\nHuﬀmann coding scheme can get close to the bound. Note that with an\\ninﬁnite set of messages, the entropy is replaced by −∫\\nPr(z)log2Pr(z)dz.\\nFrom this result we glean the following:\\nTo transmit a random variable zhaving probability density func-\\ntionPr(z), we require about −log2Pr(z)bits of information.\\nWe henceforth change notation from log2Pr(z) to log Pr( z) = logePr(z);\\nthis is for convenience, and just introduces an unimportant multiplicative\\nconstant.\\nNow we apply this result to the problem of model selection. We have\\na model Mwith parameters θ, and data Z= (X,y) consisting of both\\ninputs and outputs. Let the (conditional) probability of the outputs under\\nthe model be Pr( y|θ,M,X), assume the receiver knows all of the inputs,\\nand we wish to transmit the outputs. Then the message length required to\\ntransmit the outputs is\\nlength = −log Pr( y|θ,M,X)−log Pr( θ|M), (7.44)\\nthe log-probability of the target values given the inputs. The second term\\nis the average code length for transmitting the model parameters θ, while\\nthe ﬁrst term is the average code length for transmitting the discrepancy\\nbetween the model and actual target values. For example suppose we have\\na single target ywithy∼N(θ,σ2), parameter θ∼N(0,1) and no input\\n(for simplicity). Then the message length is\\nlength = constant + log σ+(y−θ)2\\nσ2+θ2\\n2. (7.45)\\nNote that the smaller σis, the shorter on average is the message length,\\nsinceyis more concentrated around θ.\\nThe MDL principle says that we should choose the model that mini-\\nmizes (7.44). We recognize (7.44) as the (negative) log-posterior distribu-\\ntion, and hence minimizing description length is equivalent to maximizing\\nposterior probability. Hence the BIC criterion, derived as approximation to\\nlog-posterior probability, can also be viewed as a device for (approximate)\\nmodel choice by minimum description length.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00f8409f-65df-4fc6-91df-ceedeb7dc6a7', embedding=None, metadata={'page_label': '256', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.9 Vapnik–Chervonenkis Dimension 237\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 0.0 1.0\\nxsin(50 ≤x)\\nFIGURE 7.5. The solid curve is the function sin(50 x)forx∈[0,1]. The green\\n(solid) and blue (hollow) points illustrate how the associate d indicator function\\nI(sin(αx)>0)can shatter (separate) an arbitrarily large number of points b y\\nchoosing an appropriately high frequency α.\\nNote that we have ignored the precision with which a random variable\\nzis coded. With a ﬁnite code length we cannot code a continuous variable\\nexactly. However, if we code zwithin a tolerance δz, the message length\\nneeded is the log of the probability in the interval [ z,z+δz] which is well ap-\\nproximated by δzPr(z) ifδzis small. Since log δzPr(z) = log δz+log Pr( z),\\nthis means we can just ignore the constant log δzand use log Pr( z) as our\\nmeasure of message length, as we did above.\\nThe preceding view of MDL for model selection says that we should\\nchoose the model with highest posterior probability. However, many Bayes-\\nians would instead do inference by sampling from the posterior distribution.\\n7.9 Vapnik–Chervonenkis Dimension\\nA diﬃculty in using estimates of in-sample error is the need to specify the\\nnumber of parameters (or the complexity) dused in the ﬁt. Although the\\neﬀective number of parameters introduced in Section 7.6 is useful for some\\nnonlinear models, it is not fully general. The Vapnik–Chervonenkis (VC)\\ntheory provides such a general measure of complexity, and gives associated\\nbounds on the optimism. Here we give a brief review of this theory.\\nSuppose we have a class of functions {f(x,α)}indexed by a parameter\\nvector α, with x∈IRp. Assume for now that fis an indicator function,\\nthat is, takes the values 0 or 1. If α= (α0,α1) and fis the linear indi-\\ncator function I(α0+αT\\n1x >0), then it seems reasonable to say that the\\ncomplexity of the class fis the number of parameters p+ 1. But what\\nabout f(x,α) =I(sinα≤x) where αis any real number and x∈IR? The\\nfunction sin(50 ≤x) is shown in Figure 7.5. This is a very wiggly function\\nthat gets even rougher as the frequency αincreases, but it has only one\\nparameter: despite this, it doesn’t seem reasonable to conclude that it has\\nless complexity than the linear indicator function I(α0+α1x) inp= 1\\ndimension.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27810b44-bf94-4f9f-97f4-a64f58a4db99', embedding=None, metadata={'page_label': '257', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='238 7. Model Assessment and Selection\\nFIGURE 7.6. The ﬁrst three panels show that the class of lines in the plane\\ncan shatter three points. The last panel shows that this class c annot shatter four\\npoints, as no line will put the hollow points on one side and the solid points on\\nthe other. Hence the VC dimension of the class of straight lines i n the plane is\\nthree. Note that a class of nonlinear curves could shatter four p oints, and hence\\nhas VC dimension greater than three.\\nThe Vapnik–Chervonenkis dimension is a way of measuring the com-\\nplexity of a class of functions by assessing how wiggly its members can\\nbe.\\nTheVC dimension of the class {f(x,α)}is deﬁned to be the\\nlargest number of points (in some conﬁguration) that can be\\nshattered by members of {f(x,α)}.\\nA set of points is said to be shattered by a class of functions if, no matter\\nhow we assign a binary label to each point, a member of the class can\\nperfectly separate them.\\nFigure 7.6 shows that the VC dimension of linear indicator functions\\nin the plane is 3 but not 4, since no four points can be shattered by a\\nset of lines. In general, a linear indicator function in pdimensions has VC\\ndimension p+1, which is also the number of free parameters. On the other\\nhand, it can be shown that the family sin( αx) has inﬁnite VC dimension,\\nas Figure 7.5 suggests. By appropriate choice of α, any set of points can be\\nshattered by this class (Exercise 7.8).\\nSo far we have discussed the VC dimension only of indicator functions,\\nbut this can be extended to real-valued functions. The VC dimension of a\\nclass of real-valued functions {g(x,α)}is deﬁned to be the VC dimension\\nof the indicator class {I(g(x,α)−β >0)}, where βtakes values over the\\nrange of g.\\nOne can use the VC dimension in constructing an estimate of (extra-\\nsample) prediction error; diﬀerent types of results are available. Using the\\nconcept of VC dimension, one can prove results about the optimism of the\\ntraining error when using a class of functions. An example of such a result is\\nthe following. If we ﬁt Ntraining points using a class of functions {f(x,α)}\\nhaving VC dimension h, then with probability at least 1 −ηover training', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1c17853a-e266-403e-9fd3-9bb9729fe78c', embedding=None, metadata={'page_label': '258', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.9 Vapnik–Chervonenkis Dimension 239\\nsets:\\nErrT≤err +ǫ\\n2(\\n1 +√\\n1 +4≤err\\nǫ)\\n(binary classiﬁcation)\\nErrT≤err\\n(1−c√ǫ)+(regression) (7.46)\\nwhere ǫ=a1h[log (a2N/h) + 1]−log (η/4)\\nN,\\nand 0 < a1≤4,0< a2≤2\\nThese bounds hold simultaneously for all members f(x,α), and are taken\\nfrom Cherkassky and Mulier (2007, pages 116–118). They recommend the\\nvalue c= 1. For regression they suggest a1=a2= 1, and for classiﬁcation\\nthey make no recommendation, with a1= 4 and a2= 2 corresponding\\nto worst-case scenarios. They also give an alternative practical bound for\\nregression\\nErrT≤err(\\n1−√\\nρ−ρlogρ+logN\\n2N)−1\\n+(7.47)\\nwithρ=h\\nN, which is free of tuning constants. The bounds suggest that the\\noptimism increases with hand decreases with Nin qualitative agreement\\nwith the AIC correction d/Ngiven is (7.24). However, the results in (7.46)\\nare stronger: rather than giving the expected optimism for each ﬁxed func-\\ntionf(x,α), they give probabilistic upper bounds for all functions f(x,α),\\nand hence allow for searching over the class.\\nVapnik’s structural risk minimization (SRM) approach ﬁts a nested se-\\nquence of models of increasing VC dimensions h1< h2<≤≤≤, and then\\nchooses the model with the smallest value of the upper bound.\\nWe note that upper bounds like the ones in (7.46) are often very loose,\\nbut that doesn’t rule them out as good criteria for model selection, where\\nthe relative (not absolute) size of the test error is important. The main\\ndrawback of this approach is the diﬃculty in calculating the VC dimension\\nof a class of functions. Often only a crude upper bound for VC dimension\\nis obtainable, and this may not be adequate. An example in which the\\nstructural risk minimization program can be successfully carried out is the\\nsupport vector classiﬁer, discussed in Section 12.2.\\n7.9.1 Example (Continued)\\nFigure 7.7 shows the results when AIC, BIC and SRM are used to select\\nthe model size for the examples of Figure 7.3. For the examples labeled KNN,\\nthe model index αrefers to neighborhood size, while for those labeled REGα\\nrefers to subset size. Using each selection method (e.g., AIC) we estimated\\nthe best model ˆ αand found its true prediction error Err T(ˆα) on a test\\nset. For the same training set we computed the prediction error of the best', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7cecf65c-24f1-40e1-ab61-4488a63a8c96', embedding=None, metadata={'page_label': '259', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='240 7. Model Assessment and Selection\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestAIC\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBIC\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestSRM\\nFIGURE 7.7. Boxplots show the distribution of the relative error\\n100×[Err T(ˆα)−min αErrT(α)]/[max αErrT(α)−min αErrT(α)]over the four\\nscenarios of Figure 7.3. This is the error in using the chosen mo del relative to\\nthe best model. There are 100training sets each of size 80represented in each\\nboxplot, with the errors computed on test sets of size 10,000.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bff9fece-d61a-4a68-a38b-27e240bc0c8e', embedding=None, metadata={'page_label': '260', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.10 Cross-Validation 241\\nand worst possible model choices: min αErrT(α) and max αErrT(α). The\\nboxplots show the distribution of the quantity\\n100×ErrT(ˆα)−minαErrT(α)\\nmax αErrT(α)−minαErrT(α),\\nwhich represents the error in using the chosen model relative to the best\\nmodel. For linear regression the model complexity was measured by the\\nnumber of features; as mentioned in Section 7.5, this underestimates the\\ndf, since it does not charge for the search for the best model of that size.\\nThis was also used for the VC dimension of the linear classiﬁer. For k-\\nnearest neighbors, we used the quantity N/k. Under an additive-error re-\\ngression model, this can be justiﬁed as the exact eﬀective degrees of free-\\ndom (Exercise 7.6); we do not know if it corresponds to the VC dimen-\\nsion. We used a1=a2= 1 for the constants in (7.46); the results for SRM\\nchanged with diﬀerent constants, and this choice gave the most favorable re-\\nsults. We repeated the SRM selection using the alternative practical bound\\n(7.47), and got almost identical results. For misclassiﬁcation error w e used\\nˆσε2= [N/(N−d)]≤err(α) for the least restrictive model ( k= 5 for KNN,\\nsincek= 1 results in zero training error). The AIC criterion seems to work\\nwell in all four scenarios, despite the lack of theoretical support with 0–1\\nloss. BIC does nearly as well, while the performance of SRM is mixed.\\n7.10 Cross-Validation\\nProbably the simplest and most widely used method for estimating predic-\\ntion error is cross-validation. This method directly estimates the expected\\nextra-sample error Err = E[ L(Y,ˆf(X))], the average generalization error\\nwhen the method ˆf(X) is applied to an independent test sample from the\\njoint distribution of XandY. As mentioned earlier, we might hope that\\ncross-validation estimates the conditional error, with the training set T\\nheld ﬁxed. But as we will see in Section 7.12, cross-validation typically\\nestimates well only the expected prediction error.\\n7.10.1 K-Fold Cross-Validation\\nIdeally, if we had enough data, we would set aside a validation set and use\\nit to assess the performance of our prediction model. Since data are often\\nscarce, this is usually not possible. To ﬁnesse the problem, K-fold cross-\\nvalidation uses part of the available data to ﬁt the model, and a diﬀerent\\npart to test it. We split the data into Kroughly equal-sized parts; for\\nexample, when K= 5, the scenario looks like this:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='82b1a3cc-a581-4033-8d0f-8d40c18e1e7a', embedding=None, metadata={'page_label': '261', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='242 7. Model Assessment and Selection\\nValidation Train1 2 3 4 5\\nTrain Train Train\\nFor the kth part (third above), we ﬁt the model to the other K−1 parts\\nof the data, and calculate the prediction error of the ﬁtted model when\\npredicting the kth part of the data. We do this for k= 1,2,... ,K and\\ncombine the Kestimates of prediction error.\\nHere are more details. Let κ:{1,... ,N } ↦→ { 1,... ,K }be an indexing\\nfunction that indicates the partition to which observation iis allocated by\\nthe randomization. Denote by ˆf−k(x) the ﬁtted function, computed with\\nthekth part of the data removed. Then the cross-validation estimate of\\nprediction error is\\nCV(ˆf) =1\\nNN∑\\ni=1L(yi,ˆf−κ(i)(xi)). (7.48)\\nTypical choices of Kare 5 or 10 (see below). The case K=Nis known\\nasleave-one-out cross-validation. In this case κ(i) =i, and for the ith\\nobservation the ﬁt is computed using all the data except the ith.\\nGiven a set of models f(x,α) indexed by a tuning parameter α, denote\\nbyˆf−k(x,α) theαth model ﬁt with the kth part of the data removed. Then\\nfor this set of models we deﬁne\\nCV(ˆf,α) =1\\nNN∑\\ni=1L(yi,ˆf−κ(i)(xi,α)). (7.49)\\nThe function CV( ˆf,α) provides an estimate of the test error curve, and we\\nﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model is\\nf(x,ˆα), which we then ﬁt to all the data.\\nIt is interesting to wonder about what quantity K-fold cross-validation\\nestimates. With K= 5 or 10, we might guess that it estimates the ex-\\npected error Err, since the training sets in each fold are quite diﬀerent\\nfrom the original training set. On the other hand, if K=Nwe might\\nguess that cross-validation estimates the conditional error Err T. It turns\\nout that cross-validation only estimates eﬀectively the average error Err,\\nas discussed in Section 7.12.\\nWhat value should we choose for K? With K=N, the cross-validation\\nestimator is approximately unbiased for the true (expected) prediction er-\\nror, but can have high variance because the N“training sets” are so similar\\nto one another. The computational burden is also considerable, requiring\\nNapplications of the learning method. In certain special problems, this\\ncomputation can be done quickly—see Exercises 7.3 and 5.13.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4b8390aa-2baf-451d-9546-b50b9ac561d2', embedding=None, metadata={'page_label': '262', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.10 Cross-Validation 243\\nSize of Training Set1-Err\\n0 50 100 150 2000.0 0.2 0.4 0.6 0.8\\nFIGURE 7.8. Hypothetical learning curve for a classiﬁer on a given task: a\\nplot of 1−Errversus the size of the training set N. With a dataset of 200\\nobservations, 5-fold cross-validation would use training sets of size 160, which\\nwould behave much like the full set. However, with a dataset o f50observations\\nﬁvefold cross-validation would use training sets of size 40, and this would result\\nin a considerable overestimate of prediction error.\\nOn the other hand, with K= 5 say, cross-validation has lower variance.\\nBut bias could be a problem, depending on how the performance of the\\nlearning method varies with the size of the training set. Figure 7.8 shows\\na hypothetical “learning curve” for a classiﬁer on a given task, a plot of\\n1−Err versus the size of the training set N. The performance of the\\nclassiﬁer improves as the training set size increases to 100 observations;\\nincreasing the number further to 200 brings only a small beneﬁt. If our\\ntraining set had 200 observations, ﬁvefold cross-validation would estimat e\\nthe performance of our classiﬁer over training sets of size 160, which from\\nFigure 7.8 is virtually the same as the performance for training set size\\n200. Thus cross-validation would not suﬀer from much bias. However if the\\ntraining set had 50 observations, ﬁvefold cross-validation would estimate\\nthe performance of our classiﬁer over training sets of size 40, and from the\\nﬁgure that would be an underestimate of 1 −Err. Hence as an estimate of\\nErr, cross-validation would be biased upward.\\nTo summarize, if the learning curve has a considerable slope at the given\\ntraining set size, ﬁve- or tenfold cross-validation will overestimate the tr ue\\nprediction error. Whether this bias is a drawback in practice depends on\\nthe objective. On the other hand, leave-one-out cross-validation has low\\nbias but can have high variance. Overall, ﬁve- or tenfold cross-validation\\nare recommended as a good compromise: see Breiman and Spector (1992)\\nand Kohavi (1995).\\nFigure 7.9 shows the prediction error and tenfold cross-validation curve\\nestimated from a single training set, from the scenario in the bottom righ t\\npanel of Figure 7.3. This is a two-class classiﬁcation problem, using a lin-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca5532d7-2699-4960-879a-f518f2e85eee', embedding=None, metadata={'page_label': '263', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='244 7. Model Assessment and Selection\\nSubset Size pMisclassification Error\\n5 10 15 200.0 0.1 0.2 0.3 0.4 0.5 0.6•••\\n••\\n••\\n•\\n•\\n•••••• • •••••\\n•\\n••\\n•\\n•\\n••\\n••• • •• ••• • • •\\nFIGURE 7.9. Prediction error (orange) and tenfold cross-validation curve\\n(blue) estimated from a single training set, from the scenario in the bottom right\\npanel of Figure 7.3.\\near model with best subsets regression of subset size p. Standard error bars\\nare shown, which are the standard errors of the individual misclassiﬁcation\\nerror rates for each of the ten parts. Both curves have minima at p= 10,\\nalthough the CV curve is rather ﬂat beyond 10. Often a “one-standard\\nerror” rule is used with cross-validation, in which we choose the most par-\\nsimonious model whose error is no more than one standard error above\\nthe error of the best model. Here it looks like a model with about p= 9\\npredictors would be chosen, while the true model uses p= 10.\\nGeneralized cross-validation provides a convenient approximation to leave-\\none out cross-validation, for linear ﬁtting under squared-error loss. As de-\\nﬁned in Section 7.6, a linear ﬁtting method is one for which we can write\\nˆy=Sy. (7.50)\\nNow for many linear ﬁtting methods,\\n1\\nNN∑\\ni=1[yi−ˆf−i(xi)]2=1\\nNN∑\\ni=1[yi−ˆf(xi)\\n1−Sii]2\\n, (7.51)\\nwhere Siiis the ith diagonal element of S(see Exercise 7.3). The GCV\\napproximation is\\nGCV( ˆf) =1\\nNN∑\\ni=1[\\nyi−ˆf(xi)\\n1−trace(S)/N]2\\n. (7.52)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='625b9118-c969-4cc1-83c0-4ff3f3f7674b', embedding=None, metadata={'page_label': '264', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.10 Cross-Validation 245\\nThe quantity trace( S) is the eﬀective number of parameters, as deﬁned in\\nSection 7.6.\\nGCV can have a computational advantage in some settings, where the\\ntrace of Scan be computed more easily than the individual elements Sii.\\nIn smoothing problems, GCV can also alleviate the tendency of cross-\\nvalidation to undersmooth. The similarity between GCV and AIC can be\\nseen from the approximation 1 /(1−x)2≈1 + 2x(Exercise 7.7).\\n7.10.2 The Wrong and Right Way to Do Cross-validation\\nConsider a classiﬁcation problem with a large number of predictors, as may\\narise, for example, in genomic or proteomic applications. A typical strategy\\nfor analysis might be as follows:\\n1. Screen the predictors: ﬁnd a subset of “good” predictors that show\\nfairly strong (univariate) correlation with the class labels\\n2. Using just this subset of predictors, build a multivariate classiﬁer.\\n3. Use cross-validation to estimate the unknown tuning parameters and\\nto estimate the prediction error of the ﬁnal model.\\nIs this a correct application of cross-validation? Consider a scenario with\\nN= 50 samples in two equal-sized classes, and p= 5000 quantitative\\npredictors (standard Gaussian) that are independent of the class labels.\\nThe true (test) error rate of any classiﬁer is 50%. We carried out the above\\nrecipe, choosing in step (1) the 100 predictors having highest correlation\\nwith the class labels, and then using a 1-nearest neighbor classiﬁer, based\\non just these 100 predictors, in step (2). Over 50 simulations from this\\nsetting, the average CV error rate was 3%. This is far lower than the true\\nerror rate of 50%.\\nWhat has happened? The problem is that the predictors have an unfair\\nadvantage, as they were chosen in step (1) on the basis of all of the samples .\\nLeaving samples out afterthe variables have been selected does not cor-\\nrectly mimic the application of the classiﬁer to a completely independent\\ntest set, since these predictors “have already seen” the left out samples.\\nFigure 7.10 (top panel) illustrates the problem. We selected the 100 pre-\\ndictors having largest correlation with the class labels over all 50 sampl es.\\nThen we chose a random set of 10 samples, as we would do in ﬁve-fold cross-\\nvalidation, and computed the correlations of the pre-selected 100 predictors\\nwith the class labels over just these 10 samples (top panel). We see that\\nthe correlations average about 0.28, rather than 0, as one might expect.\\nHere is the correct way to carry out cross-validation in this example:\\n1. Divide the samples into Kcross-validation folds (groups) at random.\\n2. For each fold k= 1,2,... ,K', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5bc099f1-ce64-40bb-8446-4d304012843f', embedding=None, metadata={'page_label': '265', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='246 7. Model Assessment and Selection\\nCorrelations of Selected Predictors with OutcomeFrequency\\n−1.0 −0.5 0.0 0.5 1.00 10 20 30Wrong way\\nCorrelations of Selected Predictors with OutcomeFrequency\\n−1.0 −0.5 0.0 0.5 1.00 10 20 30Right way\\nFIGURE 7.10. Cross-validation the wrong and right way: histograms shows th e\\ncorrelation of class labels, in 10randomly chosen samples, with the 100predic-\\ntors chosen using the incorrect (upper red) and correct (lower g reen) versions of\\ncross-validation.\\n(a) Find a subset of “good” predictors that show fairly strong (uni-\\nvariate) correlation with the class labels, using all of the samples\\nexcept those in fold k.\\n(b) Using just this subset of predictors, build a multivariate classi-\\nﬁer, using all of the samples except those in fold k.\\n(c) Use the classiﬁer to predict the class labels for the samples in\\nfoldk.\\nThe error estimates from step 2(c) are then accumulated over all Kfolds, to\\nproduce the cross-validation estimate of prediction error. The lower panel\\nof Figure 7.10 shows the correlations of class labels with the 100 predictor s\\nchosen in step 2(a) of the correct procedure, over the samples in a typical\\nfoldk. We see that they average about zero, as they should.\\nIn general, with a multistep modeling procedure, cross-validation must\\nbe applied to the entire sequence of modeling steps. In particular, samples\\nmust be “left out” before any selection or ﬁltering steps are applied. There\\nis one qualiﬁcation: initial unsupervised screening steps can be done be-\\nfore samples are left out. For example, we could select the 1000 predictors', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='64c5ec14-b500-44fc-b818-e6da1cb6947b', embedding=None, metadata={'page_label': '266', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.10 Cross-Validation 247\\nwith highest variance across all 50 samples, before starting cross-valida tion.\\nSince this ﬁltering does not involve the class labels, it does not give the\\npredictors an unfair advantage.\\nWhile this point may seem obvious to the reader, we have seen this\\nblunder committed many times in published papers in top rank journals.\\nWith the large numbers of predictors that are so common in genomic and\\nother areas, the potential consequences of this error have also increased\\ndramatically; see Ambroise and McLachlan (2002) for a detailed discussion\\nof this issue.\\n7.10.3 Does Cross-Validation Really Work?\\nWe once again examine the behavior of cross-validation in a high-dimensional\\nclassiﬁcation problem. Consider a scenario with N= 20 samples in two\\nequal-sized classes, and p= 500 quantitative predictors that are indepen-\\ndent of the class labels. Once again, the true error rate of any classiﬁer is\\n50%. Consider a simple univariate classiﬁer: a single split that minimizes\\nthe misclassiﬁcation error (a “stump”). Stumps are trees with a single split ,\\nand are used in boosting methods (Chapter 10). A simple argument sug-\\ngests that cross-validation will not work properly in this setting2:\\nFitting to the entire training set, we will ﬁnd a predictor th at\\nsplits the data very well If we do 5-fold cross-validation, thi s\\nsame predictor should split any 4/5ths and 1/5th of the data\\nwell too, and hence its cross-validation error will be small ( much\\nless than 50%) Thus CV does not give an accurate estimate of\\nerror.\\nTo investigate whether this argument is correct, Figure 7.11 shows the\\nresult of a simulation from this setting. There are 500 predictors and 20\\nsamples, in each of two equal-sized classes, with all predictors having a\\nstandard Gaussian distribution. The panel in the top left shows the number\\nof training errors for each of the 500 stumps ﬁt to the training data. We\\nhave marked in color the six predictors yielding the fewest errors. In the top\\nright panel, the training errors are shown for stumps ﬁt to a random 4 /5ths\\npartition of the data (16 samples), and tested on the remaining 1 /5th (four\\nsamples). The colored points indicate the same predictors marked in the\\ntop left panel. We see that the stump for the blue predictor (whose stump\\nwas the best in the top left panel), makes two out of four test errors (50%),\\nand is no better than random.\\nWhat has happened? The preceding argument has ignored the fact that\\nin cross-validation, the model must be completely retrained for each fold\\n2This argument was made to us by a scientist at a proteomics lab meeting, and led\\nto material in this section.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ef32cf3c-348a-482e-a086-981ad1fe1401', embedding=None, metadata={'page_label': '267', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='248 7. Model Assessment and Selection\\n0 100 200 300 400 5002 3 4 5 6 7 8 9\\nPredictorError on Full Training Set\\n1 2 3 4 5 6 7 80 1 2 3 4\\nError on 4/5Error on 1/5\\n−1 0 1 2\\nPredictor 436 (blue)Class Label\\n0 1\\nfull\\n4/5\\n0.0 0.2 0.4 0.6 0.8 1.0\\nCV Errors\\nFIGURE 7.11. Simulation study to investigate the performance of cross vali-\\ndation in a high-dimensional problem where the predictors are independent of the\\nclass labels. The top-left panel shows the number of errors mad e by individual\\nstump classiﬁers on the full training set ( 20observations). The top right panel\\nshows the errors made by individual stumps trained on a random sp lit of the\\ndataset into 4/5ths (16observations) and tested on the remaining 1/5th (4ob-\\nservations). The best performers are depicted by colored dot s in each panel. The\\nbottom left panel shows the eﬀect of re-estimating the split po int in each fold: the\\ncolored points correspond to the four samples in the 4/5ths validation set. The\\nsplit point derived from the full dataset classiﬁes all four sa mples correctly, but\\nwhen the split point is re-estimated on the 4/5ths data (as it should be), it com-\\nmits two errors on the four validation samples. In the bottom right we see the\\noverall result of ﬁve-fold cross-validation applied to 50simulated datasets. The\\naverage error rate is about 50%, as it should be.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7e47881a-68e9-41a2-bd50-1a6c05649cf2', embedding=None, metadata={'page_label': '268', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.11 Bootstrap Methods 249\\nof the process. In the present example, this means that the best predictor\\nand corresponding split point are found from 4 /5ths of the data. The eﬀect\\nof predictor choice is seen in the top right panel. Since the class labels are\\nindependent of the predictors, the performance of a stump on the 4 /5ths\\ntraining data contains no information about its performance in the remain-\\ning 1/5th. The eﬀect of the choice of split point is shown in the bottom left\\npanel. Here we see the data for predictor 436, corresponding to the blue\\ndot in the top left plot. The colored points indicate the 1 /5th data, while\\nthe remaining points belong to the 4 /5ths. The optimal split points for this\\npredictor based on both the full training set and 4 /5ths data are indicated.\\nThe split based on the full data makes no errors on the 1 /5ths data. But\\ncross-validation must base its split on the 4 /5ths data, and this incurs two\\nerrors out of four samples.\\nThe results of applying ﬁve-fold cross-validation to each of 50 simulated\\ndatasets is shown in the bottom right panel. As we would hope, the average\\ncross-validation error is around 50%, which is the true expected prediction\\nerror for this classiﬁer. Hence cross-validation has behaved as it should.\\nOn the other hand, there is considerable variability in the error, underscor-\\ning the importance of reporting the estimated standard error of the CV\\nestimate. See Exercise 7.10 for another variation of this problem.\\n7.11 Bootstrap Methods\\nThe bootstrap is a general tool for assessing statistical accuracy. Firs t we\\ndescribe the bootstrap in general, and then show how it can be used to\\nestimate extra-sample prediction error. As with cross-validation, the boo t-\\nstrap seeks to estimate the conditional error Err T, but typically estimates\\nwell only the expected prediction error Err.\\nSuppose we have a model ﬁt to a set of training data. We denote the\\ntraining set by Z= (z1,z2,... ,z N) where zi= (xi,yi). The basic idea is\\nto randomly draw datasets with replacement from the training data, each\\nsample the same size as the original training set. This is done Btimes\\n(B= 100 say), producing Bbootstrap datasets, as shown in Figure 7.12.\\nThen we reﬁt the model to each of the bootstrap datasets, and examine\\nthe behavior of the ﬁts over the Breplications.\\nIn the ﬁgure, S(Z) is any quantity computed from the data Z, for ex-\\nample, the prediction at some input point. From the bootstrap sampling\\nwe can estimate any aspect of the distribution of S(Z), for example, its\\nvariance,\\nˆVar[S(Z)] =1\\nB−1B∑\\nb=1(S(Z∗b)−¯S∗)2, (7.53)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8196c5cd-e322-4390-9580-ff3e43960e3a', embedding=None, metadata={'page_label': '269', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='250 7. Model Assessment and Selection\\n  Bootstrap\\nBootstrapreplications\\nsamples\\nsampleTrainingZ= (z1,z2,... ,z N)Z∗1Z∗2Z∗BS(Z∗1) S(Z∗2) S(Z∗B)\\nFIGURE 7.12. Schematic of the bootstrap process. We wish to assess the sta-\\ntistical accuracy of a quantity S(Z)computed from our dataset. Btraining sets\\nZ∗b, b= 1, . . . , B each of size Nare drawn with replacement from the original\\ndataset. The quantity of interest S(Z)is computed from each bootstrap training\\nset, and the values S(Z∗1), . . . , S (Z∗B)are used to assess the statistical accuracy\\nofS(Z).\\nwhere ¯S∗=∑\\nbS(Z∗b)/B. Note that ˆVar[S(Z)] can be thought of as a\\nMonte-Carlo estimate of the variance of S(Z) under sampling from the\\nempirical distribution function ˆFfor the data ( z1,z2,... ,z N).\\nHow can we apply the bootstrap to estimate prediction error? One ap-\\nproach would be to ﬁt the model in question on a set of bootstrap samples,\\nand then keep track of how well it predicts the original training set. If\\nˆf∗b(xi) is the predicted value at xi, from the model ﬁtted to the bth boot-\\nstrap dataset, our estimate is\\nˆErrboot=1\\nB1\\nNB∑\\nb=1N∑\\ni=1L(yi,ˆf∗b(xi)). (7.54)\\nHowever, it is easy to see that ˆErrbootdoes not provide a good estimate in\\ngeneral. The reason is that the bootstrap datasets are acting as the training\\nsamples, while the original training set is acting as the test sample, and\\nthese two samples have observations in common. This overlap can make\\noverﬁt predictions look unrealistically good, and is the reason that cross-\\nvalidation explicitly uses non-overlapping data for the training and test\\nsamples. Consider for example a 1-nearest neighbor classiﬁer applied to a\\ntwo-class classiﬁcation problem with the same number of observations in', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6aff4b0a-efaf-4cf9-ada7-df692b48b3de', embedding=None, metadata={'page_label': '270', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.11 Bootstrap Methods 251\\neach class, in which the predictors and class labels are in fact independent.\\nThen the true error rate is 0 .5. But the contributions to the bootstrap\\nestimate ˆErrbootwill be zero unless the observation idoes not appear in the\\nbootstrap sample b. In this latter case it will have the correct expectation\\n0.5. Now\\nPr{observation i∈bootstrap sample b}= 1−(\\n1−1\\nN)N\\n≈1−e−1\\n= 0.632. (7.55)\\nHence the expectation of ˆErrbootis about 0 .5×0.368 = 0 .184, far below\\nthe correct error rate 0 .5.\\nBy mimicking cross-validation, a better bootstrap estimate can be ob-\\ntained. For each observation, we only keep track of predictions from boot-\\nstrap samples not containing that observation. The leave-one-out bootstrap\\nestimate of prediction error is deﬁned by\\nˆErr(1)=1\\nNN∑\\ni=11\\n|C−i|∑\\nb∈C−iL(yi,ˆf∗b(xi)). (7.56)\\nHereC−iis the set of indices of the bootstrap samples bthat do notcontain\\nobservation i, and|C−i|is the number of such samples. In computing ˆErr(1),\\nwe either have to choose Blarge enough to ensure that all of the |C−i|are\\ngreater than zero, or we can just leave out the terms in (7.56) corresponding\\nto|C−i|’s that are zero.\\nThe leave-one out bootstrap solves the overﬁtting problem suﬀered by\\nˆErrboot, but has the training-set-size bias mentioned in the discussion of\\ncross-validation. The average number of distinct observations in each boot-\\nstrap sample is about 0 .632≤N, so its bias will roughly behave like that of\\ntwofold cross-validation. Thus if the learning curve has considerable slope\\nat sample size N/2, the leave-one out bootstrap will be biased upward as\\nan estimate of the true error.\\nThe “ .632 estimator” is designed to alleviate this bias. It is deﬁned by\\nˆErr(.632)=.368≤err +.632≤ˆErr(1). (7.57)\\nThe derivation of the .632 estimator is complex; intuitively it pulls the\\nleave-one out bootstrap estimate down toward the training error rate, and\\nhence reduces its upward bias. The use of the constant .632 relates to (7.55).\\nThe.632 estimator works well in “light ﬁtting” situations, but can break\\ndown in overﬁt ones. Here is an example due to Breiman et al. (1984).\\nSuppose we have two equal-size classes, with the targets independent of\\nthe class labels, and we apply a one-nearest neighbor rule. Then err = 0,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb726ff8-4560-4161-a29b-c335d23469c1', embedding=None, metadata={'page_label': '271', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='252 7. Model Assessment and Selection\\nˆErr(1)= 0.5 and so ˆErr(.632)=.632×0.5 =.316. However, the true error\\nrate is 0.5.\\nOne can improve the .632 estimator by taking into account the amount\\nof overﬁtting. First we deﬁne γto be the no-information error rate : this\\nis the error rate of our prediction rule if the inputs and class labels were\\nindependent. An estimate of γis obtained by evaluating the prediction rule\\non all possible combinations of targets yiand predictors xi′\\nˆγ=1\\nN2N∑\\ni=1N∑\\ni′=1L(yi,ˆf(xi′)). (7.58)\\nFor example, consider the dichotomous classiﬁcation problem: let ˆ p1be\\nthe observed proportion of responses yiequaling 1, and let ˆ q1be the ob-\\nserved proportion of predictions ˆf(xi′) equaling 1. Then\\nˆγ= ˆp1(1−ˆq1) + (1 −ˆp1)ˆq1. (7.59)\\nWith a rule like 1-nearest neighbors for which ˆ q1= ˆp1the value of ˆ γis\\n2ˆp1(1−ˆp1). The multi-category generalization of (7.59) is ˆ γ=∑\\nℓˆpℓ(1−ˆqℓ).\\nUsing this, the relative overﬁtting rate is deﬁned to be\\nˆR=ˆErr(1)−err\\nˆγ−err, (7.60)\\na quantity that ranges from 0 if there is no overﬁtting ( ˆErr(1)=err) to 1\\nif the overﬁtting equals the no-information value ˆ γ−err. Finally, we deﬁne\\nthe “.632+” estimator by\\nˆErr(.632+)= (1 −ˆw)≤err + ˆw≤ˆErr(1)(7.61)\\nwith ˆw=.632\\n1−.368ˆR.\\nThe weight wranges from .632 if ˆR= 0 to 1 if ˆR= 1, so ˆErr(.632+)\\nranges from ˆErr(.632)toˆErr(1). Again, the derivation of (7.61) is compli-\\ncated: roughly speaking, it produces a compromise between the leave-one-\\nout bootstrap and the training error rate that depends on the amount of\\noverﬁtting. For the 1-nearest-neighbor problem with class labels indepen-\\ndent of the inputs, ˆ w=ˆR= 1, so ˆErr(.632+)=ˆErr(1), which has the correct\\nexpectation of 0.5. In other problems with less overﬁtting, ˆErr(.632+)will\\nlie somewhere between err and ˆErr(1).\\n7.11.1 Example (Continued)\\nFigure 7.13 shows the results of tenfold cross-validation and the .632+ bo ot-\\nstrap estimate in the same four problems of Figures 7.7. As in that ﬁgure,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4d1d899d-8063-4025-882f-b40aa7a9a0b4', embedding=None, metadata={'page_label': '272', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.11 Bootstrap Methods 253\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestCross−validation\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBootstrap\\nFIGURE 7.13. Boxplots show the distribution of the relative error\\n100≤[Errˆα−min αErr(α)]/[max αErr(α)−min αErr(α)]over the four scenar-\\nios of Figure 7.3. This is the error in using the chosen model re lative to the best\\nmodel. There are 100training sets represented in each boxplot.\\nFigure 7.13 shows boxplots of 100 ≤[Errˆα−minαErr(α)]/[max αErr(α)−\\nminαErr(α)], the error in using the chosen model relative to the best model.\\nThere are 100 diﬀerent training sets represented in each boxplot. Both mea-\\nsures perform well overall, perhaps the same or slightly worse that the AI C\\nin Figure 7.7.\\nOur conclusion is that for these particular problems and ﬁtting methods,\\nminimization of either AIC, cross-validation or bootstrap yields a model\\nfairly close to the best available. Note that for the purpose of model selec-\\ntion, any of the measures could be biased and it wouldn’t aﬀect things, as\\nlong as the bias did not change the relative performance of the methods.\\nFor example, the addition of a constant to any of the measures would not\\nchange the resulting chosen model. However, for many adaptive, nonlinear\\ntechniques (like trees), estimation of the eﬀective number of parameters is\\nvery diﬃcult. This makes methods like AIC impractical and leaves us with\\ncross-validation or bootstrap as the methods of choice.\\nA diﬀerent question is: how well does each method estimate test error?\\nOn the average the AIC criterion overestimated prediction error of its cho-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4f828ad9-ecaa-4aef-8d8d-7e13632a04c1', embedding=None, metadata={'page_label': '273', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='254 7. Model Assessment and Selection\\nsen model by 38%, 37%, 51%, and 30%, respectively, over the four scenarios,\\nwith BIC performing similarly. In contrast, cross-validation over estimated\\nthe error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the\\nsame. Hence the extra work involved in computing a cross-validation or\\nbootstrap measure is worthwhile, if an accurate estimate of test error i s\\nrequired. With other ﬁtting methods like trees, cross-validation and boot-\\nstrap can underestimate the true error by 10%, because the search for best\\ntree is strongly aﬀected by the validation set. In these situations only a\\nseparate test set will provide an unbiased estimate of test error.\\n7.12 Conditional or Expected Test Error?\\nFigures 7.14 and 7.15 examine the question of whether cross-validation does\\na good job in estimating Err T, the error conditional on a given training set\\nT(expression (7.15) on page 228), as opposed to the expected test error.\\nFor each of 100 training sets generated from the “reg/linear” setting in\\nthe top-right panel of Figure 7.3, Figure 7.14 shows the conditional error\\ncurves Err Tas a function of subset size (top left). The next two panels show\\n10-fold and N-fold cross-validation, the latter also known as leave-one-out\\n(LOO). The thick red curve in each plot is the expected error Err, while\\nthe thick black curves are the expected cross-validation curves. The lower\\nright panel shows how well cross-validation approximates the conditional\\nand expected error.\\nOne might have expected N-fold CV to approximate Err Twell, since it\\nalmost uses the full training sample to ﬁt a new test point. 10-fold CV, on\\nthe other hand, might be expected to estimate Err well, since it averages\\nover somewhat diﬀerent training sets. From the ﬁgure it appears 10-fold\\ndoes a better job than N-fold in estimating Err T, and estimates Err even\\nbetter. Indeed, the similarity of the two black curves with the red curve\\nsuggests both CV curves are approximately unbiased for Err, with 10-fold\\nhaving less variance. Similar trends were reported by Efron (1983).\\nFigure 7.15 shows scatterplots of both 10-fold and N-fold cross-validation\\nerror estimates versus the true conditional error for the 100 simulations.\\nAlthough the scatterplots do not indicate much correlation, the lower right\\npanel shows that for the most part the correlations are negative, a curi-\\nous phenomenon that has been observed before. This negative correlation\\nexplains why neither form of CV estimates Err Twell. The broken lines in\\neach plot are drawn at Err( p), the expected error for the best subset of\\nsizep. We see again that both forms of CV are approximately unbiased for\\nexpected error, but the variation in test error for diﬀerent training sets is\\nquite substantial.\\nAmong the four experimental conditions in 7.3, this “reg/linear” scenario\\nshowed the highest correlation between actual and predicted test error. This', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='31467e34-4aa8-4e73-b0a3-fd66b34de934', embedding=None, metadata={'page_label': '274', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.12 Conditional or Expected Test Error? 255\\n5 10 15 200.1 0.2 0.3 0.4Prediction Error\\nSubset Size pError\\n5 10 15 200.1 0.2 0.3 0.410−Fold CV Error\\nSubset Size pError\\n5 10 15 200.1 0.2 0.3 0.4Leave−One−Out CV Error\\nSubset Size pError\\n5 10 15 200.015 0.025 0.035 0.045Approximation Error\\nSubset Size pMean Absolute DeviationET|CV10−Err|\\nET|CV10−ErrT|\\nET|CVN−ErrT|\\nFIGURE 7.14. Conditional prediction-error ErrT,10-fold cross-validation, and\\nleave-one-out cross-validation curves for a 100simulations from the top-right\\npanel in Figure 7.3. The thick red curve is the expected predict ion error Err,\\nwhile the thick black curves are the expected CV curves ETCV10andETCVN.\\nThe lower-right panel shows the mean absolute deviation of th e CV curves from\\nthe conditional error, ET|CVK−ErrT|forK= 10(blue) and K=N(green),\\nas well as from the expected error ET|CV10−Err|(orange).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='49c826f1-19e3-469a-8a0f-3ca3e3431bfa', embedding=None, metadata={'page_label': '275', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='256 7. Model Assessment and Selection\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 1\\nPrediction ErrorCV Error\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 5\\nPrediction ErrorCV Error\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 10\\nPrediction ErrorCV Error\\n5 10 15 20−0.6 −0.4 −0.2 0.0 0.2\\nSubset SizeCorrelation\\nLeave−one−out\\n10−Fold\\nFIGURE 7.15. Plots of the CV estimates of error versus the true conditional\\nerror for each of the 100training sets, for the simulation setup in the top right\\npanel Figure 7.3. Both 10-fold and leave-one-out CV are depicted in diﬀerent\\ncolors. The ﬁrst three panels correspond to diﬀerent subset size sp, and vertical\\nand horizontal lines are drawn at Err(p). Although there appears to be little cor-\\nrelation in these plots, we see in the lower right panel that fo r the most part the\\ncorrelation is negative .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='030a66a1-214c-4190-a577-7785277217d7', embedding=None, metadata={'page_label': '276', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 257\\nphenomenon also occurs for bootstrap estimates of error, and we would\\nguess, for any other estimate of conditional prediction error.\\nWe conclude that estimation of test error for a particular training set is\\nnot easy in general, given just the data from that same training set. Instead,\\ncross-validation and related methods may provide reasonable estimates of\\ntheexpected error Err.\\nBibliographic Notes\\nKey references for cross-validation are Stone (1974), Stone (1977) and\\nAllen (1974). The AIC was proposed by Akaike (1973), while the BIC\\nwas introduced by Schwarz (1978). Madigan and Raftery (1994) give an\\noverview of Bayesian model selection. The MDL criterion is due to Rissa-\\nnen (1983). Cover and Thomas (1991) contains a good description of coding\\ntheory and complexity. VC dimension is described in Vapnik (1996). Stone\\n(1977) showed that the AIC and leave-one out cross-validation are asymp-\\ntotically equivalent. Generalized cross-validation is described by Golub et\\nal. (1979) and Wahba (1980); a further discussion of the topic may be found\\nin the monograph by Wahba (1990). See also Hastie and Tibshirani (1990),\\nChapter 3. The bootstrap is due to Efron (1979); see Efron and Tibshi-\\nrani (1993) for an overview. Efron (1983) proposes a number of bootst rap\\nestimates of prediction error, including the optimism and .632 estimates.\\nEfron (1986) compares CV, GCV and bootstrap estimates of error rates.\\nThe use of cross-validation and the bootstrap for model selection is stud-\\nied by Breiman and Spector (1992), Breiman (1992), Shao (1996), Zhang\\n(1993) and Kohavi (1995). The .632+ estimator was proposed by Efron\\nand Tibshirani (1997).\\nCherkassky and Ma (2003) published a study on the performance of\\nSRM for model selection in regression, in response to our study of section\\n7.9.1. They complained that we had been unfair to SRM because had not\\napplied it properly. Our response can be found in the same issue of the\\njournal (Hastie et al. (2003)).\\nExercises\\nEx. 7.1 Derive the estimate of in-sample error (7.24).\\nEx. 7.2 For 0–1 loss with Y∈ {0,1}and Pr( Y= 1|x0) =f(x0), show that\\nErr(x0) = Pr( Y̸=ˆG(x0)|X=x0)\\n= Err B(x0) +|2f(x0)−1|Pr(ˆG(x0)̸=G(x0)|X=x0),\\n(7.62)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cd8ede1c-327f-42c0-8418-714543d3b229', embedding=None, metadata={'page_label': '277', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='258 7. Model Assessment and Selection\\nwhere ˆG(x) =I(ˆf(x)>1\\n2),G(x) =I(f(x)>1\\n2) is the Bayes classiﬁer,\\nand Err B(x0) = Pr( Y̸=G(x0)|X=x0), the irreducible Bayes error atx0.\\nUsing the approximation ˆf(x0)∼N(Eˆf(x0),Var(ˆf(x0)), show that\\nPr(ˆG(x0)̸=G(x0)|X=x0)≈Φ(\\nsign(1\\n2−f(x0))(Eˆf(x0)−1\\n2)√\\nVar(ˆf(x0)))\\n.(7.63)\\nIn the above,\\nΦ(t) =1√\\n2π∫t\\n−∞exp(−t2/2)dt,\\nthe cumulative Gaussian distribution function. This is an increasing func-\\ntion, with value 0 at t=−∞and value 1 at t= +∞.\\nWe can think of sign(1\\n2−f(x0))(Eˆf(x0)−1\\n2) as a kind of boundary-\\nbiasterm, as it depends on the true f(x0) only through which side of the\\nboundary (1\\n2) that it lies. Notice also that the bias and variance combine\\nin a multiplicative rather than additive fashion. If E ˆf(x0) is on the same\\nside of1\\n2asf(x0), then the bias is negative, and decreasing the variance\\nwill decrease the misclassiﬁcation error. On the other hand, if E ˆf(x0) is\\non the opposite side of1\\n2tof(x0), then the bias is positive and it pays to\\nincrease the variance! Such an increase will improve the chance that ˆf(x0)\\nfalls on the correct side of1\\n2(Friedman, 1997).\\nEx. 7.3 Letˆf=Sybe a linear smoothing of y.\\n(a) IfSiiis the ith diagonal element of S, show that for Sarising from least\\nsquares projections and cubic smoothing splines, the cross-validated\\nresidual can be written as\\nyi−ˆf−i(xi) =yi−ˆf(xi)\\n1−Sii. (7.64)\\n(b) Use this result to show that |yi−ˆf−i(xi)| ≥ |yi−ˆf(xi)|.\\n(c) Find general conditions on any smoother Sto make result (7.64) hold.\\nEx. 7.4 Consider the in-sample prediction error (7.18) and the training\\nerrorerr in the case of squared-error loss:\\nErrin=1\\nNN∑\\ni=1EY0(Y0\\ni−ˆf(xi))2\\nerr =1\\nNN∑\\ni=1(yi−ˆf(xi))2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a8164fdf-dde6-48a9-b4db-88a6cb56b47f', embedding=None, metadata={'page_label': '278', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 259\\nAdd and subtract f(xi) and E ˆf(xi) in each expression and expand. Hence\\nestablish that the average optimism in the training error is\\n2\\nNN∑\\ni=1Cov(ˆyi,yi),\\nas given in (7.21).\\nEx. 7.5 For a linear smoother ˆy=Sy, show that\\nN∑\\ni=1Cov(ˆyi,yi) = trace( S)σ2\\nε, (7.65)\\nwhich justiﬁes its use as the eﬀective number of parameters.\\nEx. 7.6 Show that for an additive-error model, the eﬀective degrees-of-\\nfreedom for the k-nearest-neighbors regression ﬁt is N/k.\\nEx. 7.7 Use the approximation 1 /(1−x)2≈1+2xto expose the relationship\\nbetween Cp/AIC (7.26) and GCV (7.52), the main diﬀerence being the\\nmodel used to estimate the noise variance σ2\\nε.\\nEx. 7.8 Show that the set of functions {I(sin(αx)>0)}can shatter the\\nfollowing points on the line:\\nz1= 10−1,... ,zℓ= 10−ℓ, (7.66)\\nfor any ℓ. Hence the VC dimension of the class {I(sin(αx)>0)}is inﬁnite.\\nEx. 7.9 For the prostate data of Chapter 3, carry out a best-subset linear\\nregression analysis, as in Table 3.3 (third column from left). Compute t he\\nAIC, BIC, ﬁve- and tenfold cross-validation, and bootstrap .632 estimat es\\nof prediction error. Discuss the results.\\nEx. 7.10 Referring to the example in Section 7.10.3, suppose instead that\\nall of the ppredictors are binary, and hence there is no need to estimate\\nsplit points. The predictors are independent of the class labels as before.\\nThen if pis very large, we can probably ﬁnd a predictor that splits the\\nentire training data perfectly, and hence would split the validation data\\n(one-ﬁfth of data) perfectly as well. This predictor would therefore have\\nzero cross-validation error. Does this mean that cross-validation does not\\nprovide a good estimate of test error in this situation? [This question wa s\\nsuggested by Li Ma.]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ed98a13d-dbeb-4796-b506-be879e6c8ce7', embedding=None, metadata={'page_label': '279', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='260 7. Model Assessment and Selection', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4cf9fd1-0c72-41e0-95b5-27c4c836b5ce', embedding=None, metadata={'page_label': '280', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 261\\nPrinter: Opaque this\\n8\\nModel Inference and Averaging\\n8.1 Introduction\\nFor most of this book, the ﬁtting (learning) of models has been achieved by\\nminimizing a sum of squares for regression, or by minimizing cross-entropy\\nfor classiﬁcation. In fact, both of these minimizations are instances of the\\nmaximum likelihood approach to ﬁtting.\\nIn this chapter we provide a general exposition of the maximum likeli-\\nhood approach, as well as the Bayesian method for inference. The boot-\\nstrap, introduced in Chapter 7, is discussed in this context, and its relation\\nto maximum likelihood and Bayes is described. Finally, we present some\\nrelated techniques for model averaging and improvement, including com-\\nmittee methods, bagging, stacking and bumping.\\n8.2 The Bootstrap and Maximum Likelihood\\nMethods\\n8.2.1 A Smoothing Example\\nThe bootstrap method provides a direct computational way of assessing\\nuncertainty, by sampling from the training data. Here we illustrate the\\nbootstrap in a simple one-dimensional smoothing problem, and show its\\nconnection to maximum likelihood.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6a109e95-3e69-4c72-9d9c-7ad92f7a87fb', embedding=None, metadata={'page_label': '281', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='262 8. Model Inference and Averaging\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.2 0.4 0.6 0.8 1.0\\nxB-spline Basis\\nFIGURE 8.1. (Left panel): Data for smoothing example. (Right panel:) Set of\\nseven B-spline basis functions. The broken vertical lines indicate the p lacement\\nof the three knots.\\nDenote the training data by Z={z1,z2,... ,z N}, with zi= (xi,yi),\\ni= 1,2,... ,N . Here xiis a one-dimensional input, and yithe outcome,\\neither continuous or categorical. As an example, consider the N= 50 data\\npoints shown in the left panel of Figure 8.1.\\nSuppose we decide to ﬁt a cubic spline to the data, with three knots\\nplaced at the quartiles of the Xvalues. This is a seven-dimensional lin-\\near space of functions, and can be represented, for example, by a linear\\nexpansion of B-spline basis functions (see Section 5.9.2):\\nθ(x) =7∑\\nj=1βjhj(x). (8.1)\\nHere the hj(x),j= 1,2,... ,7 are the seven functions shown in the right\\npanel of Figure 8.1. We can think of θ(x) as representing the conditional\\nmean E( Y|X=x).\\nLetHbe the N×7 matrix with ijth element hj(xi). The usual estimate\\nofβ, obtained by minimizing the squared error over the training set, is\\ngiven by\\nˆβ= (HTH)−1HTy. (8.2)\\nThe corresponding ﬁt ˆ θ(x) =∑7\\nj=1ˆβjhj(x) is shown in the top left panel\\nof Figure 8.2.\\nThe estimated covariance matrix of ˆβis\\nˆVar(ˆβ) = (HTH)−1ˆσ2, (8.3)\\nwhere we have estimated the noise variance by ˆ σ2=∑N\\ni=1(yi−ˆθ(xi))2/N.\\nLetting h(x)T= (h1(x),h2(x),... ,h 7(x)), the standard error of a predic-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f3d33815-4d8a-4f72-b5ac-9af4b8369187', embedding=None, metadata={'page_label': '282', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.2 The Bootstrap and Maximum Likelihood Methods 263\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\nxy\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\nFIGURE 8.2. (Top left:) B-spline smooth of data. (Top right:) B-spline smooth\\nplus and minus 1.96×standard error bands. (Bottom left:) Ten bootstrap repli-\\ncates of the B-spline smooth. (Bottom right:) B-spline smooth with 95% standard\\nerror bands computed from the bootstrap distribution.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e75032da-6f21-4c3a-abae-8247b3a51f46', embedding=None, metadata={'page_label': '283', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='264 8. Model Inference and Averaging\\ntion ˆθ(x) =h(x)Tˆβis\\nˆse[ˆθ(x)] = [h(x)T(HTH)−1h(x)]1\\n2ˆσ. (8.4)\\nIn the top right panel of Figure 8.2 we have plotted ˆ θ(x)±1.96≤ˆse[ˆθ(x)].\\nSince 1.96 is the 97.5% point of the standard normal distribution, these\\nrepresent approximate 100 −2×2.5% = 95% pointwise conﬁdence bands\\nforθ(x).\\nHere is how we could apply the bootstrap in this example. We draw B\\ndatasets each of size N= 50 with replacement from our training data, the\\nsampling unit being the pair zi= (xi,yi). To each bootstrap dataset Z∗\\nwe ﬁt a cubic spline ˆ θ∗(x); the ﬁts from ten such samples are shown in the\\nbottom left panel of Figure 8.2. Using B= 200 bootstrap samples, we can\\nform a 95% pointwise conﬁdence band from the percentiles at each x: we\\nﬁnd the 2 .5%×200 = ﬁfth largest and smallest values at each x. These are\\nplotted in the bottom right panel of Figure 8.2. The bands look similar to\\nthose in the top right, being a little wider at the endpoints.\\nThere is actually a close connection between the least squares estimates\\n(8.2) and (8.3), the bootstrap, and maximum likelihood. Suppose we further\\nassume that the model errors are Gaussian,\\nY=θ(X) +ε;ε∼N(0,σ2),\\nθ(x) =7∑\\nj=1βjhj(x). (8.5)\\nThe bootstrap method described above, in which we sample with re-\\nplacement from the training data, is called the nonparametric bootstrap .\\nThis really means that the method is “model-free,” since it uses the raw\\ndata, not a speciﬁc parametric model, to generate new datasets. Consider\\na variation of the bootstrap, called the parametric bootstrap , in which we\\nsimulate new responses by adding Gaussian noise to the predicted values:\\ny∗\\ni= ˆθ(xi) +ε∗\\ni;ε∗\\ni∼N(0,ˆσ2);i= 1,2,... ,N. (8.6)\\nThis process is repeated Btimes, where B= 200 say. The resulting boot-\\nstrap datasets have the form ( x1,y∗\\n1),... ,(xN,y∗\\nN) and we recompute the\\nB-spline smooth on each. The conﬁdence bands from this method will ex-\\nactly equal the least squares bands in the top right panel, as the number of\\nbootstrap samples goes to inﬁnity. A function estimated from a bootstrap\\nsample y∗is given by ˆ θ∗(x) =h(x)T(HTH)−1HTy∗, and has distribution\\nˆθ∗(x)∼N(ˆθ(x),h(x)T(HTH)−1h(x)ˆσ2). (8.7)\\nNotice that the mean of this distribution is the least squares estimate, and\\nthe standard deviation is the same as the approximate formula (8.4).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='81f5718b-41aa-41e1-96d6-25fb2fccecaf', embedding=None, metadata={'page_label': '284', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.2 The Bootstrap and Maximum Likelihood Methods 265\\n8.2.2 Maximum Likelihood Inference\\nIt turns out that the parametric bootstrap agrees with least squares in the\\nprevious example because the model (8.5) has additive Gaussian errors. In\\ngeneral, the parametric bootstrap agrees not with least squares but with\\nmaximum likelihood, which we now review.\\nWe begin by specifying a probability density or probability mass function\\nfor our observations\\nzi∼gθ(z). (8.8)\\nIn this expression θrepresents one or more unknown parameters that gov-\\nern the distribution of Z. This is called a parametric model forZ. As an\\nexample, if Zhas a normal distribution with mean θand variance σ2, then\\nθ= (θ,σ2), (8.9)\\nand\\ngθ(z) =1√\\n2πσe−1\\n2(z−θ)2/σ2. (8.10)\\nMaximum likelihood is based on the likelihood function , given by\\nL(θ;Z) =N∏\\ni=1gθ(zi), (8.11)\\nthe probability of the observed data under the model gθ. The likelihood is\\ndeﬁned only up to a positive multiplier, which we have taken to be one.\\nWe think of L(θ;Z) as a function of θ, with our data Zﬁxed.\\nDenote the logarithm of L(θ;Z) by\\nℓ(θ;Z) =N∑\\ni=1ℓ(θ;zi)\\n=N∑\\ni=1loggθ(zi), (8.12)\\nwhich we will sometimes abbreviate as ℓ(θ). This expression is called the\\nlog-likelihood, and each value ℓ(θ;zi) = log gθ(zi) is called a log-likelihood\\ncomponent. The method of maximum likelihood chooses the value θ=ˆθ\\nto maximize ℓ(θ;Z).\\nThe likelihood function can be used to assess the precision of ˆθ. We need\\na few more deﬁnitions. The score function is deﬁned by\\n˙ℓ(θ;Z) =N∑\\ni=1˙ℓ(θ;zi), (8.13)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f1c5f80f-ddad-4d23-b7cd-096407ab4262', embedding=None, metadata={'page_label': '285', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='266 8. Model Inference and Averaging\\nwhere ˙ℓ(θ;zi) =∂ℓ(θ;zi)/∂θ. Assuming that the likelihood takes its maxi-\\nmum in the interior of the parameter space, ˙ℓ(ˆθ;Z) = 0. The information\\nmatrix is\\nI(θ) =−N∑\\ni=1∂2ℓ(θ;zi)\\n∂θ∂θT. (8.14)\\nWhenI(θ) is evaluated at θ=ˆθ, it is often called the observed information .\\nTheFisher information (or expected information) is\\ni(θ) = E θ[I(θ)]. (8.15)\\nFinally, let θ0denote the true value of θ.\\nA standard result says that the sampling distribution of the maximum\\nlikelihood estimator has a limiting normal distribution\\nˆθ→N(θ0,i(θ0)−1), (8.16)\\nasN→ ∞. Here we are independently sampling from gθ0(z). This suggests\\nthat the sampling distribution of ˆθmay be approximated by\\nN(ˆθ,i(ˆθ)−1) orN(ˆθ,I(ˆθ)−1), (8.17)\\nwhere ˆθrepresents the maximum likelihood estimate from the observed\\ndata.\\nThe corresponding estimates for the standard errors of ˆθjare obtained\\nfrom\\n√\\ni(ˆθ)−1\\njj and√\\nI(ˆθ)−1\\njj. (8.18)\\nConﬁdence points for θjcan be constructed from either approximation\\nin (8.17). Such a conﬁdence point has the form\\nˆθj−z(1−α)≤√\\ni(ˆθ)−1\\njj or ˆθj−z(1−α)≤√\\nI(ˆθ)−1\\njj,\\nrespectively, where z(1−α)is the 1 −αpercentile of the standard normal\\ndistribution. More accurate conﬁdence intervals can be derived from the\\nlikelihood function, by using the chi-squared approximation\\n2[ℓ(ˆθ)−ℓ(θ0)]∼χ2\\np, (8.19)\\nwhere pis the number of components in θ. The resulting 1 −2αconﬁ-\\ndence interval is the set of all θ0such that 2[ ℓ(ˆθ)−ℓ(θ0)]≤χ2\\np(1−2α),\\nwhere χ2\\np(1−2α)is the 1 −2αpercentile of the chi-squared distribution with\\npdegrees of freedom.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91bc783c-9c74-4a59-b11b-bb095c6f5a27', embedding=None, metadata={'page_label': '286', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.3 Bayesian Methods 267\\nLet’s return to our smoothing example to see what maximum likelihood\\nyields. The parameters are θ= (β,σ2). The log-likelihood is\\nℓ(θ) =−N\\n2logσ22π−1\\n2σ2N∑\\ni=1(yi−h(xi)Tβ)2. (8.20)\\nThe maximum likelihood estimate is obtained by setting ∂ℓ/∂β = 0 and\\n∂ℓ/∂σ2= 0, giving\\nˆβ= (HTH)−1HTy,\\nˆσ2=1\\nN∑\\n(yi−ˆθ(xi))2,(8.21)\\nwhich are the same as the usual estimates given in (8.2) and below (8.3).\\nThe information matrix for θ= (β,σ2) is block-diagonal, and the block\\ncorresponding to βis\\nI(β) = (HTH)/σ2, (8.22)\\nso that the estimated variance ( HTH)−1ˆσ2agrees with the least squares\\nestimate (8.3).\\n8.2.3 Bootstrap versus Maximum Likelihood\\nIn essence the bootstrap is a computer implementation of nonparametric or\\nparametric maximum likelihood. The advantage of the bootstrap over the\\nmaximum likelihood formula is that it allows us to compute maximum like-\\nlihood estimates of standard errors and other quantities in settings where\\nno formulas are available.\\nIn our example, suppose that we adaptively choose by cross-validation\\nthe number and position of the knots that deﬁne the B-splines, rather\\nthan ﬁx them in advance. Denote by λthe collection of knots and their\\npositions. Then the standard errors and conﬁdence bands should account\\nfor the adaptive choice of λ, but there is no way to do this analytically.\\nWith the bootstrap, we compute the B-spline smooth with an adaptive\\nchoice of knots for each bootstrap sample. The percentiles of the resulting\\ncurves capture the variability from both the noise in the targets as well as\\nthat from ˆλ. In this particular example the conﬁdence bands (not shown)\\ndon’t look much diﬀerent than the ﬁxed λbands. But in other problems,\\nwhere more adaptation is used, this can be an important eﬀect to capture.\\n8.3 Bayesian Methods\\nIn the Bayesian approach to inference, we specify a sampling model Pr( Z|θ)\\n(density or probability mass function) for our data given the parameters,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bc77c5dc-8c1b-44fd-a912-decb4f233169', embedding=None, metadata={'page_label': '287', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='268 8. Model Inference and Averaging\\nand a prior distribution for the parameters Pr( θ) reﬂecting our knowledge\\nabout θbefore we see the data. We then compute the posterior distribution\\nPr(θ|Z) =Pr(Z|θ)≤Pr(θ)∫\\nPr(Z|θ)≤Pr(θ)dθ, (8.23)\\nwhich represents our updated knowledge about θafter we see the data. To\\nunderstand this posterior distribution, one might draw samples from it or\\nsummarize by computing its mean or mode. The Bayesian approach diﬀers\\nfrom the standard (“frequentist”) method for inference in its use of a prior\\ndistribution to express the uncertainty present before seeing the data, and\\nto allow the uncertainty remaining after seeing the data to be expressed in\\nthe form of a posterior distribution.\\nThe posterior distribution also provides the basis for predicting the values\\nof a future observation znew, via the predictive distribution :\\nPr(znew|Z) =∫\\nPr(znew|θ)≤Pr(θ|Z)dθ. (8.24)\\nIn contrast, the maximum likelihood approach would use Pr( znew|ˆθ),\\nthe data density evaluated at the maximum likelihood estimate, to predict\\nfuture data. Unlike the predictive distribution (8.24), this does not account\\nfor the uncertainty in estimating θ.\\nLet’s walk through the Bayesian approach in our smoothing example.\\nWe start with the parametric model given by equation (8.5), and assume\\nfor the moment that σ2is known. We assume that the observed feature\\nvalues x1,x2,... ,x Nare ﬁxed, so that the randomness in the data comes\\nsolely from yvarying around its mean θ(x).\\nThe second ingredient we need is a prior distribution. Distributions on\\nfunctions are fairly complex entities: one approach is to use a Gaussian\\nprocess prior in which we specify the prior covariance between any two\\nfunction values θ(x) and θ(x′) (Wahba, 1990; Neal, 1996).\\nHere we take a simpler route: by considering a ﬁnite B-spline basis for\\nθ(x), we can instead provide a prior for the coeﬃcients β, and this implicitly\\ndeﬁnes a prior for θ(x). We choose a Gaussian prior centered at zero\\nβ∼N(0,τΣ) (8.25)\\nwith the choices of the prior correlation matrix Σand variance τto be\\ndiscussed below. The implicit process prior for θ(x) is hence Gaussian,\\nwith covariance kernel\\nK(x,x′) = cov[ θ(x),θ(x′)]\\n=τ≤h(x)TΣh(x′). (8.26)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f86bf6ee-f48e-4eba-be94-d2640d7e15c2', embedding=None, metadata={'page_label': '288', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.3 Bayesian Methods 269\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-3 -2 -1 0 1 2 3θ(x)\\nx\\nFIGURE 8.3. Smoothing example: Ten draws from the Gaussian prior distri-\\nbution for the function θ(x).\\nThe posterior distribution for βis also Gaussian, with mean and covariance\\nE(β|Z) =(\\nHTH+σ2\\nτΣ−1)−1\\nHTy,\\ncov(β|Z) =(\\nHTH+σ2\\nτΣ−1)−1\\nσ2,(8.27)\\nwith the corresponding posterior values for θ(x),\\nE(θ(x)|Z) =h(x)T(\\nHTH+σ2\\nτΣ−1)−1\\nHTy,\\ncov[θ(x),θ(x′)|Z] =h(x)T(\\nHTH+σ2\\nτΣ−1)−1\\nh(x′)σ2.(8.28)\\nHow do we choose the prior correlation matrix Σ? In some settings the\\nprior can be chosen from subject matter knowledge about the parameters.\\nHere we are willing to say the function θ(x) should be smooth, and have\\nguaranteed this by expressing θin a smooth low-dimensional basis of B-\\nsplines. Hence we can take the prior correlation matrix to be the identity\\nΣ=I. When the number of basis functions is large, this might not be suf-\\nﬁcient, and additional smoothness can be enforced by imposing restrictions\\nonΣ; this is exactly the case with smoothing splines (Section 5.8.1).\\nFigure 8.3 shows ten draws from the corresponding prior for θ(x). To\\ngenerate posterior values of the function θ(x), we generate values β′from its\\nposterior (8.27), giving corresponding posterior value θ′(x) =∑7\\n1β′\\njhj(x).\\nTen such posterior curves are shown in Figure 8.4. Two diﬀerent values\\nwere used for the prior variance τ, 1 and 1000. Notice how similar the\\nright panel looks to the bootstrap distribution in the bottom left panel', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='908f9b67-8e8a-4eef-800c-777e421dc5ee', embedding=None, metadata={'page_label': '289', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='270 8. Model Inference and Averaging\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••θ(x)θ(x)\\nx xτ= 1 τ= 1000\\nFIGURE 8.4. Smoothing example: Ten draws from the posterior distribution\\nfor the function θ(x), for two diﬀerent values of the prior variance τ. The purple\\ncurves are the posterior means.\\nof Figure 8.2 on page 263. This similarity is no accident. As τ→ ∞, the\\nposterior distribution (8.27) and the bootstrap distribution (8.7) co incide.\\nOn the other hand, for τ= 1, the posterior curves θ(x) in the left panel\\nof Figure 8.4 are smoother than the bootstrap curves, because we have\\nimposed more prior weight on smoothness.\\nThe distribution (8.25) with τ→ ∞ is called a noninformative prior for\\nθ. In Gaussian models, maximum likelihood and parametric bootstrap anal -\\nyses tend to agree with Bayesian analyses that use a noninformative prior\\nfor the free parameters. These tend to agree, because with a constant prior,\\nthe posterior distribution is proportional to the likelihood. This corresp on-\\ndence also extends to the nonparametric case, where the nonparametric\\nbootstrap approximates a noninformative Bayes analysis; Section 8.4 has\\nthe details.\\nWe have, however, done some things that are not proper from a Bayesian\\npoint of view. We have used a noninformative (constant) prior for σ2and\\nreplaced it with the maximum likelihood estimate ˆ σ2in the posterior. A\\nmore standard Bayesian analysis would also put a prior on σ(typically\\ng(σ)∝1/σ), calculate a joint posterior for θ(x) and σ, and then integrate\\noutσ, rather than just extract the maximum of the posterior distribution\\n(“MAP” estimate).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68b8c5f2-bb3e-441b-8c9b-93fea048100a', embedding=None, metadata={'page_label': '290', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.4 Relationship Between the Bootstrap and Bayesian Inference 271\\n8.4 Relationship Between the Bootstrap and\\nBayesian Inference\\nConsider ﬁrst a very simple example, in which we observe a single obser-\\nvation zfrom a normal distribution\\nz∼N(θ,1). (8.29)\\nTo carry out a Bayesian analysis for θ, we need to specify a prior. The\\nmost convenient and common choice would be θ∼N(0,τ) giving posterior\\ndistribution\\nθ|z∼N(z\\n1 + 1/τ,1\\n1 + 1/τ)\\n. (8.30)\\nNow the larger we take τ, the more concentrated the posterior becomes\\naround the maximum likelihood estimate ˆθ=z. In the limit as τ→ ∞ we\\nobtain a noninformative (constant) prior, and the posterior distribution is\\nθ|z∼N(z,1). (8.31)\\nThis is the same as a parametric bootstrap distribution in which we gen-\\nerate bootstrap values z∗from the maximum likelihood estimate of the\\nsampling density N(z,1).\\nThere are three ingredients that make this correspondence work:\\n1. The choice of noninformative prior for θ.\\n2. The dependence of the log-likelihood ℓ(θ;Z) on the data Zonly\\nthrough the maximum likelihood estimate ˆθ. Hence we can write the\\nlog-likelihood as ℓ(θ;ˆθ).\\n3. The symmetry of the log-likelihood in θandˆθ, that is, ℓ(θ;ˆθ) =\\nℓ(ˆθ;θ) + constant.\\nProperties (2) and (3) essentially only hold for the Gaussian distribu-\\ntion. However, they also hold approximately for the multinomial distribu-\\ntion, leading to a correspondence between the nonparametric bootstrap\\nand Bayes inference, which we outline next.\\nAssume that we have a discrete sample space with Lcategories. Let wjbe\\nthe probability that a sample point falls in category j, and ˆwjthe observed\\nproportion in category j. Letw= (w1,w2,... ,w L),ˆw= ( ˆw1,ˆw2,... ,ˆwL).\\nDenote our estimator by S( ˆw); take as a prior distribution for wa sym-\\nmetric Dirichlet distribution with parameter a:\\nw∼DiL(a1), (8.32)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='025f53f2-b597-42b8-9338-2163cc5a6bf3', embedding=None, metadata={'page_label': '291', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='272 8. Model Inference and Averaging\\nthat is, the prior probability mass function is proportional to∏L\\nℓ=1wa−1\\nℓ.\\nThen the posterior density of wis\\nw∼DiL(a1 +Nˆw), (8.33)\\nwhere Nis the sample size. Letting a→0 to obtain a noninformative prior\\ngives\\nw∼DiL(Nˆw). (8.34)\\nNow the bootstrap distribution, obtained by sampling with replacement\\nfrom the data, can be expressed as sampling the category proportions from\\na multinomial distribution. Speciﬁcally,\\nNˆw∗∼Mult( N,ˆw), (8.35)\\nwhere Mult( N,ˆw) denotes a multinomial distribution, having probability\\nmass function(N\\nNˆw∗\\n1,...,N ˆw∗\\nL)∏ˆwNˆw∗\\nℓ\\nℓ. This distribution is similar to the pos-\\nterior distribution above, having the same support, same mean, and nearly\\nthe same covariance matrix. Hence the bootstrap distribution of S( ˆw∗) will\\nclosely approximate the posterior distribution of S(w).\\nIn this sense, the bootstrap distribution represents an (approximate)\\nnonparametric, noninformative posterior distribution for our parameter.\\nBut this bootstrap distribution is obtained painlessly—without having to\\nformally specify a prior and without having to sample from the posterior\\ndistribution. Hence we might think of the bootstrap distribution as a “poor\\nman’s” Bayes posterior. By perturbing the data, the bootstrap approxi-\\nmates the Bayesian eﬀect of perturbing the parameters, and is typically\\nmuch simpler to carry out.\\n8.5 The EM Algorithm\\nThe EM algorithm is a popular tool for simplifying diﬃcult maximum\\nlikelihood problems. We ﬁrst describe it in the context of a simple mixture\\nmodel.\\n8.5.1 Two-Component Mixture Model\\nIn this section we describe a simple mixture model for density estimation,\\nand the associated EM algorithm for carrying out maximum likelihood\\nestimation. This has a natural connection to Gibbs sampling methods for\\nBayesian inference. Mixture models are discussed and demonstrated in sev-\\neral other parts of the book, in particular Sections 6.8, 12.7 and 13.2.3.\\nThe left panel of Figure 8.5 shows a histogram of the 20 ﬁctitious data\\npoints in Table 8.1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='17a7683c-8814-4fb4-8080-471756abddf4', embedding=None, metadata={'page_label': '292', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.5 The EM Algorithm 273\\n0 2 4 60.0 0.2 0.4 0.6 0.8 1.0\\ny ydensity\\n0 2 4 60.0 0.2 0.4 0.6 0.8 1.0• •• •••••••\\n•\\n•\\n••••• •• •\\nFIGURE 8.5. Mixture example. (Left panel:) Histogram of data. (Right panel: )\\nMaximum likelihood ﬁt of Gaussian densities (solid red) and resp onsibility (dotted\\ngreen) of the left component density for observation y, as a function of y.\\nTABLE 8.1. Twenty ﬁctitious data points used in the two-component mixture\\nexample in Figure 8.5.\\n-0.39 0.12 0.94 1.67 1.76 2.44 3.72 4.28 4.92 5.53\\n0.06 0.48 1.01 1.68 1.80 3.25 4.12 4.60 5.28 6.22\\nWe would like to model the density of the data points, and due to the\\napparent bi-modality, a Gaussian distribution would not be appropriate.\\nThere seems to be two separate underlying regimes, so instead we model\\nYas a mixture of two normal distributions:\\nY1∼N(θ1,σ2\\n1),\\nY2∼N(θ2,σ2\\n2), (8.36)\\nY= (1 −∆)≤Y1+ ∆≤Y2,\\nwhere ∆ ∈ {0,1}with Pr(∆ = 1) = π. This generative representation is\\nexplicit: generate a ∆ ∈ {0,1}with probability π, and then depending on\\nthe outcome, deliver either Y1orY2. Letφθ(x) denote the normal density\\nwith parameters θ= (θ,σ2). Then the density of Yis\\ngY(y) = (1 −π)φθ1(y) +πφθ2(y). (8.37)\\nNow suppose we wish to ﬁt this model to the data in Figure 8.5 by maxi-\\nmum likelihood. The parameters are\\nθ= (π,θ1,θ2) = (π,θ1,σ2\\n1,θ2,σ2\\n2). (8.38)\\nThe log-likelihood based on the Ntraining cases is\\nℓ(θ;Z) =N∑\\ni=1log[(1 −π)φθ1(yi) +πφθ2(yi)]. (8.39)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f8407071-56d9-4ca6-82d1-46964c524114', embedding=None, metadata={'page_label': '293', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='274 8. Model Inference and Averaging\\nDirect maximization of ℓ(θ;Z) is quite diﬃcult numerically, because of\\nthe sum of terms inside the logarithm. There is, however, a simpler ap-\\nproach. We consider unobserved latent variables ∆ itaking values 0 or 1 as\\nin (8.36): if ∆ i= 1 then Yicomes from model 2, otherwise it comes from\\nmodel 1. Suppose we knew the values of the ∆ i’s. Then the log-likelihood\\nwould be\\nℓ0(θ;Z,∆) =N∑\\ni=1[(1−∆i)logφθ1(yi) + ∆ ilogφθ2(yi)]\\n+N∑\\ni=1[(1−∆i)log(1 −π) + ∆ ilogπ],(8.40)\\nand the maximum likelihood estimates of θ1andσ2\\n1would be the sample\\nmean and variance for those data with ∆ i= 0, and similarly those for θ2\\nandσ2\\n2would be the sample mean and variance of the data with ∆ i= 1.\\nThe estimate of πwould be the proportion of ∆ i= 1.\\nSince the values of the ∆ i’s are actually unknown, we proceed in an\\niterative fashion, substituting for each ∆ iin (8.40) its expected value\\nγi(θ) = E(∆ i|θ,Z) = Pr(∆ i= 1|θ,Z), (8.41)\\nalso called the responsibility of model 2 for observation i. We use a proce-\\ndure called the EM algorithm, given in Algorithm 8.1 for the special case of\\nGaussian mixtures. In the expectation step, we do a soft assignment of each\\nobservation to each model: the current estimates of the parameters are used\\nto assign responsibilities according to the relative density of the training\\npoints under each model. In the maximization step, these responsibilities\\nare used in weighted maximum-likelihood ﬁts to update the estimates of\\nthe parameters.\\nA good way to construct initial guesses for ˆ θ1and ˆθ2is simply to choose\\ntwo of the yiat random. Both ˆ σ2\\n1and ˆσ2\\n2can be set equal to the overall\\nsample variance∑N\\ni=1(yi−¯y)2/N. The mixing proportion ˆ πcan be started\\nat the value 0 .5.\\nNote that the actual maximizer of the likelihood occurs when we put a\\nspike of inﬁnite height at any one data point, that is, ˆ θ1=yifor some\\niand ˆσ2\\n1= 0. This gives inﬁnite likelihood, but is not a useful solution.\\nHence we are actually looking for a good local maximum of the likelihood,\\none for which ˆ σ2\\n1,ˆσ2\\n2>0. To further complicate matters, there can be\\nmore than one local maximum having ˆ σ2\\n1,ˆσ2\\n2>0. In our example, we\\nran the EM algorithm with a number of diﬀerent initial guesses for the\\nparameters, all having ˆ σ2\\nk>0.5, and chose the run that gave us the highest\\nmaximized likelihood. Figure 8.6 shows the progress of the EM algorithm in\\nmaximizing the log-likelihood. Table 8.2 shows ˆ π=∑\\niˆγi/N, the maximum\\nlikelihood estimate of the proportion of observations in class 2, at sel ected\\niterations of the EM procedure.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e62fea64-531f-4f86-b424-b957d93bdb87', embedding=None, metadata={'page_label': '294', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.5 The EM Algorithm 275\\nAlgorithm 8.1 EM Algorithm for Two-component Gaussian Mixture.\\n1. Take initial guesses for the parameters ˆ θ1,ˆσ2\\n1,ˆθ2,ˆσ2\\n2,ˆπ(see text).\\n2.Expectation Step : compute the responsibilities\\nˆγi=ˆπφˆθ2(yi)\\n(1−ˆπ)φˆθ1(yi) + ˆπφˆθ2(yi), i= 1,2,... ,N. (8.42)\\n3.Maximization Step : compute the weighted means and variances:\\nˆθ1=∑N\\ni=1(1−ˆγi)yi∑N\\ni=1(1−ˆγi), ˆσ2\\n1=∑N\\ni=1(1−ˆγi)(yi−ˆθ1)2\\n∑N\\ni=1(1−ˆγi),\\nˆθ2=∑N\\ni=1ˆγiyi∑N\\ni=1ˆγi, ˆσ2\\n2=∑N\\ni=1ˆγi(yi−ˆθ2)2\\n∑N\\ni=1ˆγi,\\nand the mixing probability ˆ π=∑N\\ni=1ˆγi/N.\\n4. Iterate steps 2 and 3 until convergence.\\nTABLE 8.2. Selected iterations of the EM algorithm for mixture example.\\nIteration ˆ π\\n1 0.485\\n5 0.493\\n10 0.523\\n15 0.544\\n20 0.546\\nThe ﬁnal maximum likelihood estimates are\\nˆθ1= 4.62, ˆσ2\\n1= 0.87,\\nˆθ2= 1.06, ˆσ2\\n2= 0.77,\\nˆπ= 0.546.\\nThe right panel of Figure 8.5 shows the estimated Gaussian mixture density\\nfrom this procedure (solid red curve), along with the responsibilities (dotted\\ngreen curve). Note that mixtures are also useful for supervised learning; in\\nSection 6.7 we show how the Gaussian mixture model leads to a version of\\nradial basis functions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dfa9db0b-5388-47ec-91b7-c94d54430094', embedding=None, metadata={'page_label': '295', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='276 8. Model Inference and Averaging\\nIterationObserved Data Log-likelihood\\n5 10 15 20-44 -43 -42 -41 -40 -39\\nooooooooooooooo o o o o o\\nFIGURE 8.6. EM algorithm: observed data log-likelihood as a function of t he\\niteration number.\\n8.5.2 The EM Algorithm in General\\nThe above procedure is an example of the EM (or Baum–Welch) algorithm\\nfor maximizing likelihoods in certain classes of problems. These problems\\nare ones for which maximization of the likelihood is diﬃcult, but made\\neasier by enlarging the sample with latent (unobserved) data. This is called\\ndata augmentation . Here the latent data are the model memberships ∆ i.\\nIn other problems, the latent data are actual data that should have been\\nobserved but are missing.\\nAlgorithm 8.2 gives the general formulation of the EM algorithm. Our\\nobserved data is Z, having log-likelihood ℓ(θ;Z) depending on parameters\\nθ. The latent or missing data is Zm, so that the complete data is T=\\n(Z,Zm) with log-likelihood ℓ0(θ;T),ℓ0based on the complete density. In\\nthe mixture problem ( Z,Zm) = (y,∆), and ℓ0(θ;T) is given in (8.40).\\nIn our mixture example, E( ℓ0(θ′;T)|Z,ˆθ(j)) is simply (8.40) with the ∆ i\\nreplaced by the responsibilities ˆ γi(ˆθ), and the maximizers in step 3 are just\\nweighted means and variances.\\nWe now give an explanation of why the EM algorithm works in general.\\nSince\\nPr(Zm|Z,θ′) =Pr(Zm,Z|θ′)\\nPr(Z|θ′), (8.44)\\nwe can write\\nPr(Z|θ′) =Pr(T|θ′)\\nPr(Zm|Z,θ′). (8.45)\\nIn terms of log-likelihoods, we have ℓ(θ′;Z) =ℓ0(θ′;T)−ℓ1(θ′;Zm|Z), where\\nℓ1is based on the conditional density Pr( Zm|Z,θ′). Taking conditional\\nexpectations with respect to the distribution of T|Zgoverned by parameter\\nθgives\\nℓ(θ′;Z) = E[ ℓ0(θ′;T)|Z,θ]−E[ℓ1(θ′;Zm|Z)|Z,θ]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9fd54755-f3e1-4ef5-bfe6-6239f11d92dd', embedding=None, metadata={'page_label': '296', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.5 The EM Algorithm 277\\nAlgorithm 8.2 The EM Algorithm.\\n1. Start with initial guesses for the parameters ˆθ(0).\\n2.Expectation Step : at the jth step, compute\\nQ(θ′,ˆθ(j)) = E( ℓ0(θ′;T)|Z,ˆθ(j)) (8.43)\\nas a function of the dummy argument θ′.\\n3.Maximization Step : determine the new estimate ˆθ(j+1)as the maxi-\\nmizer of Q(θ′,ˆθ(j)) over θ′.\\n4. Iterate steps 2 and 3 until convergence.\\n≡Q(θ′,θ)−R(θ′,θ). (8.46)\\nIn the Mstep, the EM algorithm maximizes Q(θ′,θ) over θ′, rather than\\nthe actual objective function ℓ(θ′;Z). Why does it succeed in maximizing\\nℓ(θ′;Z)? Note that R(θ∗,θ) is the expectation of a log-likelihood of a density\\n(indexed by θ∗), with respect to the same density indexed by θ, and hence\\n(by Jensen’s inequality) is maximized as a function of θ∗, when θ∗=θ(see\\nExercise 8.1). So if θ′maximizes Q(θ′,θ), we see that\\nℓ(θ′;Z)−ℓ(θ;Z) = [ Q(θ′,θ)−Q(θ,θ)]−[R(θ′,θ)−R(θ,θ)]\\n≥0. (8.47)\\nHence the EM iteration never decreases the log-likelihood.\\nThis argument also makes it clear that a full maximization in the M\\nstep is not necessary: we need only to ﬁnd a value ˆθ(j+1)so that Q(θ′,ˆθ(j))\\nincreases as a function of the ﬁrst argument, that is, Q(ˆθ(j+1),ˆθ(j))>\\nQ(ˆθ(j),ˆθ(j)). Such procedures are called GEM (generalized EM) algorithms.\\nThe EM algorithm can also be viewed as a minorization procedure: see\\nExercise 8.7.\\n8.5.3 EM as a Maximization–Maximization Procedure\\nHere is a diﬀerent view of the EM procedure, as a joint maximization\\nalgorithm. Consider the function\\nF(θ′,˜P) = E ˜P[ℓ0(θ′;T)]−E˜P[log˜P(Zm)]. (8.48)\\nHere ˜P(Zm) is any distribution over the latent data Zm. In the mixture\\nexample, ˜P(Zm) comprises the set of probabilities γi= Pr(∆ i= 1|θ,Z).\\nNote that Fevaluated at ˜P(Zm) = Pr( Zm|Z,θ′), is the log-likelihood of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d852e9d6-f703-4f24-b2c9-1adf89b390d4', embedding=None, metadata={'page_label': '297', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='278 8. Model Inference and Averaging\\n1 2 3 4 50 1 2 3 40.10.3\\n0.50.7\\n0.9Model Parameters\\nLatent Data ParametersEMEM\\nFIGURE 8.7. Maximization–maximization view of the EM algorithm. Shown\\nare the contours of the (augmented) observed data log-likelih oodF(θ′,˜P). The\\nEstep is equivalent to maximizing the log-likelihood over the pa rameters of the\\nlatent data distribution. The Mstep maximizes it over the parameters of the\\nlog-likelihood. The red curve corresponds to the observed da ta log-likelihood, a\\nproﬁle obtained by maximizing F(θ′,˜P)for each value of θ′.\\nthe observed data, from (8.46)1. The function Fexpands the domain of\\nthe log-likelihood, to facilitate its maximization.\\nThe EM algorithm can be viewed as a joint maximization method for F\\noverθ′and˜P(Zm), by ﬁxing one argument and maximizing over the other.\\nThe maximizer over ˜P(Zm) for ﬁxed θ′can be shown to be\\n˜P(Zm) = Pr( Zm|Z,θ′) (8.49)\\n(Exercise 8.2). This is the distribution computed by the Estep, for example,\\n(8.42) in the mixture example. In the Mstep, we maximize F(θ′,˜P) over θ′\\nwith˜Pﬁxed: this is the same as maximizing the ﬁrst term E ˜P[ℓ0(θ′;T)|Z,θ]\\nsince the second term does not involve θ′.\\nFinally, since F(θ′,˜P) and the observed data log-likelihood agree when\\n˜P(Zm) = Pr( Zm|Z,θ′), maximization of the former accomplishes maxi-\\nmization of the latter. Figure 8.7 shows a schematic view of this process.\\nThis view of the EM algorithm leads to alternative maximization proce-\\n1(8.46) holds for all θ, including θ=θ′.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ab40267-139a-402c-8f62-d06e0d4a827d', embedding=None, metadata={'page_label': '298', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.6 MCMC for Sampling from the Posterior 279\\nAlgorithm 8.3 Gibbs Sampler.\\n1. Take some initial values U(0)\\nk,k= 1,2,... ,K .\\n2. Repeat for t= 1,2,... ,. :\\nFork= 1,2,... ,K generate U(t)\\nkfrom\\nPr(U(t)\\nk|U(t)\\n1,... ,U(t)\\nk−1,U(t−1)\\nk+1,... ,U(t−1)\\nK).\\n3. Continue step 2 until the joint distribution of ( U(t)\\n1,U(t)\\n2,... ,U(t)\\nK)\\ndoes not change.\\ndures. For example, one does not need to maximize with respect to all of\\nthe latent data parameters at once, but could instead maximize over one\\nof them at a time, alternating with the Mstep.\\n8.6 MCMC for Sampling from the Posterior\\nHaving deﬁned a Bayesian model, one would like to draw samples from\\nthe resulting posterior distribution, in order to make inferences about the\\nparameters. Except for simple models, this is often a diﬃcult computa-\\ntional problem. In this section we discuss the Markov chain Monte Carlo\\n(MCMC) approach to posterior sampling. We will see that Gibbs sampling,\\nan MCMC procedure, is closely related to the EM algorithm: the main dif-\\nference is that it samples from the conditional distributions rather than\\nmaximizing over them.\\nConsider ﬁrst the following abstract problem. We have random variables\\nU1,U2,... ,U Kand we wish to draw a sample from their joint distribution.\\nSuppose this is diﬃcult to do, but it is easy to simulate from the conditional\\ndistributions Pr( Uj|U1,U2,... ,U j−1,Uj+1,... ,U K), j= 1,2,... ,K . The\\nGibbs sampling procedure alternatively simulates from each of these distri-\\nbutions and when the process stabilizes, provides a sample from the desired\\njoint distribution. The procedure is deﬁned in Algorithm 8.3.\\nUnder regularity conditions it can be shown that this procedure even-\\ntually stabilizes, and the resulting random variables are indeed a sample\\nfrom the joint distribution of U1,U2,... ,U K. This occurs despite the fact\\nthat the samples ( U(t)\\n1,U(t)\\n2,... ,U(t)\\nK) are clearly not independent for dif-\\nferent t. More formally, Gibbs sampling produces a Markov chain whose\\nstationary distribution is the true joint distribution, and hence the term\\n“Markov chain Monte Carlo.” It is not surprising that the true joint dis -\\ntribution is stationary under this process, as the successive steps leave the\\nmarginal distributions of the Uk’s unchanged.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eaf6376a-15d9-4e2f-a791-be800d1a7500', embedding=None, metadata={'page_label': '299', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='280 8. Model Inference and Averaging\\nNote that we don’t need to know the explicit form of the conditional\\ndensities, but just need to be able to sample from them. After the procedure\\nreaches stationarity, the marginal density of any subset of the variables\\ncan be approximated by a density estimate applied to the sample values.\\nHowever if the explicit form of the conditional density Pr( Uk,|Uℓ,ℓ̸=k)\\nis available, a better estimate of say the marginal density of Ukcan be\\nobtained from (Exercise 8.3):\\nˆPrUk(u) =1\\n(M−m+ 1)M∑\\nt=mPr(u|U(t)\\nℓ,ℓ̸=k). (8.50)\\nHere we have averaged over the last M−m+ 1 members of the sequence,\\nto allow for an initial “burn-in” period before stationarity is reached.\\nNow getting back to Bayesian inference, our goal is to draw a sample from\\nthe joint posterior of the parameters given the data Z. Gibbs sampling will\\nbe helpful if it is easy to sample from the conditional distribution of each\\nparameter given the other parameters and Z. An example—the Gaussian\\nmixture problem—is detailed next.\\nThere is a close connection between Gibbs sampling from a posterior and\\nthe EM algorithm in exponential family models. The key is to consider the\\nlatent data Zmfrom the EM procedure to be another parameter for the\\nGibbs sampler. To make this explicit for the Gaussian mixture problem,\\nwe take our parameters to be ( θ,Zm). For simplicity we ﬁx the variances\\nσ2\\n1,σ2\\n2and mixing proportion πat their maximum likelihood values so that\\nthe only unknown parameters in θare the means θ1andθ2. The Gibbs\\nsampler for the mixture problem is given in Algorithm 8.4. We see that\\nsteps 2(a) and 2(b) are the same as the EandMsteps of the EM pro-\\ncedure, except that we sample rather than maximize. In step 2(a), rather\\nthan compute the maximum likelihood responsibilities γi= E(∆ i|θ,Z),\\nthe Gibbs sampling procedure simulates the latent data ∆ ifrom the distri-\\nbutions Pr(∆ i|θ,Z). In step 2(b), rather than compute the maximizers of\\nthe posterior Pr( θ1,θ2,∆|Z) we simulate from the conditional distribution\\nPr(θ1,θ2|∆,Z).\\nFigure 8.8 shows 200 iterations of Gibbs sampling, with the mean param-\\netersθ1(lower) and θ2(upper) shown in the left panel, and the proportion\\nof class 2 observations∑\\ni∆i/Non the right. Horizontal broken lines have\\nbeen drawn at the maximum likelihood estimate values ˆ θ1,ˆθ2and∑\\niˆγi/N\\nin each case. The values seem to stabilize quite quickly, and are distributed\\nevenly around the maximum likelihood values.\\nThe above mixture model was simpliﬁed, in order to make the clear\\nconnection between Gibbs sampling and the EM algorithm. More realisti-\\ncally, one would put a prior distribution on the variances σ2\\n1,σ2\\n2and mixing\\nproportion π, and include separate Gibbs sampling steps in which we sam-\\nple from their posterior distributions, conditional on the other parameters.\\nOne can also incorporate proper (informative) priors for the mean param-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbb99844-fb18-44ae-a0dc-92bf037b6fbc', embedding=None, metadata={'page_label': '300', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.6 MCMC for Sampling from the Posterior 281\\nAlgorithm 8.4 Gibbs sampling for mixtures.\\n1. Take some initial values θ(0)= (θ(0)\\n1,θ(0)\\n2).\\n2. Repeat for t= 1,2,... ,.\\n(a) For i= 1,2,... ,N generate ∆(t)\\ni∈ {0,1}with Pr(∆(t)\\ni= 1) =\\nˆγi(θ(t)), from equation (8.42).\\n(b) Set\\nˆθ1=∑N\\ni=1(1−∆(t)\\ni)≤yi∑N\\ni=1(1−∆(t)\\ni),\\nˆθ2=∑N\\ni=1∆(t)\\ni≤yi∑N\\ni=1∆(t)\\ni,\\nand generate θ(t)\\n1∼N(ˆθ1,ˆσ2\\n1) and θ(t)\\n2∼N(ˆθ2,ˆσ2\\n2).\\n3. Continue step 2 until the joint distribution of ( ∆(t),θ(t)\\n1,θ(t)\\n2) doesn’t\\nchange\\nGibbs IterationMean Parameters\\n0 50 100 150 2000 2 4 6 8\\nGibbs IterationMixing Proportion\\n0 50 100 150 2000.3 0.4 0.5 0.6 0.7\\nFIGURE 8.8. Mixture example. (Left panel:) 200values of the two mean param-\\neters from Gibbs sampling; horizontal lines are drawn at the maxi mum likelihood\\nestimates ˆθ1,ˆθ2. (Right panel:) Proportion of values with ∆i= 1, for each of the\\n200Gibbs sampling iterations; a horizontal line is drawn atP\\niˆγi/N.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fbb1e5ba-3821-45df-8778-63a9a144049f', embedding=None, metadata={'page_label': '301', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='282 8. Model Inference and Averaging\\neters. These priors must not be improper as this will lead to a degenerate\\nposterior, with all the mixing weight on one component.\\nGibbs sampling is just one of a number of recently developed procedures\\nfor sampling from posterior distributions. It uses conditional sampling of\\neach parameter given the rest, and is useful when the structure of the prob-\\nlem makes this sampling easy to carry out. Other methods do not require\\nsuch structure, for example the Metropolis–Hastings algorithm. These and\\nother computational Bayesian methods have been applied to sophisticated\\nlearning algorithms such as Gaussian process models and neural networks.\\nDetails may be found in the references given in the Bibliographic Notes at\\nthe end of this chapter.\\n8.7 Bagging\\nEarlier we introduced the bootstrap as a way of assessing the accuracy of a\\nparameter estimate or a prediction. Here we show how to use the bootstrap\\nto improve the estimate or prediction itself. In Section 8.4 we investigat ed\\nthe relationship between the bootstrap and Bayes approaches, and found\\nthat the bootstrap mean is approximately a posterior average. Bagging\\nfurther exploits this connection.\\nConsider ﬁrst the regression problem. Suppose we ﬁt a model to our\\ntraining data Z={(x1,y1),(x2,y2),... ,(xN,yN)}, obtaining the predic-\\ntionˆf(x) at input x. Bootstrap aggregation or bagging averages this predic-\\ntion over a collection of bootstrap samples, thereby reducing its variance.\\nFor each bootstrap sample Z∗b,b= 1,2,... ,B , we ﬁt our model, giving\\nprediction ˆf∗b(x). The bagging estimate is deﬁned by\\nˆfbag(x) =1\\nBB∑\\nb=1ˆf∗b(x). (8.51)\\nDenote by ˆPthe empirical distribution putting equal probability 1 /Non\\neach of the data points ( xi,yi). In fact the “true” bagging estimate is\\ndeﬁned by E ˆPˆf∗(x), where Z∗= (x∗\\n1,y∗\\n1),(x∗\\n2,y∗\\n2),... ,(x∗\\nN,y∗\\nN) and each\\n(x∗\\ni,y∗\\ni)∼ˆP. Expression (8.51) is a Monte Carlo estimate of the true\\nbagging estimate, approaching it as B→ ∞.\\nThe bagged estimate (8.51) will diﬀer from the original estimate ˆf(x)\\nonly when the latter is a nonlinear or adaptive function of the data. For\\nexample, to bag the B-spline smooth of Section 8.2.1, we average the curves\\nin the bottom left panel of Figure 8.2 at each value of x. The B-spline\\nsmoother is linear in the data if we ﬁx the inputs; hence if we sample using\\nthe parametric bootstrap in equation (8.6), then ˆfbag(x)→ˆf(x) asB→ ∞\\n(Exercise 8.4). Hence bagging just reproduces the original smooth in the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='529543cf-89fd-4f85-a374-803367f9c535', embedding=None, metadata={'page_label': '302', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.7 Bagging 283\\ntop left panel of Figure 8.2. The same is approximately true if we were to\\nbag using the nonparametric bootstrap.\\nA more interesting example is a regression tree, where ˆf(x) denotes the\\ntree’s prediction at input vector x(regression trees are described in Chap-\\nter 9). Each bootstrap tree will typically involve diﬀerent features tha n the\\noriginal, and might have a diﬀerent number of terminal nodes. The bagged\\nestimate is the average prediction at xfrom these Btrees.\\nNow suppose our tree produces a classiﬁer ˆG(x) for a K-class response.\\nHere it is useful to consider an underlying indicator-vector function ˆf(x),\\nwith value a single one and K−1 zeroes, such that ˆG(x) = arg max kˆf(x).\\nThen the bagged estimate ˆfbag(x) (8.51) is a K-vector [ p1(x),p2(x),... ,\\npK(x)], with pk(x) equal to the proportion of trees predicting class katx.\\nThe bagged classiﬁer selects the class with the most “votes” from the B\\ntrees, ˆGbag(x) = arg max kˆfbag(x).\\nOften we require the class-probability estimates at x, rather than the\\nclassiﬁcations themselves. It is tempting to treat the voting proportions\\npk(x) as estimates of these probabilities. A simple two-class example shows\\nthat they fail in this regard. Suppose the true probability of class 1 at xis\\n0.75, and each of the bagged classiﬁers accurately predict a 1. Then p1(x) =\\n1, which is incorrect. For many classiﬁers ˆG(x), however, there is already\\nan underlying function ˆf(x) that estimates the class probabilities at x(for\\ntrees, the class proportions in the terminal node). An alternative bagging\\nstrategy is to average these instead, rather than the vote indicator vectors.\\nNot only does this produce improved estimates of the class probabilities,\\nbut it also tends to produce bagged classiﬁers with lower variance, especially\\nfor small B(see Figure 8.10 in the next example).\\n8.7.1 Example: Trees with Simulated Data\\nWe generated a sample of size N= 30, with two classes and p= 5 features,\\neach having a standard Gaussian distribution with pairwise correlation\\n0.95. The response Ywas generated according to Pr( Y= 1|x1≤0.5) = 0 .2,\\nPr(Y= 1|x1>0.5) = 0 .8. The Bayes error is 0 .2. A test sample of size 2000\\nwas also generated from the same population. We ﬁt classiﬁcation trees to\\nthe training sample and to each of 200 bootstrap samples (classiﬁcation\\ntrees are described in Chapter 9). No pruning was used. Figure 8.9 shows\\nthe original tree and eleven bootstrap trees. Notice how the trees are all\\ndiﬀerent, with diﬀerent splitting features and cutpoints. The test error for\\nthe original tree and the bagged tree is shown in Figure 8.10. In this ex-\\nample the trees have high variance due to the correlation in the predictors.\\nBagging succeeds in smoothing out this variance and hence reducing the\\ntest error.\\nBagging can dramatically reduce the variance of unstable procedures\\nlike trees, leading to improved prediction. A simple argument shows why', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8523806a-7931-47cc-872c-435663bbe5a3', embedding=None, metadata={'page_label': '303', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='284 8. Model Inference and Averaging\\n|x.1 < 0.395\\n010\\n101\\n10Original Tree\\n|x.1 < 0.555\\n0\\n1001b = 1\\n|x.2 < 0.205\\n0101\\n01b = 2\\n|x.2 < 0.285\\n1 1010b = 3\\n|x.3 < 0.985\\n0\\n1\\n011 1b = 4\\n|x.4 < −1.36\\n0\\n1\\n1010\\n10b = 5\\n|x.1 < 0.395\\n1 10 01b = 6\\n|x.1 < 0.395\\n01011b = 7\\n|x.3 < 0.985\\n010 010b = 8\\n|x.1 < 0.395\\n0\\n1\\n0110b = 9\\n|x.1 < 0.555\\n101\\n01b = 10\\n|x.1 < 0.555\\n0 101b = 11\\nFIGURE 8.9. Bagging trees on simulated dataset. The top left panel shows th e\\noriginal tree. Eleven trees grown on bootstrap samples are sh own. For each tree,\\nthe top split is annotated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a1f46733-fa2f-4a50-9923-be38ca6c40dd', embedding=None, metadata={'page_label': '304', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.7 Bagging 285\\n0 50 100 150 2000.20 0.25 0.30 0.35 0.40 0.45 0.50\\nNumber of Bootstrap SamplesTest ErrorBagged TreesOriginal Tree\\nBayesConsensus\\nProbability\\nFIGURE 8.10. Error curves for the bagging example of Figure 8.9. Shown is\\nthe test error of the original tree and bagged trees as a function of the number of\\nbootstrap samples. The orange points correspond to the consensus vote, while the\\ngreen points average the probabilities.\\nbagging helps under squared-error loss, in short because averaging reduces\\nvariance and leaves bias unchanged.\\nAssume our training observations ( xi,yi), i= 1,... ,N are indepen-\\ndently drawn from a distribution P, and consider the ideal aggregate es-\\ntimator fag(x) = E Pˆf∗(x). Here xis ﬁxed and the bootstrap dataset Z∗\\nconsists of observations x∗\\ni,y∗\\ni,i= 1,2,... ,N sampled from P. Note that\\nfag(x) is a bagging estimate, drawing bootstrap samples from the actual\\npopulation Prather than the data. It is not an estimate that we can use\\nin practice, but is convenient for analysis. We can write\\nEP[Y−ˆf∗(x)]2= E P[Y−fag(x) +fag(x)−ˆf∗(x)]2\\n= E P[Y−fag(x)]2+ EP[ˆf∗(x)−fag(x)]2\\n≥EP[Y−fag(x)]2. (8.52)\\nThe extra error on the right-hand side comes from the variance of ˆf∗(x)\\naround its mean fag(x). Therefore true population aggregation never in-\\ncreases mean squared error. This suggests that bagging—drawing samples\\nfrom the training data— will often decrease mean-squared error.\\nThe above argument does not hold for classiﬁcation under 0-1 loss, be-\\ncause of the nonadditivity of bias and variance. In that setting, bagging a', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4c2beb67-af05-4831-81f6-f727703d4e77', embedding=None, metadata={'page_label': '305', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='286 8. Model Inference and Averaging\\ngood classiﬁer can make it better, but bagging a bad classiﬁer can make it\\nworse. Here is a simple example, using a randomized rule. Suppose Y= 1\\nfor all x, and the classiﬁer ˆG(x) predicts Y= 1 (for all x) with proba-\\nbility 0.4 and predicts Y= 0 (for all x) with probability 0.6. Then the\\nmisclassiﬁcation error of ˆG(x) is 0.6 but that of the bagged classiﬁer is 1.0.\\nFor classiﬁcation we can understand the bagging eﬀect in terms of a\\nconsensus of independent weak learners (Dietterich, 2000a). Let the Bayes\\noptimal decision at xbeG(x) = 1 in a two-class example. Suppose each\\nof the weak learners G∗\\nbhave an error-rate eb=e <0.5, and let S1(x) =∑B\\nb=1I(G∗\\nb(x) = 1) be the consensus vote for class 1. Since the weak learn-\\ners are assumed to be independent, S1(x)∼Bin(B,1−e), and Pr( S1>\\nB/2)→1 asBgets large. This concept has been popularized outside of\\nstatistics as the “Wisdom of Crowds” (Surowiecki, 2004) — the collective\\nknowledge of a diverse and independent body of people typically exceeds\\nthe knowledge of any single individual, and can be harnessed by voting.\\nOf course, the main caveat here is “independent,” and bagged trees are\\nnot. Figure 8.11 illustrates the power of a consensus vote in a simulated\\nexample, where only 30% of the voters have some knowledge.\\nIn Chapter 15 we see how random forests improve on bagging by reducing\\nthe correlation between the sampled trees.\\nNote that when we bag a model, any simple structure in the model is\\nlost. As an example, a bagged tree is no longer a tree. For interpretation\\nof the model this is clearly a drawback. More stable procedures like near-\\nest neighbors are typically not aﬀected much by bagging. Unfortunately,\\nthe unstable models most helped by bagging are unstable because of the\\nemphasis on interpretability, and this is lost in the bagging process.\\nFigure 8.12 shows an example where bagging doesn’t help. The 100 data\\npoints shown have two features and two classes, separated by the gray\\nlinear boundary x1+x2= 1. We choose as our classiﬁer ˆG(x) a single\\naxis-oriented split, choosing the split along either x1orx2that produces\\nthe largest decrease in training misclassiﬁcation error.\\nThe decision boundary obtained from bagging the 0-1 decision rule over\\nB= 50 bootstrap samples is shown by the blue curve in the left panel.\\nIt does a poor job of capturing the true boundary. The single split rule,\\nderived from the training data, splits near 0 (the middle of the range of x1\\norx2), and hence has little contribution away from the center. Averaging\\nthe probabilities rather than the classiﬁcations does not help here. Bagging\\nestimates the expected class probabilities from the single split rule, that is,\\naveraged over many replications. Note that the expected class probabilities\\ncomputed by bagging cannot be realized on any single replication, in the\\nsame way that a woman cannot have 2.4 children. In this sense, bagging\\nincreases somewhat the space of models of the individual base classiﬁer.\\nHowever, it doesn’t help in this and many other examples where a greater\\nenlargement of the model class is needed. “Boosting” is a way of doing this', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='62cc4917-f3d6-4e71-8665-f1421d3e30cc', embedding=None, metadata={'page_label': '306', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.7 Bagging 2870 2 4 6 8 10\\nP −  Probability of Informed Person Being CorrectExpected Correct out of 10Wisdom of Crowds\\nConsensus\\nIndividual\\n0.25 0.50 0.75 1.00\\nFIGURE 8.11. Simulated academy awards voting. 50members vote in 10 cat-\\negories, each with 4nominations. For any category, only 15voters have some\\nknowledge, represented by their probability of selecting the “ correct” candidate in\\nthat category (so P= 0.25means they have no knowledge). For each category, the\\n15experts are chosen at random from the 50. Results show the expected correct\\n(based on 50simulations) for the consensus, as well as for the individuals. T he\\nerror bars indicate one standard deviation. We see, for example, t hat if the 15\\ninformed for a category have a 50% chance of selecting the correct candidate, the\\nconsensus doubles the expected performance of an individual.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db666bf2-af51-465f-845a-c3846a73ce51', embedding=None, metadata={'page_label': '307', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='288 8. Model Inference and Averaging\\n•••\\n••••\\n•••\\n••\\n••\\n•\\n••••\\n•••\\n•••\\n•••\\n•\\n••\\n•••\\n•\\n•• •• •••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n••\\n•\\n•\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•• • •••\\n••\\n•••\\n•••\\n•\\n••••\\n•\\n• ••\\n••\\n••\\n••• •••\\n••••\\n••\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••••\\n•\\n••\\n•••••\\n••\\n•••\\n•••••\\n••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n•\\n•Bagged Decision Rule\\n•••\\n••••\\n•••\\n••\\n••\\n•\\n••••\\n•••\\n•••\\n•••\\n•\\n••\\n•••\\n•\\n•• •• •••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n••\\n•\\n•\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•• • •••\\n••\\n•••\\n•••\\n•\\n••••\\n•\\n• ••\\n••\\n••\\n••• •••\\n••••\\n••\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••••\\n•\\n••\\n•••••\\n••\\n•••\\n•••••\\n••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n•\\n•Boosted Decision Rule\\nFIGURE 8.12. Data with two features and two classes, separated by a linear\\nboundary. (Left panel:) Decision boundary estimated from bagg ing the decision\\nrule from a single split, axis-oriented classiﬁer. (Right panel: ) Decision boundary\\nfrom boosting the decision rule of the same classiﬁer. The test error rates are\\n0.166, and 0.065, respectively. Boosting is described in Chapter 10.\\nand is described in Chapter 10. The decision boundary in the right panel is\\nthe result of the boosting procedure, and it roughly captures the diagonal\\nboundary.\\n8.8 Model Averaging and Stacking\\nIn Section 8.4 we viewed bootstrap values of an estimator as approximate\\nposterior values of a corresponding parameter, from a kind of nonparamet-\\nric Bayesian analysis. Viewed in this way, the bagged estimate (8.51) i s\\nan approximate posterior Bayesian mean. In contrast, the training sampl e\\nestimate ˆf(x) corresponds to the mode of the posterior. Since the posterior\\nmean (not mode) minimizes squared-error loss, it is not surprising that\\nbagging can often reduce mean squared-error.\\nHere we discuss Bayesian model averaging more generally. We have a\\nset of candidate models Mm, m= 1,... ,M for our training set Z. These\\nmodels may be of the same type with diﬀerent parameter values (e.g.,\\nsubsets in linear regression), or diﬀerent models for the same task (e.g.,\\nneural networks and regression trees).\\nSuppose ζis some quantity of interest, for example, a prediction f(x) at\\nsome ﬁxed feature value x. The posterior distribution of ζis\\nPr(ζ|Z) =M∑\\nm=1Pr(ζ|Mm,Z)Pr(Mm|Z), (8.53)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='83ceeb87-ee52-4db6-b3e9-56455dcfa692', embedding=None, metadata={'page_label': '308', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.8 Model Averaging and Stacking 289\\nwith posterior mean\\nE(ζ|Z) =M∑\\nm=1E(ζ|Mm,Z)Pr(Mm|Z). (8.54)\\nThis Bayesian prediction is a weighted average of the individual predictions,\\nwith weights proportional to the posterior probability of each model.\\nThis formulation leads to a number of diﬀerent model-averaging strate-\\ngies.Committee methods take a simple unweighted average of the predic-\\ntions from each model, essentially giving equal probability to each model.\\nMore ambitiously, the development in Section 7.7 shows the BIC criterion\\ncan be used to estimate posterior model probabilities. This is applicable\\nin cases where the diﬀerent models arise from the same parametric model,\\nwith diﬀerent parameter values. The BIC gives weight to each model de-\\npending on how well it ﬁts and how many parameters it uses. One can also\\ncarry out the Bayesian recipe in full. If each model Mmhas parameters\\nθm, we write\\nPr(Mm|Z)∝Pr(Mm)≤Pr(Z|Mm)\\n∝Pr(Mm)≤∫\\nPr(Z|θm,Mm)Pr(θm|Mm)dθm.\\n(8.55)\\nIn principle one can specify priors Pr( θm|Mm) and numerically com-\\npute the posterior probabilities from (8.55), to be used as model-averaging\\nweights. However, we have seen no real evidence that this is worth all of\\nthe eﬀort, relative to the much simpler BIC approximation.\\nHow can we approach model averaging from a frequentist viewpoint?\\nGiven predictions ˆf1(x),ˆf2(x),... ,ˆfM(x), under squared-error loss, we can\\nseek the weights w= (w1,w2,... ,w M) such that\\nˆw= argmin\\nwEP[\\nY−M∑\\nm=1wmˆfm(x)]2\\n. (8.56)\\nHere the input value xis ﬁxed and the Nobservations in the dataset Z(and\\nthe target Y) are distributed according to P. The solution is the population\\nlinear regression of YonˆF(x)T≡[ˆf1(x),ˆf2(x),... ,ˆfM(x)]:\\nˆw= EP[ˆF(x)ˆF(x)T]−1EP[ˆF(x)Y]. (8.57)\\nNow the full regression has smaller error than any single model\\nEP[\\nY−M∑\\nm=1ˆwmˆfm(x)]2\\n≤EP[\\nY−ˆfm(x)]2\\n∀m (8.58)\\nso combining models never makes things worse, at the population level.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b01dc36-2f0e-470a-a92b-a8e794735ffe', embedding=None, metadata={'page_label': '309', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='290 8. Model Inference and Averaging\\nOf course the population linear regression (8.57) is not available, and it\\nis natural to replace it with the linear regression over the training set. But\\nthere are simple examples where this does not work well. For example, if\\nˆfm(x), m= 1,2,... ,M represent the prediction from the best subset of\\ninputs of size mamong Mtotal inputs, then linear regression would put all\\nof the weight on the largest model, that is, ˆ wM= 1,ˆwm= 0, m < M . The\\nproblem is that we have not put each of the models on the same footing\\nby taking into account their complexity (the number of inputs min this\\nexample).\\nStacked generalization , orstacking , is a way of doing this. Let ˆf−i\\nm(x)\\nbe the prediction at x, using model m, applied to the dataset with the\\nith training observation removed. The stacking estimate of the weights is\\nobtained from the least squares linear regression of yionˆf−i\\nm(xi), m=\\n1,2,... ,M . In detail the stacking weights are given by\\nˆwst= argmin\\nwN∑\\ni=1[\\nyi−M∑\\nm=1wmˆf−i\\nm(xi)]2\\n. (8.59)\\nThe ﬁnal prediction is∑\\nmˆwst\\nmˆfm(x). By using the cross-validated pre-\\ndictions ˆf−i\\nm(x), stacking avoids giving unfairly high weight to models with\\nhigher complexity. Better results can be obtained by restricting the weights\\nto be nonnegative, and to sum to 1. This seems like a reasonable restriction\\nif we interpret the weights as posterior model probabilities as in equation\\n(8.54), and it leads to a tractable quadratic programming problem.\\nThere is a close connection between stacking and model selection via\\nleave-one-out cross-validation (Section 7.10). If we restrict the minimizatio n\\nin (8.59) to weight vectors wthat have one unit weight and the rest zero,\\nthis leads to a model choice ˆ mwith smallest leave-one-out cross-validation\\nerror. Rather than choose a single model, stacking combines them with\\nestimated optimal weights. This will often lead to better prediction, but\\nless interpretability than the choice of only one of the Mmodels.\\nThe stacking idea is actually more general than described above. One\\ncan use any learning method, not just linear regression, to combine the\\nmodels as in (8.59); the weights could also depend on the input location\\nx. In this way, learning methods are “stacked” on top of one another, to\\nimprove prediction performance.\\n8.9 Stochastic Search: Bumping\\nThe ﬁnal method described in this chapter does not involve averaging or\\ncombining models, but rather is a technique for ﬁnding a better single\\nmodel. Bumping uses bootstrap sampling to move randomly through model\\nspace. For problems where ﬁtting method ﬁnds many local minima, bump-\\ning can help the method to avoid getting stuck in poor solutions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2ad74f93-2532-4264-a96b-93c494a95b6d', embedding=None, metadata={'page_label': '310', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.9 Stochastic Search: Bumping 291\\nRegular 4-Node Tree\\n••\\n•••\\n••\\n•\\n•••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n••\\n•\\n•\\n•••\\n••\\n•\\n••\\n••\\n••\\n••\\n••••••\\n••••\\n•\\n•\\n• ••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n• •••\\n••\\n•\\n••\\n•••\\n••\\n•••\\n•\\n••••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n•\\n•••••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n••••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••••\\n•\\n••••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•\\n•• •••\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n••\\n••\\n•••\\n••\\n•••\\n•• •\\n•••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n•••\\n•\\n•••\\n•••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n••••••\\n•\\n••\\n••\\n••\\n•\\n•\\n••\\n••••\\n•••\\n••\\n•••\\n•••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n•••\\n•••\\n••••\\n•••••\\n•\\n• •• ••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n•\\n•••\\n••\\n••\\n•\\n•\\n••\\n••\\n••Bumped 4-Node Tree\\n••\\n•••\\n••\\n•\\n•••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n••\\n•\\n•\\n•••\\n••\\n•\\n••\\n••\\n••\\n••\\n••••••\\n••••\\n•\\n•\\n• ••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n• •••\\n••\\n•\\n••\\n•••\\n••\\n•••\\n•\\n••••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n•\\n•••••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n••••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••••\\n•\\n••••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•\\n•• •••\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n••\\n••\\n•••\\n••\\n•••\\n•• •\\n•••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n•••\\n•\\n•••\\n•••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n••••••\\n•\\n••\\n••\\n••\\n•\\n•\\n••\\n••••\\n•••\\n••\\n•••\\n•••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n•••\\n•••\\n••••\\n•••••\\n•\\n• •• ••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n•\\n•••\\n••\\n••\\n•\\n•\\n••\\n••\\n••\\nFIGURE 8.13. Data with two features and two classes (blue and orange), dis-\\nplaying a pure interaction. The left panel shows the partition fo und by three splits\\nof a standard, greedy, tree-growing algorithm. The vertical g rey line near the left\\nedge is the ﬁrst split, and the broken lines are the two subsequent splits. The al-\\ngorithm has no idea where to make a good initial split, and makes a poor choice.\\nThe right panel shows the near-optimal splits found by bumping th e tree-growing\\nalgorithm 20times.\\nAs in bagging, we draw bootstrap samples and ﬁt a model to each. But\\nrather than average the predictions, we choose the model estimated from a\\nbootstrap sample that best ﬁts the training data. In detail, we draw boot-\\nstrap samples Z∗1,... ,Z∗Band ﬁt our model to each, giving predictions\\nˆf∗b(x), b= 1,2,... ,B at input point x. We then choose the model that\\nproduces the smallest prediction error, averaged over the original training\\nset. For squared error, for example, we choose the model obtained from\\nbootstrap sample ˆb, where\\nˆb= arg min\\nbN∑\\ni=1[yi−ˆf∗b(xi)]2. (8.60)\\nThe corresponding model predictions are ˆf∗ˆb(x). By convention we also\\ninclude the original training sample in the set of bootstrap samples, so that\\nthe method is free to pick the original model if it has the lowest training\\nerror.\\nBy perturbing the data, bumping tries to move the ﬁtting procedure\\naround to good areas of model space. For example, if a few data points are\\ncausing the procedure to ﬁnd a poor solution, any bootstrap sample that\\nomits those data points should procedure a better solution.\\nFor another example, consider the classiﬁcation data in Figure 8.13, the\\nnotorious exclusive or (XOR) problem. There are two classes (blue and\\norange) and two input features, with the features exhibiting a pure inter-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c16266f6-966c-40e4-b6ef-f268df09aa6f', embedding=None, metadata={'page_label': '311', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='292 8. Model Inference and Averaging\\naction. By splitting the data at x1= 0 and then splitting each resulting\\nstrata at x2= 0, (or vice versa) a tree-based classiﬁer could achieve per-\\nfect discrimination. However, the greedy, short-sighted CART algorithm\\n(Section 9.2) tries to ﬁnd the best split on either feature, and then splits\\nthe resulting strata. Because of the balanced nature of the data, all initial\\nsplits on x1orx2appear to be useless, and the procedure essentially gener-\\nates a random split at the top level. The actual split found for these data is\\nshown in the left panel of Figure 8.13. By bootstrap sampling from the data ,\\nbumping breaks the balance in the classes, and with a reasonable number\\nof bootstrap samples (here 20), it will by chance produce at least one tree\\nwith initial split near either x1= 0 or x2= 0. Using just 20 bootstrap\\nsamples, bumping found the near optimal splits shown in the right panel\\nof Figure 8.13. This shortcoming of the greedy tree-growing algorithm is\\nexacerbated if we add a number of noise features that are independent of\\nthe class label. Then the tree-growing algorithm cannot distinguish x1or\\nx2from the others, and gets seriously lost.\\nSince bumping compares diﬀerent models on the training data, one must\\nensure that the models have roughly the same complexity. In the case of\\ntrees, this would mean growing trees with the same number of terminal\\nnodes on each bootstrap sample. Bumping can also help in problems where\\nit is diﬃcult to optimize the ﬁtting criterion, perhaps because of a lack of\\nsmoothness. The trick is to optimize a diﬀerent, more convenient criterion\\nover the bootstrap samples, and then choose the model producing the best\\nresults for the desired criterion on the training sample.\\nBibliographic Notes\\nThere are many books on classical statistical inference: Cox and Hink-\\nley (1974) and Silvey (1975) give nontechnical accounts. The bootstrap\\nis due to Efron (1979) and is described more fully in Efron and Tibshi-\\nrani (1993) and Hall (1992). A good modern book on Bayesian inference\\nis Gelman et al. (1995). A lucid account of the application of Bayesian\\nmethods to neural networks is given in Neal (1996). The statistical appli-\\ncation of Gibbs sampling is due to Geman and Geman (1984), and Gelfand\\nand Smith (1990), with related work by Tanner and Wong (1987). Markov\\nchain Monte Carlo methods, including Gibbs sampling and the Metropolis–\\nHastings algorithm, are discussed in Spiegelhalter et al. (1996). The EM\\nalgorithm is due to Dempster et al. (1977); as the discussants in that pa-\\nper make clear, there was much related, earlier work. The view of EM as\\na joint maximization scheme for a penalized complete-data log-likelihood\\nwas elucidated by Neal and Hinton (1998); they credit Csiszar and Tusn´ ady\\n(1984) and Hathaway (1986) as having noticed this connection earlier. Bag-\\nging was proposed by Breiman (1996a). Stacking is due to Wolpert (1992) ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38652904-3160-4ba1-ae04-2bbb023139ee', embedding=None, metadata={'page_label': '312', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 293\\nBreiman (1996b) contains an accessible discussion for statisticians. Lebla nc\\nand Tibshirani (1996) describe variations on stacking based on the boot-\\nstrap. Model averaging in the Bayesian framework has been recently advo-\\ncated by Madigan and Raftery (1994). Bumping was proposed by Tibshi-\\nrani and Knight (1999).\\nExercises\\nEx. 8.1 Letr(y) and q(y) be probability density functions. Jensen’s in-\\nequality states that for a random variable Xand a convex function φ(x),\\nE[φ(X)]≥φ[E(X)]. Use Jensen’s inequality to show that\\nEqlog[r(Y)/q(Y)] (8.61)\\nis maximized as a function of r(y) when r(y) =q(y). Hence show that\\nR(θ,θ)≥R(θ′,θ) as stated below equation (8.46).\\nEx. 8.2 Consider the maximization of the log-likelihood (8.48), over dis-\\ntributions ˜P(Zm) such that ˜P(Zm)≥0 and∑\\nZm˜P(Zm) = 1. Use La-\\ngrange multipliers to show that the solution is the conditional distribution\\n˜P(Zm) = Pr( Zm|Z,θ′), as in (8.49).\\nEx. 8.3 Justify the estimate (8.50), using the relationship\\nPr(A) =∫\\nPr(A|B)d(Pr(B)).\\nEx. 8.4 Consider the bagging method of Section 8.7. Let our estimate ˆf(x)\\nbe the B-spline smoother ˆ θ(x) of Section 8.2.1. Consider the parametric\\nbootstrap of equation (8.6), applied to this estimator. Show that if we ba g\\nˆf(x), using the parametric bootstrap to generate the bootstrap samples,\\nthe bagging estimate ˆfbag(x) converges to the original estimate ˆf(x) as\\nB→ ∞.\\nEx. 8.5 Suggest generalizations of each of the loss functions in Figure 10.4\\nto more than two classes, and design an appropriate plot to compare them.\\nEx. 8.6 Consider the bone mineral density data of Figure 5.6.\\n(a) Fit a cubic smooth spline to the relative change in spinal BMD, as a\\nfunction of age. Use cross-validation to estimate the optimal amount\\nof smoothing. Construct pointwise 90% conﬁdence bands for the un-\\nderlying function.\\n(b) Compute the posterior mean and covariance for the true function via\\n(8.28), and compare the posterior bands to those obtained in (a).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e60b6f2a-c2be-4708-ae3c-d258e262bc9b', embedding=None, metadata={'page_label': '313', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='294 8. Model Inference and Averaging\\n(c) Compute 100 bootstrap replicates of the ﬁtted curves, as in the bottom\\nleft panel of Figure 8.2. Compare the results to those obtained in (a)\\nand (b).\\nEx. 8.7 EM as a minorization algorithm (Hunter and Lange, 2004; Wu and\\nLange, 2007). A function g(x,y) to said to minorize a function f(x) if\\ng(x,y)≤f(x), g(x,x) =f(x) (8.62)\\nfor all x,yin the domain. This is useful for maximizing f(x) since is easy\\nto show that f(x) is non-decreasing under the update\\nxs+1= argmaxxg(x,xs) (8.63)\\nThere are analogous deﬁnitions for majorization , for minimizing a function\\nf(x). The resulting algorithms are known as MMalgorithms, for “Minorize-\\nMaximize” or “Majorize-Minimize.”\\nShow that the EM algorithm (Section 8.5.2) is an example of an MM al-\\ngorithm, using Q(θ′,θ)+log Pr( Z|θ)−Q(θ,θ) to minorize the observed data\\nlog-likelihood ℓ(θ′;Z). (Note that only the ﬁrst term involves the relevant\\nparameter θ′).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='adb05c0a-d4af-4bed-bb6c-cdc9240dc485', embedding=None, metadata={'page_label': '314', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 295\\nPrinter: Opaque this\\n9\\nAdditive Models, Trees, and Related\\nMethods\\nIn this chapter we begin our discussion of some speciﬁc methods for super-\\nvised learning. These techniques each assume a (diﬀerent) structured form\\nfor the unknown regression function, and by doing so they ﬁnesse the curse\\nof dimensionality. Of course, they pay the possible price of misspecifying\\nthe model, and so in each case there is a tradeoﬀ that has to be made. They\\ntake oﬀ where Chapters 3–6 left oﬀ. We describe ﬁve related techniques:\\ngeneralized additive models, trees, multivariate adaptive regression splines,\\nthe patient rule induction method, and hierarchical mixtures of experts.\\n9.1 Generalized Additive Models\\nRegression models play an important role in many data analyses, providi ng\\nprediction and classiﬁcation rules, and data analytic tools for understand-\\ning the importance of diﬀerent inputs.\\nAlthough attractively simple, the traditional linear model often fails in\\nthese situations: in real life, eﬀects are often not linear. In earlier chapters\\nwe described techniques that used predeﬁned basis functions to achieve\\nnonlinearities. This section describes more automatic ﬂexible statistical\\nmethods that may be used to identify and characterize nonlinear regression\\neﬀects. These methods are called “generalized additive models.”\\nIn the regression setting, a generalized additive model has the form\\nE(Y|X1,X2,... ,X p) =α+f1(X1) +f2(X2) +≤≤≤+fp(Xp).(9.1)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca62ef13-d446-498b-a32b-8fd5bcac92bd', embedding=None, metadata={'page_label': '315', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='296 9. Additive Models, Trees, and Related Methods\\nAs usual X1,X2,... ,X prepresent predictors and Yis the outcome; the fj’s\\nare unspeciﬁed smooth (“nonparametric”) functions. If we were to model\\neach function using an expansion of basis functions (as in Chapter 5), the\\nresulting model could then be ﬁt by simple least squares. Our approach\\nhere is diﬀerent: we ﬁt each function using a scatterplot smoother (e.g., a\\ncubic smoothing spline or kernel smoother), and provide an algorithm for\\nsimultaneously estimating all pfunctions (Section 9.1.1).\\nFor two-class classiﬁcation, recall the logistic regression model for binar y\\ndata discussed in Section 4.4. We relate the mean of the binary response\\nθ(X) = Pr( Y= 1|X) to the predictors via a linear regression model and\\nthelogitlink function:\\nlog(θ(X)\\n1−θ(X))\\n=α+β1X1+≤≤≤+βpXp. (9.2)\\nTheadditive logistic regression model replaces each linear term by a more\\ngeneral functional form\\nlog(θ(X)\\n1−θ(X))\\n=α+f1(X1) +≤≤≤+fp(Xp), (9.3)\\nwhere again each fjis an unspeciﬁed smooth function. While the non-\\nparametric form for the functions fjmakes the model more ﬂexible, the\\nadditivity is retained and allows us to interpret the model in much the\\nsame way as before. The additive logistic regression model is an example\\nof a generalized additive model. In general, the conditional mean θ(X) of\\na response Yis related to an additive function of the predictors via a link\\nfunction g:\\ng[θ(X)] =α+f1(X1) +≤≤≤+fp(Xp). (9.4)\\nExamples of classical link functions are the following:\\n•g(θ) =θis the identity link, used for linear and additive models for\\nGaussian response data.\\n•g(θ) = logit( θ) as above, or g(θ) = probit( θ), theprobit link function,\\nfor modeling binomial probabilities. The probit function is the inverse\\nGaussian cumulative distribution function: probit( θ) = Φ−1(θ).\\n•g(θ) = log( θ) for log-linear or log-additive models for Poisson count\\ndata.\\nAll three of these arise from exponential family sampling models, which\\nin addition include the gamma and negative-binomial distributions. These\\nfamilies generate the well-known class of generalized linear models, which\\nare all extended in the same way to generalized additive models.\\nThe functions fjare estimated in a ﬂexible manner, using an algorithm\\nwhose basic building block is a scatterplot smoother. The estimated func-\\ntionˆfjcan then reveal possible nonlinearities in the eﬀect of Xj. Not all', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b92585b-5b8e-4ed5-9846-0886a5a7af7e', embedding=None, metadata={'page_label': '316', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.1 Generalized Additive Models 297\\nof the functions fjneed to be nonlinear. We can easily mix in linear and\\nother parametric forms with the nonlinear terms, a necessity when some of\\nthe inputs are qualitative variables (factors). The nonlinear terms are not\\nrestricted to main eﬀects either; we can have nonlinear components in two\\nor more variables, or separate curves in Xjfor each level of the factor Xk.\\nThus each of the following would qualify:\\n•g(θ) =XTβ+αk+f(Z)—asemiparametric model, where Xis a\\nvector of predictors to be modeled linearly, αkthe eﬀect for the kth\\nlevel of a qualitative input V, and the eﬀect of predictor Zis modeled\\nnonparametrically.\\n•g(θ) =f(X) +gk(Z)—again kindexes the levels of a qualitative\\ninput V, and thus creates an interaction term g(V,Z) =gk(Z) for\\nthe eﬀect of VandZ.\\n•g(θ) =f(X) +g(Z,W) where gis a nonparametric function in two\\nfeatures.\\nAdditive models can replace linear models in a wide variety of settings,\\nfor example an additive decomposition of time series,\\nYt=St+Tt+εt, (9.5)\\nwhere Stis a seasonal component, Ttis a trend and εis an error term.\\n9.1.1 Fitting Additive Models\\nIn this section we describe a modular algorithm for ﬁtting additive models\\nand their generalizations. The building block is the scatterplot smoother\\nfor ﬁtting nonlinear eﬀects in a ﬂexible way. For concreteness we use as our\\nscatterplot smoother the cubic smoothing spline described in Chapter 5.\\nThe additive model has the form\\nY=α+p∑\\nj=1fj(Xj) +ε, (9.6)\\nwhere the error term εhas mean zero. Given observations xi,yi, a criterion\\nlike the penalized sum of squares (5.9) of Section 5.4 can be speciﬁed for\\nthis problem,\\nPRSS( α,f1,f2,... ,f p) =N∑\\ni=1(\\nyi−α−p∑\\nj=1fj(xij))2\\n+p∑\\nj=1λj∫\\nf′′\\nj(tj)2dtj,\\n(9.7)\\nwhere the λj≥0 are tuning parameters. It can be shown that the minimizer\\nof (9.7) is an additive cubic spline model; each of the functions fjis a', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e321eeab-acd3-4a33-b13e-dbda5f6e29c1', embedding=None, metadata={'page_label': '317', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='298 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.1 The Backﬁtting Algorithm for Additive Models.\\n1. Initialize: ˆ α=1\\nN∑N\\n1yi,ˆfj≡0,∀i,j.\\n2. Cycle: j= 1,2,... ,p,... , 1,2,... ,p,... ,\\nˆfj← S j[\\n{yi−ˆα−∑\\nk̸=jˆfk(xik)}N\\n1]\\n,\\nˆfj←ˆfj−1\\nNN∑\\ni=1ˆfj(xij).\\nuntil the functions ˆfjchange less than a prespeciﬁed threshold.\\ncubic spline in the component Xj, with knots at each of the unique values\\nofxij, i= 1,... ,N . However, without further restrictions on the model,\\nthe solution is not unique. The constant αis not identiﬁable, since we\\ncan add or subtract any constants to each of the functions fj, and adjust\\nαaccordingly. The standard convention is to assume that∑N\\n1fj(xij) =\\n0∀j—the functions average zero over the data. It is easily seen that ˆ α=\\nave(yi) in this case. If in addition to this restriction, the matrix of input\\nvalues (having ijth entry xij) has full column rank, then (9.7) is a strictly\\nconvex criterion and the minimizer is unique. If the matrix is singular, then\\nthelinear part of the components fjcannot be uniquely determined (while\\nthe nonlinear parts can!)(Buja et al., 1989).\\nFurthermore, a simple iterative procedure exists for ﬁnding the solution.\\nWe set ˆ α= ave( yi), and it never changes. We apply a cubic smoothing\\nspline Sjto the targets {yi−ˆα−∑\\nk̸=jˆfk(xik)}N\\n1, as a function of xij,\\nto obtain a new estimate ˆfj. This is done for each predictor in turn, using\\nthe current estimates of the other functions ˆfkwhen computing yi−ˆα−∑\\nk̸=jˆfk(xik). The process is continued until the estimates ˆfjstabilize. This\\nprocedure, given in detail in Algorithm 9.1, is known as “backﬁtting” and\\nthe resulting ﬁt is analogous to a multiple regression for linear models.\\nIn principle, the second step in (2) of Algorithm 9.1 is not needed, since\\nthe smoothing spline ﬁt to a mean-zero response has mean zero (Exer-\\ncise 9.1). In practice, machine rounding can cause slippage, and the ad-\\njustment is advised.\\nThis same algorithm can accommodate other ﬁtting methods in exactly\\nthe same way, by specifying appropriate smoothing operators Sj:\\n•other univariate regression smoothers such as local polynomial re-\\ngression and kernel methods;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8bb2969b-67ea-4a7a-9549-438dac3438ad', embedding=None, metadata={'page_label': '318', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.1 Generalized Additive Models 299\\n•linear regression operators yielding polynomial ﬁts, piecewise con-\\nstant ﬁts, parametric spline ﬁts, series and Fourier ﬁts;\\n•more complicated operators such as surface smoothers for second or\\nhigher-order interactions or periodic smoothers for seasonal eﬀects.\\nIf we consider the operation of smoother Sjonly at the training points, it\\ncan be represented by an N×Noperator matrix Sj(see Section 5.4.1).\\nThen the degrees of freedom for the jth term are (approximately) computed\\nas df j= trace[ Sj]−1, by analogy with degrees of freedom for smoothers\\ndiscussed in Chapters 5 and 6.\\nFor a large class of linear smoothers Sj, backﬁtting is equivalent to a\\nGauss–Seidel algorithm for solving a certain linear system of equations.\\nDetails are given in Exercise 9.2.\\nFor the logistic regression model and other generalized additive models,\\nthe appropriate criterion is a penalized log-likelihood. To maximize it, the\\nbackﬁtting procedure is used in conjunction with a likelihood maximizer.\\nThe usual Newton–Raphson routine for maximizing log-likelihoods in gen-\\neralized linear models can be recast as an IRLS (iteratively reweighted\\nleast squares) algorithm. This involves repeatedly ﬁtting a weighted linear\\nregression of a working response variable on the covariates; each regress ion\\nyields a new value of the parameter estimates, which in turn give new work-\\ning responses and weights, and the process is iterated (see Section 4.4.1).\\nIn the generalized additive model, the weighted linear regression is simply\\nreplaced by a weighted backﬁtting algorithm. We describe the algorithm in\\nmore detail for logistic regression below, and more generally in Chapter 6\\nof Hastie and Tibshirani (1990).\\n9.1.2 Example: Additive Logistic Regression\\nProbably the most widely used model in medical research is the logistic\\nmodel for binary data. In this model the outcome Ycan be coded as 0\\nor 1, with 1 indicating an event (like death or relapse of a disease) and\\n0 indicating no event. We wish to model Pr( Y= 1|X), the probability of\\nan event given values of the prognostic factors XT= (X1,... ,X p). The\\ngoal is usually to understand the roles of the prognostic factors, rather\\nthan to classify new individuals. Logistic models are also used in applica-\\ntions where one is interested in estimating the class probabilities, for use\\nin risk screening. Apart from medical applications, credit risk screening is\\na popular application.\\nThe generalized additive logistic model has the form\\nlogPr(Y= 1|X)\\nPr(Y= 0|X)=α+f1(X1) +≤≤≤+fp(Xp). (9.8)\\nThe functions f1,f2,... ,f pare estimated by a backﬁtting algorithm\\nwithin a Newton–Raphson procedure, shown in Algorithm 9.2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1e4b0cca-3d85-49ac-91ca-c26f6d604080', embedding=None, metadata={'page_label': '319', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='300 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.2 Local Scoring Algorithm for the Additive Logistic Regres-\\nsion Model.\\n1. Compute starting values: ˆ α= log[¯ y/(1−¯y)], where ¯ y= ave( yi), the\\nsample proportion of ones, and set ˆfj≡0∀j.\\n2. Deﬁne ˆ ηi= ˆα+∑\\njˆfj(xij) and ˆ pi= 1/[1 + exp( −ˆηi)].\\nIterate:\\n(a) Construct the working target variable\\nzi= ˆηi+(yi−ˆpi)\\nˆpi(1−ˆpi).\\n(b) Construct weights wi= ˆpi(1−ˆpi)\\n(c) Fit an additive model to the targets ziwith weights wi, us-\\ning a weighted backﬁtting algorithm. This gives new estimates\\nˆα,ˆfj,∀j\\n3. Continue step 2. until the change in the functions falls below a pre-\\nspeciﬁed threshold.\\nThe additive model ﬁtting in step (2) of Algorithm 9.2 requires a weighted\\nscatterplot smoother. Most smoothing procedures can accept observation\\nweights (Exercise 5.12); see Chapter 3 of Hastie and Tibshirani (1990) fo r\\nfurther details.\\nThe additive logistic regression model can be generalized further to han-\\ndle more than two classes, using the multilogit formulation as outlined in\\nSection 4.4. While the formulation is a straightforward extension of ( 9.8),\\nthe algorithms for ﬁtting such models are more complex. See Yee and Wild\\n(1996) for details, and the VGAMsoftware currently available from:\\nhttp://www.stat.auckland.ac.nz/ ∼yee.\\nExample: Predicting Email Spam\\nWe apply a generalized additive model to the spam data introduced in\\nChapter 1. The data consists of information from 4601 email messages, in\\na study to screen email for “spam” (i.e., junk email). The data is publicly\\navailable at ftp.ics.uci.edu , and was donated by George Forman from\\nHewlett-Packard laboratories, Palo Alto, California.\\nThe response variable is binary, with values email orspam, and there are\\n57 predictors as described below:\\n•48 quantitative predictors—the percentage of words in the email that\\nmatch a given word. Examples include business ,address ,internet ,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='432927f9-2466-400a-bcad-86919a1b80e1', embedding=None, metadata={'page_label': '320', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.1 Generalized Additive Models 301\\nTABLE 9.1. Test data confusion matrix for the additive logistic regress ion model\\nﬁt to the spam training data. The overall test error rate is 5.5%.\\nPredicted Class\\nTrue Class email (0)spam(1)\\nemail (0) 58.3% 2.5%\\nspam(1) 3.0% 36.3%\\nfree, andgeorge . The idea was that these could be customized for\\nindividual users.\\n•6 quantitative predictors—the percentage of characters in the email\\nthat match a given character. The characters are ch;,ch(,ch[,ch!,\\nch$, andch#.\\n•The average length of uninterrupted sequences of capital letters:\\nCAPAVE .\\n•The length of the longest uninterrupted sequence of capital letters:\\nCAPMAX .\\n•The sum of the length of uninterrupted sequences of capital letters:\\nCAPTOT .\\nWe coded spamas 1 and email as zero. A test set of size 1536 was randomly\\nchosen, leaving 3065 observations in the training set. A generalized additive\\nmodel was ﬁt, using a cubic smoothing spline with a nominal four degrees of\\nfreedom for each predictor. What this means is that for each predictor Xj,\\nthe smoothing-spline parameter λjwas chosen so that trace[ Sj(λj)]−1 = 4,\\nwhereSj(λ) is the smoothing spline operator matrix constructed using the\\nobserved values xij, i= 1,... ,N . This is a convenient way of specifying\\nthe amount of smoothing in such a complex model.\\nMost of the spampredictors have a very long-tailed distribution. Before\\nﬁtting the GAM model, we log-transformed each variable (actually log( x+\\n0.1)), but the plots in Figure 9.1 are shown as a function of the original\\nvariables.\\nThe test error rates are shown in Table 9.1; the overall error rate is 5.3 %.\\nBy comparison, a linear logistic regression has a test error rate of 7.6% .\\nTable 9.2 shows the predictors that are highly signiﬁcant in the additive\\nmodel.\\nFor ease of interpretation, in Table 9.2 the contribution for each variabl e\\nis decomposed into a linear component and the remaining nonlinear com-\\nponent. The top block of predictors are positively correlated with spam,\\nwhile the bottom block is negatively correlated. The linear component is a\\nweighted least squares linear ﬁt of the ﬁtted curve on the predictor, while\\nthe nonlinear part is the residual. The linear component of an estimated', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='730a167c-d30b-44bb-a4fe-3ab551ff8e61', embedding=None, metadata={'page_label': '321', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='302 9. Additive Models, Trees, and Related Methods\\nTABLE 9.2. Signiﬁcant predictors from the additive model ﬁt to the spam train-\\ning data. The coeﬃcients represent the linear part of ˆfj, along with their standard\\nerrors and Z-score. The nonlinear P-value is for a test of nonlineari ty of ˆfj.\\nName Num. df Coeﬃcient Std. Error ZScore Nonlinear\\nP-value\\nPositive eﬀects\\nour 5 3.9 0.566 0.114 4.970 0.052\\nover 6 3.9 0.244 0.195 1.249 0.004\\nremove 7 4.0 0.949 0.183 5.201 0.093\\ninternet 8 4.0 0.524 0.176 2.974 0.028\\nfree 16 3.9 0.507 0.127 4.010 0.065\\nbusiness 17 3.8 0.779 0.186 4.179 0.194\\nhpl 26 3.8 0.045 0.250 0.181 0.002\\nch! 52 4.0 0.674 0.128 5.283 0.164\\nch$ 53 3.9 1.419 0.280 5.062 0.354\\nCAPMAX 56 3.8 0.247 0.228 1.080 0.000\\nCAPTOT 57 4.0 0.755 0.165 4.566 0.063\\nNegative eﬀects\\nhp 25 3.9 −1.404 0.224 −6.262 0.140\\ngeorge 27 3.7 −5.003 0.744 −6.722 0.045\\n1999 37 3.8 −0.672 0.191 −3.512 0.011\\nre 45 3.9 −0.620 0.133 −4.649 0.597\\nedu 46 4.0 −1.183 0.209 −5.647 0.000\\nfunction is summarized by the coeﬃcient, standard error and Z-score; the\\nlatter is the coeﬃcient divided by its standard error, and is considered\\nsigniﬁcant if it exceeds the appropriate quantile of a standard normal dis-\\ntribution. The column labeled nonlinear P-value is a test of nonlinearity\\nof the estimated function. Note, however, that the eﬀect of each predictor\\nis fully adjusted for the entire eﬀects of the other predictors, not just for\\ntheir linear parts. The predictors shown in the table were judged signiﬁ-\\ncant by at least one of the tests (linear or nonlinear) at the p= 0.01 level\\n(two-sided).\\nFigure 9.1 shows the estimated functions for the signiﬁcant predictors\\nappearing in Table 9.2. Many of the nonlinear eﬀects appear to account for\\na strong discontinuity at zero. For example, the probability of spamdrops\\nsigniﬁcantly as the frequency of george increases from zero, but then does\\nnot change much after that. This suggests that one might replace each of\\nthe frequency predictors by an indicator variable for a zero count, and resort\\nto a linear logistic model. This gave a test error rate of 7 .4%; including the\\nlinear eﬀects of the frequencies as well dropped the test error to 6 .6%. It\\nappears that the nonlinearities in the additive model have an additional\\npredictive power.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b7aa4547-7de2-4c7e-b0fd-93f95fe6a452', embedding=None, metadata={'page_label': '322', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.1 Generalized Additive Models 303\\n0 2 4 6 8-5 0 5\\n0 1 2 3-5 0 5\\n0 2 4 6-5 0 5 10\\n0 2 4 6 8 10-5 0 5 10\\n0 2 4 6 8 10-5 0 5 10\\n0 2 4 6-5 0 5 10\\n0 5 10 15 20-10 -5 0\\n0 5 10-10 -5 0\\n0 10 20 30-10 -5 0 5\\n0 2 4 6-5 0 5\\n0 5 10 15 20-10 -5 0 5\\n0 5 10 15-10 -5 0\\n0 10 20 30-5 0 5 10\\n0 1 2 3 4 5 6-5 0 5 10\\n0 2000 6000 10000-5 0 5\\n0 5000 10000 15000-5 0 5our over remove internet\\nfree business hp hpl\\ngeorge 1999 re edu\\nch! ch$ CAPMAX CAPTOTˆf(our)\\nˆf(over)\\nˆf(remove )\\nˆf(internet )ˆf(free)\\nˆf(business )\\nˆf(hp)\\nˆf(hpl)ˆf(george )\\nˆf(1999)\\nˆf(re)\\nˆf(edu)ˆf(ch!)\\nˆf(ch$)\\nˆf(CAPMAX )\\nˆf(CAPTOT )\\nFIGURE 9.1. Spam analysis: estimated functions for signiﬁcant predictors. The\\nrug plot along the bottom of each frame indicates the observed values of the cor-\\nresponding predictor. For many of the predictors the nonlinearity picks up the\\ndiscontinuity at zero.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e05dce70-ffec-4dc9-811d-9b901101ffb8', embedding=None, metadata={'page_label': '323', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='304 9. Additive Models, Trees, and Related Methods\\nIt is more serious to classify a genuine email message as spam, since then\\na good email would be ﬁltered out and would not reach the user. We can\\nalter the balance between the class error rates by changing the losses (see\\nSection 2.4). If we assign a loss L01for predicting a true class 0 as class 1,\\nandL10for predicting a true class 1 as class 0, then the estimated Bayes\\nrule predicts class 1 if its probability is greater than L01/(L01+L10). For\\nexample, if we take L01= 10,L10= 1 then the (true) class 0 and class 1\\nerror rates change to 0.8% and 8.7%.\\nMore ambitiously, we can encourage the model to ﬁt better data in the\\nclass 0 by using weights L01for the class 0 observations and L10for the\\nclass 1 observations. As above, we then use the estimated Bayes rule to\\npredict. This gave error rates of 1.2% and 8.0% in (true) class 0 and class 1,\\nrespectively. We discuss below the issue of unequal losses further, in the\\ncontext of tree-based models.\\nAfter ﬁtting an additive model, one should check whether the inclusion\\nof some interactions can signiﬁcantly improve the ﬁt. This can be done\\n“manually,” by inserting products of some or all of the signiﬁcant inputs,\\nor automatically via the MARS procedure (Section 9.4).\\nThis example uses the additive model in an automatic fashion. As a data\\nanalysis tool, additive models are often used in a more interactive fashi on,\\nadding and dropping terms to determine their eﬀect. By calibrating the\\namount of smoothing in terms of df j, one can move seamlessly between\\nlinear models (df j= 1) and partially linear models, where some terms are\\nmodeled more ﬂexibly. See Hastie and Tibshirani (1990) for more details.\\n9.1.3 Summary\\nAdditive models provide a useful extension of linear models, making them\\nmore ﬂexible while still retaining much of their interpretability. The famil iar\\ntools for modeling and inference in linear models are also available for\\nadditive models, seen for example in Table 9.2. The backﬁtting procedure\\nfor ﬁtting these models is simple and modular, allowing one to choose a\\nﬁtting method appropriate for each input variable. As a result they have\\nbecome widely used in the statistical community.\\nHowever additive models can have limitations for large data-mining ap-\\nplications. The backﬁtting algorithm ﬁts all predictors, which is not feasi-\\nble or desirable when a large number are available. The BRUTO procedure\\n(Hastie and Tibshirani, 1990, Chapter 9) combines backﬁtting with selec-\\ntion of inputs, but is not designed for large data-mining problems. There\\nhas also been recent work using lasso-type penalties to estimate sparse ad-\\nditive models, for example the COSSO procedure of Lin and Zhang (2006)\\nand the SpAM proposal of Ravikumar et al. (2008). For large problems a\\nforward stagewise approach such as boosting (Chapter 10) is more eﬀectiv e,\\nand also allows for interactions to be included in the model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='24e444e8-8754-4b95-ba8a-3883cbeca077', embedding=None, metadata={'page_label': '324', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Tree-Based Methods 305\\n9.2 Tree-Based Methods\\n9.2.1 Background\\nTree-based methods partition the feature space into a set of rectangles, and\\nthen ﬁt a simple model (like a constant) in each one. They are conceptually\\nsimple yet powerful. We ﬁrst describe a popular method for tree-based\\nregression and classiﬁcation called CART, and later contrast it with C4. 5,\\na major competitor.\\nLet’s consider a regression problem with continuous response Yand in-\\nputsX1andX2, each taking values in the unit interval. The top left panel\\nof Figure 9.2 shows a partition of the feature space by lines that are parall el\\nto the coordinate axes. In each partition element we can model Ywith a\\ndiﬀerent constant. However, there is a problem: although each partitioning\\nline has a simple description like X1=c, some of the resulting regions are\\ncomplicated to describe.\\nTo simplify matters, we restrict attention to recursive binary partitio ns\\nlike that in the top right panel of Figure 9.2. We ﬁrst split the space into\\ntwo regions, and model the response by the mean of Yin each region.\\nWe choose the variable and split-point to achieve the best ﬁt. Then one\\nor both of these regions are split into two more regions, and this process\\nis continued, until some stopping rule is applied. For example, in the top\\nright panel of Figure 9.2, we ﬁrst split at X1=t1. Then the region X1≤t1\\nis split at X2=t2and the region X1> t1is split at X1=t3. Finally, the\\nregion X1> t3is split at X2=t4. The result of this process is a partition\\ninto the ﬁve regions R1,R2,... ,R 5shown in the ﬁgure. The corresponding\\nregression model predicts Ywith a constant cmin region Rm, that is,\\nˆf(X) =5∑\\nm=1cmI{(X1,X2)∈Rm}. (9.9)\\nThis same model can be represented by the binary tree in the bottom left\\npanel of Figure 9.2. The full dataset sits at the top of the tree. Observations\\nsatisfying the condition at each junction are assigned to the left branch,\\nand the others to the right branch. The terminal nodes or leaves of the\\ntree correspond to the regions R1,R2,... ,R 5. The bottom right panel of\\nFigure 9.2 is a perspective plot of the regression surface from this model.\\nFor illustration, we chose the node means c1=−5,c2=−7,c3= 0,c4=\\n2,c5= 4 to make this plot.\\nA key advantage of the recursive binary tree is its interpretability. The\\nfeature space partition is fully described by a single tree. With more than\\ntwo inputs, partitions like that in the top right panel of Figure 9.2 are\\ndiﬃcult to draw, but the binary tree representation works in the same\\nway. This representation is also popular among medical scientists, perhaps\\nbecause it mimics the way that a doctor thinks. The tree stratiﬁes the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90ad91fb-97f6-4c0b-a4a4-a87beebe36df', embedding=None, metadata={'page_label': '325', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='306 9. Additive Models, Trees, and Related Methods\\n|t1t2\\nt3t4\\nR1R1\\nR2R2\\nR3R3\\nR4R4\\nR5R5\\nX1X1 X1\\nX2X2X2\\nX1≤t1\\nX2≤t2 X1≤t3\\nX2≤t4\\nFIGURE 9.2. Partitions and CART. Top right panel shows a partition of a\\ntwo-dimensional feature space by recursive binary splitting, a s used in CART,\\napplied to some fake data. Top left panel shows a general partit ion that cannot\\nbe obtained from recursive binary splitting. Bottom left panel s hows the tree cor-\\nresponding to the partition in the top right panel, and a perspect ive plot of the\\nprediction surface appears in the bottom right panel.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1b831f59-14db-45dd-b2d8-547cd87fba93', embedding=None, metadata={'page_label': '326', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Tree-Based Methods 307\\npopulation into strata of high and low outcome, on the basis of patient\\ncharacteristics.\\n9.2.2 Regression Trees\\nWe now turn to the question of how to grow a regression tree. Our data\\nconsists of pinputs and a response, for each of Nobservations: that is,\\n(xi,yi) for i= 1,2,... ,N , with xi= (xi1,xi2,... ,x ip). The algorithm\\nneeds to automatically decide on the splitting variables and split points,\\nand also what topology (shape) the tree should have. Suppose ﬁrst that we\\nhave a partition into Mregions R1,R2,... ,R M, and we model the response\\nas a constant cmin each region:\\nf(x) =M∑\\nm=1cmI(x∈Rm). (9.10)\\nIf we adopt as our criterion minimization of the sum of squares∑(yi−\\nf(xi))2, it is easy to see that the best ˆ cmis just the average of yiin region\\nRm:\\nˆcm= ave( yi|xi∈Rm). (9.11)\\nNow ﬁnding the best binary partition in terms of minimum sum of squares\\nis generally computationally infeasible. Hence we proceed with a greedy\\nalgorithm. Starting with all of the data, consider a splitting variable jand\\nsplit point s, and deﬁne the pair of half-planes\\nR1(j,s) ={X|Xj≤s}andR2(j,s) ={X|Xj> s}. (9.12)\\nThen we seek the splitting variable jand split point sthat solve\\nmin\\nj, s[\\nmin\\nc1∑\\nxi∈R1(j,s)(yi−c1)2+ min\\nc2∑\\nxi∈R2(j,s)(yi−c2)2]\\n. (9.13)\\nFor any choice jands, the inner minimization is solved by\\nˆc1= ave( yi|xi∈R1(j,s)) and ˆ c2= ave( yi|xi∈R2(j,s)). (9.14)\\nFor each splitting variable, the determination of the split point scan\\nbe done very quickly and hence by scanning through all of the inputs,\\ndetermination of the best pair ( j,s) is feasible.\\nHaving found the best split, we partition the data into the two resulting\\nregions and repeat the splitting process on each of the two regions. Then\\nthis process is repeated on all of the resulting regions.\\nHow large should we grow the tree? Clearly a very large tree might overﬁt\\nthe data, while a small tree might not capture the important structure.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='11350385-e8f7-42b9-945a-c8bf323f0542', embedding=None, metadata={'page_label': '327', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='308 9. Additive Models, Trees, and Related Methods\\nTree size is a tuning parameter governing the model’s complexity, and the\\noptimal tree size should be adaptively chosen from the data. One approach\\nwould be to split tree nodes only if the decrease in sum-of-squares due to the\\nsplit exceeds some threshold. This strategy is too short-sighted, however,\\nsince a seemingly worthless split might lead to a very good split below it.\\nThe preferred strategy is to grow a large tree T0, stopping the splitting\\nprocess only when some minimum node size (say 5) is reached. Then this\\nlarge tree is pruned using cost-complexity pruning , which we now describe.\\nWe deﬁne a subtree T⊂T0to be any tree that can be obtained by\\npruning T0, that is, collapsing any number of its internal (non-terminal)\\nnodes. We index terminal nodes by m, with node mrepresenting region\\nRm. Let|T|denote the number of terminal nodes in T. Letting\\nNm= #{xi∈Rm},\\nˆcm=1\\nNm∑\\nxi∈Rmyi,\\nQm(T) =1\\nNm∑\\nxi∈Rm(yi−ˆcm)2,(9.15)\\nwe deﬁne the cost complexity criterion\\nCα(T) =|T|∑\\nm=1NmQm(T) +α|T|. (9.16)\\nThe idea is to ﬁnd, for each α, the subtree Tα⊆T0to minimize Cα(T).\\nThe tuning parameter α≥0 governs the tradeoﬀ between tree size and its\\ngoodness of ﬁt to the data. Large values of αresult in smaller trees Tα, and\\nconversely for smaller values of α. As the notation suggests, with α= 0 the\\nsolution is the full tree T0. We discuss how to adaptively choose αbelow.\\nFor each αone can show that there is a unique smallest subtree Tαthat\\nminimizes Cα(T). To ﬁnd Tαwe use weakest link pruning : we successively\\ncollapse the internal node that produces the smallest per-node increase in∑\\nmNmQm(T), and continue until we produce the single-node (root) tree.\\nThis gives a (ﬁnite) sequence of subtrees, and one can show this sequence\\nmust contain Tα. See Breiman et al. (1984) or Ripley (1996) for details.\\nEstimation of αis achieved by ﬁve- or tenfold cross-validation: we choose\\nthe value ˆ αto minimize the cross-validated sum of squares. Our ﬁnal tree\\nisTˆα.\\n9.2.3 Classiﬁcation Trees\\nIf the target is a classiﬁcation outcome taking values 1 ,2,... ,K , the only\\nchanges needed in the tree algorithm pertain to the criteria for splitting\\nnodes and pruning the tree. For regression we used the squared-error node', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97206ae7-512b-46e4-9422-359f7df1f08a', embedding=None, metadata={'page_label': '328', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Tree-Based Methods 309\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5\\npEntropy\\nGini index\\nMisclassification error\\nFIGURE 9.3. Node impurity measures for two-class classiﬁcation, as a funct ion\\nof the proportion pin class 2. Cross-entropy has been scaled to pass through\\n(0.5,0.5).\\nimpurity measure Qm(T) deﬁned in (9.15), but this is not suitable for\\nclassiﬁcation. In a node m, representing a region RmwithNmobservations,\\nlet\\nˆpmk=1\\nNm∑\\nxi∈RmI(yi=k),\\nthe proportion of class kobservations in node m. We classify the obser-\\nvations in node mto class k(m) = arg max kˆpmk, the majority class in\\nnodem. Diﬀerent measures Qm(T) of node impurity include the following:\\nMisclassiﬁcation error:1\\nNm∑\\ni∈RmI(yi̸=k(m)) = 1 −ˆpmk(m).\\nGini index:∑\\nk̸=k′ˆpmkˆpmk′=∑K\\nk=1ˆpmk(1−ˆpmk).\\nCross-entropy or deviance: −∑K\\nk=1ˆpmklog ˆpmk.\\n(9.17)\\nFor two classes, if pis the proportion in the second class, these three mea-\\nsures are 1 −max(p,1−p), 2p(1−p) and −plogp−(1−p)log (1 −p),\\nrespectively. They are shown in Figure 9.3. All three are similar, but cross -\\nentropy and the Gini index are diﬀerentiable, and hence more amenable to\\nnumerical optimization. Comparing (9.13) and (9.15), we see that we need\\nto weight the node impurity measures by the number NmLandNmRof\\nobservations in the two child nodes created by splitting node m.\\nIn addition, cross-entropy and the Gini index are more sensitive to changes\\nin the node probabilities than the misclassiﬁcation rate. For example, in\\na two-class problem with 400 observations in each class (denote this by\\n(400,400)), suppose one split created nodes (300 ,100) and (100 ,300), while', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab0db6bf-cc2c-432b-b3b8-ffbaa115426c', embedding=None, metadata={'page_label': '329', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='310 9. Additive Models, Trees, and Related Methods\\nthe other created nodes (200 ,400) and (200 ,0). Both splits produce a mis-\\nclassiﬁcation rate of 0.25, but the second split produces a pure node and is\\nprobably preferable. Both the Gini index and cross-entropy are lower for the\\nsecond split. For this reason, either the Gini index or cross-entropy should\\nbe used when growing the tree. To guide cost-complexity pruning, any of\\nthe three measures can be used, but typically it is the misclassiﬁcation rate.\\nThe Gini index can be interpreted in two interesting ways. Rather than\\nclassify observations to the majority class in the node, we could classify\\nthem to class kwith probability ˆ pmk. Then the training error rate of this\\nrule in the node is∑\\nk̸=k′ˆpmkˆpmk′—the Gini index. Similarly, if we code\\neach observation as 1 for class kand zero otherwise, the variance over the\\nnode of this 0-1 response is ˆ pmk(1−ˆpmk). Summing over classes kagain\\ngives the Gini index.\\n9.2.4 Other Issues\\nCategorical Predictors\\nWhen splitting a predictor having qpossible unordered values, there are\\n2q−1−1 possible partitions of the qvalues into two groups, and the com-\\nputations become prohibitive for large q. However, with a 0 −1 outcome,\\nthis computation simpliﬁes. We order the predictor classes according to the\\nproportion falling in outcome class 1. Then we split this predictor as if it\\nwere an ordered predictor. One can show this gives the optimal split, in\\nterms of cross-entropy or Gini index, among all possible 2q−1−1 splits. This\\nresult also holds for a quantitative outcome and square error loss—the cat-\\negories are ordered by increasing mean of the outcome. Although intuitive,\\nthe proofs of these assertions are not trivial. The proof for binary outcomes\\nis given in Breiman et al. (1984) and Ripley (1996); the proof for quanti ta-\\ntive outcomes can be found in Fisher (1958). For multicategory outcomes,\\nno such simpliﬁcations are possible, although various approximations have\\nbeen proposed (Loh and Vanichsetakul, 1988).\\nThe partitioning algorithm tends to favor categorical predictors with\\nmany levels q; the number of partitions grows exponentially in q, and the\\nmore choices we have, the more likely we can ﬁnd a good one for the data\\nat hand. This can lead to severe overﬁtting if qis large, and such variables\\nshould be avoided.\\nThe Loss Matrix\\nIn classiﬁcation problems, the consequences of misclassifying observations\\nare more serious in some classes than others. For example, it is probably\\nworse to predict that a person will not have a heart attack when he/she\\nactually will, than vice versa. To account for this, we deﬁne a K×Kloss\\nmatrix L, with Lkk′being the loss incurred for classifying a class kobser-\\nvation as class k′. Typically no loss is incurred for correct classiﬁcations,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1714c2e3-1064-470e-89f7-03e32bd75386', embedding=None, metadata={'page_label': '330', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Tree-Based Methods 311\\nthat is, Lkk= 0∀k. To incorporate the losses into the modeling process,\\nwe could modify the Gini index to∑\\nk̸=k′Lkk′ˆpmkˆpmk′; this would be the\\nexpected loss incurred by the randomized rule. This works for the multi-\\nclass case, but in the two-class case has no eﬀect, since the coeﬃcient of\\nˆpmkˆpmk′isLkk′+Lk′k. For two classes a better approach is to weight the\\nobservations in class kbyLkk′. This can be used in the multiclass case only\\nif, as a function of k,Lkk′doesn’t depend on k′. Observation weighting can\\nbe used with the deviance as well. The eﬀect of observation weighting is to\\nalter the prior probability on the classes. In a terminal node, the empirical\\nBayes rule implies that we classify to class k(m) = arg min k∑\\nℓLℓkˆpmℓ.\\nMissing Predictor Values\\nSuppose our data has some missing predictor values in some or all of the\\nvariables. We might discard any observation with some missing values, but\\nthis could lead to serious depletion of the training set. Alternatively we\\nmight try to ﬁll in (impute) the missing values, with say the mean of that\\npredictor over the nonmissing observations. For tree-based models, there\\nare two better approaches. The ﬁrst is applicable to categorical predictors:\\nwe simply make a new category for “missing.” From this we might dis-\\ncover that observations with missing values for some measurement behave\\ndiﬀerently than those with nonmissing values. The second more general\\napproach is the construction of surrogate variables. When considering a\\npredictor for a split, we use only the observations for which that predictor\\nis not missing. Having chosen the best (primary) predictor and split point,\\nwe form a list of surrogate predictors and split points. The ﬁrst surroga te\\nis the predictor and corresponding split point that best mimics the split of\\nthe training data achieved by the primary split. The second surrogate is\\nthe predictor and corresponding split point that does second best, and so\\non. When sending observations down the tree either in the training phase\\nor during prediction, we use the surrogate splits in order, if the primary\\nsplitting predictor is missing. Surrogate splits exploit correlations between\\npredictors to try and alleviate the eﬀect of missing data. The higher the cor-\\nrelation between the missing predictor and the other predictors, the smaller\\nthe loss of information due to the missing value. The general problem of\\nmissing data is discussed in Section 9.6.\\nWhy Binary Splits?\\nRather than splitting each node into just two groups at each stage (as\\nabove), we might consider multiway splits into more than two groups. Whil e\\nthis can sometimes be useful, it is not a good general strategy. The problem\\nis that multiway splits fragment the data too quickly, leaving insuﬃcient\\ndata at the next level down. Hence we would want to use such splits only\\nwhen needed. Since multiway splits can be achieved by a series of binary\\nsplits, the latter are preferred.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c0f11c1-5964-4caa-ba11-cd02a34d02a9', embedding=None, metadata={'page_label': '331', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='312 9. Additive Models, Trees, and Related Methods\\nOther Tree-Building Procedures\\nThe discussion above focuses on the CART (classiﬁcation and regression\\ntree) implementation of trees. The other popular methodology is ID3 and\\nits later versions, C4.5 and C5.0 (Quinlan, 1993). Early versions of the\\nprogram were limited to categorical predictors, and used a top-down rule\\nwith no pruning. With more recent developments, C5.0 has become quite\\nsimilar to CART. The most signiﬁcant feature unique to C5.0 is a scheme\\nfor deriving rule sets. After a tree is grown, the splitting rules that deﬁne the\\nterminal nodes can sometimes be simpliﬁed: that is, one or more condition\\ncan be dropped without changing the subset of observations that fall in\\nthe node. We end up with a simpliﬁed set of rules deﬁning each terminal\\nnode; these no longer follow a tree structure, but their simplicity might\\nmake them more attractive to the user.\\nLinear Combination Splits\\nRather than restricting splits to be of the form Xj≤s, one can allow splits\\nalong linear combinations of the form∑ajXj≤s. The weights ajand\\nsplit point sare optimized to minimize the relevant criterion (such as the\\nGini index). While this can improve the predictive power of the tree, it can\\nhurt interpretability. Computationally, the discreteness of the split point\\nsearch precludes the use of a smooth optimization for the weights. A better\\nway to incorporate linear combination splits is in the hierarchical mixtures\\nof experts (HME) model, the topic of Section 9.5.\\nInstability of Trees\\nOne major problem with trees is their high variance. Often a small change\\nin the data can result in a very diﬀerent series of splits, making interpre-\\ntation somewhat precarious. The major reason for this instability is the\\nhierarchical nature of the process: the eﬀect of an error in the top split\\nis propagated down to all of the splits below it. One can alleviate this to\\nsome degree by trying to use a more stable split criterion, but the inherent\\ninstability is not removed. It is the price to be paid for estimating a simple,\\ntree-based structure from the data. Bagging (Section 8.7) averages many\\ntrees to reduce this variance.\\nLack of Smoothness\\nAnother limitation of trees is the lack of smoothness of the prediction sur-\\nface, as can be seen in the bottom right panel of Figure 9.2. In classiﬁcation\\nwith 0/1 loss, this doesn’t hurt much, since bias in estimation of the class\\nprobabilities has a limited eﬀect. However, this can degrade performance\\nin the regression setting, where we would normally expect the underlying\\nfunction to be smooth. The MARS procedure, described in Section 9.4,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e3323121-ad8e-47cd-9936-f3d6ddb5cf02', embedding=None, metadata={'page_label': '332', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Tree-Based Methods 313\\nTABLE 9.3. Spam data: confusion rates for the 17-node tree (chosen by cross–\\nvalidation) on the test data. Overall error rate is 9.3%.\\nPredicted\\nTrueemail spam\\nemail 57.3% 4.0%\\nspam 5.3% 33.4%\\ncan be viewed as a modiﬁcation of CART designed to alleviate this lack of\\nsmoothness.\\nDiﬃculty in Capturing Additive Structure\\nAnother problem with trees is their diﬃculty in modeling additive struc-\\nture. In regression, suppose, for example, that Y=c1I(X1< t1)+c2I(X2<\\nt2) +εwhere εis zero-mean noise. Then a binary tree might make its ﬁrst\\nsplit on X1neart1. At the next level down it would have to split both nodes\\nonX2att2in order to capture the additive structure. This might happen\\nwith suﬃcient data, but the model is given no special encouragement to ﬁnd\\nsuch structure. If there were ten rather than two additive eﬀects, it would\\ntake many fortuitous splits to recreate the structure, and the data analyst\\nwould be hard pressed to recognize it in the estimated tree. The “blame”\\nhere can again be attributed to the binary tree structure, which has both\\nadvantages and drawbacks. Again the MARS method (Section 9.4) gives\\nup this tree structure in order to capture additive structure.\\n9.2.5 Spam Example (Continued)\\nWe applied the classiﬁcation tree methodology to the spamexample intro-\\nduced earlier. We used the deviance measure to grow the tree and mis-\\nclassiﬁcation rate to prune it. Figure 9.4 shows the 10-fold cross-validat ion\\nerror rate as a function of the size of the pruned tree, along with ±2 stan-\\ndard errors of the mean, from the ten replications. The test error curve is\\nshown in orange. Note that the cross-validation error rates are indexed by\\na sequence of values of αandnottree size; for trees grown in diﬀerent folds,\\na value of αmight imply diﬀerent sizes. The sizes shown at the base of the\\nplot refer to |Tα|, the sizes of the pruned original tree.\\nThe error ﬂattens out at around 17 terminal nodes, giving the pruned tree\\nin Figure 9.5. Of the 13 distinct features chosen by the tree, 11 overlap with\\nthe 16 signiﬁcant features in the additive model (Table 9.2). The overall\\nerror rate shown in Table 9.3 is about 50% higher than for the additive\\nmodel in Table 9.1.\\nConsider the rightmost branches of the tree. We branch to the right\\nwith aspamwarning if more than 5.5% of the characters are the $ sign.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cd9a0cf8-ae58-4871-ae56-47b571cd511b', embedding=None, metadata={'page_label': '333', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='314 9. Additive Models, Trees, and Related Methods\\n0 10 20 30 400.0 0.1 0.2 0.3 0.4\\nTree SizeMisclassification Rate176 21 7 5 3 2 0α\\nFIGURE 9.4. Results for spamexample. The blue curve is the 10-fold cross-val-\\nidation estimate of misclassiﬁcation rate as a function of tre e size, with standard\\nerror bars. The minimum occurs at a tree size with about 17terminal nodes (using\\nthe “one-standard-error” rule). The orange curve is the test er ror, which tracks\\nthe CV error quite closely. The cross-validation is indexed by values of α, shown\\nabove. The tree sizes shown below refer to |Tα|, the size of the original tree indexed\\nbyα.\\nHowever, if in addition the phrase hpoccurs frequently, then this is likely\\nto be company business and we classify as email. All of the 22 cases in\\nthe test set satisfying these criteria were correctly classiﬁed. If the second\\ncondition is not met, and in addition the average length of repeated capital\\nlettersCAPAVE is larger than 2.9, then we classify as spam. Of the 227 test\\ncases, only seven were misclassiﬁed.\\nIn medical classiﬁcation problems, the terms sensitivity andspeciﬁcity\\nare used to characterize a rule. They are deﬁned as follows:\\nSensitivity: probability of predicting disease given true state is disease.\\nSpeciﬁcity: probability of predicting non-disease given true state is non-\\ndisease.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='481ff83b-cd87-4972-82b6-e743b07114c3', embedding=None, metadata={'page_label': '334', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Tree-Based Methods 315\\n600/1536\\n280/1177\\n180/1065\\n 80/861\\n 80/652\\n 77/423\\n 20/238\\n 19/236   1/2 57/185\\n 48/113\\n 37/101   1/12  9/72  3/229  0/209100/204\\n 36/123\\n 16/94\\n 14/89   3/5  9/29 16/81  9/112\\n  6/109   0/3 48/359\\n 26/337\\n 19/110\\n 18/109   0/1  7/227  0/22\\nspam\\nspamspamspamspam\\nspamspam\\nspam\\nspam\\nspam\\nspamspamemail\\nemailemail\\nemailemailemailemail\\nemail\\nemail\\nemail\\nemailemailemail\\nemailemailemailemailemailemailemailemail\\nch$<0.0555\\nremove<0.06\\nch!<0.191\\ngeorge<0.005\\nhp<0.03\\nCAPMAX<10.5\\nreceive<0.125 edu<0.045\\nour<1.2CAPAVE<2.7505\\nfree<0.065\\nbusiness<0.145george<0.15hp<0.405\\nCAPAVE<2.907\\n1999<0.58ch$>0.0555\\nremove>0.06\\nch!>0.191\\ngeorge>0.005\\nhp>0.03\\nCAPMAX>10.5\\nreceive>0.125 edu>0.045\\nour>1.2CAPAVE>2.7505\\nfree>0.065\\nbusiness>0.145george>0.15hp>0.405\\nCAPAVE>2.907\\n1999>0.58\\nFIGURE 9.5. The pruned tree for the spamexample. The split variables are\\nshown in blue on the branches, and the classiﬁcation is shown in e very node.The\\nnumbers under the terminal nodes indicate misclassiﬁcation rates on the test data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4c4b3405-aa7e-4a09-a817-e78faf5040bf', embedding=None, metadata={'page_label': '335', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='316 9. Additive Models, Trees, and Related Methods\\nSpecificitySensitivity\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0• •\\n•• •\\n•\\n•\\n•\\n••\\n•\\n•••• ••••••••••••••• ••••• ••••••••• •••••••••••••\\n••\\n•\\n•\\n••\\n••\\n•••\\n•\\n•\\n•\\n•\\n•Tree (0.95)\\nGAM (0.98)\\nWeighted Tree (0.90)\\nFIGURE 9.6. ROC curves for the classiﬁcation rules ﬁt to the spamdata. Curves\\nthat are closer to the northeast corner represent better classi ﬁers. In this case the\\nGAM classiﬁer dominates the trees. The weighted tree achieves better sensitivity\\nfor higher speciﬁcity than the unweighted tree. The numbers in t he legend repre-\\nsent the area under the curve.\\nIf we think of spamandemail as the presence and absence of disease, re-\\nspectively, then from Table 9.3 we have\\nSensitivity = 100 ×33.4\\n33.4 + 5.3= 86.3%,\\nSpeciﬁcity = 100 ×57.3\\n57.3 + 4.0= 93.4%.\\nIn this analysis we have used equal losses. As before let Lkk′be the\\nloss associated with predicting a class kobject as class k′. By varying the\\nrelative sizes of the losses L01andL10, we increase the sensitivity and\\ndecrease the speciﬁcity of the rule, or vice versa. In this example, we want\\nto avoid marking good email asspam, and thus we want the speciﬁcity to\\nbe very high. We can achieve this by setting L01>1 say, with L10= 1.\\nThe Bayes’ rule in each terminal node classiﬁes to class 1 ( spam) if the\\nproportion of spamis≥L01/(L10+L01), and class zero otherwise. The', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='197a24ef-2940-4e96-adc7-33d6584ba25c', embedding=None, metadata={'page_label': '336', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 PRIM: Bump Hunting 317\\nreceiver operating characteristic curve (ROC) is a commonly used summary\\nfor assessing the tradeoﬀ between sensitivity and speciﬁcity. It is a plot of\\nthe sensitivity versus speciﬁcity as we vary the parameters of a classiﬁcation\\nrule. Varying the loss L01between 0.1 and 10, and applying Bayes’ rule to\\nthe 17-node tree selected in Figure 9.4, produced the ROC curve shown\\nin Figure 9.6. The standard error of each curve near 0.9 is approximately√\\n0.9(1−0.9)/1536 = 0 .008, and hence the standard error of the diﬀerence\\nis about 0 .01. We see that in order to achieve a speciﬁcity of close to 100%,\\nthe sensitivity has to drop to about 50%. The area under the curve is a\\ncommonly used quantitative summary; extending the curve linearly in each\\ndirection so that it is deﬁned over [0 ,100], the area is approximately 0 .95.\\nFor comparison, we have included the ROC curve for the GAM model ﬁt\\nto these data in Section 9.2; it gives a better classiﬁcation rule for any los s,\\nwith an area of 0 .98.\\nRather than just modifying the Bayes rule in the nodes, it is better to\\ntake full account of the unequal losses in growing the tree, as was done\\nin Section 9.2. With just two classes 0 and 1, losses may be incorporated\\ninto the tree-growing process by using weight Lk,1−kfor an observation in\\nclassk. Here we chose L01= 5,L10= 1 and ﬁt the same size tree as before\\n(|Tα|= 17). This tree has higher sensitivity at high values of the speciﬁcity\\nthan the original tree, but does more poorly at the other extreme. Its top\\nfew splits are the same as the original tree, and then it departs from it.\\nFor this application the tree grown using L01= 5 is clearly better than the\\noriginal tree.\\nThe area under the ROC curve, used above, is sometimes called the c-\\nstatistic . Interestingly, it can be shown that the area under the ROC curve\\nis equivalent to the Mann-Whitney U statistic (or Wilcoxon rank-sum test),\\nfor the median diﬀerence between the prediction scores in the two groups\\n(Hanley and McNeil, 1982). For evaluating the contribution of an additional\\npredictor when added to a standard model, the c-statistic may not be an\\ninformative measure. The new predictor can be very signiﬁcant in terms\\nof the change in model deviance, but show only a small increase in the c-\\nstatistic. For example, removal of the highly signiﬁcant term george from\\nthe model of Table 9.2 results in a decrease in the c-statistic of less than\\n0.01. Instead, it is useful to examine how the additional predictor changes\\nthe classiﬁcation on an individual sample basis. A good discussion of this\\npoint appears in Cook (2007).\\n9.3 PRIM: Bump Hunting\\nTree-based methods (for regression) partition the feature space into box-\\nshaped regions, to try to make the response averages in each box as diﬀer-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ba40ec82-d703-451b-beda-32683e767066', embedding=None, metadata={'page_label': '337', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='318 9. Additive Models, Trees, and Related Methods\\nent as possible. The splitting rules deﬁning the boxes are related to each\\nthrough a binary tree, facilitating their interpretation.\\nThe patient rule induction method (PRIM) also ﬁnds boxes in the feature\\nspace, but seeks boxes in which the response average is high. Hence it looks\\nfor maxima in the target function, an exercise known as bump hunting . (If\\nminima rather than maxima are desired, one simply works with the negative\\nresponse values.)\\nPRIM also diﬀers from tree-based partitioning methods in that the box\\ndeﬁnitions are not described by a binary tree. This makes interpretation of\\nthe collection of rules more diﬃcult; however, by removing the binary tree\\nconstraint, the individual rules are often simpler.\\nThe main box construction method in PRIM works from the top down,\\nstarting with a box containing all of the data. The box is compressed along\\none face by a small amount, and the observations then falling outside the\\nbox are peeled oﬀ. The face chosen for compression is the one resulting in\\nthe largest box mean, after the compression is performed. Then the process\\nis repeated, stopping when the current box contains some minimum number\\nof data points.\\nThis process is illustrated in Figure 9.7. There are 200 data points uni-\\nformly distributed over the unit square. The color-coded plot indicates the\\nresponse Ytaking the value 1 (red) when 0 .5< X1<0.8 and 0 .4< X2<\\n0.6. and zero (blue) otherwise. The panels shows the successive boxes found\\nby the top-down peeling procedure, peeling oﬀ a proportion α= 0.1 of the\\nremaining data points at each stage.\\nFigure 9.8 shows the mean of the response values in the box, as the box\\nis compressed.\\nAfter the top-down sequence is computed, PRIM reverses the process,\\nexpanding along any edge, if such an expansion increases the box mean.\\nThis is called pasting . Since the top-down procedure is greedy at each step,\\nsuch an expansion is often possible.\\nThe result of these steps is a sequence of boxes, with diﬀerent numbers\\nof observation in each box. Cross-validation, combined with the judgment\\nof the data analyst, is used to choose the optimal box size.\\nDenote by B1the indices of the observations in the box found in step 1.\\nThe PRIM procedure then removes the observations in B1from the training\\nset, and the two-step process—top down peeling, followed by bottom-up\\npasting—is repeated on the remaining dataset. This entire process is re-\\npeated several times, producing a sequence of boxes B1,B2,... ,B k. Each\\nbox is deﬁned by a set of rules involving a subset of predictors like\\n(a1≤X1≤b1) and ( b1≤X3≤b2).\\nA summary of the PRIM procedure is given Algorithm 9.3.\\nPRIM can handle a categorical predictor by considering all partitions of\\nthe predictor, as in CART. Missing values are also handled in a manner\\nsimilar to CART. PRIM is designed for regression (quantitative respo nse', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d765736-9124-4ced-94b5-7c8d07dce51d', embedding=None, metadata={'page_label': '338', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 PRIM: Bump Hunting 319\\n1\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no2\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no3\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no4\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\n5\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no6\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no7\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no8\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\n12\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no17\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no22\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no27\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\nFIGURE 9.7. Illustration of PRIM algorithm. There are two classes, indica ted\\nby the blue (class 0) and red (class 1) points. The procedure starts with a rectangle\\n(broken black lines) surrounding all of the data, and then peels a way points along\\none edge by a prespeciﬁed amount in order to maximize the mean of th e points\\nremaining in the box. Starting at the top left panel, the sequence of p eelings is\\nshown, until a pure red region is isolated in the bottom right pa nel. The iteration\\nnumber is indicated at the top of each panel.\\nNumber of Observations in BoxBox Mean\\n50 100 1500.2 0.4 0.6 0.8 1.0\\n•••••••••••••••••••••••••••\\nFIGURE 9.8. Box mean as a function of number of observations in the box.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='17acf8b5-29cd-4c3f-b800-979b6a079dfb', embedding=None, metadata={'page_label': '339', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='320 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.3 Patient Rule Induction Method.\\n1. Start with all of the training data, and a maximal box containing all\\nof the data.\\n2. Consider shrinking the box by compressing one face, so as to peel oﬀ\\nthe proportion αof observations having either the highest values of\\na predictor Xj, or the lowest. Choose the peeling that produces the\\nhighest response mean in the remaining box. (Typically α= 0.05 or\\n0.10.)\\n3. Repeat step 2 until some minimal number of observations (say 10)\\nremain in the box.\\n4. Expand the box along any face, as long as the resulting box mean\\nincreases.\\n5. Steps 1–4 give a sequence of boxes, with diﬀerent numbers of obser-\\nvations in each box. Use cross-validation to choose a member of the\\nsequence. Call the box B1.\\n6. Remove the data in box B1from the dataset and repeat steps 2–5 to\\nobtain a second box, and continue to get as many boxes as desired.\\nvariable); a two-class outcome can be handled simply by coding it as 0 and\\n1. There is no simple way to deal with k >2 classes simultaneously: one\\napproach is to run PRIM separately for each class versus a baseline class.\\nAn advantage of PRIM over CART is its patience. Because of its bi-\\nnary splits, CART fragments the data quite quickly. Assuming splits of\\nequal size, with Nobservations it can only make log2(N)−1 splits before\\nrunning out of data. If PRIM peels oﬀ a proportion αof training points\\nat each stage, it can perform approximately −log(N)/log(1−α) peeling\\nsteps before running out of data. For example, if N= 128 and α= 0.10,\\nthen log2(N)−1 = 6 while −log(N)/log(1−α)≈46. Taking into account\\nthat there must be an integer number of observations at each stage, PRIM\\nin fact can peel only 29 times. In any case, the ability of PRIM to be more\\npatient should help the top-down greedy algorithm ﬁnd a better solution.\\n9.3.1 Spam Example (Continued)\\nWe applied PRIM to the spamdata, with the response coded as 1 for spam\\nand 0 for email.\\nThe ﬁrst two boxes found by PRIM are summarized below:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='41f62f91-f85c-4e35-9aa6-dc4cf73779a2', embedding=None, metadata={'page_label': '340', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.4 MARS: Multivariate Adaptive Regression Splines 321\\nRule 1 Global Mean Box Mean Box Support\\nTraining 0.3931 0.9607 0.1413\\nTest 0.3958 1.0000 0.1536\\nRule 1\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3ch!>0.029\\nCAPAVE >2.331\\nyour >0.705\\n1999 <0.040\\nCAPTOT >79.50\\nedu <0.070\\nre<0.535\\nch;<0.030\\nRule 2 Remain Mean Box Mean Box Support\\nTraining 0.2998 0.9560 0.1043\\nTest 0.2862 0.9264 0.1061\\nRule 2{\\nremove >0.010\\ngeorge <0.110\\nThe box support is the proportion of observations falling in the box.\\nThe ﬁrst box is purely spam, and contains about 15% of the test data.\\nThe second box contains 10.6% of the test observations, 92.6% of which\\narespam. Together the two boxes contain 26% of the data and are about\\n97%spam. The next few boxes (not shown) are quite small, containing only\\nabout 3% of the data.\\nThe predictors are listed in order of importance. Interestingly the top\\nsplitting variables in the CART tree (Figure 9.5) do not appear in PRIM’s\\nﬁrst box.\\n9.4 MARS: Multivariate Adaptive Regression\\nSplines\\nMARS is an adaptive procedure for regression, and is well suited for high-\\ndimensional problems (i.e., a large number of inputs). It can be viewed as a\\ngeneralization of stepwise linear regression or a modiﬁcation of the CART\\nmethod to improve the latter’s performance in the regression setting. We\\nintroduce MARS from the ﬁrst point of view, and later make the connection\\nto CART.\\nMARS uses expansions in piecewise linear basis functions of the form\\n(x−t)+and (t−x)+. The “+” means positive part, so\\n(x−t)+={\\nx−t,ifx > t,\\n0,otherwise,and ( t−x)+={\\nt−x,ifx < t,\\n0,otherwise .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a6628c9-42ae-4a57-bc20-3f67bf2f08f7', embedding=None, metadata={'page_label': '341', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='322 9. Additive Models, Trees, and Related Methods\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5(x−t)+ (t−x)+\\nxtBasis Function\\nFIGURE 9.9. The basis functions (x−t)+(solid orange) and (t−x)+(broken\\nblue) used by MARS.\\nAs an example, the functions ( x−0.5)+and (0 .5−x)+are shown in Fig-\\nure 9.9.\\nEach function is piecewise linear, with a knotat the value t. In the\\nterminology of Chapter 5, these are linear splines. We call the two functions\\nareﬂected pair in the discussion below. The idea is to form reﬂected pairs\\nfor each input Xjwith knots at each observed value xijof that input.\\nTherefore, the collection of basis functions is\\nC={(Xj−t)+,(t−Xj)+}t∈ {x1j, x2j, . . . , x Nj}\\nj= 1,2, . . . , p.(9.18)\\nIf all of the input values are distinct, there are 2 Npbasis functions alto-\\ngether. Note that although each basis function depends only on a single\\nXj, for example, h(X) = (Xj−t)+, it is considered as a function over the\\nentire input space IRp.\\nThe model-building strategy is like a forward stepwise linear regression,\\nbut instead of using the original inputs, we are allowed to use functions\\nfrom the set Cand their products. Thus the model has the form\\nf(X) =β0+M∑\\nm=1βmhm(X), (9.19)\\nwhere each hm(X) is a function in C, or a product of two or more such\\nfunctions.\\nGiven a choice for the hm, the coeﬃcients βmare estimated by minimiz-\\ning the residual sum-of-squares, that is, by standard linear regression. The\\nreal art, however, is in the construction of the functions hm(x). We start\\nwith only the constant function h0(X) = 1 in our model, and all functions\\nin the set Care candidate functions. This is depicted in Figure 9.10.\\nAt each stage we consider as a new basis function pair all products of a\\nfunction hmin the model set Mwith one of the reﬂected pairs in C. We\\nadd to the model Mthe term of the form\\nˆβM+1hℓ(X)≤(Xj−t)++ˆβM+2hℓ(X)≤(t−Xj)+, hℓ∈ M,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2c355b29-fd32-4554-8046-9935208f1c27', embedding=None, metadata={'page_label': '342', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.4 MARS: Multivariate Adaptive Regression Splines 323\\nX1\\nX1X1\\nX1\\nX2X2\\nX2X2\\nX2\\nXpXpXpConstant\\nFIGURE 9.10. Schematic of the MARS forward model-building procedure. On\\nthe left are the basis functions currently in the model: initiall y, this is the constant\\nfunction h(X) = 1. On the right are all candidate basis functions to be considered\\nin building the model. These are pairs of piecewise linear basi s functions as in\\nFigure 9.9, with knots tat all unique observed values xijof each predictor Xj.\\nAt each stage we consider all products of a candidate pair with a basis function\\nin the model. The product that decreases the residual error t he most is added into\\nthe current model. Above we illustrate the ﬁrst three steps of t he procedure, with\\nthe selected functions shown in red.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c2cbc4e8-e3f0-48dd-9bc0-23e8da6a4ec1', embedding=None, metadata={'page_label': '343', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='324 9. Additive Models, Trees, and Related Methods\\nX1X2h(X1, X2)\\nFIGURE 9.11. The function h(X1, X2) = (X1−x51)+≤(x72−X2)+, resulting\\nfrom multiplication of two piecewise linear MARS basis functio ns.\\nthat produces the largest decrease in training error. Here ˆβM+1andˆβM+2\\nare coeﬃcients estimated by least squares, along with all the other M+ 1\\ncoeﬃcients in the model. Then the winning products are added to the\\nmodel and the process is continued until the model set Mcontains some\\npreset maximum number of terms.\\nFor example, at the ﬁrst stage we consider adding to the model a function\\nof the form β1(Xj−t)++β2(t−Xj)+;t∈ {xij}, since multiplication by\\nthe constant function just produces the function itself. Suppose the best\\nchoice is ˆβ1(X2−x72)++ˆβ2(x72−X2)+. Then this pair of basis functions\\nis added to the set M, and at the next stage we consider including a pair\\nof products the form\\nhm(X)≤(Xj−t)+and hm(X)≤(t−Xj)+, t∈ {xij},\\nwhere for hmwe have the choices\\nh0(X) = 1 ,\\nh1(X) = ( X2−x72)+,or\\nh2(X) = ( x72−X2)+.\\nThe third choice produces functions such as ( X1−x51)+≤(x72−X2)+,\\ndepicted in Figure 9.11.\\nAt the end of this process we have a large model of the form (9.19). This\\nmodel typically overﬁts the data, and so a backward deletion procedure\\nis applied. The term whose removal causes the smallest increase in resid-\\nual squared error is deleted from the model at each stage, producing an\\nestimated best model ˆfλof each size (number of terms) λ. One could use\\ncross-validation to estimate the optimal value of λ, but for computational', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cdc9a204-7379-46b5-b359-4ba713cdd037', embedding=None, metadata={'page_label': '344', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.4 MARS: Multivariate Adaptive Regression Splines 325\\nsavings the MARS procedure instead uses generalized cross-validation. This\\ncriterion is deﬁned as\\nGCV( λ) =∑N\\ni=1(yi−ˆfλ(xi))2\\n(1−M(λ)/N)2. (9.20)\\nThe value M(λ) is the eﬀective number of parameters in the model: this\\naccounts both for the number of terms in the models, plus the number\\nof parameters used in selecting the optimal positions of the knots. Some\\nmathematical and simulation results suggest that one should pay a price\\nof three parameters for selecting a knot in a piecewise linear regression.\\nThus if there are rlinearly independent basis functions in the model, and\\nKknots were selected in the forward process, the formula is M(λ) =r+cK,\\nwhere c= 3. (When the model is restricted to be additive—details below—\\na penalty of c= 2 is used). Using this, we choose the model along the\\nbackward sequence that minimizes GCV( λ).\\nWhy these piecewise linear basis functions, and why this particular model\\nstrategy? A key property of the functions of Figure 9.9 is their ability to\\noperate locally; they are zero over part of their range. When they are mul-\\ntiplied together, as in Figure 9.11, the result is nonzero only over the small\\npart of the feature space where both component functions are nonzero. As\\na result, the regression surface is built up parsimoniously, using nonzero\\ncomponents locally—only where they are needed. This is important, since\\none should “spend” parameters carefully in high dimensions, as they can\\nrun out quickly. The use of other basis functions such as polynomials, would\\nproduce a nonzero product everywhere, and would not work as well.\\nThe second important advantage of the piecewise linear basis function\\nconcerns computation. Consider the product of a function in Mwith each\\nof the Nreﬂected pairs for an input Xj. This appears to require the ﬁtting\\nofNsingle-input linear regression models, each of which uses O(N) oper-\\nations, making a total of O(N2) operations. However, we can exploit the\\nsimple form of the piecewise linear function. We ﬁrst ﬁt the reﬂected pair\\nwith rightmost knot. As the knot is moved successively one position at a\\ntime to the left, the basis functions diﬀer by zero over the left part of the\\ndomain, and by a constant over the right part. Hence after each such move\\nwe can update the ﬁt in O(1) operations. This allows us to try every knot\\nin only O(N) operations.\\nThe forward modeling strategy in MARS is hierarchical, in the sense that\\nmultiway products are built up from products involving terms already in\\nthe model. For example, a four-way product can only be added to the model\\nif one of its three-way components is already in the model. The philosophy\\nhere is that a high-order interaction will likely only exist if some of its lo wer-\\norder “footprints” exist as well. This need not be true, but is a reasonable\\nworking assumption and avoids the search over an exponentially growing\\nspace of alternatives.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f42d33b7-4f76-4e0e-8934-7180e7782546', embedding=None, metadata={'page_label': '345', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='326 9. Additive Models, Trees, and Related Methods\\nRank of ModelTest Misclassification Error\\n0 20 40 60 80 1000.1 0.2 0.3 0.4\\n• • • • • • • • • • •••••• •• •• • • •••• • ••• • • •• • ••••• ••••••• ••••••••••••••• ••• ••••• ••••••••••••••••••••••••••••••\\n0.055GCV choice\\nFIGURE 9.12. Spam data: test error misclassiﬁcation rate for the MARS pro-\\ncedure, as a function of the rank (number of independent basis functi ons) in the\\nmodel.\\nThere is one restriction put on the formation of model terms: each input\\ncan appear at most once in a product. This prevents the formation of\\nhigher-order powers of an input, which increase or decrease too sharply\\nnear the boundaries of the feature space. Such powers can be approximated\\nin a more stable way with piecewise linear functions.\\nA useful option in the MARS procedure is to set an upper limit on\\nthe order of interaction. For example, one can set a limit of two, allowing\\npairwise products of piecewise linear functions, but not three- or higher-\\nway products. This can aid in the interpretation of the ﬁnal model. An\\nupper limit of one results in an additive model.\\n9.4.1 Spam Example (Continued)\\nWe applied MARS to the “spam” data analyzed earlier in this chapter. To\\nenhance interpretability, we restricted MARS to second-degree interactions.\\nAlthough the target is a two-class variable, we used the squared-error loss\\nfunction nonetheless (see Section 9.4.3). Figure 9.12 shows the test error\\nmisclassiﬁcation rate as a function of the rank (number of independent ba-\\nsis functions) in the model. The error rate levels oﬀ at about 5 .5%, which is\\nslightly higher than that of the generalized additive model (5 .3%) discussed\\nearlier. GCV chose a model size of 60, which is roughly the smallest model\\ngiving optimal performance. The leading interactions found by MARS in-\\nvolved inputs ( ch$, remove ), (ch$, free ) and (hp, CAPTOT ). However, these\\ninteractions give no improvement in performance over the generalized ad-\\nditive model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e0c0724d-e305-4781-9685-ef7112b4ab81', embedding=None, metadata={'page_label': '346', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.4 MARS: Multivariate Adaptive Regression Splines 327\\n9.4.2 Example (Simulated Data)\\nHere we examine the performance of MARS in three contrasting scenarios.\\nThere are N= 100 observations, and the predictors X1,X2,... ,X pand\\nerrors εhave independent standard normal distributions.\\nScenario 1: The data generation model is\\nY= (X1−1)++ (X1−1)+≤(X2−.8)++ 0.12≤ε. (9.21)\\nThe noise standard deviation 0.12 was chosen so that the signal-to-\\nnoise ratio was about 5. We call this the tensor-product scenario; the\\nproduct term gives a surface that looks like that of Figure 9.11.\\nScenario 2: This is the same as scenario 1, but with p= 20 total predictors;\\nthat is, there are 18 inputs that are independent of the response.\\nScenario 3: This has the structure of a neural network:\\nℓ1=X1+X2+X3+X4+X5,\\nℓ2=X6−X7+X8−X9+X10,\\nσ(t) = 1 /(1 +e−t),\\nY=σ(ℓ1) +σ(ℓ2) + 0.12≤ε.(9.22)\\nScenarios 1 and 2 are ideally suited for MARS, while scenario 3 contains\\nhigh-order interactions and may be diﬃcult for MARS to approximate. We\\nran ﬁve simulations from each model, and recorded the results.\\nIn scenario 1, MARS typically uncovered the correct model almost per-\\nfectly. In scenario 2, it found the correct structure but also found a few\\nextraneous terms involving other predictors.\\nLetθ(x) be the true mean of Y, and let\\nMSE 0= ave x∈Test(¯y−θ(x))2,\\nMSE = ave x∈Test(ˆf(x)−θ(x))2.(9.23)\\nThese represent the mean-square error of the constant model and the ﬁtted\\nMARS model, estimated by averaging at the 1000 test values of x. Table 9.4\\nshows the proportional decrease in model error or R2for each scenario:\\nR2=MSE 0−MSE\\nMSE 0. (9.24)\\nThe values shown are means and standard error over the ﬁve simulations.\\nThe performance of MARS is degraded only slightly by the inclusion of the\\nuseless inputs in scenario 2; it performs substantially worse in scenario 3.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d724ca76-68c9-47dc-91be-3dc23a846a4f', embedding=None, metadata={'page_label': '347', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='328 9. Additive Models, Trees, and Related Methods\\nTABLE 9.4. Proportional decrease in model error ( R2) when MARS is applied\\nto three diﬀerent scenarios.\\nScenario Mean (S.E.)\\n1: Tensor product p= 2 0.97 (0.01)\\n2: Tensor product p= 20 0.96 (0.01)\\n3: Neural network 0.79 (0.01)\\n9.4.3 Other Issues\\nMARS for Classiﬁcation\\nThe MARS method and algorithm can be extended to handle classiﬁcation\\nproblems. Several strategies have been suggested.\\nFor two classes, one can code the output as 0/1 and treat the problem as\\na regression; we did this for the spamexample. For more than two classes,\\none can use the indicator response approach described in Section 4.2. One\\ncodes the Kresponse classes via 0/1 indicator variables, and then per-\\nforms a multi-response MARS regression. For the latter we use a common\\nset of basis functions for all response variables. Classiﬁcation is made to\\nthe class with the largest predicted response value. There are, however, po-\\ntential masking problems with this approach, as described in Section 4.2.\\nA generally superior approach is the “optimal scoring” method discussed\\nin Section 12.5.\\nStone et al. (1997) developed a hybrid of MARS called PolyMARS specif-\\nically designed to handle classiﬁcation problems. It uses the multiple logistic\\nframework described in Section 4.4. It grows the model in a forward stage-\\nwise fashion like MARS, but at each stage uses a quadratic approximation\\nto the multinomial log-likelihood to search for the next basis-function pair.\\nOnce found, the enlarged model is ﬁt by maximum likelihood, and the\\nprocess is repeated.\\nRelationship of MARS to CART\\nAlthough they might seem quite diﬀerent, the MARS and CART strategies\\nactually have strong similarities. Suppose we take the MARS procedure and\\nmake the following changes:\\n•Replace the piecewise linear basis functions by step functions I(x−t >\\n0) and I(x−t≤0).\\n•When a model term is involved in a multiplication by a candidate\\nterm, it gets replaced by the interaction, and hence is not available\\nfor further interactions.\\nWith these changes, the MARS forward procedure is the same as the CART\\ntree-growing algorithm. Multiplying a step function by a pair of reﬂected', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38641ba8-4abf-47e6-8bff-d9e97d862f7e', embedding=None, metadata={'page_label': '348', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.5 Hierarchical Mixtures of Experts 329\\nstep functions is equivalent to splitting a node at the step. The second\\nrestriction implies that a node may not be split more than once, and leads\\nto the attractive binary-tree representation of the CART model. On the\\nother hand, it is this restriction that makes it diﬃcult for CART to model\\nadditive structures. MARS forgoes the tree structure and gains the ability\\nto capture additive eﬀects.\\nMixed Inputs\\nMars can handle “mixed” predictors—quantitative and qualitative—in a\\nnatural way, much like CART does. MARS considers all possible binary\\npartitions of the categories for a qualitative predictor into two groups.\\nEach such partition generates a pair of piecewise constant basis functions—\\nindicator functions for the two sets of categories. This basis pair is now\\ntreated as any other, and is used in forming tensor products with other\\nbasis functions already in the model.\\n9.5 Hierarchical Mixtures of Experts\\nThe hierarchical mixtures of experts (HME) procedure can be viewed as a\\nvariant of tree-based methods. The main diﬀerence is that the tree splits\\nare not hard decisions but rather soft probabilistic ones. At each node an\\nobservation goes left or right with probabilities depending on its input val-\\nues. This has some computational advantages since the resulting parameter\\noptimization problem is smooth, unlike the discrete split point search in the\\ntree-based approach. The soft splits might also help in prediction accuracy\\nand provide a useful alternative description of the data.\\nThere are other diﬀerences between HMEs and the CART implementa-\\ntion of trees. In an HME, a linear (or logistic regression) model is ﬁt in\\neach terminal node, instead of a constant as in CART. The splits can be\\nmultiway, not just binary, and the splits are probabilistic functions of a\\nlinear combination of inputs, rather than a single input as in the standard\\nuse of CART. However, the relative merits of these choices are not clear,\\nand most were discussed at the end of Section 9.2.\\nA simple two-level HME model in shown in Figure 9.13. It can be thought\\nof as a tree with soft splits at each non-terminal node. However, the inven-\\ntors of this methodology use a diﬀerent terminology. The terminal nodes\\nare called experts , and the non-terminal nodes are called gating networks .\\nThe idea is that each expert provides an opinion (prediction) about the\\nresponse, and these are combined together by the gating networks. As we\\nwill see, the model is formally a mixture model, and the two-level model\\nin the ﬁgure can be extend to multiple levels, hence the name hierarchical\\nmixtures of experts .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac52a0d4-835a-4b79-a61c-f4f88472422e', embedding=None, metadata={'page_label': '349', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='330 9. Additive Models, Trees, and Related Methods\\ng1 g2\\ng1|1 g2|1g1|2 g2|2Gating GatingGatingGating\\nGating GatingGating GatingGating\\nNetwork Network NetworkNetwork\\nNetwork\\nNetworkNetwork\\nNetwork Network NetworkNetwork\\nNetworkNetwork NetworkNetwork Network\\nNetwork Network NetworkNetwork\\nExpert Expert Expert Expert Expert Expert Expert Expert Expert Expert Expert\\nPr(y|x, θ11) Pr( y|x, θ21) Pr( y|x, θ12) Pr( y|x, θ22)\\nFIGURE 9.13. A two-level hierarchical mixture of experts (HME) model.\\nConsider the regression or classiﬁcation problem, as described earlier in\\nthe chapter. The data is ( xi,yi),i= 1,2,... ,N , with yieither a continuous\\nor binary-valued response, and xia vector-valued input. For ease of nota-\\ntion we assume that the ﬁrst element of xiis one, to account for intercepts.\\nHere is how an HME is deﬁned. The top gating network has the output\\ngj(x,γj) =eγT\\njx\\n∑K\\nk=1eγT\\nkx, j= 1,2,... ,K, (9.25)\\nwhere each γjis a vector of unknown parameters. This represents a soft\\nK-way split ( K= 2 in Figure 9.13.) Each gj(x,γj) is the probability of\\nassigning an observation with feature vector xto the jth branch. Notice\\nthat with K= 2 groups, if we take the coeﬃcient of one of the elements of\\nxto be + ∞, then we get a logistic curve with inﬁnite slope. In this case,\\nthe gating probabilities are either 0 or 1, corresponding to a hard split on\\nthat input.\\nAt the second level, the gating networks have a similar form:\\ngℓ|j(x,γjℓ) =eγT\\njℓx\\n∑K\\nk=1eγT\\njkx, ℓ= 1,2,... ,K. (9.26)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3aed52a-b2eb-4b90-a964-0415e2ea5ec4', embedding=None, metadata={'page_label': '350', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.5 Hierarchical Mixtures of Experts 331\\nThis is the probability of assignment to the ℓth branch, given assignment\\nto the jth branch at the level above.\\nAt each expert (terminal node), we have a model for the response variable\\nof the form\\nY∼Pr(y|x,θjℓ). (9.27)\\nThis diﬀers according to the problem.\\nRegression: The Gaussian linear regression model is used, with θjℓ=\\n(βjℓ,σ2\\njℓ):\\nY=βT\\njℓx+εandε∼N(0,σ2\\njℓ). (9.28)\\nClassiﬁcation: The linear logistic regression model is used:\\nPr(Y= 1|x,θjℓ) =1\\n1 +e−θT\\njℓx. (9.29)\\nDenoting the collection of all parameters by Ψ = {γj,γjℓ,θjℓ}, the total\\nprobability that Y=yis\\nPr(y|x,Ψ) =K∑\\nj=1gj(x,γj)K∑\\nℓ=1gℓ|j(x,γjℓ)Pr(y|x,θjℓ). (9.30)\\nThis is a mixture model, with the mixture probabilities determined by the\\ngating network models.\\nTo estimate the parameters, we maximize the log-likelihood of the data,∑\\nilog Pr( yi|xi,Ψ), over the parameters in Ψ. The most convenient method\\nfor doing this is the EM algorithm, which we describe for mixtures in\\nSection 8.5. We deﬁne latent variables ∆ j, all of which are zero except for\\na single one. We interpret these as the branching decisions made by the top\\nlevel gating network. Similarly we deﬁne latent variables ∆ ℓ|jto describe\\nthe gating decisions at the second level.\\nIn the E-step, the EM algorithm computes the expectations of the ∆ j\\nand ∆ ℓ|jgiven the current values of the parameters. These expectations\\nare then used as observation weights in the M-step of the procedure, to\\nestimate the parameters in the expert networks. The parameters in the\\ninternal nodes are estimated by a version of multiple logistic regression.\\nThe expectations of the ∆ jor ∆ ℓ|jare probability proﬁles, and these are\\nused as the response vectors for these logistic regressions.\\nThe hierarchical mixtures of experts approach is a promising competitor\\nto CART trees. By using soft splits rather than hard decision rules it can\\ncapture situations where the transition from low to high response is gradual .\\nThe log-likelihood is a smooth function of the unknown weights and hence\\nis amenable to numerical optimization. The model is similar to CART with\\nlinear combination splits, but the latter is more diﬃcult to optimize. On', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c152199f-4cd3-438f-8991-37ce8442d53d', embedding=None, metadata={'page_label': '351', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='332 9. Additive Models, Trees, and Related Methods\\nthe other hand, to our knowledge there are no methods for ﬁnding a good\\ntree topology for the HME model, as there are in CART. Typically one uses\\na ﬁxed tree of some depth, possibly the output of the CART procedure.\\nThe emphasis in the research on HMEs has been on prediction rather than\\ninterpretation of the ﬁnal model. A close cousin of the HME is the latent\\nclass model (Lin et al., 2000), which typically has only one layer; here\\nthe nodes or latent classes are interpreted as groups of subjects that show\\nsimilar response behavior.\\n9.6 Missing Data\\nIt is quite common to have observations with missing values for one or mor e\\ninput features. The usual approach is to impute (ﬁll-in) the missing values\\nin some way.\\nHowever, the ﬁrst issue in dealing with the problem is determining wheth-\\ner the missing data mechanism has distorted the observed data. Roughly\\nspeaking, data are missing at random if the mechanism resulting in its\\nomission is independent of its (unobserved) value. A more precise deﬁnition\\nis given in Little and Rubin (2002). Suppose yis the response vector and X\\nis the N×pmatrix of inputs (some of which are missing). Denote by Xobs\\nthe observed entries in Xand let Z= (y,X),Zobs= (y,Xobs). Finally, if R\\nis an indicator matrix with ijth entry 1 if xijis missing and zero otherwise,\\nthen the data is said to be missing at random (MAR) if the distribution of\\nRdepends on the data Zonly through Zobs:\\nPr(R|Z,θ) = Pr( R|Zobs,θ). (9.31)\\nHereθare any parameters in the distribution of R. Data are said to be\\nmissing completely at random (MCAR) if the distribution of Rdoesn’t\\ndepend on the observed or missing data:\\nPr(R|Z,θ) = Pr( R|θ). (9.32)\\nMCAR is a stronger assumption than MAR: most imputation methods rely\\non MCAR for their validity.\\nFor example, if a patient’s measurement was not taken because the doctor\\nfelt he was too sick, that observation would not be MAR or MCAR. In this\\ncase the missing data mechanism causes our observed training data to give a\\ndistorted picture of the true population, and data imputation is dangerous\\nin this instance. Often the determination of whether features are MCAR\\nmust be made from information about the data collection process. For\\ncategorical features, one way to diagnose this problem is to code “missing”\\nas an additional class. Then we ﬁt our model to the training data and see\\nif class “missing” is predictive of the response.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b4ee7c4-b8ae-4f59-ad51-9c341c1a9d79', embedding=None, metadata={'page_label': '352', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.6 Missing Data 333\\nAssuming the features are missing completely at random, there are a\\nnumber of ways of proceeding:\\n1. Discard observations with any missing values.\\n2. Rely on the learning algorithm to deal with missing values in its\\ntraining phase.\\n3. Impute all missing values before training.\\nApproach (1) can be used if the relative amount of missing data is small,\\nbut otherwise should be avoided. Regarding (2), CART is one learning\\nalgorithm that deals eﬀectively with missing values, through surrogate splits\\n(Section 9.2.4). MARS and PRIM use similar approaches. In generalized\\nadditive modeling, all observations missing for a given input feature are\\nomitted when the partial residuals are smoothed against that feature in\\nthe backﬁtting algorithm, and their ﬁtted values are set to zero. Since the\\nﬁtted curves have mean zero (when the model includes an intercept), this\\namounts to assigning the average ﬁtted value to the missing observations.\\nFor most learning methods, the imputation approach (3) is necessary.\\nThe simplest tactic is to impute the missing value with the mean or median\\nof the nonmissing values for that feature. (Note that the above procedure\\nfor generalized additive models is analogous to this.)\\nIf the features have at least some moderate degree of dependence, one\\ncan do better by estimating a predictive model for each feature given the\\nother features and then imputing each missing value by its prediction from\\nthe model. In choosing the learning method for imputation of the features,\\none must remember that this choice is distinct from the method used for\\npredicting yfromX. Thus a ﬂexible, adaptive method will often be pre-\\nferred, even for the eventual purpose of carrying out a linear regression of y\\nonX. In addition, if there are many missing feature values in the training\\nset, the learning method must itself be able to deal with missing feature\\nvalues. CART therefore is an ideal choice for this imputation “engine.”\\nAfter imputation, missing values are typically treated as if they were ac-\\ntually observed. This ignores the uncertainty due to the imputation, which\\nwill itself introduce additional uncertainty into estimates and predictions\\nfrom the response model. One can measure this additional uncertainty by\\ndoing multiple imputations and hence creating many diﬀerent training sets.\\nThe predictive model for ycan be ﬁt to each training set, and the variation\\nacross training sets can be assessed. If CART was used for the imputation\\nengine, the multiple imputations could be done by sampling from the values\\nin the corresponding terminal nodes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='279da600-203c-45fb-9b3e-a20d742d6064', embedding=None, metadata={'page_label': '353', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='334 9. Additive Models, Trees, and Related Methods\\n9.7 Computational Considerations\\nWith Nobservations and ppredictors, additive model ﬁtting requires some\\nnumber mpof applications of a one-dimensional smoother or regression\\nmethod. The required number of cycles mof the backﬁtting algorithm is\\nusually less than 20 and often less than 10, and depends on the amount\\nof correlation in the inputs. With cubic smoothing splines, for example,\\nNlogNoperations are needed for an initial sort and Noperations for the\\nspline ﬁt. Hence the total operations for an additive model ﬁt is pNlogN+\\nmpN.\\nTrees require pNlogNoperations for an initial sort for each predictor,\\nand typically another pNlogNoperations for the split computations. If the\\nsplits occurred near the edges of the predictor ranges, this number could\\nincrease to N2p.\\nMARS requires Nm2+pmN operations to add a basis function to a\\nmodel with mterms already present, from a pool of ppredictors. Hence to\\nbuild an M-term model requires NM3+pM2Ncomputations, which can\\nbe quite prohibitive if Mis a reasonable fraction of N.\\nEach of the components of an HME are typically inexpensive to ﬁt at\\neach M-step: Np2for the regressions, and Np2K2for a K-class logistic\\nregression. The EM algorithm, however, can take a long time to converge,\\nand so sizable HME models are considered costly to ﬁt.\\nBibliographic Notes\\nThe most comprehensive source for generalized additive models is the text\\nof that name by Hastie and Tibshirani (1990). Diﬀerent applications of\\nthis work in medical problems are discussed in Hastie et al. (1989) and\\nHastie and Herman (1990), and the software implementation in Splus is\\ndescribed in Chambers and Hastie (1991). Green and Silverman (1994)\\ndiscuss penalization and spline models in a variety of settings. Efron and\\nTibshirani (1991) give an exposition of modern developments in statisti cs\\n(including generalized additive models), for a nonmathematical audience.\\nClassiﬁcation and regression trees date back at least as far as Morgan and\\nSonquist (1963). We have followed the modern approaches of Breiman et\\nal. (1984) and Quinlan (1993). The PRIM method is due to Friedman\\nand Fisher (1999), while MARS is introduced in Friedman (1991), with an\\nadditive precursor in Friedman and Silverman (1989). Hierarchical mixtures\\nof experts were proposed in Jordan and Jacobs (1994); see also Jacobs et\\nal. (1991).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bab2326f-e89f-4272-b6d9-63e73869680b', embedding=None, metadata={'page_label': '354', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 335\\nExercises\\nEx. 9.1 Show that a smoothing spline ﬁt of yitoxipreserves the linear\\npartof the ﬁt. In other words, if yi= ˆyi+ri, where ˆ yirepresents the\\nlinear regression ﬁts, and Sis the smoothing matrix, then Sy=ˆy+Sr.\\nShow that the same is true for local linear regression (Section 6.1.1). Hence\\nargue that the adjustment step in the second line of (2) in Algorithm 9.1\\nis unnecessary.\\nEx. 9.2 LetAbe a known k×kmatrix, bbe a known k-vector, and z\\nbe an unknown k-vector. A Gauss–Seidel algorithm for solving the linear\\nsystem of equations Az=bworks by successively solving for element zjin\\nthejth equation, ﬁxing all other zj’s at their current guesses. This process\\nis repeated for j= 1,2,... ,k, 1,2,... ,k,... , until convergence (Golub and\\nVan Loan, 1983).\\n(a) Consider an additive model with Nobservations and pterms, with\\nthejth term to be ﬁt by a linear smoother Sj. Consider the following\\nsystem of equations:\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edI S 1S1≤≤≤S1\\nS2I S 2≤≤≤S2\\n...............\\nSpSpSp≤≤≤I\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edf1\\nf2\\n...\\nfp\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edS1y\\nS2y\\n...\\nSpy\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8. (9.33)\\nHere each fjis an N-vector of evaluations of the jth function at\\nthe data points, and yis anN-vector of the response values. Show\\nthat backﬁtting is a blockwise Gauss–Seidel algorithm for solving this\\nsystem of equations.\\n(b) Let S1andS2be symmetric smoothing operators (matrices) with\\neigenvalues in [0 ,1). Consider a backﬁtting algorithm with response\\nvector yand smoothers S1,S2. Show that with any starting values,\\nthe algorithm converges and give a formula for the ﬁnal iterates.\\nEx. 9.3 Backﬁtting equations. Consider a backﬁtting procedure with orthog-\\nonal projections, and let Dbe the overall regression matrix whose columns\\nspanV=Lcol(S1)⊕ Lcol(S2)⊕ ≤≤≤ ⊕ L col(Sp), where Lcol(S) denotes the\\ncolumn space of a matrix S. Show that the estimating equations\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edI S 1S1≤≤≤S1\\nS2I S 2≤≤≤S2\\n...............\\nSpSpSp≤≤≤I\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edf1\\nf2\\n...\\nfp\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edS1y\\nS2y\\n...\\nSpy\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nare equivalent to the least squares normal equations DTDβ=DTywhere\\nβis the vector of coeﬃcients.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e9347d5-7cb0-43c6-85f5-ba24d8c9b6ef', embedding=None, metadata={'page_label': '355', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='336 9. Additive Models, Trees, and Related Methods\\nEx. 9.4 Suppose the same smoother Sis used to estimate both terms in a\\ntwo-term additive model (i.e., both variables are identical). Assume that S\\nis symmetric with eigenvalues in [0 ,1). Show that the backﬁtting residual\\nconverges to ( I+S)−1(I−S)y, and that the residual sum of squares con-\\nverges upward. Can the residual sum of squares converge upward in less\\nstructured situations? How does this ﬁt compare to the ﬁt with a single\\nterm ﬁt by S? [Hint: Use the eigen-decomposition of Sto help with this\\ncomparison.]\\nEx. 9.5 Degrees of freedom of a tree . Given data yiwith mean f(xi) and\\nvariance σ2, and a ﬁtting operation y→ˆy, let’s deﬁne the degrees of\\nfreedom of a ﬁt by∑\\nicov(yi,ˆyi)/σ2.\\nConsider a ﬁt ˆyestimated by a regression tree, ﬁt to a set of predictors\\nX1,X2,... ,X p.\\n(a) In terms of the number of terminal nodes m, give a rough formula for\\nthe degrees of freedom of the ﬁt.\\n(b) Generate 100 observations with predictors X1,X2,... ,X 10as inde-\\npendent standard Gaussian variates and ﬁx these values.\\n(c) Generate response values also as standard Gaussian ( σ2= 1), indepen-\\ndent of the predictors. Fit regression trees to the data of ﬁxed size 1,5\\nand 10 terminal nodes and hence estimate the degrees of freedom of\\neach ﬁt. [Do ten simulations of the response and average the results,\\nto get a good estimate of degrees of freedom.]\\n(d) Compare your estimates of degrees of freedom in (a) and (c) and\\ndiscuss.\\n(e) If the regression tree ﬁt were a linear operation, we could write ˆy=Sy\\nfor some matrix S. Then the degrees of freedom would be tr( S).\\nSuggest a way to compute an approximate Smatrix for a regression\\ntree, compute it and compare the resulting degrees of freedom to\\nthose in (a) and (c).\\nEx. 9.6 Consider the ozone data of Figure 6.9.\\n(a) Fit an additive model to the cube root of ozone concentration. as a\\nfunction of temperature, wind speed, and radiation. Compare your\\nresults to those obtained via the trellis display in Figure 6.9.\\n(b) Fit trees, MARS, and PRIM to the same data, and compare the results\\nto those found in (a) and in Figure 6.9.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de767f80-7936-4e18-878f-c33802347499', embedding=None, metadata={'page_label': '356', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 337\\nPrinter: Opaque this\\n10\\nBoosting and Additive Trees\\n10.1 Boosting Methods\\nBoosting is one of the most powerful learning ideas introduced in the last\\ntwenty years. It was originally designed for classiﬁcation problems, but as\\nwill be seen in this chapter, it can proﬁtably be extended to regression\\nas well. The motivation for boosting was a procedure that combines the\\noutputs of many “weak” classiﬁers to produce a powerful “committee.”\\nFrom this perspective boosting bears a resemblance to bagging and other\\ncommittee-based approaches (Section 8.8). However we shall see that the\\nconnection is at best superﬁcial and that boosting is fundamentally diﬀer-\\nent.\\nWe begin by describing the most popular boosting algorithm due to\\nFreund and Schapire (1997) called “AdaBoost.M1.” Consider a two-class\\nproblem, with the output variable coded as Y∈ {−1,1}. Given a vector of\\npredictor variables X, a classiﬁer G(X) produces a prediction taking one\\nof the two values {−1,1}. The error rate on the training sample is\\nerr =1\\nNN∑\\ni=1I(yi̸=G(xi)),\\nand the expected error rate on future predictions is E XYI(Y̸=G(X)).\\nA weak classiﬁer is one whose error rate is only slightly better than\\nrandom guessing. The purpose of boosting is to sequentially apply the\\nweak classiﬁcation algorithm to repeatedly modiﬁed versions of the data,\\nthereby producing a sequence of weak classiﬁers Gm(x),m= 1,2,... ,M .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34492a4e-9a7b-40df-af3b-82e2cde8db7f', embedding=None, metadata={'page_label': '357', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='338 10. Boosting and Additive Trees\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleG(x) = sign[∑M\\nm=1αmGm(x)]\\nGM(x)\\nG3(x)\\nG2(x)\\nG1(x)Final Classifier\\nFIGURE 10.1. Schematic of AdaBoost. Classiﬁers are trained on weighted ver-\\nsions of the dataset, and then combined to produce a ﬁnal predictio n.\\nThe predictions from all of them are then combined through a weighted\\nmajority vote to produce the ﬁnal prediction:\\nG(x) = sign(M∑\\nm=1αmGm(x))\\n. (10.1)\\nHereα1,α2,... ,α Mare computed by the boosting algorithm, and weight\\nthe contribution of each respective Gm(x). Their eﬀect is to give higher\\ninﬂuence to the more accurate classiﬁers in the sequence. Figure 10.1 shows\\na schematic of the AdaBoost procedure.\\nThe data modiﬁcations at each boosting step consist of applying weights\\nw1,w2,... ,w Nto each of the training observations ( xi,yi), i= 1,2,... ,N .\\nInitially all of the weights are set to wi= 1/N, so that the ﬁrst step simply\\ntrains the classiﬁer on the data in the usual manner. For each successive\\niteration m= 2,3,... ,M the observation weights are individually modi-\\nﬁed and the classiﬁcation algorithm is reapplied to the weighted observa-\\ntions. At step m, those observations that were misclassiﬁed by the classiﬁer\\nGm−1(x) induced at the previous step have their weights increased, whereas\\nthe weights are decreased for those that were classiﬁed correctly. Thus as\\niterations proceed, observations that are diﬃcult to classify correctly re-\\nceive ever-increasing inﬂuence. Each successive classiﬁer is thereby forced', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8c520ea0-da19-4f33-890e-6327b0d9cc8f', embedding=None, metadata={'page_label': '358', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.1 Boosting Methods 339\\nAlgorithm 10.1 AdaBoost.M1.\\n1. Initialize the observation weights wi= 1/N, i = 1,2,... ,N .\\n2. For m= 1 to M:\\n(a) Fit a classiﬁer Gm(x) to the training data using weights wi.\\n(b) Compute\\nerrm=∑N\\ni=1wiI(yi̸=Gm(xi))\\n∑N\\ni=1wi.\\n(c) Compute αm= log((1 −errm)/errm).\\n(d) Set wi←wi≤exp[αm≤I(yi̸=Gm(xi))], i= 1,2,... ,N .\\n3. Output G(x) = sign[∑M\\nm=1αmGm(x)]\\n.\\nto concentrate on those training observations that are missed by previous\\nones in the sequence.\\nAlgorithm 10.1 shows the details of the AdaBoost.M1 algorithm. The\\ncurrent classiﬁer Gm(x) is induced on the weighted observations at line 2a.\\nThe resulting weighted error rate is computed at line 2b. Line 2c calculates\\nthe weight αmgiven to Gm(x) in producing the ﬁnal classiﬁer G(x) (line\\n3). The individual weights of each of the observations are updated for the\\nnext iteration at line 2d. Observations misclassiﬁed by Gm(x) have their\\nweights scaled by a factor exp( αm), increasing their relative inﬂuence for\\ninducing the next classiﬁer Gm+1(x) in the sequence.\\nThe AdaBoost.M1 algorithm is known as “Discrete AdaBoost” in Fried-\\nman et al. (2000), because the base classiﬁer Gm(x) returns a discrete class\\nlabel. If the base classiﬁer instead returns a real-valued prediction (e.g.,\\na probability mapped to the interval [ −1,1]), AdaBoost can be modiﬁed\\nappropriately (see “Real AdaBoost” in Friedman et al. (2000)).\\nThe power of AdaBoost to dramatically increase the performance of even\\na very weak classiﬁer is illustrated in Figure 10.2. The features X1,... ,X 10\\nare standard independent Gaussian, and the deterministic target Yis de-\\nﬁned by\\nY={\\n1 if∑10\\nj=1X2\\nj> χ2\\n10(0.5),\\n−1 otherwise .(10.2)\\nHereχ2\\n10(0.5) = 9 .34 is the median of a chi-squared random variable with\\n10 degrees of freedom (sum of squares of 10 standard Gaussians). There are\\n2000 training cases, with approximately 1000 cases in each class, and 10,0 00\\ntest observations. Here the weak classiﬁer is just a “stump”: a two terminal-\\nnode classiﬁcation tree. Applying this classiﬁer alone to the training data\\nset yields a very poor test set error rate of 45.8%, compared to 50% for', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7a244066-b191-44f0-ab68-5af6abb979fd', embedding=None, metadata={'page_label': '359', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='340 10. Boosting and Additive Trees\\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5\\nBoosting IterationsTest ErrorSingle Stump\\n244 Node Tree\\nFIGURE 10.2. Simulated data (10.2): test error rate for boosting with stump s,\\nas a function of the number of iterations. Also shown are the test error rate for\\na single stump, and a 244-node classiﬁcation tree.\\nrandom guessing. However, as boosting iterations proceed the error rate\\nsteadily decreases, reaching 5.8% after 400 iterations. Thus, boosting this\\nsimple very weak classiﬁer reduces its prediction error rate by almost a\\nfactor of four. It also outperforms a single large classiﬁcation tree ( error\\nrate 24 .7%). Since its introduction, much has been written to explain the\\nsuccess of AdaBoost in producing accurate classiﬁers. Most of this work\\nhas centered on using classiﬁcation trees as the “base learner” G(x), where\\nimprovements are often most dramatic. In fact, Breiman (NIPS Workshop,\\n1996) referred to AdaBoost with trees as the “best oﬀ-the-shelf classiﬁer in\\nthe world” (see also Breiman (1998)). This is especially the case for data -\\nmining applications, as discussed more fully in Section 10.7 later in this\\nchapter.\\n10.1.1 Outline of This Chapter\\nHere is an outline of the developments in this chapter:\\n•We show that AdaBoost ﬁts an additive model in a base learner,\\noptimizing a novel exponential loss function. This loss function is', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='17d3dddb-81b1-4873-a2f7-0f3d0144c92c', embedding=None, metadata={'page_label': '360', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.2 Boosting Fits an Additive Model 341\\nvery similar to the (negative) binomial log-likelihood (Sections 10.2–\\n10.4).\\n•The population minimizer of the exponential loss function is shown\\nto be the log-odds of the class probabilities (Section 10.5).\\n•We describe loss functions for regression and classiﬁcation that are\\nmore robust than squared error or exponential loss (Section 10.6).\\n•It is argued that decision trees are an ideal base learner for data\\nmining applications of boosting (Sections 10.7 and 10.9).\\n•We develop a class of gradient boosted models (GBMs), for boosting\\ntrees with any loss function (Section 10.10).\\n•The importance of “slow learning” is emphasized, and implemented\\nby shrinkage of each new term that enters the model (Section 10.12),\\nas well as randomization (Section 10.12.2).\\n•Tools for interpretation of the ﬁtted model are described (Section 10.13).\\n10.2 Boosting Fits an Additive Model\\nThe success of boosting is really not very mysterious. The key lies in ex-\\npression (10.1). Boosting is a way of ﬁtting an additive expansion in a set\\nof elementary “basis” functions. Here the basis functions are the individual\\nclassiﬁers Gm(x)∈ {−1,1}. More generally, basis function expansions take\\nthe form\\nf(x) =M∑\\nm=1βmb(x;γm), (10.3)\\nwhere βm,m= 1,2,... ,M are the expansion coeﬃcients, and b(x;γ)∈IR\\nare usually simple functions of the multivariate argument x, characterized\\nby a set of parameters γ. We discuss basis expansions in some detail in\\nChapter 5.\\nAdditive expansions like this are at the heart of many of the learning\\ntechniques covered in this book:\\n•In single-hidden-layer neural networks (Chapter 11), b(x;γ) =σ(γ0+\\nγT\\n1x), where σ(t) = 1/(1+e−t) is the sigmoid function, and γparam-\\neterizes a linear combination of the input variables.\\n•In signal processing, wavelets (Section 5.9.1) are a popular choice with\\nγparameterizing the location and scale shifts of a “mother” wavelet.\\n•Multivariate adaptive regression splines (Section 9.4) uses truncated-\\npower spline basis functions where γparameterizes the variables and\\nvalues for the knots.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='811db287-525e-49ce-a20c-f5b6198d0fdc', embedding=None, metadata={'page_label': '361', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='342 10. Boosting and Additive Trees\\nAlgorithm 10.2 Forward Stagewise Additive Modeling.\\n1. Initialize f0(x) = 0.\\n2. For m= 1 to M:\\n(a) Compute\\n(βm,γm) = arg min\\nβ,γN∑\\ni=1L(yi,fm−1(xi) +βb(xi;γ)).\\n(b) Set fm(x) =fm−1(x) +βmb(x;γm).\\n•For trees, γparameterizes the split variables and split points at the\\ninternal nodes, and the predictions at the terminal nodes.\\nTypically these models are ﬁt by minimizing a loss function averaged\\nover the training data, such as the squared-error or a likelihood-based loss\\nfunction,\\nmin\\n{βm,γm}M\\n1N∑\\ni=1L(\\nyi,M∑\\nm=1βmb(xi;γm))\\n. (10.4)\\nFor many loss functions L(y,f(x)) and/or basis functions b(x;γ), this re-\\nquires computationally intensive numerical optimization techniques. How-\\never, a simple alternative often can be found when it is feasible to rapidly\\nsolve the subproblem of ﬁtting just a single basis function,\\nmin\\nβ,γN∑\\ni=1L(yi,βb(xi;γ)). (10.5)\\n10.3 Forward Stagewise Additive Modeling\\nForward stagewise modeling approximates the solution to (10.4) by sequen-\\ntially adding new basis functions to the expansion without adjusting the\\nparameters and coeﬃcients of those that have already been added. This is\\noutlined in Algorithm 10.2. At each iteration m, one solves for the optimal\\nbasis function b(x;γm) and corresponding coeﬃcient βmto add to the cur-\\nrent expansion fm−1(x). This produces fm(x), and the process is repeated.\\nPreviously added terms are not modiﬁed.\\nFor squared-error loss\\nL(y,f(x)) = ( y−f(x))2, (10.6)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a48df3c-1d84-46a4-924b-4cef10bafb6c', embedding=None, metadata={'page_label': '362', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.4 Exponential Loss and AdaBoost 343\\none has\\nL(yi,fm−1(xi) +βb(xi;γ)) = ( yi−fm−1(xi)−βb(xi;γ))2\\n= (rim−βb(xi;γ))2, (10.7)\\nwhere rim=yi−fm−1(xi) is simply the residual of the current model\\non the ith observation. Thus, for squared-error loss, the term βmb(x;γm)\\nthat best ﬁts the current residuals is added to the expansion at each step.\\nThis idea is the basis for “least squares” regression boosting discussed in\\nSection 10.10.2. However, as we show near the end of the next section,\\nsquared-error loss is generally not a good choice for classiﬁcation; hence\\nthe need to consider other loss criteria.\\n10.4 Exponential Loss and AdaBoost\\nWe now show that AdaBoost.M1 (Algorithm 10.1) is equivalent to forwar d\\nstagewise additive modeling (Algorithm 10.2) using the loss function\\nL(y,f(x)) = exp( −y f(x)). (10.8)\\nThe appropriateness of this criterion is addressed in the next section.\\nFor AdaBoost the basis functions are the individual classiﬁers Gm(x)∈\\n{−1,1}. Using the exponential loss function, one must solve\\n(βm,Gm) = arg min\\nβ,GN∑\\ni=1exp[−yi(fm−1(xi) +β G(xi))]\\nfor the classiﬁer Gmand corresponding coeﬃcient βmto be added at each\\nstep. This can be expressed as\\n(βm,Gm) = arg min\\nβ,GN∑\\ni=1w(m)\\niexp(−β yiG(xi)) (10.9)\\nwithw(m)\\ni= exp( −yifm−1(xi)). Since each w(m)\\nidepends neither on β\\nnorG(x), it can be regarded as a weight that is applied to each observa-\\ntion. This weight depends on fm−1(xi), and so the individual weight values\\nchange with each iteration m.\\nThe solution to (10.9) can be obtained in two steps. First, for any value\\nofβ >0, the solution to (10.9) for Gm(x) is\\nGm= arg min\\nGN∑\\ni=1w(m)\\niI(yi̸=G(xi)), (10.10)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='829478c7-0efe-4d73-ba9e-180a7ab2eb3a', embedding=None, metadata={'page_label': '363', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='344 10. Boosting and Additive Trees\\nwhich is the classiﬁer that minimizes the weighted error rate in predicting\\ny. This can be easily seen by expressing the criterion in (10.9) as\\ne−β≤∑\\nyi=G(xi)w(m)\\ni+eβ≤∑\\nyi̸=G(xi)w(m)\\ni,\\nwhich in turn can be written as\\n(\\neβ−e−β)\\n≤N∑\\ni=1w(m)\\niI(yi̸=G(xi)) +e−β≤N∑\\ni=1w(m)\\ni. (10.11)\\nPlugging this Gminto (10.9) and solving for βone obtains\\nβm=1\\n2log1−errm\\nerrm, (10.12)\\nwhere err mis the minimized weighted error rate\\nerrm=∑N\\ni=1w(m)\\niI(yi̸=Gm(xi))\\n∑N\\ni=1w(m)\\ni. (10.13)\\nThe approximation is then updated\\nfm(x) =fm−1(x) +βmGm(x),\\nwhich causes the weights for the next iteration to be\\nw(m+1)\\ni =w(m)\\ni≤e−βmyiGm(xi). (10.14)\\nUsing the fact that −yiGm(xi) = 2≤I(yi̸=Gm(xi))−1, (10.14) becomes\\nw(m+1)\\ni =w(m)\\ni≤eαmI(yi̸=Gm(xi))≤e−βm, (10.15)\\nwhere αm= 2βmis the quantity deﬁned at line 2c of AdaBoost.M1 (Al-\\ngorithm 10.1). The factor e−βmin (10.15) multiplies all weights by the\\nsame value, so it has no eﬀect. Thus (10.15) is equivalent to line 2(d) of\\nAlgorithm 10.1.\\nOne can view line 2(a) of the Adaboost.M1 algorithm as a method for\\napproximately solving the minimization in (10.11) and hence (10.10). Hence\\nwe conclude that AdaBoost.M1 minimizes the exponential loss criterion\\n(10.8) via a forward-stagewise additive modeling approach.\\nFigure 10.3 shows the training-set misclassiﬁcation error rate and aver-\\nage exponential loss for the simulated data problem (10.2) of Figure 10.2 .\\nThe training-set misclassiﬁcation error decreases to zero at around 250 it-\\nerations (and remains there), but the exponential loss keeps decreasing.\\nNotice also in Figure 10.2 that the test-set misclassiﬁcation error conti nues\\nto improve after iteration 250. Clearly Adaboost is not optimizing tra ining-\\nset misclassiﬁcation error; the exponential loss is more sensitive to cha nges\\nin the estimated class probabilities.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80478b6d-6f05-46bb-a772-c998ae26f27a', embedding=None, metadata={'page_label': '364', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.5 Why Exponential Loss? 345\\n0 100 200 300 4000.0 0.2 0.4 0.6 0.8 1.0\\nBoosting IterationsTraining Error\\nMisclassification RateExponential Loss\\nFIGURE 10.3. Simulated data, boosting with stumps: misclassiﬁcation error\\nrate on the training set, and average exponential loss: (1/N)PN\\ni=1exp(−yif(xi)).\\nAfter about 250iterations, the misclassiﬁcation error is zero, while the expo nential\\nloss continues to decrease.\\n10.5 Why Exponential Loss?\\nThe AdaBoost.M1 algorithm was originally motivated from a very diﬀer -\\nent perspective than presented in the previous section. Its equivalence to\\nforward stagewise additive modeling based on exponential loss was only\\ndiscovered ﬁve years after its inception. By studying the properties of the\\nexponential loss criterion, one can gain insight into the procedure and dis-\\ncover ways it might be improved.\\nThe principal attraction of exponential loss in the context of additive\\nmodeling is computational; it leads to the simple modular reweighting Ad-\\naBoost algorithm. However, it is of interest to inquire about its stat istical\\nproperties. What does it estimate and how well is it being estimated? The\\nﬁrst question is answered by seeking its population minimizer.\\nIt is easy to show (Friedman et al., 2000) that\\nf∗(x) = arg min\\nf(x)EY|x(e−Y f(x)) =1\\n2logPr(Y= 1|x)\\nPr(Y=−1|x), (10.16)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f0ef362-4c02-4687-aabb-9a7a474a384f', embedding=None, metadata={'page_label': '365', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='346 10. Boosting and Additive Trees\\nor equivalently\\nPr(Y= 1|x) =1\\n1 +e−2f∗(x).\\nThus, the additive expansion produced by AdaBoost is estimating one-\\nhalf the log-odds of P(Y= 1|x). This justiﬁes using its sign as the classiﬁ-\\ncation rule in (10.1).\\nAnother loss criterion with the same population minimizer is the bi-\\nnomial negative log-likelihood or deviance (also known as cross-entropy),\\ninterpreting fas the logit transform. Let\\np(x) = Pr( Y= 1|x) =ef(x)\\ne−f(x)+ef(x)=1\\n1 +e−2f(x)(10.17)\\nand deﬁne Y′= (Y+ 1)/2∈ {0,1}. Then the binomial log-likelihood loss\\nfunction is\\nl(Y,p(x)) =Y′logp(x) + (1 −Y′)log(1 −p(x)),\\nor equivalently the deviance is\\n−l(Y,f(x)) = log(\\n1 +e−2Y f(x))\\n. (10.18)\\nSince the population maximizer of log-likelihood is at the true probabilities\\np(x) = Pr( Y= 1|x), we see from (10.17) that the population minimizers of\\nthe deviance E Y|x[−l(Y,f(x))] and E Y|x[e−Y f(x)] are the same. Thus, using\\neither criterion leads to the same solution at the population level. Note that\\ne−Y fitself is not a proper log-likelihood, since it is not the logarithm of\\nany probability mass function for a binary random variable Y∈ {−1,1}.\\n10.6 Loss Functions and Robustness\\nIn this section we examine the diﬀerent loss functions for classiﬁcation and\\nregression more closely, and characterize them in terms of their robustness\\nto extreme data.\\nRobust Loss Functions for Classiﬁcation\\nAlthough both the exponential (10.8) and binomial deviance (10.18) yield\\nthe same solution when applied to the population joint distribution, the\\nsame is not true for ﬁnite data sets. Both criteria are monotone decreasing\\nfunctions of the “margin” yf(x). In classiﬁcation (with a −1/1 response)\\nthe margin plays a role analogous to the residuals y−f(x) in regression. The\\nclassiﬁcation rule G(x) = sign[ f(x)] implies that observations with positive\\nmargin yif(xi)>0 are classiﬁed correctly whereas those with negative\\nmargin yif(xi)<0 are misclassiﬁed. The decision boundary is deﬁned by', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='19859273-21c8-44b9-8bdb-304fa2949e2e', embedding=None, metadata={'page_label': '366', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.6 Loss Functions and Robustness 347\\n−2 −1 0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0Misclassification\\nExponential\\nBinomial Deviance\\nSquared Error\\nSupport VectorLoss\\ny≤f\\nFIGURE 10.4. Loss functions for two-class classiﬁcation. The response is\\ny=±1; the prediction is f, with class prediction sign(f). The losses are\\nmisclassiﬁcation: I(sign( f)̸=y); exponential: exp(−yf); binomial deviance:\\nlog(1 + exp( −2yf)); squared error: (y−f)2; and support vector: (1−yf)+(see\\nSection 12.3). Each function has been scaled so that it passes t hrough the point\\n(0,1).\\nf(x) = 0. The goal of the classiﬁcation algorithm is to produce positive\\nmargins as frequently as possible. Any loss criterion used for classiﬁcati on\\nshould penalize negative margins more heavily than positive ones since\\npositive margin observations are already correctly classiﬁed.\\nFigure 10.4 shows both the exponential (10.8) and binomial deviance\\ncriteria as a function of the margin y≤f(x). Also shown is misclassiﬁcation\\nlossL(y,f(x)) =I(y≤f(x)<0), which gives unit penalty for negative mar-\\ngin values, and no penalty at all for positive ones. Both the exponential\\nand deviance loss can be viewed as monotone continuous approximations\\nto misclassiﬁcation loss. They continuously penalize increasingly negative\\nmargin values more heavily than they reward increasingly positive ones.\\nThe diﬀerence between them is in degree. The penalty associated with bi-\\nnomial deviance increases linearly for large increasingly negative margin,\\nwhereas the exponential criterion increases the inﬂuence of such observa-\\ntions exponentially.\\nAt any point in the training process the exponential criterion concen-\\ntrates much more inﬂuence on observations with large negative margins.\\nBinomial deviance concentrates relatively less inﬂuence on such observa-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00426161-70ed-4c7a-b3d0-aab4b84b0964', embedding=None, metadata={'page_label': '367', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='348 10. Boosting and Additive Trees\\ntions, more evenly spreading the inﬂuence among all of the data. It is\\ntherefore far more robust in noisy settings where the Bayes error rate is\\nnot close to zero, and especially in situations where there is misspeciﬁcation\\nof the class labels in the training data. The performance of AdaBoost has\\nbeen empirically observed to dramatically degrade in such situations.\\nAlso shown in the ﬁgure is squared-error loss. The minimizer of the cor-\\nresponding risk on the population is\\nf∗(x) = arg min\\nf(x)EY|x(Y−f(x))2= E(Y|x) = 2≤Pr(Y= 1|x)−1.(10.19)\\nAs before the classiﬁcation rule is G(x) = sign[ f(x)]. Squared-error loss\\nis not a good surrogate for misclassiﬁcation error. As seen in Figure 10 .4, it\\nis not a monotone decreasing function of increasing margin yf(x). For mar-\\ngin values yif(xi)>1 it increases quadratically, thereby placing increasing\\ninﬂuence (error) on observations that are correctly classiﬁed with increas-\\ning certainty, thereby reducing the relative inﬂuence of those incorrectly\\nclassiﬁed yif(xi)<0. Thus, if class assignment is the goal, a monotone de-\\ncreasing criterion serves as a better surrogate loss function. Figure 12.4 on\\npage 426 in Chapter 12 includes a modiﬁcation of quadratic loss, the “Hu-\\nberized” square hinge loss (Rosset et al., 2004b), which enjoys the favorable\\nproperties of the binomial deviance, quadratic loss and the SVM hinge loss.\\nIt has the same population minimizer as the quadratic (10.19), is zero for\\ny≤f(x)>1, and becomes linear for y≤f(x)<−1. Since quadratic functions\\nare easier to compute with than exponentials, our experience suggests this\\nto be a useful alternative to the binomial deviance.\\nWith K-class classiﬁcation, the response Ytakes values in the unordered\\nsetG={G1,... ,Gk}(see Sections 2.4 and 4.4). We now seek a classiﬁer\\nG(x) taking values in G. It is suﬃcient to know the class conditional proba-\\nbilities pk(x) = Pr( Y=Gk|x),k= 1,2,... ,K , for then the Bayes classiﬁer\\nis\\nG(x) =Gkwhere k= arg max\\nℓpℓ(x). (10.20)\\nIn principal, though, we need not learn the pk(x), but simply which one is\\nlargest. However, in data mining applications the interest is often more in\\nthe class probabilities pℓ(x), ℓ= 1,... ,K themselves, rather than in per-\\nforming a class assignment. As in Section 4.4, the logistic model generalizes\\nnaturally to Kclasses,\\npk(x) =efk(x)\\n∑K\\nl=1efl(x), (10.21)\\nwhich ensures that 0 ≤pk(x)≤1 and that they sum to one. Note that\\nhere we have Kdiﬀerent functions, one per class. There is a redundancy\\nin the functions fk(x), since adding an arbitrary h(x) to each leaves the\\nmodel unchanged. Traditionally one of them is set to zero: for example,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d614abd-9a23-4fc0-8299-61e46378f378', embedding=None, metadata={'page_label': '368', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.6 Loss Functions and Robustness 349\\nfK(x) = 0, as in (4.17). Here we prefer to retain the symmetry, and impose\\nthe constraint∑K\\nk=1fk(x) = 0. The binomial deviance extends naturally\\nto the K-class multinomial deviance loss function:\\nL(y,p(x)) = −K∑\\nk=1I(y=Gk)logpk(x)\\n=−K∑\\nk=1I(y=Gk)fk(x) + log(K∑\\nℓ=1efℓ(x))\\n.(10.22)\\nAs in the two-class case, the criterion (10.22) penalizes incorrect predictions\\nonly linearly in their degree of incorrectness.\\nZhu et al. (2005) generalize the exponential loss for K-class problems.\\nSee Exercise 10.5 for details.\\nRobust Loss Functions for Regression\\nIn the regression setting, analogous to the relationship between exponential\\nloss and binomial log-likelihood is the relationship between squared-error\\nlossL(y,f(x)) = ( y−f(x))2and absolute loss L(y,f(x)) =|y−f(x)|. The\\npopulation solutions are f(x) = E( Y|x) for squared-error loss, and f(x) =\\nmedian( Y|x) for absolute loss; for symmetric error distributions these are\\nthe same. However, on ﬁnite samples squared-error loss places much more\\nemphasis on observations with large absolute residuals |yi−f(xi)|during\\nthe ﬁtting process. It is thus far less robust, and its performance severely\\ndegrades for long-tailed error distributions and especially for grossly mis-\\nmeasured y-values (“outliers”). Other more robust criteria, such as abso-\\nlute loss, perform much better in these situations. In the statistical ro -\\nbustness literature, a variety of regression loss criteria have been proposed\\nthat provide strong resistance (if not absolute immunity) to gross outliers\\nwhile being nearly as eﬃcient as least squares for Gaussian errors. They\\nare often better than either for error distributions with moderately heavy\\ntails. One such criterion is the Huber loss criterion used for M-regression\\n(Huber, 1964)\\nL(y,f(x)) ={\\n[y−f(x)]2for|y−f(x)| ≤δ,\\n2δ|y−f(x)| −δ2otherwise.(10.23)\\nFigure 10.5 compares these three loss functions.\\nThese considerations suggest than when robustness is a concern, as is\\nespecially the case in data mining applications (see Section 10.7), squared-\\nerror loss for regression and exponential loss for classiﬁcation are not the\\nbest criteria from a statistical perspective. However, they both lead to the\\nelegant modular boosting algorithms in the context of forward stagewis e\\nadditive modeling. For squared-error loss one simply ﬁts the base learner\\nto the residuals from the current model yi−fm−1(xi) at each step. For', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f1be6eda-9758-4c8d-a5ca-7ced85577bf0', embedding=None, metadata={'page_label': '369', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='350 10. Boosting and Additive Trees\\n−3 −2 −1 0 1 2 30 2 4 6 8Squared Error\\nAbsolute Error\\nHuberLoss\\ny−f\\nFIGURE 10.5. A comparison of three loss functions for regression, plotted as a\\nfunction of the margin y−f. The Huber loss function combines the good properties\\nof squared-error loss near zero and absolute error loss when |y−f|is large.\\nexponential loss one performs a weighted ﬁt of the base learner to the\\noutput values yi, with weights wi= exp( −yifm−1(xi)). Using other more\\nrobust criteria directly in their place does not give rise to such simple\\nfeasible boosting algorithms. However, in Section 10.10.2 we show how one\\ncan derive simple elegant boosting algorithms based on any diﬀerentiable\\nloss criterion, thereby producing highly robust boosting procedures for data\\nmining.\\n10.7 “Oﬀ-the-Shelf” Procedures for Data Mining\\nPredictive learning is an important aspect of data mining. As can be seen\\nfrom this book, a wide variety of methods have been developed for predic-\\ntive learning from data. For each particular method there are situations\\nfor which it is particularly well suited, and others where it performs badly\\ncompared to the best that can be done with that data. We have attempted\\nto characterize appropriate situations in our discussions of each of the re-\\nspective methods. However, it is seldom known in advance which procedure\\nwill perform best or even well for any given problem. Table 10.1 summarizes\\nsome of the characteristics of a number of learning methods.\\nIndustrial and commercial data mining applications tend to be especially\\nchallenging in terms of the requirements placed on learning procedures.\\nData sets are often very large in terms of number of observations and\\nnumber of variables measured on each of them. Thus, computational con-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ce78408-4c08-4b3d-9f83-116c24a0646a', embedding=None, metadata={'page_label': '370', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.7 “Oﬀ-the-Shelf” Procedures for Data Mining 351\\nTABLE 10.1. Some characteristics of diﬀerent learning methods. Key: ▲= good,\\n◆=fair, and ▼=poor.\\nCharacteristic Neural SVM Trees MARS k-NN,\\nNets Kernels\\nNatural handling of data\\nof “mixed” type▼ ▼ ▲ ▲ ▼\\nHandling of missing values ▼ ▼ ▲ ▲ ▲\\nRobustness to outliers in\\ninput space▼ ▼ ▲▼ ▲\\nInsensitive to monotone\\ntransformations of inputs▼ ▼ ▲▼ ▼\\nComputational scalability\\n(large N)▼ ▼ ▲ ▲ ▼\\nAbility to deal with irrel-\\nevant inputs▼ ▼ ▲ ▲ ▼\\nAbility to extract linear\\ncombinations of features▲ ▲ ▼ ▼ ◆\\nInterpretability ▼ ▼ ◆▲ ▼\\nPredictive power ▲ ▲ ▼◆ ▲\\nsiderations play an important role. Also, the data are usually messy : the\\ninputs tend to be mixtures of quantitative, binary, and categorical vari-\\nables, the latter often with many levels. There are generally many missing\\nvalues, complete observations being rare. Distributions of numeric predic-\\ntor and response variables are often long-tailed and highly skewed. This\\nis the case for the spam data (Section 9.1.2); when ﬁtting a generalized\\nadditive model, we ﬁrst log-transformed each of the predictors in order to\\nget a reasonable ﬁt. In addition they usually contain a substantial fraction\\nof gross mis-measurements (outliers). The predictor variables are generally\\nmeasured on very diﬀerent scales.\\nIn data mining applications, usually only a small fraction of the large\\nnumber of predictor variables that have been included in the analysis are\\nactually relevant to prediction. Also, unlike many applications such as pat-\\ntern recognition, there is seldom reliable domain knowledge to help create\\nespecially relevant features and/or ﬁlter out the irrelevant ones, the inclu-\\nsion of which dramatically degrades the performance of many methods.\\nIn addition, data mining applications generally require interpretable mod-\\nels. It is not enough to simply produce predictions. It is also desirable to\\nhave information providing qualitative understanding of the relationship', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0a38262b-5525-4f60-99db-b32a47d511d0', embedding=None, metadata={'page_label': '371', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='352 10. Boosting and Additive Trees\\nbetween joint values of the input variables and the resulting predicted re-\\nsponse value. Thus, black box methods such as neural networks, which can\\nbe quite useful in purely predictive settings such as pattern recognition,\\nare far less useful for data mining.\\nThese requirements of speed, interpretability and the messy nature of\\nthe data sharply limit the usefulness of most learning procedures as oﬀ-\\nthe-shelf methods for data mining. An “oﬀ-the-shelf” method is one that\\ncan be directly applied to the data without requiring a great deal of time-\\nconsuming data preprocessing or careful tuning of the learning procedure.\\nOf all the well-known learning methods, decision trees come closest to\\nmeeting the requirements for serving as an oﬀ-the-shelf procedure for data\\nmining. They are relatively fast to construct and they produce interpretable\\nmodels (if the trees are small). As discussed in Section 9.2, they naturally\\nincorporate mixtures of numeric and categorical predictor variables and\\nmissing values. They are invariant under (strictly monotone) transforma-\\ntions of the individual predictors. As a result, scaling and/or more general\\ntransformations are not an issue, and they are immune to the eﬀects of pre-\\ndictor outliers. They perform internal feature selection as an integral part\\nof the procedure. They are thereby resistant, if not completely immune,\\nto the inclusion of many irrelevant predictor variables. These properties of\\ndecision trees are largely the reason that they have emerged as the most\\npopular learning method for data mining.\\nTrees have one aspect that prevents them from being the ideal tool for\\npredictive learning, namely inaccuracy. They seldom provide predictive ac-\\ncuracy comparable to the best that can be achieved with the data at hand.\\nAs seen in Section 10.1, boosting decision trees improves their accuracy,\\noften dramatically. At the same time it maintains most of their desirabl e\\nproperties for data mining. Some advantages of trees that are sacriﬁced by\\nboosting are speed, interpretability, and, for AdaBoost, robustness against\\noverlapping class distributions and especially mislabeling of the training\\ndata. A gradient boosted model (GBM) is a generalization of tree boosting\\nthat attempts to mitigate these problems, so as to produce an accurate and\\neﬀective oﬀ-the-shelf procedure for data mining.\\n10.8 Example: Spam Data\\nBefore we go into the details of gradient boosting, we demonstrate its abi li-\\nties on a two-class classiﬁcation problem. The spam data are introduced in\\nChapter 1, and used as an example for many of the procedures in Chapter 9\\n(Sections 9.1.2, 9.2.5, 9.3.1 and 9.4.1).\\nApplying gradient boosting to these data resulted in a test error rate of\\n4.5%, using the same test set as was used in Section 9.1.2. By comparison,\\nan additive logistic regression achieved 5.5%, a CART tree fully grown and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f1d344e9-0ee2-45f1-b21b-8c7d7cf967af', embedding=None, metadata={'page_label': '372', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.9 Boosting Trees 353\\npruned by cross-validation 8.7%, and MARS 5.5%. The standard error of\\nthese estimates is around 0.6%, although gradient boosting is signiﬁcantly\\nbetter than all of them using the McNemar test (Exercise 10.6).\\nIn Section 10.13 below we develop a relative importance measure for\\neach predictor, as well as a partial dependence plot describing a predictor’s\\ncontribution to the ﬁtted model. We now illustrate these for the spam data.\\nFigure 10.6 displays the relative importance spectrum for all 57 predictor\\nvariables. Clearly some predictors are more important than others in sep-\\naratingspamfromemail. The frequencies of the character strings !,$,hp,\\nandremove are estimated to be the four most relevant predictor variables.\\nAt the other end of the spectrum, the character strings 857,415,table, and\\n3dhave virtually no relevance.\\nThe quantity being modeled here is the log-odds of spamversusemail\\nf(x) = logPr(spam|x)\\nPr(email|x)(10.24)\\n(see Section 10.13 below). Figure 10.7 shows the partial dependence of the\\nlog-odds on selected important predictors, two positively associated with\\nspam(!andremove ), and two negatively associated ( eduandhp). These\\nparticular dependencies are seen to be essentially monotonic. There is a\\ngeneral agreement with the corresponding functions found by the additive\\nlogistic regression model; see Figure 9.1 on page 303.\\nRunning a gradient boosted model on these data with J= 2 terminal-\\nnode trees produces a purely additive (main eﬀects) model for the log-\\nodds, with a corresponding error rate of 4.7%, as compared to 4.5% for the\\nfull gradient boosted model (with J= 5 terminal-node trees). Although\\nnot signiﬁcant, this slightly higher error rate suggests that there may be\\ninteractions among some of the important predictor variables. This can\\nbe diagnosed through two-variable partial dependence plots. Figure 10.8\\nshows one of the several such plots displaying strong interaction eﬀects.\\nOne sees that for very low frequencies of hp, the log-odds of spamare\\ngreatly increased. For high frequencies of hp, the log-odds of spamtend to\\nbe much lower and roughly constant as a function of !. As the frequency\\nofhpdecreases, the functional relationship with !strengthens.\\n10.9 Boosting Trees\\nRegression and classiﬁcation trees are discussed in detail in Section 9.2.\\nThey partition the space of all joint predictor variable values into disjoin t\\nregions Rj,j= 1,2,... ,J , as represented by the terminal nodes of the tree.\\nA constant γjis assigned to each such region and the predictive rule is\\nx∈Rj⇒f(x) =γj.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1e517029-f6d8-4844-a909-55bce6d1bf0e', embedding=None, metadata={'page_label': '373', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='354 10. Boosting and Additive Trees\\n!$hpremovefreeCAPAVEyourCAPMAXgeorgeCAPTOTeduyouourmoneywill1999businessre(receiveinternet000emailmeeting;650overmailpmpeopletechnologyhplallorderaddressmakefontprojectdataoriginalreportconferencelab[creditparts#85tablecsdirect415857telnetlabsaddresses3d\\n0 20 40 60 80 100\\nRelative Importance\\nFIGURE 10.6. Predictor variable importance spectrum for the spamdata. The\\nvariable names are written on the vertical axis.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='abf3c79d-d319-4f1f-8f15-5a8cad07809a', embedding=None, metadata={'page_label': '374', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.9 Boosting Trees 355\\n!Partial Dependence\\n0.0 0.2 0.4 0.6 0.8 1.0-0.2 0.0 0.2 0.4 0.6 0.8 1.0\\nremovePartial Dependence\\n0.0 0.2 0.4 0.6-0.2 0.0 0.2 0.4 0.6 0.8 1.0\\neduPartial Dependence\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.6 -0.2 0.0 0.2\\nhpPartial Dependence\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1.0 -0.6 -0.2 0.0 0.2\\nFIGURE 10.7. Partial dependence of log-odds of spamon four important pre-\\ndictors. The red ticks at the base of the plots are deciles of t he input variable.\\n0.51.01.52.02.53.00.20.40.60.81.0-1.0-0.5 0.0 0.5 1.0\\nhp!\\nFIGURE 10.8. Partial dependence of the log-odds of spamvs.email as a func-\\ntion of joint frequencies of hpand the character !.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2c59e80b-5c13-466a-9b21-9cde70dca012', embedding=None, metadata={'page_label': '375', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='356 10. Boosting and Additive Trees\\nThus a tree can be formally expressed as\\nT(x;Θ) =J∑\\nj=1γjI(x∈Rj), (10.25)\\nwith parameters Θ = {Rj,γj}J\\n1.Jis usually treated as a meta-parameter.\\nThe parameters are found by minimizing the empirical risk\\nˆΘ = arg min\\nΘJ∑\\nj=1∑\\nxi∈RjL(yi,γj). (10.26)\\nThis is a formidable combinatorial optimization problem, and we usually\\nsettle for approximate suboptimal solutions. It is useful to divide the opti -\\nmization problem into two parts:\\nFinding γjgiven Rj:Given the Rj, estimating the γjis typically trivial,\\nand often ˆ γj= ¯yj, the mean of the yifalling in region Rj. For mis-\\nclassiﬁcation loss, ˆ γjis the modal class of the observations falling in\\nregion Rj.\\nFinding Rj:This is the diﬃcult part, for which approximate solutions are\\nfound. Note also that ﬁnding the Rjentails estimating the γjas well.\\nA typical strategy is to use a greedy, top-down recursive partitioning\\nalgorithm to ﬁnd the Rj. In addition, it is sometimes necessary to\\napproximate (10.26) by a smoother and more convenient criterion for\\noptimizing the Rj:\\n˜Θ = arg min\\nΘN∑\\ni=1˜L(yi,T(xi,Θ)). (10.27)\\nThen given the ˆRj=˜Rj, the γjcan be estimated more precisely\\nusing the original criterion.\\nIn Section 9.2 we described such a strategy for classiﬁcation trees. The Gini\\nindex replaced misclassiﬁcation loss in the growing of the tree (identifying\\ntheRj).\\nThe boosted tree model is a sum of such trees,\\nfM(x) =M∑\\nm=1T(x;Θm), (10.28)\\ninduced in a forward stagewise manner (Algorithm 10.2). At each step in\\nthe forward stagewise procedure one must solve\\nˆΘm= arg min\\nΘmN∑\\ni=1L(yi,fm−1(xi) +T(xi;Θm)) (10.29)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7bdb3026-950f-43e7-b04f-dc9639eea601', embedding=None, metadata={'page_label': '376', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.9 Boosting Trees 357\\nfor the region set and constants Θ m={Rjm,γjm}Jm\\n1of the next tree, given\\nthe current model fm−1(x).\\nGiven the regions Rjm, ﬁnding the optimal constants γjmin each region\\nis typically straightforward:\\nˆγjm= arg min\\nγjm∑\\nxi∈RjmL(yi,fm−1(xi) +γjm). (10.30)\\nFinding the regions is diﬃcult, and even more diﬃcult than for a single\\ntree. For a few special cases, the problem simpliﬁes.\\nFor squared-error loss, the solution to (10.29) is no harder than for a\\nsingle tree. It is simply the regression tree that best predicts the current\\nresiduals yi−fm−1(xi), and ˆ γjmis the mean of these residuals in each\\ncorresponding region.\\nFor two-class classiﬁcation and exponential loss, this stagewise approac h\\ngives rise to the AdaBoost method for boosting classiﬁcation trees (Algo -\\nrithm 10.1). In particular, if the trees T(x;Θm) are restricted to be scaled\\nclassiﬁcation trees, then we showed in Section 10.4 that the solution to\\n(10.29) is the tree that minimizes the weighted error rate∑N\\ni=1w(m)\\niI(yi̸=\\nT(xi;Θm)) with weights w(m)\\ni=e−yifm−1(xi). By a scaled classiﬁcation\\ntree, we mean βmT(x;Θm), with the restriction that γjm∈ {−1,1}).\\nWithout this restriction, (10.29) still simpliﬁes for exponential loss t o a\\nweighted exponential criterion for the new tree:\\nˆΘm= arg min\\nΘmN∑\\ni=1w(m)\\niexp[−yiT(xi;Θm)]. (10.31)\\nIt is straightforward to implement a greedy recursive-partitioning algori thm\\nusing this weighted exponential loss as a splitting criterion. Given the Rjm,\\none can show (Exercise 10.7) that the solution to (10.30) is the weighted\\nlog-odds in each corresponding region\\nˆγjm= log∑\\nxi∈Rjmw(m)\\niI(yi= 1)\\n∑\\nxi∈Rjmw(m)\\niI(yi=−1). (10.32)\\nThis requires a specialized tree-growing algorithm; in practice, we prefer\\nthe approximation presented below that uses a weighted least squares re-\\ngression tree.\\nUsing loss criteria such as the absolute error or the Huber loss (10.23) in\\nplace of squared-error loss for regression, and the deviance (10.22) in place\\nof exponential loss for classiﬁcation, will serve to robustify boosting trees.\\nUnfortunately, unlike their nonrobust counterparts, these robust criteria\\ndo not give rise to simple fast boosting algorithms.\\nFor more general loss criteria the solution to (10.30), given the Rjm,\\nis typically straightforward since it is a simple “location” estimat e. For', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb27e323-bc26-4287-a236-3da7ca70f882', embedding=None, metadata={'page_label': '377', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='358 10. Boosting and Additive Trees\\nabsolute loss it is just the median of the residuals in each respective region.\\nFor the other criteria fast iterative algorithms exist for solving (10 .30),\\nand usually their faster “single-step” approximations are adequate. The\\nproblem is tree induction. Simple fast algorithms do not exist for solving\\n(10.29) for these more general loss criteria, and approximations like (1 0.27)\\nbecome essential.\\n10.10 Numerical Optimization via Gradient\\nBoosting\\nFast approximate algorithms for solving (10.29) with any diﬀerenti able loss\\ncriterion can be derived by analogy to numerical optimization. The loss in\\nusing f(x) to predict yon the training data is\\nL(f) =N∑\\ni=1L(yi,f(xi)). (10.33)\\nThe goal is to minimize L(f) with respect to f, where here f(x) is con-\\nstrained to be a sum of trees (10.28). Ignoring this constraint, minimizing\\n(10.33) can be viewed as a numerical optimization\\nˆf= arg min\\nfL(f), (10.34)\\nwhere the “parameters” f∈IRNare the values of the approximating func-\\ntionf(xi) at each of the Ndata points xi:\\nf={f(x1),f(x2)),... ,f (xN)}.\\nNumerical optimization procedures solve (10.34) as a sum of component\\nvectors\\nfM=M∑\\nm=0hm,hm∈IRN,\\nwhere f0=h0is an initial guess, and each successive fmis induced based\\non the current parameter vector fm−1, which is the sum of the previously\\ninduced updates. Numerical optimization methods diﬀer in their prescrip-\\ntions for computing each increment vector hm(“step”).\\n10.10.1 Steepest Descent\\nSteepest descent chooses hm=−ρmgmwhere ρmis a scalar and gm∈IRN\\nis the gradient of L(f) evaluated at f=fm−1. The components of the\\ngradient gmare\\ngim=[∂L(yi,f(xi))\\n∂f(xi)]\\nf(xi)=fm−1(xi)(10.35)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb61fafd-822c-4d01-905d-41c90e48f26d', embedding=None, metadata={'page_label': '378', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.10 Numerical Optimization via Gradient Boosting 359\\nThestep length ρmis the solution to\\nρm= arg min\\nρL(fm−1−ρgm). (10.36)\\nThe current solution is then updated\\nfm=fm−1−ρmgm\\nand the process repeated at the next iteration. Steepest descent can be\\nviewed as a very greedy strategy, since −gmis the local direction in IRN\\nfor which L(f) is most rapidly decreasing at f=fm−1.\\n10.10.2 Gradient Boosting\\nForward stagewise boosting (Algorithm 10.2) is also a very greedy st rategy.\\nAt each step the solution tree is the one that maximally reduces (10.29),\\ngiven the current model fm−1and its ﬁts fm−1(xi). Thus, the tree predic-\\ntionsT(xi;Θm) are analogous to the components of the negative gradient\\n(10.35). The principal diﬀerence between them is that the tree compo-\\nnentstm= (T(x1;Θm),... ,T (xN;Θm) are not independent. They are con-\\nstrained to be the predictions of a Jm-terminal node decision tree, whereas\\nthe negative gradient is the unconstrained maximal descent direction.\\nThe solution to (10.30) in the stagewise approach is analogous to the li ne\\nsearch (10.36) in steepest descent. The diﬀerence is that (10.30) performs\\na separate line search for those components of tmthat correspond to each\\nseparate terminal region {T(xi;Θm)}xi∈Rjm.\\nIf minimizing loss on the training data (10.33) were the only goal, steep-\\nest descent would be the preferred strategy. The gradient (10.35) is trivial\\nto calculate for any diﬀerentiable loss function L(y,f(x)), whereas solving\\n(10.29) is diﬃcult for the robust criteria discussed in Section 10.6. Unfor-\\ntunately the gradient (10.35) is deﬁned only at the training data points xi,\\nwhereas the ultimate goal is to generalize fM(x) to new data not repre-\\nsented in the training set.\\nA possible resolution to this dilemma is to induce a tree T(x;Θm) at the\\nmth iteration whose predictions tmare as close as possible to the negative\\ngradient. Using squared error to measure closeness, this leads us to\\n˜Θm= arg min\\nΘN∑\\ni=1(−gim−T(xi;Θ))2. (10.37)\\nThat is, one ﬁts the tree Tto the negative gradient values (10.35) by least\\nsquares. As noted in Section 10.9 fast algorithms exist for least squares\\ndecision tree induction. Although the solution regions ˜Rjmto (10.37) will\\nnot be identical to the regions Rjmthat solve (10.29), it is generally sim-\\nilar enough to serve the same purpose. In any case, the forward stagewise', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='abed0d4f-dd14-46b9-aa3b-1682be9a09e3', embedding=None, metadata={'page_label': '379', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='360 10. Boosting and Additive Trees\\nTABLE 10.2. Gradients for commonly used loss functions.\\nSetting Loss Function −∂L(yi,f(xi))/∂f(xi)\\nRegression1\\n2[yi−f(xi)]2yi−f(xi)\\nRegression |yi−f(xi)| sign[yi−f(xi)]\\nRegression Huber yi−f(xi) for|yi−f(xi)| ≤δm\\nδmsign[yi−f(xi)] for|yi−f(xi)|> δm\\nwhere δm=αth-quantile {|yi−f(xi)|}\\nClassiﬁcation Deviance kth component: I(yi=Gk)−pk(xi)\\nboosting procedure, and top-down decision tree induction, are themselves\\napproximation procedures. After constructing the tree (10.37), the corre-\\nsponding constants in each region are given by (10.30).\\nTable 10.2 summarizes the gradients for commonly used loss functions.\\nFor squared error loss, the negative gradient is just the ordinary residual\\n−gim=yi−fm−1(xi), so that (10.37) on its own is equivalent standard\\nleast squares boosting. With absolute error loss, the negative gradient i s\\nthesignof the residual, so at each iteration (10.37) ﬁts the tree to the\\nsign of the current residuals by least squares. For Huber M-regression, the\\nnegative gradient is a compromise between these two (see the table).\\nFor classiﬁcation the loss function is the multinomial deviance (10.22),\\nandKleast squares trees are constructed at each iteration. Each tree Tkm\\nis ﬁt to its respective negative gradient vector gkm,\\n−gikm=∂L(yi,f1m(xi),... ,f 1m(xi))\\n∂fkm(xi)\\n=I(yi=Gk)−pk(xi), (10.38)\\nwithpk(x) given by (10.21). Although Kseparate trees are built at each\\niteration, they are related through (10.21). For binary classiﬁcation ( K=\\n2), only one tree is needed (exercise 10.10).\\n10.10.3 Implementations of Gradient Boosting\\nAlgorithm 10.3 presents the generic gradient tree-boosting algorithm for\\nregression. Speciﬁc algorithms are obtained by inserting diﬀerent loss cri-\\nteriaL(y,f(x)). The ﬁrst line of the algorithm initializes to the optimal\\nconstant model, which is just a single terminal node tree. The components\\nof the negative gradient computed at line 2(a) are referred to as general-\\nized or pseudo residuals, r. Gradients for commonly used loss functions are\\nsummarized in Table 10.2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1836a82-bc82-4458-9349-84a58fbe3984', embedding=None, metadata={'page_label': '380', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.11 Right-Sized Trees for Boosting 361\\nAlgorithm 10.3 Gradient Tree Boosting Algorithm.\\n1. Initialize f0(x) = arg min γ∑N\\ni=1L(yi,γ).\\n2. For m= 1 to M:\\n(a) For i= 1,2,... ,N compute\\nrim=−[∂L(yi,f(xi))\\n∂f(xi)]\\nf=fm−1.\\n(b) Fit a regression tree to the targets rimgiving terminal regions\\nRjm, j= 1,2,... ,J m.\\n(c) For j= 1,2,... ,J mcompute\\nγjm= arg min\\nγ∑\\nxi∈RjmL(yi,fm−1(xi) +γ).\\n(d) Update fm(x) =fm−1(x) +∑Jm\\nj=1γjmI(x∈Rjm).\\n3. Output ˆf(x) =fM(x).\\nThe algorithm for classiﬁcation is similar. Lines 2(a)–(d) are repeated\\nKtimes at each iteration m, once for each class using (10.38). The result\\nat line 3 is Kdiﬀerent (coupled) tree expansions fkM(x),k= 1,2,... ,K .\\nThese produce probabilities via (10.21) or do classiﬁcation as in (10.20).\\nDetails are given in Exercise 10.9. Two basic tuning parameters are the\\nnumber of iterations Mand the sizes of each of the constituent trees\\nJm, m= 1,2,... ,M .\\nThe original implementation of this algorithm was called MART for\\n“multiple additive regression trees,” and was referred to in the ﬁrst edi-\\ntion of this book. Many of the ﬁgures in this chapter were produced by\\nMART. Gradient boosting as described here is implemented in the R gbm\\npackage (Ridgeway, 1999, “Gradient Boosted Models”), and is freely avai l-\\nable. The gbmpackage is used in Section 10.14.2, and extensively in Chap-\\nters 16 and 15. Another R implementation of boosting is mboost (Hothorn\\nand B¨ uhlmann, 2006). A commercial implementation of gradient boost-\\ning/MART called TreeNet\\uf8e8is available from Salford Systems, Inc.\\n10.11 Right-Sized Trees for Boosting\\nHistorically, boosting was considered to be a technique for combining mod-\\nels, here trees. As such, the tree building algorithm was regarded as a', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e29291f4-d1fb-4dae-bda6-1560e199bec6', embedding=None, metadata={'page_label': '381', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='362 10. Boosting and Additive Trees\\nprimitive that produced models to be combined by the boosting proce-\\ndure. In this scenario, the optimal size of each tree is estimated separately\\nin the usual manner when it is built (Section 9.2). A very large (oversized)\\ntree is ﬁrst induced, and then a bottom-up procedure is employed to prune\\nit to the estimated optimal number of terminal nodes. This approach as-\\nsumes implicitly that each tree is the last one in the expansion (10.28).\\nExcept perhaps for the very last tree, this is clearly a very poor assump-\\ntion. The result is that trees tend to be much too large, especially during\\nthe early iterations. This substantially degrades performance and increases\\ncomputation.\\nThe simplest strategy for avoiding this problem is to restrict all trees\\nto be the same size, Jm=J∀m. At each iteration a J-terminal node\\nregression tree is induced. Thus Jbecomes a meta-parameter of the entire\\nboosting procedure, to be adjusted to maximize estimated performance for\\nthe data at hand.\\nOne can get an idea of useful values for Jby considering the properties\\nof the “target” function\\nη= arg min\\nfEXYL(Y,f(X)). (10.39)\\nHere the expected value is over the population joint distribution of ( X,Y).\\nThe target function η(x) is the one with minimum prediction risk on future\\ndata. This is the function we are trying to approximate.\\nOne relevant property of η(X) is the degree to which the coordinate vari-\\nables XT= (X1,X2,... ,X p) interact with one another. This is captured\\nby its ANOVA (analysis of variance) expansion\\nη(X) =∑\\njηj(Xj)+∑\\njkηjk(Xj,Xk)+∑\\njklηjkl(Xj,Xk,Xl)+≤≤≤.(10.40)\\nThe ﬁrst sum in (10.40) is over functions of only a single predictor variable\\nXj. The particular functions ηj(Xj) are those that jointly best approximate\\nη(X) under the loss criterion being used. Each such ηj(Xj) is called the\\n“main eﬀect” of Xj. The second sum is over those two-variable functions\\nthat when added to the main eﬀects best ﬁt η(X). These are called the\\nsecond-order interactions of each respective variable pair ( Xj,Xk). The\\nthird sum represents third-order interactions, and so on. For many problems\\nencountered in practice, low-order interaction eﬀects tend to dominate.\\nWhen this is the case, models that produce strong higher-order interaction\\neﬀects, such as large decision trees, suﬀer in accuracy.\\nThe interaction level of tree-based approximations is limited by the tree\\nsizeJ. Namely, no interaction eﬀects of level greater that J−1 are pos-\\nsible. Since boosted models are additive in the trees (10.28), this limit\\nextends to them as well. Setting J= 2 (single split “decision stump”)\\nproduces boosted models with only main eﬀects; no interactions are per-\\nmitted. With J= 3, two-variable interaction eﬀects are also allowed, and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3dafd7b6-e04a-4661-8f25-5f7da8412215', embedding=None, metadata={'page_label': '382', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.11 Right-Sized Trees for Boosting 363\\nNumber of TermsTest Error\\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4Stumps\\n10 Node\\n100 Node\\nAdaboost\\nFIGURE 10.9. Boosting with diﬀerent sized trees, applied to the example (10. 2)\\nused in Figure 10.2. Since the generative model is additive, stu mps perform the\\nbest. The boosting algorithm used the binomial deviance loss in Algorithm 10.3;\\nshown for comparison is the AdaBoost Algorithm 10.1.\\nso on. This suggests that the value chosen for Jshould reﬂect the level\\nof dominant interactions of η(x). This is of course generally unknown, but\\nin most situations it will tend to be low. Figure 10.9 illustrates the eﬀ ect\\nof interaction order (choice of J) on the simulation example (10.2). The\\ngenerative function is additive (sum of quadratic monomials), so boosting\\nmodels with J >2 incurs unnecessary variance and hence the higher test\\nerror. Figure 10.10 compares the coordinate functions found by boosted\\nstumps with the true functions.\\nAlthough in many applications J= 2 will be insuﬃcient, it is unlikely\\nthatJ >10 will be required. Experience so far indicates that 4 ≤J≤8\\nworks well in the context of boosting, with results being fairly insensiti ve\\nto particular choices in this range. One can ﬁne-tune the value for Jby\\ntrying several diﬀerent values and choosing the one that produces the low-\\nest risk on a validation sample. However, this seldom provides signiﬁcant\\nimprovement over using J≃6.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb416e10-96b3-4033-9620-9dea01854ccf', embedding=None, metadata={'page_label': '383', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='364 10. Boosting and Additive Trees\\nCoordinate Functions for Additive Logistic Trees\\nf1(x1) f2(x2) f3(x3) f4(x4) f5(x5)\\nf6(x6) f7(x7) f8(x8) f9(x9) f10(x10)\\nFIGURE 10.10. Coordinate functions estimated by boosting stumps for the sim-\\nulated example used in Figure 10.9. The true quadratic functio ns are shown for\\ncomparison.\\n10.12 Regularization\\nBesides the size of the constituent trees, J, the other meta-parameter of\\ngradient boosting is the number of boosting iterations M. Each iteration\\nusually reduces the training risk L(fM), so that for Mlarge enough this risk\\ncan be made arbitrarily small. However, ﬁtting the training data too well\\ncan lead to overﬁtting, which degrades the risk on future predictions. Thus,\\nthere is an optimal number M∗minimizing future risk that is application\\ndependent. A convenient way to estimate M∗is to monitor prediction risk\\nas a function of Mon a validation sample. The value of Mthat minimizes\\nthis risk is taken to be an estimate of M∗. This is analogous to the early\\nstopping strategy often used with neural networks (Section 11.4).\\n10.12.1 Shrinkage\\nControlling the value of Mis not the only possible regularization strategy.\\nAs with ridge regression and neural networks, shrinkage techniques can be\\nemployed as well (see Sections 3.4.1 and 11.5). The simplest implementation\\nof shrinkage in the context of boosting is to scale the contribution of each\\ntree by a factor 0 < ν < 1 when it is added to the current approximation.\\nThat is, line 2(d) of Algorithm 10.3 is replaced by\\nfm(x) =fm−1(x) +ν≤J∑\\nj=1γjmI(x∈Rjm). (10.41)\\nThe parameter νcan be regarded as controlling the learning rate of the\\nboosting procedure. Smaller values of ν(more shrinkage) result in larger\\ntraining risk for the same number of iterations M. Thus, both νandM\\ncontrol prediction risk on the training data. However, these parameters do', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bcb1d1a4-c6d5-4947-93b8-013b3a07130b', embedding=None, metadata={'page_label': '384', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.12 Regularization 365\\nnot operate independently. Smaller values of νlead to larger values of M\\nfor the same training risk, so that there is a tradeoﬀ between them.\\nEmpirically it has been found (Friedman, 2001) that smaller values of ν\\nfavor better test error, and require correspondingly larger values of M. In\\nfact, the best strategy appears to be to set νto be very small ( ν <0.1)\\nand then choose Mby early stopping. This yields dramatic improvements\\n(over no shrinkage ν= 1) for regression and for probability estimation. The\\ncorresponding improvements in misclassiﬁcation risk via (10.20) are les s,\\nbut still substantial. The price paid for these improvements is computa-\\ntional: smaller values of νgive rise to larger values of M, and computation\\nis proportional to the latter. However, as seen below, many iterations ar e\\ngenerally computationally feasible even on very large data sets. This is\\npartly due to the fact that small trees are induced at each step with no\\npruning.\\nFigure 10.11 shows test error curves for the simulated example (10.2) of\\nFigure 10.2. A gradient boosted model (MART) was trained using binomial\\ndeviance, using either stumps or six terminal-node trees, and with or with-\\nout shrinkage. The beneﬁts of shrinkage are evident, especially when the\\nbinomial deviance is tracked. With shrinkage, each test error curve reaches\\na lower value, and stays there for many iterations.\\nSection 16.2.1 draws a connection between forward stagewise shrinkage\\nin boosting and the use of an L1penalty for regularizing model parame-\\nters (the “lasso”). We argue that L1penalties may be superior to the L2\\npenalties used by methods such as the support vector machine.\\n10.12.2 Subsampling\\nWe saw in Section 8.7 that bootstrap averaging (bagging) improves the\\nperformance of a noisy classiﬁer through averaging. Chapter 15 discusses\\nin some detail the variance-reduction mechanism of this sampling followed\\nby averaging. We can exploit the same device in gradient boosting, both\\nto improve performance and computational eﬃciency.\\nWithstochastic gradient boosting (Friedman, 1999), at each iteration we\\nsample a fraction ηof the training observations (without replacement),\\nand grow the next tree using that subsample. The rest of the algorithm is\\nidentical. A typical value for ηcan be1\\n2, although for large N,ηcan be\\nsubstantially smaller than1\\n2.\\nNot only does the sampling reduce the computing time by the same\\nfraction η, but in many cases it actually produces a more accurate model.\\nFigure 10.12 illustrates the eﬀect of subsampling using the simulated\\nexample (10.2), both as a classiﬁcation and as a regression example. We\\nsee in both cases that sampling along with shrinkage slightly outperform ed\\nthe rest. It appears here that subsampling without shrinkage does poorly.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d41cb2ca-2525-4195-ba22-bb39121afb06', embedding=None, metadata={'page_label': '385', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='366 10. Boosting and Additive Trees\\nBoosting IterationsTest Set Deviance\\n0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\\nShrinkage=0.2Stumps\\nDeviance\\nBoosting IterationsTest Set Misclassification Error\\n0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\\nShrinkage=0.2Stumps\\nMisclassification Error\\nBoosting IterationsTest Set Deviance\\n0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\\nShrinkage=0.66-Node Trees\\nDeviance\\nBoosting IterationsTest Set Misclassification Error\\n0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\\nShrinkage=0.66-Node Trees\\nMisclassification Error\\nFIGURE 10.11. Test error curves for simulated example (10.2) of Figure 10.9 ,\\nusing gradient boosting (MART). The models were trained using bino mial de-\\nviance, either stumps or six terminal-node trees, and with or wit hout shrinkage.\\nThe left panels report test deviance, while the right panels sho w misclassiﬁcation\\nerror. The beneﬁcial eﬀect of shrinkage can be seen in all cases, especially for\\ndeviance in the left panels.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1fbb50ef-887a-4b43-a9d0-a64ed9429119', embedding=None, metadata={'page_label': '386', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.13 Interpretation 367\\n0 200 400 600 800 10000.4 0.6 0.8 1.0 1.2 1.4\\nBoosting IterationsTest Set DevianceDeviance4−Node Trees\\n0 200 400 600 800 10000.30 0.35 0.40 0.45 0.50\\nBoosting IterationsTest Set Absolute ErrorNo shrinkage\\nShrink=0.1\\nSample=0.5\\nShrink=0.1 Sample=0.5Absolute Error\\nFIGURE 10.12. Test-error curves for the simulated example (10.2), showing\\nthe eﬀect of stochasticity. For the curves labeled “Sample = 0.5”, a diﬀerent 50%\\nsubsample of the training data was used each time a tree was grow n. In the left\\npanel the models were ﬁt by gbmusing a binomial deviance loss function; in the\\nright-hand panel using square-error loss.\\nThe downside is that we now have four parameters to set: J,M,νand\\nη. Typically some early explorations determine suitable values for J,νand\\nη, leaving Mas the primary parameter.\\n10.13 Interpretation\\nSingle decision trees are highly interpretable. The entire model can be com-\\npletely represented by a simple two-dimensional graphic (binary tree) that\\nis easily visualized. Linear combinations of trees (10.28) lose this import ant\\nfeature, and must therefore be interpreted in a diﬀerent way.\\n10.13.1 Relative Importance of Predictor Variables\\nIn data mining applications the input predictor variables are seldom equally\\nrelevant. Often only a few of them have substantial inﬂuence on the re-\\nsponse; the vast majority are irrelevant and could just as well have not\\nbeen included. It is often useful to learn the relative importance or contri-\\nbution of each input variable in predicting the response.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='39fcfb93-9e0f-4cbc-a363-4a1a27e077d0', embedding=None, metadata={'page_label': '387', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='368 10. Boosting and Additive Trees\\nFor a single decision tree T, Breiman et al. (1984) proposed\\nI2\\nℓ(T) =J−1∑\\nt=1ˆı2\\ntI(v(t) =ℓ) (10.42)\\nas a measure of relevance for each predictor variable Xℓ. The sum is over\\ntheJ−1 internal nodes of the tree. At each such node t, one of the input\\nvariables Xv(t)is used to partition the region associated with that node into\\ntwo subregions; within each a separate constant is ﬁt to the response values.\\nThe particular variable chosen is the one that gives maximal estimated\\nimprovement ˆ ı2\\ntin squared error risk over that for a constant ﬁt over the\\nentire region. The squared relative importance of variable Xℓis the sum of\\nsuch squared improvements over all internal nodes for which it was chosen\\nas the splitting variable.\\nThis importance measure is easily generalized to additive tree expansions\\n(10.28); it is simply averaged over the trees\\nI2\\nℓ=1\\nMM∑\\nm=1I2\\nℓ(Tm). (10.43)\\nDue to the stabilizing eﬀect of averaging, this measure turns out to be more\\nreliable than is its counterpart (10.42) for a single tree. Also, because of\\nshrinkage (Section 10.12.1) the masking of important variables by other s\\nwith which they are highly correlated is much less of a problem. Note\\nthat (10.42) and (10.43) refer to squared relevance; the actual relevances\\nare their respective square roots. Since these measures are relative, it is\\ncustomary to assign the largest a value of 100 and then scale the others\\naccordingly. Figure 10.6 shows the relevant importance of the 57 inputs in\\npredicting spamversusemail.\\nForK-class classiﬁcation, Kseparate models fk(x),k= 1,2,... ,K are\\ninduced, each consisting of a sum of trees\\nfk(x) =M∑\\nm=1Tkm(x). (10.44)\\nIn this case (10.43) generalizes to\\nI2\\nℓk=1\\nMM∑\\nm=1I2\\nℓ(Tkm). (10.45)\\nHereIℓkis the relevance of Xℓin separating the class kobservations from\\nthe other classes. The overall relevance of Xℓis obtained by averaging over\\nall of the classes\\nI2\\nℓ=1\\nKK∑\\nk=1I2\\nℓk. (10.46)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1270472e-a561-44b3-ba47-05da1c3266be', embedding=None, metadata={'page_label': '388', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.13 Interpretation 369\\nFigures 10.23 and 10.24 illustrate the use of these averaged and separate\\nrelative importances.\\n10.13.2 Partial Dependence Plots\\nAfter the most relevant variables have been identiﬁed, the next step is to\\nattempt to understand the nature of the dependence of the approximation\\nf(X) on their joint values. Graphical renderings of the f(X) as a function\\nof its arguments provides a comprehensive summary of its dependence on\\nthe joint values of the input variables.\\nUnfortunately, such visualization is limited to low-dimensional views.\\nWe can easily display functions of one or two arguments, either continuous\\nor discrete (or mixed), in a variety of diﬀerent ways; this book is ﬁlled\\nwith such displays. Functions of slightly higher dimensions can be plotted\\nby conditioning on particular sets of values of all but one or two of the\\narguments, producing a trellis of plots (Becker et al., 1996).1\\nFor more than two or three variables, viewing functions of the corre-\\nsponding higher-dimensional arguments is more diﬃcult. A useful alterna-\\ntive can sometimes be to view a collection of plots, each one of which shows\\nthe partial dependence of the approximation f(X) on a selected small sub-\\nset of the input variables. Although such a collection can seldom provide a\\ncomprehensive depiction of the approximation, it can often produce helpful\\nclues, especially when f(x) is dominated by low-order interactions (10.40).\\nConsider the subvector XSofℓ < pof the input predictor variables XT=\\n(X1,X2,... ,X p), indexed by S ⊂ { 1,2,... ,p }. LetCbe the complement\\nset, with S ∪ C ={1,2,... ,p }. A general function f(X) will in principle\\ndepend on all of the input variables: f(X) =f(XS,XC). One way to deﬁne\\nthe average or partial dependence of f(X) onXSis\\nfS(XS) = E XCf(XS,XC). (10.47)\\nThis is a marginal average of f, and can serve as a useful description of the\\neﬀect of the chosen subset on f(X) when, for example, the variables in XS\\ndo not have strong interactions with those in XC.\\nPartial dependence functions can be used to interpret the results of any\\n“black box” learning method. They can be estimated by\\n¯fS(XS) =1\\nNN∑\\ni=1f(XS,xiC), (10.48)\\nwhere {x1C,x2C,... ,x NC}are the values of XCoccurring in the training\\ndata. This requires a pass over the data for each set of joint values of XSfor\\nwhich ¯fS(XS) is to be evaluated. This can be computationally intensive,\\n1lattice in R.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='47445b42-7ce3-42fc-8cd5-0c681deea8a3', embedding=None, metadata={'page_label': '389', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='370 10. Boosting and Additive Trees\\neven for moderately sized data sets. Fortunately with decision trees, ¯fS(XS)\\n(10.48) can be rapidly computed from the tree itself without reference to\\nthe data (Exercise 10.11).\\nIt is important to note that partial dependence functions deﬁned in\\n(10.47) represent the eﬀect of XSonf(X) after accounting for the (av-\\nerage) eﬀects of the other variables XConf(X). They are notthe eﬀect\\nofXSonf(X)ignoring the eﬀects of XC. The latter is given by the con-\\nditional expectation\\n˜fS(XS) = E( f(XS,XC)|XS), (10.49)\\nand is the best least squares approximation to f(X) by a function of XS\\nalone. The quantities ˜fS(XS) and ¯fS(XS) will be the same only in the\\nunlikely event that XSandXCare independent. For example, if the eﬀect\\nof the chosen variable subset happens to be purely additive,\\nf(X) =h1(XS) +h2(XC). (10.50)\\nThen (10.47) produces the h1(XS) up to an additive constant. If the eﬀect\\nis purely multiplicative,\\nf(X) =h1(XS)≤h2(XC), (10.51)\\nthen (10.47) produces h1(XS) up to a multiplicative constant factor. On\\nthe other hand, (10.49) will not produce h1(XS) in either case. In fact,\\n(10.49) can produce strong eﬀects on variable subsets for which f(X) has\\nno dependence at all.\\nViewing plots of the partial dependence of the boosted-tree approxima-\\ntion (10.28) on selected variables subsets can help to provide a qualitative\\ndescription of its properties. Illustrations are shown in Sections 10.8 and\\n10.14. Owing to the limitations of computer graphics, and human percep-\\ntion, the size of the subsets XSmust be small ( l≈1,2,3). There are of\\ncourse a large number of such subsets, but only those chosen from among\\nthe usually much smaller set of highly relevant predictors are likely to be\\ninformative. Also, those subsets whose eﬀect on f(X) is approximately\\nadditive (10.50) or multiplicative (10.51) will be most revealing.\\nForK-class classiﬁcation, there are Kseparate models (10.44), one for\\neach class. Each one is related to the respective probabilities (10.21) thro ugh\\nfk(X) = log pk(X)−1\\nKK∑\\nl=1logpl(X). (10.52)\\nThus each fk(X) is a monotone increasing function of its respective prob-\\nability on a logarithmic scale. Partial dependence plots of each respective\\nfk(X) (10.44) on its most relevant predictors (10.45) can help reveal how\\nthe log-odds of realizing that class depend on the respective input variables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c749c9c7-4188-46b4-9eba-5a8380d1585b', embedding=None, metadata={'page_label': '390', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.14 Illustrations 371\\n10.14 Illustrations\\nIn this section we illustrate gradient boosting on a number of larger data sets,\\nusing diﬀerent loss functions as appropriate.\\n10.14.1 California Housing\\nThis data set (Pace and Barry, 1997) is available from the Carnegie-Mellon\\nStatLib repository2. It consists of aggregated data from each of 20,460\\nneighborhoods (1990 census block groups) in California. The response vari-\\nableYis the median house value in each neighborhood measured in units of\\n$100,000. The predictor variables are demographics such as median income\\nMedInc , housing density as reﬂected by the number of houses House, and the\\naverage occupancy in each house AveOccup . Also included as predictors are\\nthe location of each neighborhood ( longitude andlatitude ), and several\\nquantities reﬂecting the properties of the houses in the neighborhood: av-\\nerage number of rooms AveRooms and bedrooms AveBedrms . There are thus\\na total of eight predictors, all numeric.\\nWe ﬁt a gradient boosting model using the MART procedure, with J= 6\\nterminal nodes, a learning rate (10.41) of ν= 0.1, and the Huber loss\\ncriterion for predicting the numeric response. We randomly divided the\\ndataset into a training set (80%) and a test set (20%).\\nFigure 10.13 shows the average absolute error\\nAAE = E|y−ˆfM(x)| (10.53)\\nas a function for number of iterations Mon both the training data and test\\ndata. The test error is seen to decrease monotonically with increasing M,\\nmore rapidly during the early stages and then leveling oﬀ to being nearly\\nconstant as iterations increase. Thus, the choice of a particular value of M\\nis not critical, as long as it is not too small. This tends to be the case in\\nmany applications. The shrinkage strategy (10.41) tends to eliminate the\\nproblem of overﬁtting, especially for larger data sets.\\nThe value of AAE after 800 iterations is 0.31. This can be compared to\\nthat of the optimal constant predictor median {yi}which is 0.89. In terms of\\nmore familiar quantities, the squared multiple correlation coeﬃcient of t his\\nmodel is R2= 0.84. Pace and Barry (1997) use a sophisticated spatial auto-\\nregression procedure, where prediction for each neighborhood is based on\\nmedian house values in nearby neighborhoods, using the other predictors as\\ncovariates. Experimenting with transformations they achieved R2= 0.85,\\npredicting log Y. Using log Yas the response the corresponding value for\\ngradient boosting was R2= 0.86.\\n2http://lib.stat.cmu.edu.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb67d52d-f7f9-41df-b051-ffc9ad4ce39b', embedding=None, metadata={'page_label': '391', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='372 10. Boosting and Additive Trees\\n0 200 400 600 8000.0 0.2 0.4 0.6 0.8\\nIterations MAbsolute ErrorTraining and Test Absolute Error\\nTrain Error\\nTest Error\\nFIGURE 10.13. Average-absolute error as a function of number of iterations\\nfor the California housing data.\\nFigure 10.14 displays the relative variable importances for each of the\\neight predictor variables. Not surprisingly, median income in the neigh-\\nborhood is the most relevant predictor. Longitude, latitude, and average\\noccupancy all have roughly half the relevance of income, whereas the others\\nare somewhat less inﬂuential.\\nFigure 10.15 shows single-variable partial dependence plots on the most\\nrelevant nonlocation predictors. Note that the plots are not strictly smoot h.\\nThis is a consequence of using tree-based models. Decision trees produce\\ndiscontinuous piecewise constant models (10.25). This carries over to sums\\nof trees (10.28), with of course many more pieces. Unlike most of the meth-\\nods discussed in this book, there is no smoothness constraint imposed on\\nthe result. Arbitrarily sharp discontinuities can be modeled. The fact that\\nthese curves generally exhibit a smooth trend is because that is what is\\nestimated to best predict the response for this problem. This is often the\\ncase.\\nThe hash marks at the base of each plot delineate the deciles of the\\ndata distribution of the corresponding variables. Note that here the data\\ndensity is lower near the edges, especially for larger values. This causes the\\ncurves to be somewhat less well determined in those regions. The vertical\\nscales of the plots are the same, and give a visual comparison of the relativ e\\nimportance of the diﬀerent variables.\\nThe partial dependence of median house value on median income is\\nmonotonic increasing, being nearly linear over the main body of data. House\\nvalue is generally monotonic decreasing with increasing average occupancy,\\nexcept perhaps for average occupancy rates less than one. Median house', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='689d8605-0a88-4052-9ab6-670addf37c6b', embedding=None, metadata={'page_label': '392', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.14 Illustrations 373\\nMedIncLongitudeAveOccupLatitudeHouseAgeAveRoomsAveBedrmsPopulation\\n0 20 40 60 80 100\\nRelative importance\\nFIGURE 10.14. Relative importance of the predictors for the California hous ing\\ndata.\\nvalue has a nonmonotonic partial dependence on average number of rooms.\\nIt has a minimum at approximately three rooms and is increasing both for\\nsmaller and larger values.\\nMedian house value is seen to have a very weak partial dependence on\\nhouse age that is inconsistent with its importance ranking (Figure 10.14) .\\nThis suggests that this weak main eﬀect may be masking stronger interac-\\ntion eﬀects with other variables. Figure 10.16 shows the two-variable part ial\\ndependence of housing value on joint values of median age and average oc-\\ncupancy. An interaction between these two variables is apparent. For values\\nof average occupancy greater than two, house value is nearly independent\\nof median age, whereas for values less than two there is a strong dependence\\non age.\\nFigure 10.17 shows the two-variable partial dependence of the ﬁtted\\nmodel on joint values of longitude and latitude, displayed as a shaded\\ncontour plot. There is clearly a very strong dependence of median house\\nvalue on the neighborhood location in California. Note that Figure 10. 17 is\\nnota plot of house value versus location ignoring the eﬀects of the other\\npredictors (10.49). Like all partial dependence plots, it represents the eﬀect\\nof location after accounting for the eﬀects of the other neighborhood and\\nhouse attributes (10.47). It can be viewed as representing an extra premium\\none pays for location. This premium is seen to be relatively large near the\\nPaciﬁc coast especially in the Bay Area and Los Angeles–San Diego re-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b8e77fde-bc9c-49a1-98f6-a148867c0670', embedding=None, metadata={'page_label': '393', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='374 10. Boosting and Additive Trees\\nMedIncPartial Dependence\\n2 4 6 8 10-0.5 0.0 0.5 1.0 1.5 2.0\\nAveOccupPartial Dependence\\n2 3 4 5-1.0 -0.5 0.0 0.5 1.0 1.5\\nHouseAgePartial Dependence\\n10 20 30 40 50-1.0 -0.5 0.0 0.5 1.0\\nAveRoomsPartial Dependence\\n4 6 8 10-1.0 -0.5 0.0 0.5 1.0 1.5\\nFIGURE 10.15. Partial dependence of housing value on the nonlocation vari-\\nables for the California housing data. The red ticks at the base of the plot are\\ndeciles of the input variables.\\n2\\n3\\n4\\n510203040500.00.51.0\\nAveOccupHouseAge\\nFIGURE 10.16. Partial dependence of house value on median age and aver-\\nage occupancy. There appears to be a strong interaction eﬀect be tween these two\\nvariables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4daf2eac-7944-464b-93ae-3cba0a2fd55a', embedding=None, metadata={'page_label': '394', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.14 Illustrations 375\\n−124 −122 −120 −118 −116 −11434 36 38 40 42\\nLongitudeLatitude\\n−1.0−0.5 0.0 0.5 1.0\\nFIGURE 10.17. Partial dependence of median house value on location in Cal-\\nifornia. One unit is $100,000, at1990prices, and the values plotted are relative\\nto the overall median of $180,000.\\ngions. In the northern, central valley, and southeastern desert regions of\\nCalifornia, location costs considerably less.\\n10.14.2 New Zealand Fish\\nPlant and animal ecologists use regression models to predict species pres-\\nence, abundance and richness as a function of environmental variables.\\nAlthough for many years simple linear and parametric models were popu-\\nlar, recent literature shows increasing interest in more sophisticated mod-\\nels such as generalized additive models (Section 9.1, GAM), multivariate\\nadaptive regression splines (Section 9.4, MARS) and boosted regression\\ntrees (Leathwick et al., 2005; Leathwick et al., 2006). Here we model the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8104ed00-037d-4da7-9f22-fd87e51f79a2', embedding=None, metadata={'page_label': '395', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='376 10. Boosting and Additive Trees\\npresence and abundance of the Black Oreo Dory , a marine ﬁsh found in the\\noceanic waters around New Zealand.3\\nFigure 10.18 shows the locations of 17,000 trawls (deep-water net ﬁshing,\\nwith a maximum depth of 2km), and the red points indicate those 2353\\ntrawls for which the Black Oreo was present, one of over a hundred species\\nregularly recorded. The catch size in kg for each species was recorded for\\neach trawl. Along with the species catch, a number of environmental mea-\\nsurements are available for each trawl. These include the average depth of\\nthe trawl ( AvgDepth ), and the temperature and salinity of the water. Since\\nthe latter two are strongly correlated with depth, Leathwick et al. (2006)\\nderived instead TempResid andSalResid , the residuals obtained when these\\ntwo measures are adjusted for depth (via separate non-parametric regres-\\nsions).SSTGrad is a measure of the gradient of the sea surface temperature,\\nandChlais a broad indicator of ecosytem productivity via satellite-image\\nmeasurements. SusPartMatter provides a measure of suspended particulate\\nmatter, particularly in coastal waters, and is also satellite derived.\\nThe goal of this analysis is to estimate the probability of ﬁnding Black\\nOreo in a trawl, as well as the expected catch size, standardized to take\\ninto account the eﬀects of variation in trawl speed and distance, as well\\nas the mesh size of the trawl net. The authors used logistic regression\\nfor estimating the probability. For the catch size, it might seem natural\\nto assume a Poisson distribution and model the log of the mean count,\\nbut this is often not appropriate because of the excessive number of zeros.\\nAlthough specialized approaches have been developed, such as the zero-\\ninﬂated Poisson (Lambert, 1992), they chose a simpler approach. If Yis\\nthe (non-negative) catch size,\\nE(Y|X) = E( Y|Y >0,X)≤Pr(Y >0|X). (10.54)\\nThe second term is estimated by the logistic regression, and the ﬁrst term\\ncan be estimated using only the 2353 trawls with a positive catch.\\nFor the logistic regression the authors used a gradient boosted model\\n(GBM)4with binomial deviance loss function, depth-10 trees, and a shrink-\\nage factor ν= 0.025. For the positive-catch regression, they modeled\\nlog(Y) using a GBM with squared-error loss (also depth-10 trees, but\\nν= 0.01), and un-logged the predictions. In both cases they used 10-fold\\ncross-validation for selecting the number of terms, as well as the shrinkage\\nfactor.\\n3The models, data, and maps shown here were kindly provided by Dr John Leathwick\\nof the National Institute of Water and Atmospheric Research in New Zealand, and Dr\\nJane Elith, School of Botany, University of Melbourne. The co llection of the research\\ntrawl data took place from 1979–2005, and was funded by the Ne w Zealand Ministry of\\nFisheries.\\n4Version 1.5-7 of package gbmin R, ver. 2.2.0.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c74bb57-002d-4996-8750-0cf030a99a6e', embedding=None, metadata={'page_label': '396', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.14 Illustrations 377\\nFIGURE 10.18. Map of New Zealand and its surrounding exclusive economic\\nzone, showing the locations of 17,000 trawls (small blue dots) t aken between 1979\\nand 2005. The red points indicate trawls for which the species Black Oreo Dory\\nwere present.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3df38e35-fc68-47df-8578-9a2a7bb8b55c', embedding=None, metadata={'page_label': '397', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='378 10. Boosting and Additive Trees\\n0 500 1000 15000.24 0.26 0.28 0.30 0.32 0.34\\nNumber of TreesMean DevianceGBM Test\\nGBM CV\\nGAM Test\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\nSpecificitySensitivity\\nAUC\\nGAM 0.97\\nGBM 0.98\\nFIGURE 10.19. The left panel shows the mean deviance as a function of the\\nnumber of trees for the GBM logistic regression model ﬁt to the p resence/absence\\ndata. Shown are 10-fold cross-validation on the training data ( and1×s.e. bars),\\nand test deviance on the test data. Also shown for comparison is the test deviance\\nusing a GAM model with 8d ffor each term. The right panel shows ROC curves\\non the test data for the chosen GBM model (vertical line in left plot) and the\\nGAM model.\\nFigure 10.19 (left panel) shows the mean binomial deviance for the se-\\nquence of GBM models, both for 10-fold CV and test data. There is a mod-\\nest improvement over the performance of a GAM model, ﬁt using smoothing\\nsplines with 8 degrees-of-freedom (df) per term. The right panel shows the\\nROC curves (see Section 9.2.5) for both models, which measures predictive\\nperformance. From this point of view, the performance looks very simi-\\nlar, with GBM perhaps having a slight edge as summarized by the AUC\\n(area under the curve). At the point of equal sensitivity/speciﬁcity, GBM\\nachieves 91%, and GAM 90%.\\nFigure 10.20 summarizes the contributions of the variables in the logistic\\nGBM ﬁt. We see that there is a well-deﬁned depth range over which Black\\nOreo are caught, with much more frequent capture in colder waters. We do\\nnot give details of the quantitative catch model; the important variabl es\\nwere much the same.\\nAll the predictors used in these models are available on a ﬁne geographi-\\ncal grid; in fact they were derived from environmental atlases, satellite im -\\nages and the like—see Leathwick et al. (2006) for details. This also means\\nthat predictions can be made on this grid, and imported into GIS mapping\\nsystems. Figure 10.21 shows prediction maps for both presence and catch\\nsize, with both standardized to a common set of trawl conditions; since the\\npredictors vary in a continuous fashion with geographical location, so do\\nthe predictions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e3020755-38bf-48d5-b5f9-b645a1a4bab0', embedding=None, metadata={'page_label': '398', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.14 Illustrations 379\\nOrbVelSpeedDistanceDisOrgMatterCodendSizePentadeTidalCurrSlopeChlaCase2SSTGradSalResidSusPartMatterAvgDepthTempResid\\nRelative influence0 10 25 −4 0 2 4 6−7 −5 −3 −1\\nTempResidf(TempResid)\\n0 500 1000 2000−6 −4 −2\\nAvgDepthf(AvgDepth)\\n0 5 10 15−7 −5 −3\\nSusPartMatterf(SusPartMatter)\\n−0.8 −0.4 0.0 0.4−7 −5 −3 −1\\nSalResidf(SalResid)\\n0.00 0.05 0.10 0.15−7 −5 −3 −1\\nSSTGradf(SSTGrad)\\nFIGURE 10.20. The top-left panel shows the relative inﬂuence computed from\\nthe GBM logistic regression model. The remaining panels show th e partial de-\\npendence plots for the leading ﬁve variables, all plotted on the s ame scale for\\ncomparison.\\nBecause of their ability to model interactions and automatically select\\nvariables, as well as robustness to outliers and missing data, GBM models\\nare rapidly gaining popularity in this data-rich and enthusiastic community .\\n10.14.3 Demographics Data\\nIn this section we illustrate gradient boosting on a multiclass classiﬁca -\\ntion problem, using MART. The data come from 9243 questionnaires ﬁlled\\nout by shopping mall customers in the San Francisco Bay Area (Impact\\nResources, Inc., Columbus, OH). Among the questions are 14 concerning\\ndemographics. For this illustration the goal is to predict occupation us-\\ning the other 13 variables as predictors, and hence identify demographic\\nvariables that discriminate between diﬀerent occupational categories. We\\nrandomly divided the data into a training set (80%) and test set (20%),\\nand used J= 6 node trees with a learning rate ν= 0.1.\\nFigure 10.22 shows the K= 9 occupation class values along with their\\ncorresponding error rates. The overall error rate is 42.5%, which can be\\ncompared to the null rate of 69% obtained by predicting the most numerous', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='114f9184-2b43-4c1b-8fbc-3751dfe2dfd5', embedding=None, metadata={'page_label': '399', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='380 10. Boosting and Additive Trees\\nFIGURE 10.21. Geological prediction maps of the presence probability (lef t\\nmap) and catch size (right map) obtained from the gradient boost ed models.\\nclassProf/Man (Professional/Managerial). The four best predicted classes\\nare seen to be Retired ,Student ,Prof/Man , andHomemaker .\\nFigure 10.23 shows the relative predictor variable importances as aver-\\naged over all classes (10.46). Figure 10.24 displays the individual relati ve\\nimportance distributions (10.45) for each of the four best predicted classes.\\nOne sees that the most relevant predictors are generally diﬀerent for each\\nrespective class. An exception is agewhich is among the three most relevant\\nfor predicting Retired ,Student , andProf/Man .\\nFigure 10.25 shows the partial dependence of the log-odds (10.52) on age\\nfor these three classes. The abscissa values are ordered codes for respective\\nequally spaced age intervals. One sees that after accounting for the contri-\\nbutions of the other variables, the odds of being retired are higher for older\\npeople, whereas the opposite is the case for being a student. The odds of\\nbeing professional/managerial are highest for middle-aged people. These\\nresults are of course not surprising. They illustrate that inspecting partial\\ndependences separately for each class can lead to sensible results.\\nBibliographic Notes\\nSchapire (1990) developed the ﬁrst simple boosting procedure in the PAC\\nlearning framework (Valiant, 1984; Kearns and Vazirani, 1994). Schapire', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2cc9f8bc-2e34-47ea-ab42-d21b2191025a', embedding=None, metadata={'page_label': '400', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.14 Illustrations 381\\nSalesUnemployedMilitaryClericalLaborHomemakerProf/ManRetiredStudent\\n0.0 0.2 0.4 0.6 0.8 1.0\\nError RateOverall Error Rate = 0.425\\nFIGURE 10.22. Error rate for each occupation in the demographics data.\\nageincomeeduhsld-statmar-dlincsexethnicmar-stattyp-homelangnum-hsldchildrenyrs-BA\\n0 20 40 60 80 100\\nRelative Importance\\nFIGURE 10.23. Relative importance of the predictors as averaged over all\\nclasses for the demographics data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='262943d4-39e3-475c-8b19-b8c2b0eb212d', embedding=None, metadata={'page_label': '401', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='382 10. Boosting and Additive Trees\\nagemar-dlincsexethnicincomehsld-statmar-statlangtyp-homechildrenedunum-hsldyrs-BA\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Retired\\nhsld-statageincomemar-stateduethnicnum-hsldtyp-homesexmar-dlinclangyrs-BAchildren\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Student\\neduincomeagemar-dlincethnichsld-stattyp-homesexnum-hsldlangmar-statyrs-BAchildren\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Prof/Man\\nsexmar-dlincchildrenethnicnum-hsldedumar-statlangtyp-homeincomeagehsld-statyrs-BA\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Homemaker\\nFIGURE 10.24. Predictor variable importances separately for each of the fo ur\\nclasses with lowest error rate for the demographics data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9415fc2-2fae-4ce8-88c0-3c0133174cae', embedding=None, metadata={'page_label': '402', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.14 Illustrations 383\\nagePartial Dependence\\n1 2 3 4 5 6 70 1 2 3 4Retired\\nagePartial Dependence\\n1 2 3 4 5 6 7-2 -1 0 1 2Student\\nagePartial Dependence\\n1 2 3 4 5 6 7-2 -1 0 1 2Prof/Man\\nFIGURE 10.25. Partial dependence of the odds of three diﬀerent occupations\\non age, for the demographics data.\\nshowed that a weak learner could always improve its performance by train-\\ning two additional classiﬁers on ﬁltered versions of the input data stream.\\nA weak learner is an algorithm for producing a two-class classiﬁer with\\nperformance guaranteed (with high probability) to be signiﬁcantly better\\nthan a coin-ﬂip. After learning an initial classiﬁer G1on the ﬁrst Ntraining\\npoints,\\n•G2is learned on a new sample of Npoints, half of which are misclas-\\nsiﬁed by G1;\\n•G3is learned on Npoints for which G1andG2disagree;\\n•the boosted classiﬁer is GB=majority vote (G1,G2,G3).\\nSchapire’s “Strength of Weak Learnability” theorem proves that GBhas\\nimproved performance over G1.\\nFreund (1995) proposed a “boost by majority” variation which combined\\nmany weak learners simultaneously and improved the performance of the\\nsimple boosting algorithm of Schapire. The theory supporting both of these', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f78ca51d-a132-41d0-bb84-18814d664da3', embedding=None, metadata={'page_label': '403', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='384 10. Boosting and Additive Trees\\nalgorithms requires the weak learner to produce a classiﬁer with a ﬁxed\\nerror rate. This led to the more adaptive and realistic AdaBoost (Freund\\nand Schapire, 1996a) and its oﬀspring, where this assumption was dropped.\\nFreund and Schapire (1996a) and Schapire and Singer (1999) provide\\nsome theory to support their algorithms, in the form of upper bounds on\\ngeneralization error. This theory has evolved in the computational learning\\ncommunity, initially based on the concepts of PAC learning. Other theo-\\nries attempting to explain boosting come from game theory (Freund and\\nSchapire, 1996b; Breiman, 1999; Breiman, 1998), and VC theory (Schapire\\net al., 1998). The bounds and the theory associated with the AdaBoost\\nalgorithms are interesting, but tend to be too loose to be of practical im-\\nportance. In practice, boosting achieves results far more impressive than\\nthe bounds would imply. Schapire (2002) and Meir and R¨ atsch (2003) give\\nuseful overviews more recent than the ﬁrst edition of this book.\\nFriedman et al. (2000) and Friedman (2001) form the basis for our expo-\\nsition in this chapter. Friedman et al. (2000) analyze AdaBoost statist ically,\\nderive the exponential criterion, and show that it estimates the log-odds\\nof the class probability. They propose additive tree models, the right-sized\\ntrees and ANOVA representation of Section 10.11, and the multiclass logit\\nformulation. Friedman (2001) developed gradient boosting and shrinkage\\nfor classiﬁcation and regression, while Friedman (1999) explored stochast ic\\nvariants of boosting. Mason et al. (2000) also embraced a gradient appro ach\\nto boosting. As the published discussions of Friedman et al. (2000) shows,\\nthere is some controversy about how and why boosting works.\\nSince the publication of the ﬁrst edition of this book, these debates have\\ncontinued, and spread into the statistical community with a series of papers\\non consistency of boosting (Jiang, 2004; Lugosi and Vayatis, 2004; Zhang\\nand Yu, 2005; Bartlett and Traskin, 2007). Mease and Wyner (2008),\\nthrough a series of simulation examples, challenge some of our interpre-\\ntations of boosting; our response (Friedman et al., 2008a) puts most of\\nthese objections to rest. A recent survey by B¨ uhlmann and Hothorn (2007)\\nsupports our approach to boosting.\\nExercises\\nEx. 10.1 Derive expression (10.12) for the update parameter in AdaBoost.\\nEx. 10.2 Prove result (10.16), that is, the minimizer of the population\\nversion of the AdaBoost criterion, is one-half of the log odds.\\nEx. 10.3 Show that the marginal average (10.47) recovers additive and\\nmultiplicative functions (10.50) and (10.51), while the conditional expec-\\ntation (10.49) does not.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='14eeab2f-8909-49e2-8ee5-451a38fcd3c2', embedding=None, metadata={'page_label': '404', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 385\\nEx. 10.4\\n(a) Write a program implementing AdaBoost with trees.\\n(b) Redo the computations for the example of Figure 10.2. Plot the train-\\ning error as well as test error, and discuss its behavior.\\n(c) Investigate the number of iterations needed to make the test error\\nﬁnally start to rise.\\n(d) Change the setup of this example as follows: deﬁne two classes, with\\nthe features in Class 1 being X1,X2,... ,X 10, standard indepen-\\ndent Gaussian variates. In Class 2, the features X1,X2,... ,X 10are\\nalso standard independent Gaussian, but conditioned on the event∑\\njX2\\nj>12. Now the classes have signiﬁcant overlap in feature space.\\nRepeat the AdaBoost experiments as in Figure 10.2 and discuss the\\nresults.\\nEx. 10.5 Multiclass exponential loss (Zhu et al., 2005). For a K-class clas-\\nsiﬁcation problem, consider the coding Y= (Y1,... ,Y K)Twith\\nYk={1, ifG=Gk\\n−1\\nK−1,otherwise .(10.55)\\nLetf= (f1,... ,f K)Twith∑K\\nk=1fk= 0, and deﬁne\\nL(Y,f) = exp(\\n−1\\nKYTf)\\n. (10.56)\\n(a) Using Lagrange multipliers, derive the population minimizer f∗of\\nE(Y,f), subject to the zero-sum constraint, and relate these to the\\nclass probabilities.\\n(b) Show that a multiclass boosting using this loss function leads to a\\nreweighting algorithm similar to Adaboost, as in Section 10.4.\\nEx. 10.6 McNemar test (Agresti, 1996). We report the test error rates on\\nthe spam data to be 5.5% for a generalized additive model (GAM), and\\n4.5% for gradient boosting (GBM), with a test sample of size 1536.\\n(a) Show that the standard error of these estimates is about 0.6%.\\nSince the same test data are used for both methods, the error rates are\\ncorrelated, and we cannot perform a two-sample t-test. We can compare\\nthe methods directly on each test observation, leading to the summary\\nGBM\\nGAM Correct Error\\nCorrect 1434 18\\nError 33 51', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4f8c4b88-807e-4c41-b403-d5354d4791df', embedding=None, metadata={'page_label': '405', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='386 10. Boosting and Additive Trees\\nThe McNemar test focuses on the discordant errors, 33 vs. 18.\\n(b) Conduct a test to show that GAM makes signiﬁcantly more errors\\nthan gradient boosting, with a two-sided p-value of 0 .036.\\nEx. 10.7 Derive expression (10.32).\\nEx. 10.8 Consider a K-class problem where the targets yikare coded as\\n1 if observation iis in class kand zero otherwise. Suppose we have a\\ncurrent model fk(x), k= 1,... ,K , with∑K\\nk=1fk(x) = 0 (see (10.21) in\\nSection 10.6). We wish to update the model for observations in a region R\\nin predictor space, by adding constants fk(x) +γk, with γK= 0.\\n(a) Write down the multinomial log-likelihood for this problem, and its\\nﬁrst and second derivatives.\\n(b) Using only the diagonal of the Hessian matrix in (1), and starting\\nfromγk= 0∀k, show that a one-step approximate Newton update\\nforγkis\\nγ1\\nk=∑\\nxi∈R(yik−pik)∑\\nxi∈Rpik(1−pik), k= 1,... ,K −1, (10.57)\\nwhere pik= exp( fk(xi))/(∑K\\nℓ=1fℓ(xi)).\\n(c) We prefer our update to sum to zero, as the current model does. Using\\nsymmetry arguments, show that\\nˆγk=K−1\\nK(γ1\\nk−1\\nKK∑\\nℓ=1γ1\\nℓ), k= 1,... ,K (10.58)\\nis an appropriate update, where γ1\\nkis deﬁned as in (10.57) for all\\nk= 1,... ,K .\\nEx. 10.9 Consider a K-class problem where the targets yikare coded as\\n1 if observation iis in class kand zero otherwise. Using the multinomial\\ndeviance loss function (10.22) and the symmetric logistic transform, use\\nthe arguments leading to the gradient boosting Algorithm 10.3 to derive\\nAlgorithm 10.4. Hint: See exercise 10.8 for step 2(b)iii.\\nEx. 10.10 Show that for K= 2 class classiﬁcation, only one tree needs to\\nbe grown at each gradient-boosting iteration.\\nEx. 10.11 Show how to compute the partial dependence function fS(XS)\\nin (10.47) eﬃciently.\\nEx. 10.12 Referring to (10.49), let S={1}andC={2}, with f(X1,X2) =\\nX1. Assume X1andX2are bivariate Gaussian, each with mean zero, vari-\\nance one, and E( X1,X2) =ρ. Show that E(f(X1,X2|X2) =ρX2, even\\nthough fis not a function of X2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c94a566f-ed94-4bbd-9ad2-dca6490b7da1', embedding=None, metadata={'page_label': '406', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 387\\nAlgorithm 10.4 Gradient Boosting for K-class Classiﬁcation.\\n1. Initialize fk0(x) = 0, k= 1,2,... ,K .\\n2. For m=1 to M:\\n(a) Set\\npk(x) =efk(x)\\n∑K\\nℓ=1efℓ(x), k= 1,2,... ,K.\\n(b) For k= 1 to K:\\ni. Compute rikm=yik−pk(xi), i= 1,2,... ,N .\\nii. Fit a regression tree to the targets rikm, i= 1,2,... ,N ,\\ngiving terminal regions Rjkm, j= 1,2,... ,J m.\\niii. Compute\\nγjkm=K−1\\nK∑\\nxi∈Rjkmrikm∑\\nxi∈Rjkm|rikm|(1− |rikm|), j= 1,2,... ,J m.\\niv. Update fkm(x) =fk,m−1(x) +∑Jm\\nj=1γjkmI(x∈Rjkm).\\n3. Output ˆfk(x) =fkM(x), k= 1,2,... ,K .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ecf7279-9139-4343-bf57-3e31622e8ac8', embedding=None, metadata={'page_label': '407', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='388 10. Boosting and Additive Trees', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a10fddde-d21a-4e2f-b871-8fd6c4deb48e', embedding=None, metadata={'page_label': '408', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 389\\nPrinter: Opaque this\\n11\\nNeural Networks\\n11.1 Introduction\\nIn this chapter we describe a class of learning methods that was developed\\nseparately in diﬀerent ﬁelds—statistics and artiﬁcial intelligence—based\\non essentially identical models. The central idea is to extract linear com-\\nbinations of the inputs as derived features, and then model the target as\\na nonlinear function of these features. The result is a powerful learning\\nmethod, with widespread applications in many ﬁelds. We ﬁrst discuss the\\nprojection pursuit model, which evolved in the domain of semiparamet-\\nric statistics and smoothing. The rest of the chapter is devoted to neural\\nnetwork models.\\n11.2 Projection Pursuit Regression\\nAs in our generic supervised learning problem, assume we have an input\\nvector Xwithpcomponents, and a target Y. Letωm, m= 1,2,... ,M, be\\nunitp-vectors of unknown parameters. The projection pursuit regression\\n(PPR) model has the form\\nf(X) =M∑\\nm=1gm(ωT\\nmX). (11.1)\\nThis is an additive model, but in the derived features Vm=ωT\\nmXrather\\nthan the inputs themselves. The functions gmare unspeciﬁed and are esti-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60bf09dd-30d4-4a5d-a3bf-7d88679bb62a', embedding=None, metadata={'page_label': '409', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='390 Neural Networks\\ng(V)\\nX1X2g(V)\\nX1X2\\nFIGURE 11.1. Perspective plots of two ridge functions.\\n(Left:) g(V) = 1/[1 + exp( −5(V−0.5))], where V= (X1+X2)/√\\n2.\\n(Right:) g(V) = (V+ 0.1) sin(1 /(V/3 + 0.1)), where V=X1.\\nmated along with the directions ωmusing some ﬂexible smoothing method\\n(see below).\\nThe function gm(ωT\\nmX) is called a ridge function in IRp. It varies only\\nin the direction deﬁned by the vector ωm. The scalar variable Vm=ωT\\nmX\\nis the projection of Xonto the unit vector ωm, and we seek ωmso that\\nthe model ﬁts well, hence the name “projection pursuit.” Figure 11.1 shows\\nsome examples of ridge functions. In the example on the left ω= (1/√\\n2)(1,1)T,\\nso that the function only varies in the direction X1+X2. In the example\\non the right, ω= (1,0).\\nThe PPR model (11.1) is very general, since the operation of forming\\nnonlinear functions of linear combinations generates a surprisingly large\\nclass of models. For example, the product X1≤X2can be written as [( X1+\\nX2)2−(X1−X2)2]/4, and higher-order products can be represented simi-\\nlarly.\\nIn fact, if Mis taken arbitrarily large, for appropriate choice of gmthe\\nPPR model can approximate any continuous function in IRparbitrarily\\nwell. Such a class of models is called a universal approximator . However\\nthis generality comes at a price. Interpretation of the ﬁtted model is usually\\ndiﬃcult, because each input enters into the model in a complex and multi-\\nfaceted way. As a result, the PPR model is most useful for prediction, and\\nnot very useful for producing an understandable model for the data. The\\nM= 1 model, known as the single index model in econometrics, is an\\nexception. It is slightly more general than the linear regression model, and\\noﬀers a similar interpretation.\\nHow do we ﬁt a PPR model, given training data ( xi,yi),i= 1,2,... ,N ?\\nWe seek the approximate minimizers of the error function\\nN∑\\ni=1[\\nyi−M∑\\nm=1gm(ωT\\nmxi)]2\\n(11.2)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a7735f1-ae93-4dd1-a6ac-bbaa2dc47d64', embedding=None, metadata={'page_label': '410', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.2 Projection Pursuit Regression 391\\nover functions gmand direction vectors ωm,m= 1,2,... ,M . As in other\\nsmoothing problems, we need either explicitly or implicitly to impose com-\\nplexity constraints on the gm, to avoid overﬁt solutions.\\nConsider just one term ( M= 1, and drop the subscript). Given the\\ndirection vector ω, we form the derived variables vi=ωTxi. Then we have\\na one-dimensional smoothing problem, and we can apply any scatterplot\\nsmoother, such as a smoothing spline, to obtain an estimate of g.\\nOn the other hand, given g, we want to minimize (11.2) over ω. A Gauss–\\nNewton search is convenient for this task. This is a quasi-Newton method,\\nin which the part of the Hessian involving the second derivative of gis\\ndiscarded. It can be simply derived as follows. Let ωoldbe the current\\nestimate for ω. We write\\ng(ωTxi)≈g(ωT\\noldxi) +g′(ωT\\noldxi)(ω−ωold)Txi (11.3)\\nto give\\nN∑\\ni=1[\\nyi−g(ωTxi)]2≈N∑\\ni=1g′(ωT\\noldxi)2[(\\nωT\\noldxi+yi−g(ωT\\noldxi)\\ng′(ωT\\noldxi))\\n−ωTxi]2\\n.\\n(11.4)\\nTo minimize the right-hand side, we carry out a least squares regression\\nwith target ωT\\noldxi+(yi−g(ωT\\noldxi))/g′(ωT\\noldxi) on the input xi, with weights\\ng′(ωT\\noldxi)2and no intercept (bias) term. This produces the updated coef-\\nﬁcient vector ωnew.\\nThese two steps, estimation of gandω, are iterated until convergence.\\nWith more than one term in the PPR model, the model is built in a forward\\nstage-wise manner, adding a pair ( ωm,gm) at each stage.\\nThere are a number of implementation details.\\n•Although any smoothing method can in principle be used, it is conve-\\nnient if the method provides derivatives. Local regression and smooth-\\ning splines are convenient.\\n•After each step the gm’s from previous steps can be readjusted using\\nthe backﬁtting procedure described in Chapter 9. While this may\\nlead ultimately to fewer terms, it is not clear whether it improves\\nprediction performance.\\n•Usually the ωmare not readjusted (partly to avoid excessive compu-\\ntation), although in principle they could be as well.\\n•The number of terms Mis usually estimated as part of the forward\\nstage-wise strategy. The model building stops when the next term\\ndoes not appreciably improve the ﬁt of the model. Cross-validation\\ncan also be used to determine M.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0ab12f2-5470-4204-86f7-55fd6cd14058', embedding=None, metadata={'page_label': '411', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='392 Neural Networks\\nThere are many other applications, such as density estimation (Friedman\\net al., 1984; Friedman, 1987), where the projection pursuit idea can be used.\\nIn particular, see the discussion of ICA in Section 14.7 and its relationship\\nwith exploratory projection pursuit. However the projection pursuit re-\\ngression model has not been widely used in the ﬁeld of statistics, perhaps\\nbecause at the time of its introduction (1981), its computational demands\\nexceeded the capabilities of most readily available computers. But it does\\nrepresent an important intellectual advance, one that has blossomed in its\\nreincarnation in the ﬁeld of neural networks, the topic of the rest of this\\nchapter.\\n11.3 Neural Networks\\nThe term neural network has evolved to encompass a large class of models\\nand learning methods. Here we describe the most widely used “vanilla” neu-\\nral net, sometimes called the single hidden layer back-propagation network,\\nor single layer perceptron. There has been a great deal of hypesurrounding\\nneural networks, making them seem magical and mysterious. As we make\\nclear in this section, they are just nonlinear statistical models, much like\\nthe projection pursuit regression model discussed above.\\nA neural network is a two-stage regression or classiﬁcation model, typ-\\nically represented by a network diagram as in Figure 11.2. This network\\napplies both to regression or classiﬁcation. For regression, typically K= 1\\nand there is only one output unit Y1at the top. However, these networks\\ncan handle multiple quantitative responses in a seamless fashion, so we will\\ndeal with the general case.\\nForK-class classiﬁcation, there are Kunits at the top, with the kth\\nunit modeling the probability of class k. There are Ktarget measurements\\nYk, k= 1,... ,K , each being coded as a 0 −1 variable for the kth class.\\nDerived features Zmare created from linear combinations of the inputs,\\nand then the target Ykis modeled as a function of linear combinations of\\ntheZm,\\nZm=σ(α0m+αT\\nmX), m= 1,... ,M,\\nTk=β0k+βT\\nkZ, k= 1,... ,K,\\nfk(X) =gk(T), k= 1,... ,K,(11.5)\\nwhere Z= (Z1,Z2,... ,Z M), and T= (T1,T2,... ,T K).\\nThe activation function σ(v) is usually chosen to be the sigmoid σ(v) =\\n1/(1 +e−v); see Figure 11.3 for a plot of 1 /(1 +e−v). Sometimes Gaussian\\nradial basis functions (Chapter 6) are used for the σ(v), producing what is\\nknown as a radial basis function network .\\nNeural network diagrams like Figure 11.2 are sometimes drawn with an\\nadditional biasunit feeding into every unit in the hidden and output layers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bc69c637-54a8-4579-a6b7-3070e8912a88', embedding=None, metadata={'page_label': '412', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.3 Neural Networks 393\\n Y  Y Y 2 1 K\\n Z  Z  Z1 Z2 3 m\\n X  X Z  Z1 Z2 3\\n1  Xp  X p-1  X2  X3M\\n X p-1 3 X 2 X 1p Z Y  Y Y\\n XK 1 2\\n                                                                                                                                                /0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\nFIGURE 11.2. Schematic of a single hidden layer, feed-forward neural network .\\nThinking of the constant “1” as an additional input feature, this bias unit\\ncaptures the intercepts α0mandβ0kin model (11.5).\\nThe output function gk(T) allows a ﬁnal transformation of the vector of\\noutputs T. For regression we typically choose the identity function gk(T) =\\nTk. Early work in K-class classiﬁcation also used the identity function, but\\nthis was later abandoned in favor of the softmax function\\ngk(T) =eTk\\n∑K\\nℓ=1eTℓ. (11.6)\\nThis is of course exactly the transformation used in the multilogit model\\n(Section 4.4), and produces positive estimates that sum to one. In Sec-\\ntion 4.2 we discuss other problems with linear activation functions, in par-\\nticular potentially severe masking eﬀects.\\nThe units in the middle of the network, computing the derived features\\nZm, are called hidden units because the values Zmare not directly ob-\\nserved. In general there can be more than one hidden layer, as illustrated\\nin the example at the end of this chapter. We can think of the Zmas a\\nbasis expansion of the original inputs X; the neural network is then a stan-\\ndard linear model, or linear multilogit model, using these transformations\\nas inputs. There is, however, an important enhancement over the basis-\\nexpansion techniques discussed in Chapter 5; here the parameters of the\\nbasis functions are learned from the data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='646be183-5448-485e-aff4-1bc9398775e7', embedding=None, metadata={'page_label': '413', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='394 Neural Networks\\n-10 -5 0 5 100.0 0.5 1.01/(1 +e−v)\\nv\\nFIGURE 11.3. Plot of the sigmoid function σ(v) = 1/(1+exp( −v))(red curve),\\ncommonly used in the hidden layer of a neural network. Included ar eσ(sv)for\\ns=1\\n2(blue curve) and s= 10(purple curve). The scale parameter scontrols\\nthe activation rate, and we can see that large samounts to a hard activation at\\nv= 0. Note that σ(s(v−v0))shifts the activation threshold from 0tov0.\\nNotice that if σis the identity function, then the entire model collapses\\nto a linear model in the inputs. Hence a neural network can be thought of\\nas a nonlinear generalization of the linear model, both for regression and\\nclassiﬁcation. By introducing the nonlinear transformation σ, it greatly\\nenlarges the class of linear models. In Figure 11.3 we see that the rate of\\nactivation of the sigmoid depends on the norm of αm, and if ∥αm∥is very\\nsmall, the unit will indeed be operating in the linear part of its activation\\nfunction.\\nNotice also that the neural network model with one hidden layer has\\nexactly the same form as the projection pursuit model described above.\\nThe diﬀerence is that the PPR model uses nonparametric functions gm(v),\\nwhile the neural network uses a far simpler function based on σ(v), with\\nthree free parameters in its argument. In detail, viewing the neural network\\nmodel as a PPR model, we identify\\ngm(ωT\\nmX) = βmσ(α0m+αT\\nmX)\\n=βmσ(α0m+∥αm∥(ωT\\nmX)), (11.7)\\nwhere ωm=αm/∥αm∥is the mth unit-vector. Since σβ,α0,s(v) =βσ(α0+\\nsv) has lower complexity than a more general nonparametric g(v), it is not\\nsurprising that a neural network might use 20 or 100 such functions, while\\nthe PPR model typically uses fewer terms ( M= 5 or 10, for example).\\nFinally, we note that the name “neural networks” derives from the fact\\nthat they were ﬁrst developed as models for the human brain. Each unit\\nrepresents a neuron, and the connections (links in Figure 11.2) represent\\nsynapses. In early models, the neurons ﬁred when the total signal passed to\\nthat unit exceeded a certain threshold. In the model above, this corresponds', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f91be737-fb31-40d2-9d0e-a4645b78e7e0', embedding=None, metadata={'page_label': '414', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.4 Fitting Neural Networks 395\\nto use of a step function for σ(Z) and gm(T). Later the neural network was\\nrecognized as a useful tool for nonlinear statistical modeling, and for this\\npurpose the step function is not smooth enough for optimization. Hence the\\nstep function was replaced by a smoother threshold function, the sigmoid\\nin Figure 11.3.\\n11.4 Fitting Neural Networks\\nThe neural network model has unknown parameters, often called weights ,\\nand we seek values for them that make the model ﬁt the training data well.\\nWe denote the complete set of weights by θ, which consists of\\n{α0m,αm;m= 1,2,... ,M }M(p+ 1) weights ,\\n{β0k,βk;k= 1,2,... ,K }K(M+ 1) weights .(11.8)\\nFor regression, we use sum-of-squared errors as our measure of ﬁt (error\\nfunction)\\nR(θ) =K∑\\nk=1N∑\\ni=1(yik−fk(xi))2. (11.9)\\nFor classiﬁcation we use either squared error or cross-entropy (deviance):\\nR(θ) =−N∑\\ni=1K∑\\nk=1yiklogfk(xi), (11.10)\\nand the corresponding classiﬁer is G(x) = argmaxkfk(x). With the softmax\\nactivation function and the cross-entropy error function, the neural network\\nmodel is exactly a linear logistic regression model in the hidden units, and\\nall the parameters are estimated by maximum likelihood.\\nTypically we don’t want the global minimizer of R(θ), as this is likely\\nto be an overﬁt solution. Instead some regularization is needed: this is\\nachieved directly through a penalty term, or indirectly by early stopping.\\nDetails are given in the next section.\\nThe generic approach to minimizing R(θ) is by gradient descent, called\\nback-propagation in this setting. Because of the compositional form of the\\nmodel, the gradient can be easily derived using the chain rule for diﬀeren-\\ntiation. This can be computed by a forward and backward sweep over the\\nnetwork, keeping track only of quantities local to each unit.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='01288cdc-0e4d-431a-9413-ca49e05d9170', embedding=None, metadata={'page_label': '415', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='396 Neural Networks\\nHere is back-propagation in detail for squared error loss. Let zmi=\\nσ(α0m+αT\\nmxi), from (11.5) and let zi= (z1i,z2i,... ,z Mi). Then we have\\nR(θ)≡N∑\\ni=1Ri\\n=N∑\\ni=1K∑\\nk=1(yik−fk(xi))2, (11.11)\\nwith derivatives\\n∂Ri\\n∂βkm=−2(yik−fk(xi))g′\\nk(βT\\nkzi)zmi,\\n∂Ri\\n∂αmℓ=−K∑\\nk=12(yik−fk(xi))g′\\nk(βT\\nkzi)βkmσ′(αT\\nmxi)xiℓ.(11.12)\\nGiven these derivatives, a gradient descent update at the ( r+ 1)st iter-\\nation has the form\\nβ(r+1)\\nkm=β(r)\\nkm−γrN∑\\ni=1∂Ri\\n∂β(r)\\nkm,\\nα(r+1)\\nmℓ=α(r)\\nmℓ−γrN∑\\ni=1∂Ri\\n∂α(r)\\nmℓ,(11.13)\\nwhere γris the learning rate , discussed below.\\nNow write (11.12) as\\n∂Ri\\n∂βkm=δkizmi,\\n∂Ri\\n∂αmℓ=smixiℓ.(11.14)\\nThe quantities δkiandsmiare “errors” from the current model at the\\noutput and hidden layer units, respectively. From their deﬁnitions, these\\nerrors satisfy\\nsmi=σ′(αT\\nmxi)K∑\\nk=1βkmδki, (11.15)\\nknown as the back-propagation equations . Using this, the updates in (11.13)\\ncan be implemented with a two-pass algorithm. In the forward pass , the\\ncurrent weights are ﬁxed and the predicted values ˆfk(xi) are computed\\nfrom formula (11.5). In the backward pass , the errors δkiare computed,\\nand then back-propagated via (11.15) to give the errors smi. Both sets of\\nerrors are then used to compute the gradients for the updates in (11.13),\\nvia (11.14).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27a32989-2c14-4b43-9779-dfd2813d62e5', embedding=None, metadata={'page_label': '416', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.5 Some Issues in Training Neural Networks 397\\nThis two-pass procedure is what is known as back-propagation. It has\\nalso been called the delta rule (Widrow and Hoﬀ, 1960). The computational\\ncomponents for cross-entropy have the same form as those for the sum of\\nsquares error function, and are derived in Exercise 11.3.\\nThe advantages of back-propagation are its simple, local nature. In the\\nback propagation algorithm, each hidden unit passes and receives infor-\\nmation only to and from units that share a connection. Hence it can be\\nimplemented eﬃciently on a parallel architecture computer.\\nThe updates in (11.13) are a kind of batch learning , with the parame-\\nter updates being a sum over all of the training cases. Learning can also\\nbe carried out online—processing each observation one at a time, updat-\\ning the gradient after each training case, and cycling through the training\\ncases many times. In this case, the sums in equations (11.13) are replaced\\nby a single summand. A training epoch refers to one sweep through the\\nentire training set. Online training allows the network to handle very large\\ntraining sets, and also to update the weights as new observations come in.\\nThe learning rate γrfor batch learning is usually taken to be a con-\\nstant, and can also be optimized by a line search that minimizes the error\\nfunction at each update. With online learning γrshould decrease to zero\\nas the iteration r→ ∞. This learning is a form of stochastic approxima-\\ntion(Robbins and Munro, 1951); results in this ﬁeld ensure convergence if\\nγr→0,∑\\nrγr=∞, and∑\\nrγ2\\nr<∞(satisﬁed, for example, by γr= 1/r).\\nBack-propagation can be very slow, and for that reason is usually not\\nthe method of choice. Second-order techniques such as Newton’s method\\nare not attractive here, because the second derivative matrix of R(the\\nHessian) can be very large. Better approaches to ﬁtting include conjugate\\ngradients and variable metric methods. These avoid explicit computation\\nof the second derivative matrix while still providing faster convergence.\\n11.5 Some Issues in Training Neural Networks\\nThere is quite an art in training neural networks. The model is generally\\noverparametrized, and the optimization problem is nonconvex and unstable\\nunless certain guidelines are followed. In this section we summarize some\\nof the important issues.\\n11.5.1 Starting Values\\nNote that if the weights are near zero, then the operative part of the sigmoid\\n(Figure 11.3) is roughly linear, and hence the neural network collapses into\\nan approximately linear model (Exercise 11.2). Usually starting values fo r\\nweights are chosen to be random values near zero. Hence the model starts\\nout nearly linear, and becomes nonlinear as the weights increase. Individual', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d187501a-409b-45bb-a2f5-87c7381e4194', embedding=None, metadata={'page_label': '417', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='398 Neural Networks\\nunits localize to directions and introduce nonlinearities where needed. Use\\nof exact zero weights leads to zero derivatives and perfect symmetry, and\\nthe algorithm never moves. Starting instead with large weights often leads\\nto poor solutions.\\n11.5.2 Overﬁtting\\nOften neural networks have too many weights and will overﬁt the data at\\nthe global minimum of R. In early developments of neural networks, either\\nby design or by accident, an early stopping rule was used to avoid over-\\nﬁtting. Here we train the model only for a while, and stop well before we\\napproach the global minimum. Since the weights start at a highly regular-\\nized (linear) solution, this has the eﬀect of shrinking the ﬁnal model toward\\na linear model. A validation dataset is useful for determining when to stop,\\nsince we expect the validation error to start increasing.\\nA more explicit method for regularization is weight decay , which is anal-\\nogous to ridge regression used for linear models (Section 3.4.1). We add a\\npenalty to the error function R(θ) +λJ(θ), where\\nJ(θ) =∑\\nkmβ2\\nkm+∑\\nmℓα2\\nmℓ (11.16)\\nandλ≥0 is a tuning parameter. Larger values of λwill tend to shrink\\nthe weights toward zero: typically cross-validation is used to estimate λ.\\nThe eﬀect of the penalty is to simply add terms 2 βkmand 2 αmℓto the\\nrespective gradient expressions (11.13). Other forms for the penalty have\\nbeen proposed, for example,\\nJ(θ) =∑\\nkmβ2\\nkm\\n1 +β2\\nkm+∑\\nmℓα2\\nmℓ\\n1 +α2\\nmℓ, (11.17)\\nknown as the weight elimination penalty. This has the eﬀect of shrinking\\nsmaller weights more than (11.16) does.\\nFigure 11.4 shows the result of training a neural network with ten hidden\\nunits, without weight decay (upper panel) and with weight decay (lower\\npanel), to the mixture example of Chapter 2. Weight decay has clearly\\nimproved the prediction. Figure 11.5 shows heat maps of the estimated\\nweights from the training (grayscale versions of these are called Hinton\\ndiagrams. ) We see that weight decay has dampened the weights in both\\nlayers: the resulting weights are spread fairly evenly over the ten hidden\\nunits.\\n11.5.3 Scaling of the Inputs\\nSince the scaling of the inputs determines the eﬀective scaling of the weights\\nin the bottom layer, it can have a large eﬀect on the quality of the ﬁnal', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f9916bc2-5152-4e02-857b-a7767827efad', embedding=None, metadata={'page_label': '418', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.5 Some Issues in Training Neural Networks 399\\nNeural Network - 10 Units, No Weight Decay\\n. . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . .. . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.100\\nTest Error:       0.259\\nBayes Error:    0.210\\nNeural Network - 10 Units, Weight Decay=0.02 \\n. .. .. .. .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.160\\nTest Error:       0.223\\nBayes Error:    0.210\\nFIGURE 11.4. A neural network on the mixture example of Chapter 2. The\\nupper panel uses no weight decay, and overﬁts the training data. The lower panel\\nuses weight decay, and achieves close to the Bayes error rate ( broken purple\\nboundary). Both use the softmax activation function and cross- entropy error.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06232618-f36a-40ee-8c3a-d52156ff7d1a', embedding=None, metadata={'page_label': '419', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='400 Neural Networks\\n1 1\\n11\\nx1 x1x2 x2y1 y1y2 y2\\nz1z1\\nz1z1\\nz2z2\\nz2z2\\nz3z3\\nz3z3\\nz1z1\\nz1z1\\nz5z5\\nz5z5\\nz6z6\\nz6z6\\nz7z7\\nz7z7\\nz8z8\\nz8z8\\nz9z9\\nz9z9\\nz10z10\\nz10z10No weight decay Weight decay\\nFIGURE 11.5. Heat maps of the estimated weights from the training of neural\\nnetworks from Figure 11.4. The display ranges from bright gree n (negative) to\\nbright red (positive).\\nsolution. At the outset it is best to standardize all inputs to have mean zero\\nand standard deviation one. This ensures all inputs are treated equally in\\nthe regularization process, and allows one to choose a meaningful range for\\nthe random starting weights. With standardized inputs, it is typical to take\\nrandom uniform weights over the range [ −0.7,+0.7].\\n11.5.4 Number of Hidden Units and Layers\\nGenerally speaking it is better to have too many hidden units than too few.\\nWith too few hidden units, the model might not have enough ﬂexibility to\\ncapture the nonlinearities in the data; with too many hidden units, the\\nextra weights can be shrunk toward zero if appropriate regularization is\\nused. Typically the number of hidden units is somewhere in the range of\\n5 to 100, with the number increasing with the number of inputs and num-\\nber of training cases. It is most common to put down a reasonably large\\nnumber of units and train them with regularization. Some researchers use\\ncross-validation to estimate the optimal number, but this seems unneces-\\nsary if cross-validation is used to estimate the regularization parameter .\\nChoice of the number of hidden layers is guided by background knowledge\\nand experimentation. Each layer extracts features of the input for regres-\\nsion or classiﬁcation. Use of multiple hidden layers allows construction of\\nhierarchical features at diﬀerent levels of resolution. An example of the\\neﬀective use of multiple layers is given in Section 11.6.\\n11.5.5 Multiple Minima\\nThe error function R(θ) is nonconvex, possessing many local minima. As a\\nresult, the ﬁnal solution obtained is quite dependent on the choice of start-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e0779df6-f938-4495-adc6-2e882655647c', embedding=None, metadata={'page_label': '420', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.6 Example: Simulated Data 401\\ning weights. One must at least try a number of random starting conﬁgura-\\ntions, and choose the solution giving lowest (penalized) error. Probably a\\nbetter approach is to use the average predictions over the collection of net-\\nworks as the ﬁnal prediction (Ripley, 1996). This is preferable to averaging\\nthe weights, since the nonlinearity of the model implies that this averaged\\nsolution could be quite poor. Another approach is via bagging , which aver-\\nages the predictions of networks training from randomly perturbed versions\\nof the training data. This is described in Section 8.7.\\n11.6 Example: Simulated Data\\nWe generated data from two additive error models Y=f(X) +ε:\\nSum of sigmoids: Y=σ(aT\\n1X) +σ(aT\\n2X) +ε1;\\nRadial: Y=10∏\\nm=1φ(Xm) +ε2.\\nHereXT= (X1,X2,... ,X p), each Xjbeing a standard Gaussian variate,\\nwithp= 2 in the ﬁrst model, and p= 10 in the second.\\nFor the sigmoid model, a1= (3,3), a2= (3,−3); for the radial model,\\nφ(t) = (1 /2π)1/2exp(−t2/2). Both ε1andε2are Gaussian errors, with\\nvariance chosen so that the signal-to-noise ratio\\nVar(E( Y|X))\\nVar(Y−E(Y|X))=Var(f(X))\\nVar(ε)(11.18)\\nis 4 in both models. We took a training sample of size 100 and a test sample\\nof size 10 ,000. We ﬁt neural networks with weight decay and various num-\\nbers of hidden units, and recorded the average test error E Test(Y−ˆf(X))2\\nfor each of 10 random starting weights. Only one training set was gen-\\nerated, but the results are typical for an “average” training set. The test\\nerrors are shown in Figure 11.6. Note that the zero hidden unit model refers\\nto linear least squares regression. The neural network is perfectly suited to\\nthe sum of sigmoids model, and the two-unit model does perform the best,\\nachieving an error close to the Bayes rate. (Recall that the Bayes rate for\\nregression with squared error is the error variance; in the ﬁgures, we report\\ntest error relative to the Bayes error). Notice, however, that with more hid-\\nden units, overﬁtting quickly creeps in, and with some starting weights the\\nmodel does worse than the linear model (zero hidden unit) model. Even\\nwith two hidden units, two of the ten starting weight conﬁgurations pro-\\nduced results no better than the linear model, conﬁrming the importance\\nof multiple starting values.\\nA radial function is in a sense the most diﬃcult for the neural net, as it is\\nspherically symmetric and with no preferred directions. We see in the right', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='99164909-8c9b-4f1c-88a3-eef157a907d4', embedding=None, metadata={'page_label': '421', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='402 Neural Networks\\n1.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorSum of Sigmoids\\n0 5 10 15 20 25 30\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorRadial\\nFIGURE 11.6. Boxplots of test error, for simulated data example, relative t o\\nthe Bayes error (broken horizontal line). True function is a sum of two sigmoids\\non the left, and a radial function is on the right. The test error is displayed for\\n10diﬀerent starting weights, for a single hidden layer neural netwo rk with the\\nnumber of units as indicated.\\npanel of Figure 11.6 that it does poorly in this case, with the test error\\nstaying well above the Bayes error (note the diﬀerent vertical scale from\\nthe left panel). In fact, since a constant ﬁt (such as the sample average)\\nachieves a relative error of 5 (when the SNR is 4), we see that the neural\\nnetworks perform increasingly worse than the mean.\\nIn this example we used a ﬁxed weight decay parameter of 0 .0005, rep-\\nresenting a mild amount of regularization. The results in the left panel of\\nFigure 11.6 suggest that more regularization is needed with greater num-\\nbers of hidden units.\\nIn Figure 11.7 we repeated the experiment for the sum of sigmoids model,\\nwith no weight decay in the left panel, and stronger weight decay ( λ= 0.1)\\nin the right panel. With no weight decay, overﬁtting becomes even more\\nsevere for larger numbers of hidden units. The weight decay value λ= 0.1\\nproduces good results for all numbers of hidden units, and there does not\\nappear to be overﬁtting as the number of units increase. Finally, Figure 11.8\\nshows the test error for a ten hidden unit network, varying the weight decay\\nparameter over a wide range. The value 0 .1 is approximately optimal.\\nIn summary, there are two free parameters to select: the weight decay λ\\nand number of hidden units M. As a learning strategy, one could ﬁx either\\nparameter at the value corresponding to the least constrained model, to\\nensure that the model is rich enough, and use cross-validation to choose\\nthe other parameter. Here the least constrained values are zero weight decay\\nand ten hidden units. Comparing the left panel of Figure 11.7 to Figure\\n11.8, we see that the test error is less sensitive to the value of the weight', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='83859d98-847a-4ae1-976b-2f477e7d45cf', embedding=None, metadata={'page_label': '422', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.6 Example: Simulated Data 4031.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorNo Weight Decay\\n1.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorWeight Decay=0.1\\nFIGURE 11.7. Boxplots of test error, for simulated data example, relative t o the\\nBayes error. True function is a sum of two sigmoids. The test er ror is displayed\\nfor ten diﬀerent starting weights, for a single hidden layer neur al network with\\nthe number units as indicated. The two panels represent no weight de cay (left)\\nand strong weight decay λ= 0.1(right).1.0 1.2 1.4 1.6 1.8 2.0 2.2\\n0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14\\nWeight Decay ParameterTest ErrorSum of Sigmoids, 10 Hidden Unit Model\\nFIGURE 11.8. Boxplots of test error, for simulated data example. True functi on\\nis a sum of two sigmoids. The test error is displayed for ten di ﬀerent starting\\nweights, for a single hidden layer neural network with ten hidde n units and weight\\ndecay parameter value as indicated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b06d746b-66f2-4884-8779-60e8891d50d0', embedding=None, metadata={'page_label': '423', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='404 Neural Networks\\nFIGURE 11.9. Examples of training cases from ZIP code data. Each image is\\na16×16 8-bit grayscale representation of a handwritten digit.\\ndecay parameter, and hence cross-validation of this parameter would be\\npreferred.\\n11.7 Example: ZIP Code Data\\nThis example is a character recognition task: classiﬁcation of handwritten\\nnumerals. This problem captured the attention of the machine learning and\\nneural network community for many years, and has remained a benchmark\\nproblem in the ﬁeld. Figure 11.9 shows some examples of normalized hand-\\nwritten digits, automatically scanned from envelopes by the U.S. Postal\\nService. The original scanned digits are binary and of diﬀerent sizes and\\norientations; the images shown here have been deslanted and size normal-\\nized, resulting in 16 ×16 grayscale images (Le Cun et al., 1990). These 256\\npixel values are used as inputs to the neural network classiﬁer.\\nAblack box neural network is not ideally suited to this pattern recogni-\\ntion task, partly because the pixel representation of the images lack certain\\ninvariances (such as small rotations of the image). Consequently early at -\\ntempts with neural networks yielded misclassiﬁcation rates around 4 .5%\\non various examples of the problem. In this section we show some of the\\npioneering eﬀorts to handcraft the neural network to overcome some these\\ndeﬁciencies (Le Cun, 1989), which ultimately led to the state of the art in\\nneural network performance(Le Cun et al., 1998)1.\\nAlthough current digit datasets have tens of thousands of training and\\ntest examples, the sample size here is deliberately modest in order to em-\\n1The ﬁgures and tables in this example were recreated from Le C un (1989).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a74cb99b-f8ab-4f3b-ac66-2cc51b7c4351', embedding=None, metadata={'page_label': '424', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.7 Example: ZIP Code Data 405\\n16x168x8x2\\n16x1610\\n4x44x4\\n8x8x210\\nShared WeightsNet-5Net-4Net-1\\n4x4x4Local Connectivity10\\n1010\\nNet-3Net-28x812\\n16x1616x1616x16\\nFIGURE 11.10. Architecture of the ﬁve networks used in the ZIP code example.\\nphasize the eﬀects. The examples were obtained by scanning some actual\\nhand-drawn digits, and then generating additional images by random hor-\\nizontal shifts. Details may be found in Le Cun (1989). There are 320 digi ts\\nin the training set, and 160 in the test set.\\nFive diﬀerent networks were ﬁt to the data:\\nNet-1: No hidden layer, equivalent to multinomial logistic regression.\\nNet-2: One hidden layer, 12 hidden units fully connected.\\nNet-3: Two hidden layers locally connected.\\nNet-4: Two hidden layers, locally connected with weight sharing.\\nNet-5: Two hidden layers, locally connected, two levels of weight sharing.\\nThese are depicted in Figure 11.10. Net-1 for example has 256 inputs, one\\neach for the 16 ×16 input pixels, and ten output units for each of the digits\\n0–9. The predicted value ˆfk(x) represents the estimated probability that\\nan image xhas digit class k, fork= 0,1,2,... ,9.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='84235d92-5714-49b9-ba45-53643a8c39df', embedding=None, metadata={'page_label': '425', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='406 Neural Networks\\nTraining Epochs% Correct on Test Data\\n0 5 10 15 20 25 3060708090100\\nNet-1Net-2Net-3Net-4Net-5\\nFIGURE 11.11. Test performance curves, as a function of the number of train-\\ning epochs, for the ﬁve networks of Table 11.1 applied to the ZIP c ode data.\\n(Le Cun, 1989)\\nThe networks all have sigmoidal output units, and were all ﬁt with the\\nsum-of-squares error function. The ﬁrst network has no hidden layer, and\\nhence is nearly equivalent to a linear multinomial regression model (Exer-\\ncise 11.4). Net-2 is a single hidden layer network with 12 hidden units, of\\nthe kind described above.\\nThe training set error for all of the networks was 0%, since in all cases\\nthere are more parameters than training observations. The evolution of the\\ntest error during the training epochs is shown in Figure 11.11. The linear\\nnetwork (Net-1) starts to overﬁt fairly quickly, while test performance o f\\nthe others level oﬀ at successively superior values.\\nThe other three networks have additional features which demonstrate\\nthe power and ﬂexibility of the neural network paradigm. They introduce\\nconstraints on the network, natural for the problem at hand, which allow\\nfor more complex connectivity but fewer parameters.\\nNet-3 uses local connectivity: this means that each hidden unit is con-\\nnected to only a small patch of units in the layer below. In the ﬁrst hidden\\nlayer (an 8 ×8 array), each unit takes inputs from a 3 ×3 patch of the input\\nlayer; for units in the ﬁrst hidden layer that are one unit apart, their recep-\\ntive ﬁelds overlap by one row or column, and hence are two pixels apart.\\nIn the second hidden layer, inputs are from a 5 ×5 patch, and again units\\nthat are one unit apart have receptive ﬁelds that are two units apart. The\\nweights for all other connections are set to zero. Local connectivity makes\\neach unit responsible for extracting local features from the layer below, and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c36a963d-2961-4ef5-b9a7-b052f6de76b3', embedding=None, metadata={'page_label': '426', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.7 Example: ZIP Code Data 407\\nTABLE 11.1. Test set performance of ﬁve diﬀerent neural networks on a hand-\\nwritten digit classiﬁcation example (Le Cun, 1989).\\nNetwork Architecture Links Weights % Correct\\nNet-1: Single layer network 2570 2570 80.0%\\nNet-2: Two layer network 3214 3214 87.0%\\nNet-3: Locally connected 1226 1226 88.5%\\nNet-4: Constrained network 1 2266 1132 94.0%\\nNet-5: Constrained network 2 5194 1060 98.4%\\nreduces considerably the total number of weights. With many more hidden\\nunits than Net-2, Net-3 has fewer links and hence weights (1226 vs. 3214),\\nand achieves similar performance.\\nNet-4 and Net-5 have local connectivity with shared weights. All units\\nin a local feature map perform the sameoperation on diﬀerent parts of the\\nimage, achieved by sharing the same weights. The ﬁrst hidden layer of Net-\\n4 has two 8 ×8 arrays, and each unit takes input from a 3 ×3 patch just like\\nin Net-3. However, each of the units in a single 8 ×8 feature map share the\\nsame set of nine weights (but have their own bias parameter). This forces\\nthe extracted features in diﬀerent parts of the image to be computed by\\nthe same linear functional, and consequently these networks are sometimes\\nknown as convolutional networks . The second hidden layer of Net-4 has\\nno weight sharing, and is the same as in Net-3. The gradient of the error\\nfunction Rwith respect to a shared weight is the sum of the gradients of\\nRwith respect to each connection controlled by the weights in question.\\nTable 11.1 gives the number of links, the number of weights and the\\noptimal test performance for each of the networks. We see that Net-4 has\\nmore links but fewer weights than Net-3, and superior test performance.\\nNet-5 has four 4 ×4 feature maps in the second hidden layer, each unit\\nconnected to a 5 ×5 local patch in the layer below. Weights are shared\\nin each of these feature maps. We see that Net-5 does the best, having\\nerrors of only 1.6%, compared to 13% for the “vanilla” network Net-2.\\nThe clever design of network Net-5, motivated by the fact that features of\\nhandwriting style should appear in more than one part of a digit, was the\\nresult of many person years of experimentation. This and similar networks\\ngave better performance on ZIP code problems than any other learning\\nmethod at that time (early 1990s). This example also shows that neural\\nnetworks are not a fully automatic tool, as they are sometimes advertised.\\nAs with all statistical models, subject matter knowledge can and should be\\nused to improve their performance.\\nThis network was later outperformed by the tangent distance approach\\n(Simard et al., 1993) described in Section 13.3.3, which explicitly incorpo-\\nrates natural aﬃne invariances. At this point the digit recognition datasets\\nbecome test beds for every new learning procedure, and researchers worked', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='01a36507-9aa3-494a-8e56-4009b913717a', embedding=None, metadata={'page_label': '427', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='408 Neural Networks\\nhard to drive down the error rates. As of this writing, the best error rates o n\\na large database (60 ,000 training, 10 ,000 test observations), derived from\\nstandard NIST2databases, were reported to be the following: (Le Cun et\\nal., 1998):\\n•1.1% for tangent distance with a 1-nearest neighbor classiﬁer (Sec-\\ntion 13.3.3);\\n•0.8% for a degree-9 polynomial SVM (Section 12.3);\\n•0.8% for LeNet-5 , a more complex version of the convolutional net-\\nwork described here;\\n•0.7% for boosted LeNet-4 . Boosting is described in Chapter 8. LeNet-\\n4is a predecessor of LeNet-5.\\nLe Cun et al. (1998) report a much larger table of performance results, and\\nit is evident that many groups have been working very hard to bring these\\ntest error rates down. They report a standard error of 0 .1% on the error\\nestimates, which is based on a binomial average with N= 10,000 and\\np≈0.01. This implies that error rates within 0 .1—0.2% of one another\\nare statistically equivalent. Realistically the standard error is even hi gher,\\nsince the test data has been implicitly used in the tuning of the various\\nprocedures.\\n11.8 Discussion\\nBoth projection pursuit regression and neural networks take nonlinear func-\\ntions of linear combinations (“derived features”) of the inputs. This is a\\npowerful and very general approach for regression and classiﬁcation, and\\nhas been shown to compete well with the best learning methods on many\\nproblems.\\nThese tools are especially eﬀective in problems with a high signal-to-noise\\nratio and settings where prediction without interpretation is the goal. They\\nare less eﬀective for problems where the goal is to describe the physical pro-\\ncess that generated the data and the roles of individual inputs. Each input\\nenters into the model in many places, in a nonlinear fashion. Some authors\\n(Hinton, 1989) plot a diagram of the estimated weights into each hidden\\nunit, to try to understand the feature that each unit is extracting. This\\nis limited however by the lack of identiﬁability of the parameter vectors\\nαm, m= 1,... ,M . Often there are solutions with αmspanning the same\\nlinear space as the ones found during training, giving predicted values that\\n2The National Institute of Standards and Technology maintai n large databases, in-\\ncluding handwritten character databases; http://www.nist.gov/srd/ .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1a353a0b-e2ff-4d43-a328-8dcd6d72ea00', embedding=None, metadata={'page_label': '428', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 409\\nare roughly the same. Some authors suggest carrying out a principal com-\\nponent analysis of these weights, to try to ﬁnd an interpretable solution. In\\ngeneral, the diﬃculty of interpreting these models has limited their use in\\nﬁelds like medicine, where interpretation of the model is very important.\\nThere has been a great deal of research on the training of neural net-\\nworks. Unlike methods like CART and MARS, neural networks are smooth\\nfunctions of real-valued parameters. This facilitates the development of\\nBayesian inference for these models. The next sections discusses a success-\\nful Bayesian implementation of neural networks.\\n11.9 Bayesian Neural Nets and the NIPS 2003\\nChallenge\\nA classiﬁcation competition was held in 2003, in which ﬁve labeled train-\\ning datasets were provided to participants. It was organized for a Neural\\nInformation Processing Systems (NIPS) workshop. Each of the data sets\\nconstituted a two-class classiﬁcation problems, with diﬀerent sizes and from\\na variety of domains (see Table 11.2). Feature measurements for a valida-\\ntion dataset were also available.\\nParticipants developed and applied statistical learning procedures to\\nmake predictions on the datasets, and could submit predictions to a web-\\nsite on the validation set for a period of 12 weeks. With this feedback,\\nparticipants were then asked to submit predictions for a separate test set\\nand they received their results. Finally, the class labels for the validation\\nset were released and participants had one week to train their algorithms\\non the combined training and validation sets, and submit their ﬁnal pre-\\ndictions to the competition website. A total of 75 groups participated, with\\n20 and 16 eventually making submissions on the validation and test sets,\\nrespectively.\\nThere was an emphasis on feature extraction in the competition. Arti-\\nﬁcial “probes” were added to the data: these are noise features with dis-\\ntributions resembling the real features but independent of the class labels.\\nThe percentage of probes that were added to each dataset, relative to the\\ntotal set of features, is shown on Table 11.2. Thus each learning algorithm\\nhad to ﬁgure out a way of identifying the probes and downweighting or\\neliminating them.\\nA number of metrics were used to evaluate the entries, including the\\npercentage correct on the test set, the area under the ROC curve, and a\\ncombined score that compared each pair of classiﬁers head-to-head. The\\nresults of the competition are very interesting and are detailed in Guyon et\\nal. (2006). The most notable result: the entries of Neal and Zhang (2006)\\nwere the clear overall winners. In the ﬁnal competition they ﬁnished ﬁrst', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f701cd3e-4896-4634-b202-3f5aa41a3f68', embedding=None, metadata={'page_label': '429', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='410 Neural Networks\\nTABLE 11.2. NIPS 2003 challenge data sets. The column labeled pis the number\\nof features. For the Dorothea dataset the features are binary. Ntr,NvalandNte\\nare the number of training, validation and test cases, respectiv ely\\nDataset Domain Feature p Percent Ntr Nval Nte\\nType Probes\\nArcene Mass spectrometry Dense 10,000 30 100 100 700\\nDexter Text classiﬁcation Sparse 20,000 50 300 300 2000\\nDorothea Drug discovery Sparse 100,000 50 800 350 800\\nGisette Digit recognition Dense 5000 30 6000 1000 6500\\nMadelon Artiﬁcial Dense 500 96 2000 600 1800\\nin three of the ﬁve datasets, and were 5th and 7th on the remaining two\\ndatasets.\\nIn their winning entries, Neal and Zhang (2006) used a series of pre-\\nprocessing feature-selection steps, followed by Bayesian neural networks,\\nDirichlet diﬀusion trees, and combinations of these methods. Here we focus\\nonly on the Bayesian neural network approach, and try to discern which\\naspects of their approach were important for its success. We rerun their\\nprograms and compare the results to boosted neural networks and boosted\\ntrees, and other related methods.\\n11.9.1 Bayes, Boosting and Bagging\\nLet us ﬁrst review brieﬂy the Bayesian approach to inference and its appli-\\ncation to neural networks. Given training data Xtr,ytr, we assume a sam-\\npling model with parameters θ; Neal and Zhang (2006) use a two-hidden-\\nlayer neural network, with output nodes the class probabilities Pr( Y|X,θ)\\nfor the binary outcomes. Given a prior distribution Pr( θ), the posterior\\ndistribution for the parameters is\\nPr(θ|Xtr,ytr) =Pr(θ)Pr(ytr|Xtr,θ)∫\\nPr(θ)Pr(ytr|Xtr,θ)dθ(11.19)\\nFor a test case with features Xnew, the predictive distribution for the\\nlabelYnewis\\nPr(Ynew|Xnew,Xtr,ytr) =∫\\nPr(Ynew|Xnew,θ)Pr(θ|Xtr,ytr)dθ(11.20)\\n(c.f. equation 8.24). Since the integral in (11.20) is intractable, sophis ticated\\nMarkov Chain Monte Carlo (MCMC) methods are used to sample from the\\nposterior distribution Pr( Ynew|Xnew,Xtr,ytr). A few hundred values θare\\ngenerated and then a simple average of these values estimates the integral.\\nNeal and Zhang (2006) use diﬀuse Gaussian priors for all of the parame-\\nters. The particular MCMC approach that was used is called hybrid Monte\\nCarlo, and may be important for the success of the method. It includes\\nan auxiliary momentum vector and implements Hamiltonian dynamics in\\nwhich the potential function is the target density. This is done to avoid', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='94b4e7da-f69f-487c-a841-1c8d82fb2a5e', embedding=None, metadata={'page_label': '430', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 411\\nrandom walk behavior; the successive candidates move across the sample\\nspace in larger steps. They tend to be less correlated and hence converge\\nto the target distribution more rapidly.\\nNeal and Zhang (2006) also tried diﬀerent forms of pre-processing of the\\nfeatures:\\n1. univariate screening using t-tests, and\\n2. automatic relevance determination.\\nIn the latter method (ARD), the weights (coeﬃcients) for the jth feature\\nto each of the ﬁrst hidden layer units all share a common prior variance\\nσ2\\nj, and prior mean zero. The posterior distributions for each variance σ2\\nj\\nare computed, and the features whose posterior variance concentrates on\\nsmall values are discarded.\\nThere are thus three main features of this approach that could be im-\\nportant for its success:\\n(a) the feature selection and pre-processing,\\n(b) the neural network model, and\\n(c) the Bayesian inference for the model using MCMC.\\nAccording to Neal and Zhang (2006), feature screening in (a) is carried\\nout purely for computational eﬃciency; the MCMC procedure is slow with\\na large number of features. There is no need to use feature selection to avoid\\noverﬁtting. The posterior average (11.20) takes care of this automatica lly.\\nWe would like to understand the reasons for the success of the Bayesian\\nmethod. In our view, power of modern Bayesian methods does not lie in\\ntheir use as a formal inference procedure; most people would not believe\\nthat the priors in a high-dimensional, complex neural network model are\\nactually correct. Rather the Bayesian/MCMC approach gives an eﬃcient\\nway of sampling the relevant parts of model space, and then averaging the\\npredictions for the high-probability models.\\nBagging and boosting are non-Bayesian procedures that have some simi-\\nlarity to MCMC in a Bayesian model. The Bayesian approach ﬁxes the data\\nand perturbs the parameters, according to current estimate of the poste-\\nrior distribution. Bagging perturbs the data in an i.i.d fashion and then\\nre-estimates the model to give a new set of model parameters. At the end,\\na simple average of the model predictions from diﬀerent bagged samples is\\ncomputed. Boosting is similar to bagging, but ﬁts a model that is additive\\nin the models of each individual base learner, which are learned using non\\ni.i.d. samples. We can write all of these models in the form\\nˆf(xnew) =L∑\\nℓ=1wℓE(Ynew|xnew,ˆθℓ) (11.21)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f2030938-663d-4b3c-8b6f-b6c55ed75131', embedding=None, metadata={'page_label': '431', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='412 Neural Networks\\nIn all cases the ˆθℓare a large collection of model parameters. For the\\nBayesian model the wℓ= 1/L, and the average estimates the posterior\\nmean (11.21) by sampling θℓfrom the posterior distribution. For bagging,\\nwℓ= 1/Las well, and the ˆθℓare the parameters reﬁt to bootstrap re-\\nsamples of the training data. For boosting, the weights are all equal to\\n1, but the ˆθℓare typically chosen in a nonrandom sequential fashion to\\nconstantly improve the ﬁt.\\n11.9.2 Performance Comparisons\\nBased on the similarities above, we decided to compare Bayesian neural\\nnetworks to boosted trees, boosted neural networks, random forests and\\nbagged neural networks on the ﬁve datasets in Table 11.2. Bagging and\\nboosting of neural networks are not methods that we have previously used\\nin our work. We decided to try them here, because of the success of Bayesian\\nneural networks in this competition, and the good performance of bagging\\nand boosting with trees. We also felt that by bagging and boosting neural\\nnets, we could assess both the choice of model as well as the model search\\nstrategy.\\nHere are the details of the learning methods that were compared:\\nBayesian neural nets. The results here are taken from Neal and Zhang\\n(2006), using their Bayesian approach to ﬁtting neural networks. The\\nmodels had two hidden layers of 20 and 8 units. We re-ran some\\nnetworks for timing purposes only.\\nBoosted trees. We used the gbmpackage (version 1.5-7) in the R language.\\nTree depth and shrinkage factors varied from dataset to dataset. We\\nconsistently bagged 80% of the data at each boosting iteration (the\\ndefault is 50%). Shrinkage was between 0.001 and 0.1. Tree depth was\\nbetween 2 and 9.\\nBoosted neural networks. Since boosting is typically most eﬀective with\\n“weak” learners, we boosted a single hidden layer neural network with\\ntwo or four units, ﬁt with the nnetpackage (version 7.2-36) in R.\\nRandom forests. We used the R package randomForest (version 4.5-16)\\nwith default settings for the parameters.\\nBagged neural networks. We used the same architecture as in the Bayesian\\nneural network above (two hidden layers of 20 and 8 units), ﬁt using\\nboth Neal’s C language package “Flexible Bayesian Modeling” (2004-\\n11-10 release), and Matlab neural-net toolbox (version 5.1).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='28b7a5aa-447f-4608-80b4-58821a71d472', embedding=None, metadata={'page_label': '432', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 413Test Error (%)\\nArcene Dexter Dorothea Gisette Madelon5 15 25Univariate Screened Features\\nBayesian neural nets\\nboosted trees \\nboosted neural nets\\nrandom forests\\nbagged neural networks \\nTest Error (%)\\nArcene Dexter Dorothea Gisette Madelon5 15 25ARD Reduced Features\\nFIGURE 11.12. Performance of diﬀerent learning methods on ﬁve problems,\\nusing both univariate screening of features (top panel) and a reduc ed feature set\\nfrom automatic relevance determination. The error bars at the t op of each plot\\nhave width equal to one standard error of the diﬀerence between t wo error rates.\\nOn most of the problems several competitors are within this e rror bound.\\nThis analysis was carried out by Nicholas Johnson, and full details may\\nbe found in Johnson (2008)3. The results are shown in Figure 11.12 and\\nTable 11.3.\\nThe ﬁgure and table show Bayesian, boosted and bagged neural networks,\\nboosted trees, and random forests, using both the screened and reduced\\nfeatures sets. The error bars at the top of each plot indicate one standard\\nerror of the diﬀerence between two error rates. Bayesian neural networks\\nagain emerge as the winner, although for some datasets the diﬀerences\\nbetween the test error rates is not statistically signiﬁcant. Random forests\\nperforms the best among the competitors using the selected feature set,\\nwhile the boosted neural networks perform best with the reduced feature\\nset, and nearly match the Bayesian neural net.\\nThe superiority of boosted neural networks over boosted trees suggest\\nthat the neural network model is better suited to these particular prob-\\nlems. Speciﬁcally, individual features might not be good predictors here\\n3We also thank Isabelle Guyon for help in preparing the result s of this section.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ad74bd3-6ed2-447e-8bb6-0915327fd302', embedding=None, metadata={'page_label': '433', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='414 Neural Networks\\nTABLE 11.3. Performance of diﬀerent methods. Values are average rank of tes t\\nerror across the ﬁve problems (low is good), and mean computati on time and\\nstandard error of the mean, in minutes.\\nScreened Features ARD Reduced Features\\nMethod Average Average Average Average\\nRank Time Rank Time\\nBayesian neural networks 1.5 384(138) 1.6 600(186)\\nBoosted trees 3.4 3.03(2.5) 4.0 34.1(32.4)\\nBoosted neural networks 3.8 9.4(8.6) 2.2 35.6(33.5)\\nRandom forests 2.7 1.9(1.7) 3.2 11.2(9.3)\\nBagged neural networks 3.6 3.5(1.1) 4.0 6.4(4.4)\\nand linear combinations of features work better. However the impressive\\nperformance of random forests is at odds with this explanation, and came\\nas a surprise to us.\\nSince the reduced feature sets come from the Bayesian neural network\\napproach, only the methods that use the screened features are legitimate,\\nself-contained procedures. However, this does suggest that better methods\\nfor internal feature selection might help the overall performance of boosted\\nneural networks.\\nThe table also shows the approximate training time required for each\\nmethod. Here the non-Bayesian methods show a clear advantage.\\nOverall, the superior performance of Bayesian neural networks here may\\nbe due to the fact that\\n(a) the neural network model is well suited to these ﬁve problems, and\\n(b) the MCMC approach provides an eﬃcient way of exploring the im-\\nportant part of the parameter space, and then averaging the resulting\\nmodels according to their quality.\\nThe Bayesian approach works well for smoothly parametrized models like\\nneural nets; it is not yet clear that it works as well for non-smooth models\\nlike trees.\\n11.10 Computational Considerations\\nWithNobservations, ppredictors, Mhidden units and Ltraining epochs, a\\nneural network ﬁt typically requires O(NpML ) operations. There are many\\npackages available for ﬁtting neural networks, probably many more than\\nexist for mainstream statistical methods. Because the available softwar e\\nvaries widely in quality, and the learning problem for neural networks is\\nsensitive to issues such as input scaling, such software should be carefully\\nchosen and tested.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='902b90f1-680f-4328-abe8-acc7fb404c6f', embedding=None, metadata={'page_label': '434', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 415\\nBibliographic Notes\\nProjection pursuit was proposed by Friedman and Tukey (1974), and spe-\\ncialized to regression by Friedman and Stuetzle (1981). Huber (1985) gives\\na scholarly overview, and Roosen and Hastie (1994) present a formulatio n\\nusing smoothing splines. The motivation for neural networks dates back\\nto McCulloch and Pitts (1943), Widrow and Hoﬀ (1960) (reprinted in An-\\nderson and Rosenfeld (1988)) and Rosenblatt (1962). Hebb (1949) heavily\\ninﬂuenced the development of learning algorithms. The resurgence of neural\\nnetworks in the mid 1980s was due to Werbos (1974), Parker (1985) and\\nRumelhart et al. (1986), who proposed the back-propagation algorithm.\\nToday there are many books written on the topic, for a broad range of\\naudiences. For readers of this book, Hertz et al. (1991), Bishop (1995) and\\nRipley (1996) may be the most informative. Bayesian learning for neural\\nnetworks is described in Neal (1996). The ZIP code example was taken from\\nLe Cun (1989); see also Le Cun et al. (1990) and Le Cun et al. (1998).\\nWe do not discuss theoretical topics such as approximation properties of\\nneural networks, such as the work of Barron (1993), Girosi et al. (1995 )\\nand Jones (1992). Some of these results are summarized by Ripley (1996).\\nExercises\\nEx. 11.1 Establish the exact correspondence between the projection pur-\\nsuit regression model (11.1) and the neural network (11.5). In particular,\\nshow that the single-layer regression network is equivalent to a PPR model\\nwithgm(ωT\\nmx) =βmσ(α0m+sm(ωT\\nmx)), where ωmis the mth unit vector.\\nEstablish a similar equivalence for a classiﬁcation network.\\nEx. 11.2 Consider a neural network for a quantitative outcome as in (11.5),\\nusing squared-error loss and identity output function gk(t) =t. Suppose\\nthat the weights αmfrom the input to hidden layer are nearly zero. Show\\nthat the resulting model is nearly linear in the inputs.\\nEx. 11.3 Derive the forward and backward propagation equations for the\\ncross-entropy loss function.\\nEx. 11.4 Consider a neural network for a Kclass outcome that uses cross-\\nentropy loss. If the network has no hidden layer, show that the model is\\nequivalent to the multinomial logistic model described in Chapter 4.\\nEx. 11.5\\n(a) Write a program to ﬁt a single hidden layer neural network (ten hidden\\nunits) via back-propagation and weight decay.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2a59e96d-7919-4289-aeef-a37da76ebd21', embedding=None, metadata={'page_label': '435', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='416 Neural Networks\\n(b) Apply it to 100 observations from the model\\nY=σ(aT\\n1X) + (aT\\n2X)2+ 0.30≤Z,\\nwhere σis the sigmoid function, Zis standard normal, XT= (X1,X2),\\neachXjbeing independent standard normal, and a1= (3,3),a2=\\n(3,−3). Generate a test sample of size 1000, and plot the training and\\ntest error curves as a function of the number of training epochs, for\\ndiﬀerent values of the weight decay parameter. Discuss the overﬁtting\\nbehavior in each case.\\n(c) Vary the number of hidden units in the network, from 1 up to 10, and\\ndetermine the minimum number needed to perform well for this task.\\nEx. 11.6 Write a program to carry out projection pursuit regression, using\\ncubic smoothing splines with ﬁxed degrees of freedom. Fit it to the data\\nfrom the previous exercise, for various values of the smoothing parameter\\nand number of model terms. Find the minimum number of model terms\\nnecessary for the model to perform well and compare this to the number\\nof hidden units from the previous exercise.\\nEx. 11.7 Fit a neural network to the spamdata of Section 9.1.2, and compare\\nthe results to those for the additive model given in that chapter. Compare\\nboth the classiﬁcation performance and interpretability of the ﬁnal model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44418291-df4a-429c-b945-472785345a0c', embedding=None, metadata={'page_label': '436', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 417\\nPrinter: Opaque this\\n12\\nSupport Vector Machines and\\nFlexible Discriminants\\n12.1 Introduction\\nIn this chapter we describe generalizations of linear decision boundaries\\nfor classiﬁcation. Optimal separating hyperplanes are introduced in Chap-\\nter 4 for the case when two classes are linearly separable. Here we cover\\nextensions to the nonseparable case, where the classes overlap. These tech-\\nniques are then generalized to what is known as the support vector machine ,\\nwhich produces nonlinear boundaries by constructing a linear boundary in\\na large, transformed version of the feature space. The second set of methods\\ngeneralize Fisher’s linear discriminant analysis (LDA). The generalizations\\ninclude ﬂexible discriminant analysis which facilitates construction of non-\\nlinear boundaries in a manner very similar to the support vector machines,\\npenalized discriminant analysis for problems such as signal and image clas-\\nsiﬁcation where the large number of features are highly correlated, and\\nmixture discriminant analysis for irregularly shaped classes.\\n12.2 The Support Vector Classiﬁer\\nIn Chapter 4 we discussed a technique for constructing an optimal separat-\\ning hyperplane between two perfectly separated classes. We review this and\\ngeneralize to the nonseparable case, where the classes may not be separable\\nby a linear boundary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb9016ed-f0ae-4206-be5e-8faf21d6c08b', embedding=None, metadata={'page_label': '437', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='418 12. Flexible Discriminants\\n••\\n•\\n••\\n•••\\n•\\n••\\n•••\\n••\\n••\\n•\\n•\\nmarginM=1\\n∥β∥\\nM=1\\n∥β∥xTβ+β0= 0\\n••\\n•\\n••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n••\\n•\\n• •\\nmarginξ∗\\n1ξ∗\\n1ξ∗\\n1\\nξ∗\\n2ξ∗\\n2ξ∗\\n2ξ∗\\n3ξ∗\\n3ξ∗\\n4ξ∗\\n4ξ∗\\n4ξ∗\\n5\\nM=1\\n∥β∥\\nM=1\\n∥β∥xTβ+β0= 0\\nFIGURE 12.1. Support vector classiﬁers. The left panel shows the separable\\ncase. The decision boundary is the solid line, while broken line s bound the shaded\\nmaximal margin of width 2M= 2/∥β∥. The right panel shows the nonseparable\\n(overlap) case. The points labeled ξ∗\\njare on the wrong side of their margin by\\nan amount ξ∗\\nj=Mξj; points on the correct side have ξ∗\\nj= 0. The margin is\\nmaximized subject to a total budgetPξi≤constant. HencePξ∗\\njis the total\\ndistance of points on the wrong side of their margin.\\nOur training data consists of Npairs ( x1,y1),(x2,y2),... ,(xN,yN), with\\nxi∈IRpandyi∈ {− 1,1}. Deﬁne a hyperplane by\\n{x:f(x) =xTβ+β0= 0}, (12.1)\\nwhere βis a unit vector: ∥β∥= 1. A classiﬁcation rule induced by f(x) is\\nG(x) = sign[ xTβ+β0]. (12.2)\\nThe geometry of hyperplanes is reviewed in Section 4.5, where we show that\\nf(x) in (12.1) gives the signed distance from a point xto the hyperplane\\nf(x) =xTβ+β0= 0. Since the classes are separable, we can ﬁnd a function\\nf(x) =xTβ+β0withyif(xi)>0∀i. Hence we are able to ﬁnd the\\nhyperplane that creates the biggest margin between the training points for\\nclass 1 and −1 (see Figure 12.1). The optimization problem\\nmax\\nβ,β0,∥β∥=1M\\nsubject to yi(xT\\niβ+β0)≥M, i= 1,... ,N,(12.3)\\ncaptures this concept. The band in the ﬁgure is Munits away from the\\nhyperplane on either side, and hence 2 Munits wide. It is called the margin .\\nWe showed that this problem can be more conveniently rephrased as\\nmin\\nβ,β0∥β∥\\nsubject to yi(xT\\niβ+β0)≥1, i= 1,... ,N,(12.4)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='39e156b2-1eae-4245-a5ba-2838393c951e', embedding=None, metadata={'page_label': '438', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.2 The Support Vector Classiﬁer 419\\nwhere we have dropped the norm constraint on β. Note that M= 1/∥β∥.\\nExpression (12.4) is the usual way of writing the support vector criterion\\nfor separated data. This is a convex optimization problem (quadratic cri-\\nterion, linear inequality constraints), and the solution is characterized in\\nSection 4.5.2.\\nSuppose now that the classes overlap in feature space. One way to deal\\nwith the overlap is to still maximize M, but allow for some points to be on\\nthe wrong side of the margin. Deﬁne the slack variables ξ= (ξ1,ξ2,... ,ξ N).\\nThere are two natural ways to modify the constraint in (12.3):\\nyi(xT\\niβ+β0)≥M−ξi, (12.5)\\nor\\nyi(xT\\niβ+β0)≥M(1−ξi), (12.6)\\n∀i, ξi≥0,∑N\\ni=1ξi≤constant. The two choices lead to diﬀerent solutions.\\nThe ﬁrst choice seems more natural, since it measures overlap in actual\\ndistance from the margin; the second choice measures the overlap in relative\\ndistance, which changes with the width of the margin M. However, the ﬁrst\\nchoice results in a nonconvex optimization problem, while the second is\\nconvex; thus (12.6) leads to the “standard” support vector classiﬁer, which\\nwe use from here on.\\nHere is the idea of the formulation. The value ξiin the constraint yi(xT\\niβ+\\nβ0)≥M(1−ξi) is the proportional amount by which the prediction\\nf(xi) =xT\\niβ+β0is on the wrong side of its margin. Hence by bounding the\\nsum∑ξi, we bound the total proportional amount by which predictions\\nfall on the wrong side of their margin. Misclassiﬁcations occur when ξi>1,\\nso bounding∑ξiat a value Ksay, bounds the total number of training\\nmisclassiﬁcations at K.\\nAs in (4.48) in Section 4.5.2, we can drop the norm constraint on β,\\ndeﬁne M= 1/∥β∥, and write (12.4) in the equivalent form\\nmin∥β∥subject to{\\nyi(xT\\niβ+β0)≥1−ξi∀i,\\nξi≥0,∑ξi≤constant .(12.7)\\nThis is the usual way the support vector classiﬁer is deﬁned for the non-\\nseparable case. However we ﬁnd confusing the presence of the ﬁxed scale\\n“1” in the constraint yi(xT\\niβ+β0)≥1−ξi, and prefer to start with (12.6).\\nThe right panel of Figure 12.1 illustrates this overlapping case.\\nBy the nature of the criterion (12.7), we see that points well inside their\\nclass boundary do not play a big role in shaping the boundary. This seems\\nlike an attractive property, and one that diﬀerentiates it from linear dis-\\ncriminant analysis (Section 4.3). In LDA, the decision boundary is deter-\\nmined by the covariance of the class distributions and the positions of the\\nclass centroids. We will see in Section 12.3.3 that logistic regression i s more\\nsimilar to the support vector classiﬁer in this regard.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f75b0703-9b8f-482b-9416-230891db7f62', embedding=None, metadata={'page_label': '439', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='420 12. Flexible Discriminants\\n12.2.1 Computing the Support Vector Classiﬁer\\nThe problem (12.7) is quadratic with linear inequality constraints, hence it\\nis a convex optimization problem. We describe a quadratic programming\\nsolution using Lagrange multipliers. Computationally it is convenient to\\nre-express (12.7) in the equivalent form\\nmin\\nβ,β01\\n2∥β∥2+CN∑\\ni=1ξi\\nsubject to ξi≥0, yi(xT\\niβ+β0)≥1−ξi∀i,(12.8)\\nwhere the “cost” parameter Creplaces the constant in (12.7); the separable\\ncase corresponds to C=∞.\\nThe Lagrange (primal) function is\\nLP=1\\n2∥β∥2+CN∑\\ni=1ξi−N∑\\ni=1αi[yi(xT\\niβ+β0)−(1−ξi)]−N∑\\ni=1θiξi,(12.9)\\nwhich we minimize w.r.t β,β0andξi. Setting the respective derivatives to\\nzero, we get\\nβ=N∑\\ni=1αiyixi, (12.10)\\n0 =N∑\\ni=1αiyi, (12.11)\\nαi=C−θi,∀i, (12.12)\\nas well as the positivity constraints αi, θi, ξi≥0∀i. By substituting\\n(12.10)–(12.12) into (12.9), we obtain the Lagrangian (Wolfe) dua l objec-\\ntive function\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\ni′=1αiαi′yiyi′xT\\nixi′, (12.13)\\nwhich gives a lower bound on the objective function (12.8) for any feasible\\npoint. We maximize LDsubject to 0 ≤αi≤Cand∑N\\ni=1αiyi= 0. In\\naddition to (12.10)–(12.12), the Karush–Kuhn–Tucker conditions include\\nthe constraints\\nαi[yi(xT\\niβ+β0)−(1−ξi)] = 0 , (12.14)\\nθiξi= 0, (12.15)\\nyi(xT\\niβ+β0)−(1−ξi)≥0, (12.16)\\nfori= 1,... ,N . Together these equations (12.10)–(12.16) uniquely char-\\nacterize the solution to the primal and dual problem.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='69065cd0-db4b-4720-acdd-04b2e2f38ece', embedding=None, metadata={'page_label': '440', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.2 The Support Vector Classiﬁer 421\\nFrom (12.10) we see that the solution for βhas the form\\nˆβ=N∑\\ni=1ˆαiyixi, (12.17)\\nwith nonzero coeﬃcients ˆ αionly for those observations ifor which the\\nconstraints in (12.16) are exactly met (due to (12.14)). These observati ons\\nare called the support vectors , since ˆβis represented in terms of them\\nalone. Among these support points, some will lie on the edge of the margin\\n(ˆξi= 0), and hence from (12.15) and (12.12) will be characterized by\\n0<ˆαi< C; the remainder ( ˆξi>0) have ˆ αi=C. From (12.14) we can\\nsee that any of these margin points (0 <ˆαi,ˆξi= 0) can be used to solve\\nforβ0, and we typically use an average of all the solutions for numerical\\nstability.\\nMaximizing the dual (12.13) is a simpler convex quadratic programming\\nproblem than the primal (12.9), and can be solved with standard techniques\\n(Murray et al., 1981, for example).\\nGiven the solutions ˆβ0andˆβ, the decision function can be written as\\nˆG(x) = sign[ ˆf(x)]\\n= sign[ xTˆβ+ˆβ0]. (12.18)\\nThe tuning parameter of this procedure is the cost parameter C.\\n12.2.2 Mixture Example (Continued)\\nFigure 12.2 shows the support vector boundary for the mixture example\\nof Figure 2.5 on page 21, with two overlapping classes, for two diﬀerent\\nvalues of the cost parameter C. The classiﬁers are rather similar in their\\nperformance. Points on the wrong side of the boundary are support vectors.\\nIn addition, points on the correct side of the boundary but close to it (in\\nthe margin), are also support vectors. The margin is larger for C= 0.01\\nthan it is for C= 10,000. Hence larger values of Cfocus attention more\\non (correctly classiﬁed) points near the decision boundary, while smaller\\nvalues involve data further away. Either way, misclassiﬁed points are gi ven\\nweight, no matter how far away. In this example the procedure is not very\\nsensitive to choices of C, because of the rigidity of a linear boundary.\\nThe optimal value for Ccan be estimated by cross-validation, as dis-\\ncussed in Chapter 7. Interestingly, the leave-one-out cross-validation error\\ncan be bounded above by the proportion of support points in the data. The\\nreason is that leaving out an observation that is not a support vector will\\nnot change the solution. Hence these observations, being classiﬁed correctly\\nby the original boundary, will be classiﬁed correctly in the cross-validatio n\\nprocess. However this bound tends to be too high, and not generally useful\\nfor choosing C(62% and 85%, respectively, in our examples).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20c873f3-80eb-4d8b-be83-c211cf175987', embedding=None, metadata={'page_label': '441', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='422 12. Flexible Discriminants\\n.. . . .. . . . . . .. . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . .. . . . ..\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n••\\n•\\nTraining Error: 0.270\\nTest Error:       0.288\\nBayes Error:    0.210\\nC= 10000\\n.. .. . . .. . . . .. . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .. . . . .. . . .. ..\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no•\\nTraining Error: 0.26\\nTest Error:       0.30\\nBayes Error:    0.21\\nC= 0.01\\nFIGURE 12.2. The linear support vector boundary for the mixture data exam-\\nple with two overlapping classes, for two diﬀerent values of C. The broken lines\\nindicate the margins, where f(x) =±1. The support points ( αi>0) are all the\\npoints on the wrong side of their margin. The black solid dots are those support\\npoints falling exactly on the margin ( ξi= 0, αi>0). In the upper panel 62%of\\nthe observations are support points, while in the lower panel 85%are. The broken\\npurple curve in the background is the Bayes decision boundary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca3b441c-529a-436f-b0d6-2f40282a9226', embedding=None, metadata={'page_label': '442', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Support Vector Machines and Kernels 423\\n12.3 Support Vector Machines and Kernels\\nThe support vector classiﬁer described so far ﬁnds linear boundaries in the\\ninput feature space. As with other linear methods, we can make the pro-\\ncedure more ﬂexible by enlarging the feature space using basis expansions\\nsuch as polynomials or splines (Chapter 5). Generally linear boundaries\\nin the enlarged space achieve better training-class separation, and trans-\\nlate to nonlinear boundaries in the original space. Once the basis functions\\nhm(x), m= 1,... ,M are selected, the procedure is the same as before. We\\nﬁt the SV classiﬁer using input features h(xi) = (h1(xi),h2(xi),... ,h M(xi)),\\ni= 1,... ,N , and produce the (nonlinear) function ˆf(x) =h(x)Tˆβ+ˆβ0.\\nThe classiﬁer is ˆG(x) = sign( ˆf(x)) as before.\\nThesupport vector machine classiﬁer is an extension of this idea, where\\nthe dimension of the enlarged space is allowed to get very large, inﬁnite\\nin some cases. It might seem that the computations would become pro-\\nhibitive. It would also seem that with suﬃcient basis functions, the data\\nwould be separable, and overﬁtting would occur. We ﬁrst show how the\\nSVM technology deals with these issues. We then see that in fact the SVM\\nclassiﬁer is solving a function-ﬁtting problem using a particular criterion\\nand form of regularization, and is part of a much bigger class of problems\\nthat includes the smoothing splines of Chapter 5. The reader may wish\\nto consult Section 5.8, which provides background material and overlaps\\nsomewhat with the next two sections.\\n12.3.1 Computing the SVM for Classiﬁcation\\nWe can represent the optimization problem (12.9) and its solution in a\\nspecial way that only involves the input features via inner products. We do\\nthis directly for the transformed feature vectors h(xi). We then see that for\\nparticular choices of h, these inner products can be computed very cheaply.\\nThe Lagrange dual function (12.13) has the form\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\ni′=1αiαi′yiyi′⟨h(xi),h(xi′)⟩. (12.19)\\nFrom (12.10) we see that the solution function f(x) can be written\\nf(x) = h(x)Tβ+β0\\n=N∑\\ni=1αiyi⟨h(x),h(xi)⟩+β0. (12.20)\\nAs before, given αi,β0can be determined by solving yif(xi) = 1 in (12.20)\\nfor any (or all) xifor which 0 < αi< C.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='887fd714-a273-4bdf-9fca-9413a5923e56', embedding=None, metadata={'page_label': '443', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='424 12. Flexible Discriminants\\nSo both (12.19) and (12.20) involve h(x) only through inner products. In\\nfact, we need not specify the transformation h(x) at all, but require only\\nknowledge of the kernel function\\nK(x,x′) =⟨h(x),h(x′)⟩ (12.21)\\nthat computes inner products in the transformed space. Kshould be a\\nsymmetric positive (semi-) deﬁnite function; see Section 5.8.1.\\nThree popular choices for Kin the SVM literature are\\ndth-Degree polynomial: K(x,x′) = (1 + ⟨x,x′⟩)d,\\nRadial basis: K(x,x′) = exp( −γ∥x−x′∥2),\\nNeural network: K(x,x′) = tanh( κ1⟨x,x′⟩+κ2).(12.22)\\nConsider for example a feature space with two inputs X1andX2, and a\\npolynomial kernel of degree 2. Then\\nK(X,X′) = (1 + ⟨X,X′⟩)2\\n= (1 + X1X′\\n1+X2X′\\n2)2\\n= 1 + 2 X1X′\\n1+ 2X2X′\\n2+ (X1X′\\n1)2+ (X2X′\\n2)2+ 2X1X′\\n1X2X′\\n2.\\n(12.23)\\nThen M= 6, and if we choose h1(X) = 1, h2(X) =√\\n2X1,h3(X) =√\\n2X2,h4(X) =X2\\n1,h5(X) =X2\\n2, andh6(X) =√\\n2X1X2, then K(X,X′) =\\n⟨h(X),h(X′)⟩. From (12.20) we see that the solution can be written\\nˆf(x) =N∑\\ni=1ˆαiyiK(x,xi) +ˆβ0. (12.24)\\nThe role of the parameter Cis clearer in an enlarged feature space,\\nsince perfect separation is often achievable there. A large value of Cwill\\ndiscourage any positive ξi, and lead to an overﬁt wiggly boundary in the\\noriginal feature space; a small value of Cwill encourage a small value of\\n∥β∥, which in turn causes f(x) and hence the boundary to be smoother.\\nFigure 12.3 show two nonlinear support vector machines applied to the\\nmixture example of Chapter 2. The regularization parameter was chosen\\nin both cases to achieve good test error. The radial basis kernel produces\\na boundary quite similar to the Bayes optimal boundary for this example;\\ncompare Figure 2.5.\\nIn the early literature on support vectors, there were claims that the\\nkernel property of the support vector machine is unique to it and allows\\none to ﬁnesse the curse of dimensionality. Neither of these claims is true,\\nand we go into both of these issues in the next three subsections.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='184b7532-5a33-44ff-ab4b-42c185a0c3a8', embedding=None, metadata={'page_label': '444', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Support Vector Machines and Kernels 425\\nSVM - Degree-4 Polynomial in Feature Space\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . .. . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••••••\\n•\\n• ••\\n•\\n•••\\nTraining Error: 0.180\\nTest Error:       0.245\\nBayes Error:    0.210\\nSVM - Radial Kernel in Feature Space\\n. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . .. . . .. . . .. . . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•\\n•••\\n•\\n••••\\n••••\\n••••\\n••\\n••\\n••••\\n••\\n•\\n••\\n•\\nTraining Error: 0.160\\nTest Error:       0.218\\nBayes Error:    0.210\\nFIGURE 12.3. Two nonlinear SVMs for the mixture data. The upper plot uses\\na4th degree polynomial kernel, the lower a radial basis kernel (wi thγ= 1). In\\neach case Cwas tuned to approximately achieve the best test error perform ance,\\nandC= 1worked well in both cases. The radial basis kernel performs th e best\\n(close to Bayes optimal), as might be expected given the data a rise from mixtures\\nof Gaussians. The broken purple curve in the background is the B ayes decision\\nboundary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e214e961-5132-40f4-93aa-566f6009f413', embedding=None, metadata={'page_label': '445', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='426 12. Flexible Discriminants\\n−3 −2 −1 0 1 2 30.0 0.5 1.0 1.5 2.0 2.5 3.0Hinge Loss\\nBinomial Deviance\\nSquared Error\\nClass HuberLoss\\nyf\\nFIGURE 12.4. The support vector loss function (hinge loss), compared to the\\nnegative log-likelihood loss (binomial deviance) for logisti c regression, squared-er-\\nror loss, and a “Huberized” version of the squared hinge loss. A ll are shown as a\\nfunction of yfrather than f, because of the symmetry between the y= +1 and\\ny=−1case. The deviance and Huber have the same asymptotes as the SVM\\nloss, but are rounded in the interior. All are scaled to have the limiting left-tail\\nslope of −1.\\n12.3.2 The SVM as a Penalization Method\\nWith f(x) =h(x)Tβ+β0, consider the optimization problem\\nmin\\nβ0, βN∑\\ni=1[1−yif(xi)]++λ\\n2∥β∥2(12.25)\\nwhere the subscript “+” indicates positive part. This has the form loss+\\npenalty , which is a familiar paradigm in function estimation. It is easy to\\nshow (Exercise 12.1) that the solution to (12.25), with λ= 1/C, is the\\nsame as that for (12.8).\\nExamination of the “hinge” loss function L(y,f) = [1 −yf]+shows that\\nit is reasonable for two-class classiﬁcation, when compared to other more\\ntraditional loss functions. Figure 12.4 compares it to the log-likelihood l oss\\nfor logistic regression, as well as squared-error loss and a variant thereo f.\\nThe (negative) log-likelihood or binomial deviance has similar tails as the\\nSVM loss, giving zero penalty to points well inside their margin, and a', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c167a486-d3de-453f-99fd-0d2789002108', embedding=None, metadata={'page_label': '446', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Support Vector Machines and Kernels 427\\nTABLE 12.1. The population minimizers for the diﬀerent loss functions in Fig -\\nure 12.4. Logistic regression uses the binomial log-likelih ood or deviance. Linear\\ndiscriminant analysis (Exercise 4.2) uses squared-error loss. The SVM hinge loss\\nestimates the mode of the posterior class probabilities, wh ereas the others estimate\\na linear transformation of these probabilities.\\nLoss Function L[y, f(x)] Minimizing Function\\nBinomial\\nDeviance log[1 + e−yf(x)]f(x) = logPr(Y= +1|x)\\nPr(Y= -1|x)\\nSVM Hinge\\nLoss[1−yf(x)]+ f(x) = sign[Pr( Y= +1|x)−1\\n2]\\nSquared\\nError[y−f(x)]2= [1−yf(x)]2f(x) = 2Pr( Y= +1|x)−1\\n“Huberised”\\nSquare\\nHinge Loss−4yf(x), yf (x)<-1\\n[1−yf(x)]2\\n+otherwisef(x) = 2Pr( Y= +1|x)−1\\nlinear penalty to points on the wrong side and far away. Squared-error, on\\nthe other hand gives a quadratic penalty, and points well inside their own\\nmargin have a strong inﬂuence on the model as well. The squared hinge\\nlossL(y,f) = [1 −yf]2\\n+is like the quadratic, except it is zero for points\\ninside their margin. It still rises quadratically in the left tail, and wil l be\\nless robust than hinge or deviance to misclassiﬁed observations. Recently\\nRosset and Zhu (2007) proposed a “Huberized” version of the squared hinge\\nloss, which converts smoothly to a linear loss at yf=−1.\\nWe can characterize these loss functions in terms of what they are es-\\ntimating at the population level. We consider minimizing E L(Y,f(X)).\\nTable 12.1 summarizes the results. Whereas the hinge loss estimates the\\nclassiﬁer G(x) itself, all the others estimate a transformation of the class\\nposterior probabilities. The “Huberized” square hinge loss shares attractive\\nproperties of logistic regression (smooth loss function, estimates proba bili-\\nties), as well as the SVM hinge loss (support points).\\nFormulation (12.25) casts the SVM as a regularized function estimation\\nproblem, where the coeﬃcients of the linear expansion f(x) =β0+h(x)Tβ\\nare shrunk toward zero (excluding the constant). If h(x) represents a hierar-\\nchical basis having some ordered structure (such as ordered in roughness),', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aae3c77a-1ca8-4236-a185-1040e46e1bdf', embedding=None, metadata={'page_label': '447', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='428 12. Flexible Discriminants\\nthen the uniform shrinkage makes more sense if the rougher elements hjin\\nthe vector hhave smaller norm.\\nAll the loss-function in Table 12.1 except squared-error are so called\\n“margin maximizing loss-functions” (Rosset et al., 2004b). This means that\\nif the data are separable, then the limit of ˆβλin (12.25) as λ→0 deﬁnes\\nthe optimal separating hyperplane1.\\n12.3.3 Function Estimation and Reproducing Kernels\\nHere we describe SVMs in terms of function estimation in reproducing\\nkernel Hilbert spaces, where the kernel property abounds. This material is\\ndiscussed in some detail in Section 5.8. This provides another view of the\\nsupport vector classiﬁer, and helps to clarify how it works.\\nSuppose the basis harises from the (possibly ﬁnite) eigen-expansion of\\na positive deﬁnite kernel K,\\nK(x,x′) =∞∑\\nm=1φm(x)φm(x′)δm (12.26)\\nandhm(x) =√δmφm(x). Then with θm=√δmβm, we can write (12.25)\\nas\\nmin\\nβ0, θN∑\\ni=1[\\n1−yi(β0+∞∑\\nm=1θmφm(xi))]\\n++λ\\n2∞∑\\nm=1θ2\\nm\\nδm. (12.27)\\nNow (12.27) is identical in form to (5.49) on page 169 in Section 5.8, a nd\\nthe theory of reproducing kernel Hilbert spaces described there guarantees\\na ﬁnite-dimensional solution of the form\\nf(x) =β0+N∑\\ni=1αiK(x,xi). (12.28)\\nIn particular we see there an equivalent version of the optimization crite-\\nrion (12.19) [Equation (5.67) in Section 5.8.2; see also Wahba et al. (2000)],\\nmin\\nβ0,αN∑\\ni=1(1−yif(xi))++λ\\n2αTKα, (12.29)\\nwhereKis the N×Nmatrix of kernel evaluations for all pairs of training\\nfeatures (Exercise 12.2).\\nThese models are quite general, and include, for example, the entire fam-\\nily of smoothing splines, additive and interaction spline models discussed\\n1For logistic regression with separable data, ˆβλdiverges, but ˆβλ/||ˆβλconverges to\\nthe optimal separating direction.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='581f7e1e-8712-4932-bbcc-69b18bad3276', embedding=None, metadata={'page_label': '448', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Support Vector Machines and Kernels 429\\nin Chapters 5 and 9, and in more detail in Wahba (1990) and Hastie and\\nTibshirani (1990). They can be expressed more generally as\\nmin\\nf∈HN∑\\ni=1[1−yif(xi)]++λJ(f), (12.30)\\nwhere His the structured space of functions, and J(f) an appropriate reg-\\nularizer on that space. For example, suppose His the space of additive\\nfunctions f(x) =∑p\\nj=1fj(xj), and J(f) =∑\\nj∫\\n{f′′\\nj(xj)}2dxj. Then the\\nsolution to (12.30) is an additive cubic spline, and has a kernel representa-\\ntion (12.28) with K(x,x′) =∑p\\nj=1Kj(xj,x′\\nj). Each of the Kjis the kernel\\nappropriate for the univariate smoothing spline in xj(Wahba, 1990).\\nConversely this discussion also shows that, for example, anyof the kernels\\ndescribed in (12.22) above can be used with anyconvex loss function, and\\nwill also lead to a ﬁnite-dimensional representation of the form (12.28).\\nFigure 12.5 uses the same kernel functions as in Figure 12.3, except using\\nthe binomial log-likelihood as a loss function2. The ﬁtted function is hence\\nan estimate of the log-odds,\\nˆf(x) = logˆPr(Y= +1|x)\\nˆPr(Y=−1|x)\\n=ˆβ0+N∑\\ni=1ˆαiK(x,xi), (12.31)\\nor conversely we get an estimate of the class probabilities\\nˆPr(Y= +1|x) =1\\n1 +e−ˆβ0−PN\\ni=1ˆαiK(x,xi). (12.32)\\nThe ﬁtted models are quite similar in shape and performance. Examples\\nand more details are given in Section 5.8.\\nIt does happen that for SVMs, a sizable fraction of the Nvalues of αi\\ncan be zero (the nonsupport points). In the two examples in Figure 12.3,\\nthese fractions are 42% and 45%, respectively. This is a consequence of the\\npiecewise linear nature of the ﬁrst part of the criterion (12.25). The lower\\nthe class overlap (on the training data), the greater this fraction will be.\\nReducing λwill generally reduce the overlap (allowing a more ﬂexible f).\\nA small number of support points means that ˆf(x) can be evaluated more\\nquickly, which is important at lookup time. Of course, reducing the overlap\\ntoo much can lead to poor generalization.\\n2Ji Zhu assisted in the preparation of these examples.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3ab7c29c-e628-4e87-96d7-a552074e9c96', embedding=None, metadata={'page_label': '449', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='430 12. Flexible Discriminants\\nLR - Degree-4 Polynomial in Feature Space\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . .. . . . . . .. . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.190\\nTest Error:       0.263\\nBayes Error:    0.210\\nLR - Radial Kernel in Feature Space\\n.. . . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.150\\nTest Error:       0.221\\nBayes Error:    0.210\\nFIGURE 12.5. The logistic regression versions of the SVM models in Fig-\\nure 12.3, using the identical kernels and hence penalties, but the l og-likelihood\\nloss instead of the SVM loss function. The two broken contours corr espond to\\nposterior probabilities of 0.75and0.25for the +1 class (or vice versa). The bro-\\nken purple curve in the background is the Bayes decision bounda ry.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='230f8b77-b57a-4951-b600-4ecf91ee0683', embedding=None, metadata={'page_label': '450', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Support Vector Machines and Kernels 431\\nTABLE 12.2. Skin of the orange: Shown are mean (standard error of the mean)\\nof the test error over 50simulations. BRUTO ﬁts an additive spline model adap-\\ntively, while MARS ﬁts a low-order interaction model adaptivel y.\\nTest Error (SE)\\nMethod No Noise Features Six Noise Features\\n1 SV Classiﬁer 0.450 (0.003) 0.472 (0.003)\\n2 SVM/poly 2 0.078 (0.003) 0.152 (0.004)\\n3 SVM/poly 5 0.180 (0.004) 0.370 (0.004)\\n4 SVM/poly 10 0.230 (0.003) 0.434 (0.002)\\n5 BRUTO 0.084 (0.003) 0.090 (0.003)\\n6 MARS 0.156 (0.004) 0.173 (0.005)\\nBayes 0.029 0.029\\n12.3.4 SVMs and the Curse of Dimensionality\\nIn this section, we address the question of whether SVMs have some edge\\non the curse of dimensionality. Notice that in expression (12.23) we are not\\nallowed a fully general inner product in the space of powers and products.\\nFor example, all terms of the form 2 XjX′\\njare given equal weight, and the\\nkernel cannot adapt itself to concentrate on subspaces. If the number of\\nfeatures pwere large, but the class separation occurred only in the linear\\nsubspace spanned by say X1andX2, this kernel would not easily ﬁnd the\\nstructure and would suﬀer from having many dimensions to search over.\\nOne would have to build knowledge about the subspace into the kernel;\\nthat is, tell it to ignore all but the ﬁrst two inputs. If such knowledge were\\navailable a priori, much of statistical learning would be made much easier .\\nA major goal of adaptive methods is to discover such structure.\\nWe support these statements with an illustrative example. We generated\\n100 observations in each of two classes. The ﬁrst class has four standard\\nnormal independent features X1,X2,X3,X4. The second class also has four\\nstandard normal independent features, but conditioned on 9 ≤∑X2\\nj≤16.\\nThis is a relatively easy problem. As a second harder problem, we aug-\\nmented the features with an additional six standard Gaussian noise fea-\\ntures. Hence the second class almost completely surrounds the ﬁrst, like the\\nskin surrounding the orange, in a four-dimensional subspace. The Bayes er-\\nror rate for this problem is 0 .029 (irrespective of dimension). We generated\\n1000 test observations to compare diﬀerent procedures. The average test\\nerrors over 50 simulations, with and without noise features, are shown in\\nTable 12.2.\\nLine 1 uses the support vector classiﬁer in the original feature space.\\nLines 2–4 refer to the support vector machine with a 2-, 5- and 10-dimension-\\nal polynomial kernel. For all support vector procedures, we chose the cost\\nparameter Cto minimize the test error, to be as fair as possible to the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='24222083-f03d-4a01-aa8b-94e2ea2ae365', embedding=None, metadata={'page_label': '451', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='432 12. Flexible Discriminants\\n1e−01 1e+01 1e+030.20 0.25 0.30 0.35\\n1e−01 1e+01 1e+03 1e−01 1e+01 1e+03 1e−01 1e+01 1e+03Test Error\\nCTest Error Curves − SVM with Radial Kernel\\nγ= 5 γ= 1 γ= 0.5 γ= 0.1\\nFIGURE 12.6. Test-error curves as a function of the cost parameter Cfor the\\nradial-kernel SVM classiﬁer on the mixture data. At the top of eac h plot is the\\nscale parameter γfor the radial kernel: Kγ(x, y) = exp −γ||x−y||2. The optimal\\nvalue for Cdepends quite strongly on the scale of the kernel. The Bayes erro r\\nrate is indicated by the broken horizontal lines.\\nmethod. Line 5 ﬁts an additive spline model to the ( −1,+1) response by\\nleast squares, using the BRUTO algorithm for additive models, described\\nin Hastie and Tibshirani (1990). Line 6 uses MARS (multivariate adaptiv e\\nregression splines) allowing interaction of all orders, as described in Chap-\\nter 9; as such it is comparable with the SVM/poly 10. Both BRUTO and\\nMARS have the ability to ignore redundant variables. Test error was not\\nused to choose the smoothing parameters in either of lines 5 or 6.\\nIn the original feature space, a hyperplane cannot separate the classes,\\nand the support vector classiﬁer (line 1) does poorly. The polynomial sup-\\nport vector machine makes a substantial improvement in test error rate,\\nbut is adversely aﬀected by the six noise features. It is also very sensitive to\\nthe choice of kernel: the second degree polynomial kernel (line 2) does best,\\nsince the true decision boundary is a second-degree polynomial. However,\\nhigher-degree polynomial kernels (lines 3 and 4) do much worse. BRUTO\\nperforms well, since the boundary is additive. BRUTO and MARS adapt\\nwell: their performance does not deteriorate much in the presence of noise.\\n12.3.5 A Path Algorithm for the SVM Classiﬁer\\nThe regularization parameter for the SVM classiﬁer is the cost parameter\\nC, or its inverse λin (12.25). Common usage is to set Chigh, leading often\\nto somewhat overﬁt classiﬁers.\\nFigure 12.6 shows the test error on the mixture data as a function of\\nC, using diﬀerent radial-kernel parameters γ. When γ= 5 (narrow peaked\\nkernels), the heaviest regularization (small C) is called for. With γ= 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5cc5b98b-d4c1-4451-85c8-c7eefcfa093d', embedding=None, metadata={'page_label': '452', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Support Vector Machines and Kernels 433\\n−0.5 0.0 0.5 1.0 1.5 2.0−1.0 −0.5 0.0 0.5 1.0 1.5789\\n1011\\n12\\n123\\n45\\n61/||β|| f(x) = 0f(x) = +1\\nf(x) =−1\\n0.0 0.2 0.4 0.6 0.8 1.00 2 4 6 8 101\\n2\\n34\\n56\\n789\\n1011\\n12\\nαi(λ)λ\\nFIGURE 12.7. A simple example illustrates the SVM path algorithm. (left\\npanel:) This plot illustrates the state of the model at λ= 0.05. The ‘ ‘ + 1”\\npoints are orange, the “ −1” blue. λ= 1/2, and the width of the soft margin\\nis2/||β||= 2×0.587. Two blue points {3,5}are misclassiﬁed, while the two or-\\nange points {10,12}are correctly classiﬁed, but on the wrong side of their margin\\nf(x) = +1 ; each of these has yif(xi)<1. The three square shaped points {2,6,7}\\nare exactly on their margins. (right panel:) This plot shows the piecewise linear\\nproﬁles αi(λ). The horizontal broken line at λ= 1/2indicates the state of the αi\\nfor the model in the left plot.\\n(the value used in Figure 12.3), an intermediate value of Cis required.\\nClearly in situations such as these, we need to determine a good choice\\nforC, perhaps by cross-validation. Here we describe a path algorithm (in\\nthe spirit of Section 3.8) for eﬃciently ﬁtting the entire sequence of SVM\\nmodels obtained by varying C.\\nIt is convenient to use the loss+penalty formulation (12.25), along with\\nFigure 12.4. This leads to a solution for βat a given value of λ:\\nβλ=1\\nλN∑\\ni=1αiyixi. (12.33)\\nTheαiare again Lagrange multipliers, but in this case they all lie in [0 ,1].\\nFigure 12.7 illustrates the setup. It can be shown that the KKT optimal-\\nity conditions imply that the labeled points ( xi,yi) fall into three distinct\\ngroups:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ba0e131-0e1c-458b-86ad-e9515406ccf5', embedding=None, metadata={'page_label': '453', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='434 12. Flexible Discriminants\\n•Observations correctly classiﬁed and outside their margins. They have\\nyif(xi)>1, and Lagrange multipliers αi= 0. Examples are the\\norange points 8, 9 and 11, and the blue points 1 and 4.\\n•Observations sitting on their margins with yif(xi) = 1, with Lagrange\\nmultipliers αi∈[0,1]. Examples are the orange 7 and the blue 2 and\\n8.\\n•Observations inside their margins have yif(xi)<1, with αi= 1.\\nExamples are the blue 3 and 5, and the orange 10 and 12.\\nThe idea for the path algorithm is as follows. Initially λis large, the\\nmargin 1 /||βλ||is wide, and all points are inside their margin and have\\nαi= 1. As λdecreases, 1 /||βλ||decreases, and the margin gets narrower.\\nSome points will move from inside their margins to outside their margins,\\nand their αiwill change from 1 to 0. By continuity of the αi(λ), these points\\nwilllinger on the margin during this transition. From (12.33) we see that\\nthe points with αi= 1 make ﬁxed contributions to β(λ), and those with\\nαi= 0 make no contribution. So all that changes as λdecreases are the\\nαi∈[0,1] of those (small number) of points on the margin. Since all these\\npoints have yif(xi) = 1, this results in a small set of linear equations that\\nprescribe how αi(λ) and hence βλchanges during these transitions. This\\nresults in piecewise linear paths for each of the αi(λ). The breaks occur\\nwhen points cross the margin. Figure 12.7 (right panel) shows the αi(λ)\\nproﬁles for the small example in the left panel.\\nAlthough we have described this for linear SVMs, exactly the same idea\\nworks for nonlinear models, in which (12.33) is replaced by\\nfλ(x) =1\\nλN∑\\ni=1αiyiK(x,xi). (12.34)\\nDetails can be found in Hastie et al. (2004). An Rpackagesvmpath is\\navailable on CRAN for ﬁtting these models.\\n12.3.6 Support Vector Machines for Regression\\nIn this section we show how SVMs can be adapted for regression with a\\nquantitative response, in ways that inherit some of the properties of the\\nSVM classiﬁer. We ﬁrst discuss the linear regression model\\nf(x) =xTβ+β0, (12.35)\\nand then handle nonlinear generalizations. To estimate β, we consider min-\\nimization of\\nH(β,β0) =N∑\\ni=1V(yi−f(xi)) +λ\\n2∥β∥2, (12.36)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d637ecca-74a0-49fe-ad2d-15c5d1f30700', embedding=None, metadata={'page_label': '454', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Support Vector Machines and Kernels 435\\n-4 -2 0 2 4-1 0 1 2 3 4\\n-4 -2 0 2 40 2 4 6 8 10 12\\nǫ −ǫ c −cVH(r)Vǫ(r)\\nr r\\nFIGURE 12.8. The left panel shows the ǫ-insensitive error function used by the\\nsupport vector regression machine. The right panel shows the e rror function used\\nin Huber’s robust regression (blue curve). Beyond |c|, the function changes from\\nquadratic to linear.\\nwhere\\nVǫ(r) ={\\n0 if |r|< ǫ,\\n|r| −ǫ,otherwise.(12.37)\\nThis is an “ ǫ-insensitive” error measure, ignoring errors of size less than\\nǫ(left panel of Figure 12.8). There is a rough analogy with the support\\nvector classiﬁcation setup, where points on the correct side of the deci-\\nsion boundary and far away from it, are ignored in the optimization. In\\nregression, these “low error” points are the ones with small residuals.\\nIt is interesting to contrast this with error measures used in robust re-\\ngression in statistics. The most popular, due to Huber (1964), has the for m\\nVH(r) ={\\nr2/2 if |r| ≤c,\\nc|r| −c2/2,|r|> c,(12.38)\\nshown in the right panel of Figure 12.8. This function reduces from quadratic\\nto linear the contributions of observations with absolute residual greater\\nthan a prechosen constant c. This makes the ﬁtting less sensitive to out-\\nliers. The support vector error measure (12.37) also has linear tails (beyo nd\\nǫ), but in addition it ﬂattens the contributions of those cases with small\\nresiduals.\\nIfˆβ,ˆβ0are the minimizers of H, the solution function can be shown to\\nhave the form\\nˆβ=N∑\\ni=1(ˆα∗\\ni−ˆαi)xi, (12.39)\\nˆf(x) =N∑\\ni=1(ˆα∗\\ni−ˆαi)⟨x,xi⟩+β0, (12.40)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec7225ec-cb41-41f0-83b2-8aef32eb7439', embedding=None, metadata={'page_label': '455', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='436 12. Flexible Discriminants\\nwhere ˆ αi,ˆα∗\\niare positive and solve the quadratic programming problem\\nmin\\nαi,α∗\\niǫN∑\\ni=1(α∗\\ni+αi)−N∑\\ni=1yi(α∗\\ni−αi) +1\\n2N∑\\ni,i′=1(α∗\\ni−αi)(α∗\\ni′−αi′)⟨xi,xi′⟩\\nsubject to the constraints\\n0≤αi, α∗\\ni≤1/λ,\\nN∑\\ni=1(α∗\\ni−αi) = 0, (12.41)\\nαiα∗\\ni= 0.\\nDue to the nature of these constraints, typically only a subset of the solution\\nvalues (ˆ α∗\\ni−ˆαi) are nonzero, and the associated data values are called the\\nsupport vectors. As was the case in the classiﬁcation setting, the solution\\ndepends on the input values only through the inner products ⟨xi,xi′⟩. Thus\\nwe can generalize the methods to richer spaces by deﬁning an appropriate\\ninner product, for example, one of those deﬁned in (12.22).\\nNote that there are parameters, ǫandλ, associated with the criterion\\n(12.36). These seem to play diﬀerent roles. ǫis a parameter of the loss\\nfunction Vǫ, just like cis for VH. Note that both VǫandVHdepend on the\\nscale of yand hence r. If we scale our response (and hence use VH(r/σ) and\\nVǫ(r/σ) instead), then we might consider using preset values for candǫ(the\\nvalue c= 1.345 achieves 95% eﬃciency for the Gaussian). The quantity λ\\nis a more traditional regularization parameter, and can be estimated for\\nexample by cross-validation.\\n12.3.7 Regression and Kernels\\nAs discussed in Section 12.3.3, this kernel property is not unique to sup-\\nport vector machines. Suppose we consider approximation of the regression\\nfunction in terms of a set of basis functions {hm(x)},m= 1,2,... ,M :\\nf(x) =M∑\\nm=1βmhm(x) +β0. (12.42)\\nTo estimate βandβ0we minimize\\nH(β,β0) =N∑\\ni=1V(yi−f(xi)) +λ\\n2∑\\nβ2\\nm (12.43)\\nfor some general error measure V(r). For any choice of V(r), the solution\\nˆf(x) =∑ˆβmhm(x) +ˆβ0has the form\\nˆf(x) =N∑\\ni=1ˆaiK(x,xi) (12.44)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bab80eb0-3623-4626-be5b-76d6600ab12c', embedding=None, metadata={'page_label': '456', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Support Vector Machines and Kernels 437\\nwithK(x,y) =∑M\\nm=1hm(x)hm(y). Notice that this has the same form\\nas both the radial basis function expansion and a regularization estimate,\\ndiscussed in Chapters 5 and 6.\\nFor concreteness, let’s work out the case V(r) =r2. LetHbe the N×M\\nbasis matrix with imth element hm(xi), and suppose that M > N is large.\\nFor simplicity we assume that β0= 0, or that the constant is absorbed in\\nh; see Exercise 12.3 for an alternative.\\nWe estimate βby minimizing the penalized least squares criterion\\nH(β) = (y−Hβ)T(y−Hβ) +λ∥β∥2. (12.45)\\nThe solution is\\nˆy=Hˆβ (12.46)\\nwithˆβdetermined by\\n−HT(y−Hˆβ) +λˆβ= 0. (12.47)\\nFrom this it appears that we need to evaluate the M×Mmatrix of inner\\nproducts in the transformed space. However, we can premultiply by Hto\\ngive\\nHˆβ= (HHT+λI)−1HHTy. (12.48)\\nTheN×Nmatrix HHTconsists of inner products between pairs of obser-\\nvations i,i′; that is, the evaluation of an inner product kernel {HHT}i,i′=\\nK(xi,xi′). It is easy to show (12.44) directly in this case, that the predicted\\nvalues at an arbitrary xsatisfy\\nˆf(x) = h(x)Tˆβ\\n=N∑\\ni=1ˆαiK(x,xi), (12.49)\\nwhere ˆ α= (HHT+λI)−1y. As in the support vector machine, we need not\\nspecify or evaluate the large set of functions h1(x),h2(x),... ,h M(x). Only\\nthe inner product kernel K(xi,xi′) need be evaluated, at the Ntraining\\npoints for each i,i′and at points xfor predictions there. Careful choice\\nofhm(such as the eigenfunctions of particular, easy-to-evaluate kernels\\nK) means, for example, that HHTcan be computed at a cost of N2/2\\nevaluations of K, rather than the direct cost N2M.\\nNote, however, that this property depends on the choice of squared norm\\n∥β∥2in the penalty. It does not hold, for example, for the L1norm |β|,\\nwhich may lead to a superior model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='71ab91f3-92ac-4a18-922a-195486b3bedd', embedding=None, metadata={'page_label': '457', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='438 12. Flexible Discriminants\\n12.3.8 Discussion\\nThe support vector machine can be extended to multiclass problems, es-\\nsentially by solving many two-class problems. A classiﬁer is built for each\\npair of classes, and the ﬁnal classiﬁer is the one that dominates the most\\n(Kressel, 1999; Friedman, 1996; Hastie and Tibshirani, 1998). Alternati vely,\\none could use the multinomial loss function along with a suitable kernel,\\nas in Section 12.3.3. SVMs have applications in many other supervised\\nand unsupervised learning problems. At the time of this writing, empirical\\nevidence suggests that it performs well in many real learning problems.\\nFinally, we mention the connection of the support vector machine and\\nstructural risk minimization (7.9). Suppose the training points (or their\\nbasis expansion) are contained in a sphere of radius R, and let G(x) =\\nsign[f(x)] = sign[ βTx+β0] as in (12.2). Then one can show that the class\\nof functions {G(x),∥β∥ ≤A}has VC-dimension hsatisfying\\nh≤R2A2. (12.50)\\nIff(x) separates the training data, optimally for ∥β∥ ≤A, then with\\nprobability at least 1 −ηover training sets (Vapnik, 1996, page 139):\\nError Test≤4h[log (2 N/h) + 1]−log (η/4)\\nN. (12.51)\\nThe support vector classiﬁer was one of the ﬁrst practical learning pro-\\ncedures for which useful bounds on the VC dimension could be obtained,\\nand hence the SRM program could be carried out. However in the deriva-\\ntion, balls are put around the data points—a process that depends on the\\nobserved values of the features. Hence in a strict sense, the VC complexity\\nof the class is not ﬁxed a priori, before seeing the features.\\nThe regularization parameter Ccontrols an upper bound on the VC\\ndimension of the classiﬁer. Following the SRM paradigm, we could choose C\\nby minimizing the upper bound on the test error, given in (12.51). However,\\nit is not clear that this has any advantage over the use of cross-validation\\nfor choice of C.\\n12.4 Generalizing Linear Discriminant Analysis\\nIn Section 4.3 we discussed linear discriminant analysis (LDA), a funda-\\nmental tool for classiﬁcation. For the remainder of this chapter we discuss\\na class of techniques that produce better classiﬁers than LDA by directly\\ngeneralizing LDA.\\nSome of the virtues of LDA are as follows:\\n•It is a simple prototype classiﬁer. A new observation is classiﬁed to the\\nclass with closest centroid. A slight twist is that distance is measured\\nin the Mahalanobis metric, using a pooled covariance estimate.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80b63cc0-2b4b-46a8-bf53-93ecae19e1f5', embedding=None, metadata={'page_label': '458', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.4 Generalizing Linear Discriminant Analysis 439\\n•LDA is the estimated Bayes classiﬁer if the observations are multi-\\nvariate Gaussian in each class, with a common covariance matrix.\\nSince this assumption is unlikely to be true, this might not seem to\\nbe much of a virtue.\\n•The decision boundaries created by LDA are linear, leading to deci-\\nsion rules that are simple to describe and implement.\\n•LDA provides natural low-dimensional views of the data. For exam-\\nple, Figure 12.12 is an informative two-dimensional view of data in\\n256 dimensions with ten classes.\\n•Often LDA produces the best classiﬁcation results, because of its\\nsimplicity and low variance. LDA was among the top three classiﬁers\\nfor 11 of the 22 datasets studied in the STATLOG project (Michie et\\nal., 1994)3.\\nUnfortunately the simplicity of LDA causes it to fail in a number of situa-\\ntions as well:\\n•Often linear decision boundaries do not adequately separate the classes.\\nWhen Nis large, it is possible to estimate more complex decision\\nboundaries. Quadratic discriminant analysis (QDA) is often useful\\nhere, and allows for quadratic decision boundaries. More generally\\nwe would like to be able to model irregular decision boundaries.\\n•The aforementioned shortcoming of LDA can often be paraphrased\\nby saying that a single prototype per class is insuﬃcient. LDA uses\\na single prototype (class centroid) plus a common covariance matrix\\nto describe the spread of the data in each class. In many situations,\\nseveral prototypes are more appropriate.\\n•At the other end of the spectrum, we may have way too many (corre-\\nlated) predictors, for example, in the case of digitized analogue signals\\nand images. In this case LDA uses too many parameters, which are\\nestimated with high variance, and its performance suﬀers. In cases\\nsuch as this we need to restrict or regularize LDA even further.\\nIn the remainder of this chapter we describe a class of techniques that\\nattend to all these issues by generalizing the LDA model. This is achieved\\nlargely by three diﬀerent ideas.\\nThe ﬁrst idea is to recast the LDA problem as a linear regression problem.\\nMany techniques exist for generalizing linear regression to more ﬂexible,\\nnonparametric forms of regression. This in turn leads to more ﬂexible forms\\nof discriminant analysis, which we call FDA. In most cases of interest, t he\\n3This study predated the emergence of SVMs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5e79bfe6-8922-4e3e-90dd-f201fa22f698', embedding=None, metadata={'page_label': '459', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='440 12. Flexible Discriminants\\nregression procedures can be seen to identify an enlarged set of predictors\\nvia basis expansions. FDA amounts to LDA in this enlarged space, the\\nsame paradigm used in SVMs.\\nIn the case of too many predictors, such as the pixels of a digitized image,\\nwe do not want to expand the set: it is already too large. The second idea is\\nto ﬁt an LDA model, but penalize its coeﬃcients to be smooth or otherwise\\ncoherent in the spatial domain, that is, as an image. We call this procedure\\npenalized discriminant analysis or PDA. With FDA itself, the expanded\\nbasis set is often so large that regularization is also required (again a s in\\nSVMs). Both of these can be achieved via a suitably regularized regression\\nin the context of the FDA model.\\nThe third idea is to model each class by a mixture of two or more Gaus-\\nsians with diﬀerent centroids, but with every component Gaussian, both\\nwithin and between classes, sharing the same covariance matrix. This allows\\nfor more complex decision boundaries, and allows for subspace reduction\\nas in LDA. We call this extension mixture discriminant analysis or MDA.\\nAll three of these generalizations use a common framework by exploiting\\ntheir connection with LDA.\\n12.5 Flexible Discriminant Analysis\\nIn this section we describe a method for performing LDA using linear re-\\ngression on derived responses. This in turn leads to nonparametric and ﬂex-\\nible alternatives to LDA. As in Chapter 4, we assume we have observations\\nwith a quantitative response Gfalling into one of Kclasses G={1,... ,K },\\neach having measured features X. Suppose θ:G ↦→IR1is a function that\\nassigns scores to the classes, such that the transformed class labels are op-\\ntimally predicted by linear regression on X: If our training sample has the\\nform ( gi,xi), i= 1,2,... ,N , then we solve\\nmin\\nβ,θN∑\\ni=1(\\nθ(gi)−xT\\niβ)2, (12.52)\\nwith restrictions on θto avoid a trivial solution (mean zero and unit vari-\\nance over the training data). This produces a one-dimensional separation\\nbetween the classes.\\nMore generally, we can ﬁnd up to L≤K−1 sets of independent scorings\\nfor the class labels, θ1,θ2,... ,θ L, andLcorresponding linear maps ηℓ(X) =\\nXTβℓ, ℓ= 1,... ,L , chosen to be optimal for multiple regression in IRp. The\\nscores θℓ(g) and the maps βℓare chosen to minimize the average squared\\nresidual,\\nASR=1\\nNL∑\\nℓ=1[N∑\\ni=1(\\nθℓ(gi)−xT\\niβℓ)2]\\n. (12.53)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe220a03-f899-4712-b6d3-9c43ea13183e', embedding=None, metadata={'page_label': '460', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.5 Flexible Discriminant Analysis 441\\nThe set of scores are assumed to be mutually orthogonal and normalized\\nwith respect to an appropriate inner product to prevent trivial zero\\nsolutions.\\nWhy are we going down this road? It can be shown that the sequence\\nof discriminant (canonical) vectors νℓderived in Section 4.3.3 are identical\\nto the sequence βℓup to a constant (Mardia et al., 1979; Hastie et al.,\\n1995). Moreover, the Mahalanobis distance of a test point xto the kth\\nclass centroid ˆ θkis given by\\nδJ(x,ˆθk) =K−1∑\\nℓ=1wℓ(ˆηℓ(x)−¯ηk\\nℓ)2+D(x), (12.54)\\nwhere ¯ ηk\\nℓis the mean of the ˆ ηℓ(xi) in the kth class, and D(x) does not\\ndepend on k. Here wℓare coordinate weights that are deﬁned in terms of\\nthe mean squared residual r2\\nℓof the ℓth optimally scored ﬁt\\nwℓ=1\\nr2\\nℓ(1−r2\\nℓ). (12.55)\\nIn Section 4.3.2 we saw that these canonical distances are all that is needed\\nfor classiﬁcation in the Gaussian setup, with equal covariances in each class.\\nTo summarize:\\nLDA can be performed by a sequence of linear regressions, fol-\\nlowed by classiﬁcation to the closest class centroid in the space\\nof ﬁts. The analogy applies both to the reduced rank version,\\nor the full rank case when L=K−1.\\nThe real power of this result is in the generalizations that it invites. We\\ncan replace the linear regression ﬁts ηℓ(x) =xTβℓby far more ﬂexible,\\nnonparametric ﬁts, and by analogy achieve a more ﬂexible classiﬁer than\\nLDA. We have in mind generalized additive ﬁts, spline functions, MARS\\nmodels and the like. In this more general form the regression problems are\\ndeﬁned via the criterion\\nASR({θℓ,ηℓ}L\\nℓ=1) =1\\nNL∑\\nℓ=1[N∑\\ni=1(θℓ(gi)−ηℓ(xi))2+λJ(ηℓ)]\\n,(12.56)\\nwhere Jis a regularizer appropriate for some forms of nonparametric regres-\\nsion, such as smoothing splines, additive splines and lower-order ANOVA\\nspline models. Also included are the classes of functions and associated\\npenalties generated by kernels, as in Section 12.3.3.\\nBefore we describe the computations involved in this generalization, let\\nus consider a very simple example. Suppose we use degree-2 polynomial\\nregression for each ηℓ. The decision boundaries implied by the (12.54) will\\nbe quadratic surfaces, since each of the ﬁtted functions is quadratic, and as', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='758335d6-2a51-4c72-bad6-8c20e3c21894', embedding=None, metadata={'page_label': '461', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='442 12. Flexible Discriminants\\n-2 0 2-2 0 2o\\nooo\\noo\\nooo\\noo\\no o\\no\\noooo\\noo\\noo oo\\no\\no\\nooo\\noooo\\noo\\nooo\\noo\\noo\\noo\\noo\\nooo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\nooo\\noo o\\noo\\nooo\\noo\\no\\nFIGURE 12.9. The data consist of 50points generated from each of N(0, I)and\\nN(0,9\\n4I). The solid black ellipse is the decision boundary found by FDA u sing\\ndegree-two polynomial regression. The dashed purple circle i s the Bayes decision\\nboundary.\\nin LDA their squares cancel out when comparing distances. We could have\\nachieved identical quadratic boundaries in a more conventional way, by\\naugmenting our original predictors with their squares and cross-products.\\nIn the enlarged space one performs an LDA, and the linear boundaries in\\nthe enlarged space map down to quadratic boundaries in the original space.\\nA classic example is a pair of multivariate Gaussians centered at the origi n,\\none having covariance matrix I, and the other cIforc >1; Figure 12.9\\nillustrates. The Bayes decision boundary is the sphere ∥x∥=pclogc\\n2(c−1), which\\nis a linear boundary in the enlarged space.\\nMany nonparametric regression procedures operate by generating a basis\\nexpansion of derived variables, and then performing a linear regression in\\nthe enlarged space. The MARS procedure (Chapter 9) is exactly of this\\nform. Smoothing splines and additive spline models generate an extremely\\nlarge basis set ( N×pbasis functions for additive splines), but then perform\\na penalized regression ﬁt in the enlarged space. SVMs do as well; see also\\nthe kernel-based regression example in Section 12.3.7. FDA in this case can\\nbe shown to perform a penalized linear discriminant analysis in the enlarged\\nspace. We elaborate in Section 12.6. Linear boundaries in the enlarged space\\nmap down to nonlinear boundaries in the reduced space. This is exactly the\\nsame paradigm that is used with support vector machines (Section 12.3).\\nWe illustrate FDA on the speech recognition example used in Chapter\\n4.), with K= 11 classes and p= 10 predictors. The classes correspond to', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b545e09d-ce58-4ce9-a34d-65de896c735b', embedding=None, metadata={'page_label': '462', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.5 Flexible Discriminant Analysis 443\\noooo oo\\noooooooo\\noooo\\noo\\no\\noo\\no\\noooooooooooo\\nooo\\no\\noooooooo\\nooooooo\\no\\noo\\no\\nooooo\\no\\nooooooo\\noooooo\\no\\no\\nooooooooooooo\\nooooooooooo\\noooo\\noooo\\no\\nooooo\\noooooo o\\no\\noooooooooooo\\noo\\no\\noo\\nooooooooooooo\\noooooooooooooooooooooooooooooo\\no\\no\\nooooooo\\no\\no\\nooooooo\\noooooooooooo\\no\\nooooooooooooooooo\\noo\\no\\no\\noooooooo\\nooooooooooooooooo\\noooo\\no\\no\\nooooo\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\nooooooooooooo\\noo\\noooooo\\nooooooooo\\nooo\\nooooooooo\\no\\noo\\no\\nooo\\nooooooooooooooo\\nooo\\noooooooo\\noooo\\nooooooooooooo\\no\\noooo\\nooooo\\noooooo\\noo\\no\\no\\no\\no\\noooooo\\no\\noooooo\\noooo\\no\\nooooooooooooo\\no ooooooooooooooooo\\no\\no\\nooooo\\nooooooooooo\\no\\no\\nooo\\nooooooo\\noooooo\\noooooo\\noooooooooo\\no\\nooooooo\\noooooooooooo\\noo\\no\\nooo\\nCoordinate 1 for Training DataCoordinate 2 for Training DataLinear Discriminant Analysis\\noo oooooo\\noooo\\noooooooooooo\\noooooo\\no\\nooo\\no\\noooooo\\noooo\\nooo\\nooooooo\\nooooo\\noooooo\\nooooooooooo\\nooooooo\\nooooooo\\no\\noooo\\noooooo\\nooo ooooooooo\\noooooooo\\noooo\\noooooo\\no\\noooo\\nooooooo\\noooooo\\noooooooooooooooooo\\noooooooooooooooooooooooooooo\\nooooooooooooo\\nooo\\no\\no oooooooooooooooooooo\\noooo\\no\\nooooooo\\noo\\noooo\\nooo\\noo\\nooooo\\nooo\\no\\noooo ooooooooo\\no\\no\\noooo\\noooooooooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\no\\no\\noooooo\\no\\noooo\\no\\noooooo\\nooo\\nooooooo\\nooo\\nooooooooo\\noooo\\no\\nooo\\nooooo\\no\\nooooooo\\no\\noooooo\\no\\noooo\\noooo\\no\\nooo\\noooo\\no\\noooooo\\no\\noooooooooooo\\noo\\no\\nooo\\no\\no\\no\\noooooo\\no\\noooooooooooooooooo\\noooooooo\\noooo\\noooooo\\nooooooooo\\no\\noo\\nooooooo\\noooooo\\no\\noooo\\nooooooooooo\\nooooo oo\\noooo\\noooo\\noooo\\nCoordinate 1 for Training DataCoordinate 2 for Training DataFlexible Discriminant Analysis -- Bruto\\nFIGURE 12.10. The left plot shows the ﬁrst two LDA canonical variates for\\nthe vowel training data. The right plot shows the corresponding p rojection when\\nFDA/BRUTO is used to ﬁt the model; plotted are the ﬁtted regress ion functions\\nˆη1(xi)andˆη2(xi). Notice the improved separation. The colors represent the ele ven\\ndiﬀerent vowel sounds.\\n11 vowel sounds, each contained in 11 diﬀerent words. Here are the words,\\npreceded by the symbols that represent them:\\nVowel Word Vowel Word Vowel Word Vowel Word\\ni: heed O hod I hid C: hoard\\nE head U hood A had u: who’d\\na: hard 3: heard Y hud\\nEach of eight speakers spoke each word six times in the training set, and\\nlikewise seven speakers in the test set. The ten predictors are derived from\\nthe digitized speech in a rather complicated way, but standard in the speech\\nrecognition world. There are thus 528 training observations, and 462 test\\nobservations. Figure 12.10 shows two-dimensional projections produced by\\nLDA and FDA. The FDA model used adaptive additive-spline regression\\nfunctions to model the ηℓ(x), and the points plotted in the right plot have\\ncoordinates ˆ η1(xi) and ˆ η2(xi). The routine used in S-PLUS is called bruto ,\\nhence the heading on the plot and in Table 12.3. We see that ﬂexible model-\\ning has helped to separate the classes in this case. Table 12.3 shows training\\nand test error rates for a number of classiﬁcation techniques. FDA/MARS\\nrefers to Friedman’s multivariate adaptive regression splines; degree = 2\\nmeans pairwise products are permitted. Notice that for FDA/MARS, the\\nbest classiﬁcation results are obtained in a reduced-rank subspace.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58d3d512-9f70-401a-9116-8e0d2a96e43c', embedding=None, metadata={'page_label': '463', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='444 12. Flexible Discriminants\\nTABLE 12.3. Vowel recognition data performance results. The results for ne ural\\nnetworks are the best among a much larger set, taken from a neural network\\narchive. The notation FDA/BRUTO refers to the regression met hod used with\\nFDA.\\nTechnique Error Rates\\nTraining Test\\n(1) LDA 0.32 0.56\\nSoftmax 0.48 0.67\\n(2) QDA 0.01 0.53\\n(3) CART 0.05 0.56\\n(4) CART (linear combination splits) 0.05 0.54\\n(5) Single-layer perceptron 0.67\\n(6) Multi-layer perceptron (88 hidden units) 0.49\\n(7) Gaussian node network (528 hidden units) 0.45\\n(8) Nearest neighbor 0.44\\n(9) FDA/BRUTO 0.06 0.44\\nSoftmax 0.11 0.50\\n(10) FDA/MARS (degree = 1) 0.09 0.45\\nBest reduced dimension (=2) 0.18 0.42\\nSoftmax 0.14 0.48\\n(11) FDA/MARS (degree = 2) 0.02 0.42\\nBest reduced dimension (=6) 0.13 0.39\\nSoftmax 0.10 0.50\\n12.5.1 Computing the FDA Estimates\\nThe computations for the FDA coordinates can be simpliﬁed in many im-\\nportant cases, in particular when the nonparametric regression procedure\\ncan be represented as a linear operator. We will denote this operator by\\nSλ; that is, ˆy=Sλy, where yis the vector of responses and ˆythe vector\\nof ﬁts. Additive splines have this property, if the smoothing parameters are\\nﬁxed, as does MARS once the basis functions are selected. The subscript λ\\ndenotes the entire set of smoothing parameters. In this case optimal scoring\\nis equivalent to a canonical correlation problem, and the solution can be\\ncomputed by a single eigen-decomposition. This is pursued in Exercise 12.6,\\nand the resulting algorithm is presented here.\\nWe create an N×Kindicator response matrix Yfrom the responses gi,\\nsuch that yik= 1 if gi=k, otherwise yik= 0. For a ﬁve-class problem Y\\nmight look like the following:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6747ea35-b632-40aa-9d87-77851b8bd80f', embedding=None, metadata={'page_label': '464', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.5 Flexible Discriminant Analysis 445\\n0\\nBBBBBBBBB@C1C2C3C4C5\\ng1= 2 0 1 0 0 0\\ng2= 1 1 0 0 0 0\\ng3= 1 1 0 0 0 0\\ng4= 5 0 0 0 0 1\\ng5= 4 0 0 0 1 0\\n......\\ngN= 3 0 0 1 0 01\\nCCCCCCCCCA\\nHere are the computational steps:\\n1.Multivariate nonparametric regression. Fit a multiresponse, adaptive\\nnonparametric regression of YonX, giving ﬁtted values ˆY. LetSλ\\nbe the linear operator that ﬁts the ﬁnal chosen model, and η∗(x) be\\nthe vector of ﬁtted regression functions.\\n2.Optimal scores. Compute the eigen-decomposition of YTˆY=YTSλY,\\nwhere the eigenvectors Θare normalized: ΘTDπΘ=I. Here Dπ=\\nYTY/Nis a diagonal matrix of the estimated class prior\\nprobabilities.\\n3.Update the model from step 1 using the optimal scores: η(x) =ΘTη∗(x).\\nThe ﬁrst of the Kfunctions in η(x) is the constant function— a trivial\\nsolution; the remaining K−1 functions are the discriminant functions. The\\nconstant function, along with the normalization, causes all the remaining\\nfunctions to be centered.\\nAgainSλcan correspond to any regression method. When Sλ=HX, the\\nlinear regression projection operator, then FDA is linear discriminant anal-\\nysis. The software that we reference in the Computational Considerations\\nsection on page 455 makes good use of this modularity; the fdafunction\\nhas amethod= argument that allows one to supply anyregression function,\\nas long as it follows some natural conventions. The regression functions\\nwe provide allow for polynomial regression, adaptive additive models and\\nMARS. They all eﬃciently handle multiple responses, so step (1) is a single\\ncall to a regression routine. The eigen-decomposition in step (2) simulta-\\nneously computes all the optimal scoring functions.\\nIn Section 4.2 we discussed the pitfalls of using linear regression on an\\nindicator response matrix as a method for classiﬁcation. In particular, se-\\nvere masking can occur with three or more classes. FDA uses the ﬁts from\\nsuch a regression in step (1), but then transforms them further to produce\\nuseful discriminant functions that are devoid of these pitfalls. Exercise 12.9\\ntakes another view of this phenomenon.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='72858dd7-9f20-431b-9b8f-76a78ff86b36', embedding=None, metadata={'page_label': '465', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='446 12. Flexible Discriminants\\n12.6 Penalized Discriminant Analysis\\nAlthough FDA is motivated by generalizing optimal scoring, it can also be\\nviewed directly as a form of regularized discriminant analysis. Suppose the\\nregression procedure used in FDA amounts to a linear regression onto a\\nbasis expansion h(X), with a quadratic penalty on the coeﬃcients:\\nASR({θℓ,βℓ}L\\nℓ=1) =1\\nNL∑\\nℓ=1[N∑\\ni=1(θℓ(gi)−hT(xi)βℓ)2+λβT\\nℓΩβℓ]\\n.(12.57)\\nThe choice of Ωdepends on the problem. If ηℓ(x) =h(x)βℓis an expansion\\non spline basis functions, Ωmight constrain ηℓto be smooth over IRp. In\\nthe case of additive splines, there are Nspline basis functions for each\\ncoordinate, resulting in a total of Npbasis functions in h(x);Ωin this case\\nisNp×Npand block diagonal.\\nThe steps in FDA can then be viewed as a generalized form of LDA,\\nwhich we call penalized discriminant analysis , or PDA:\\n•Enlarge the set of predictors Xvia a basis expansion h(X).\\n•Use (penalized) LDA in the enlarged space, where the penalized\\nMahalanobis distance is given by\\nD(x,θ) = (h(x)−h(θ))T(ΣW+λΩ)−1(h(x)−h(θ)),(12.58)\\nwhere ΣWis the within-class covariance matrix of the derived vari-\\nables h(xi).\\n•Decompose the classiﬁcation subspace using a penalized metric:\\nmaxuTΣBetusubject to uT(ΣW+λΩ)u= 1.\\nLoosely speaking, the penalized Mahalanobis distance tends to give less\\nweight to “rough” coordinates, and more weight to “smooth” ones; since\\nthe penalty is not diagonal, the same applies to linear combinations that\\nare rough or smooth.\\nFor some classes of problems, the ﬁrst step, involving the basis expansion,\\nis not needed; we already have far too many (correlated) predictors. A\\nleading example is when the objects to be classiﬁed are digitized analog\\nsignals:\\n•the log-periodogram of a fragment of spoken speech, sampled at a set\\nof 256 frequencies; see Figure 5.5 on page 149.\\n•the grayscale pixel values in a digitized image of a handwritten digit.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7dde008c-dcfe-4f2e-8f5f-a74fa398a871', embedding=None, metadata={'page_label': '466', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.6 Penalized Discriminant Analysis 447\\nLDA: Coefficient 1 PDA: Coefficient 1 LDA: Coefficient 2 PDA: Coefficient 2 LDA: Coefficient 3 PDA: Coefficient 3\\nLDA: Coefficient 4 PDA: Coefficient 4 LDA: Coefficient 5 PDA: Coefficient 5 LDA: Coefficient 6 PDA: Coefficient 6\\nLDA: Coefficient 7 PDA: Coefficient 7 LDA: Coefficient 8 PDA: Coefficient 8 LDA: Coefficient 9 PDA: Coefficient 9\\nFIGURE 12.11. The images appear in pairs, and represent the nine discrim-\\ninant coeﬃcient functions for the digit recognition problem. The l eft member of\\neach pair is the LDA coeﬃcient, while the right member is the PD A coeﬃcient,\\nregularized to enforce spatial smoothness.\\nIt is also intuitively clear in these cases why regularization is needed.\\nTake the digitized image as an example. Neighboring pixel values will tend\\nto be correlated, being often almost the same. This implies that the pair\\nof corresponding LDA coeﬃcients for these pixels can be wildly diﬀerent\\nand opposite in sign, and thus cancel when applied to similar pixel values.\\nPositively correlated predictors lead to noisy, negatively correlated coeﬃ-\\ncient estimates, and this noise results in unwanted sampling variance. A\\nreasonable strategy is to regularize the coeﬃcients to be smooth over the\\nspatial domain, as with images. This is what PDA does. The computations\\nproceed just as for FDA, except that an appropriate penalized regression\\nmethod is used. Here hT(X)βℓ=Xβℓ, and Ω is chosen so that βT\\nℓΩβℓ\\npenalizes roughness in βℓwhen viewed as an image. Figure 1.2 on page 4\\nshows some examples of handwritten digits. Figure 12.11 shows the dis-\\ncriminant variates using LDA and PDA. Those produced by LDA appear\\nassalt-and-pepper images, while those produced by PDA are smooth im-\\nages. The ﬁrst smooth image can be seen as the coeﬃcients of a linear\\ncontrast functional for separating images with a dark central vertical stri p\\n(ones, possibly sevens) from images that are hollow in the middle (zeros,\\nsome fours). Figure 12.12 supports this interpretation, and with more di f-\\nﬁculty allows an interpretation of the second coordinate. This and other', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90714705-cd92-4862-bf33-d484d56bd808', embedding=None, metadata={'page_label': '467', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='448 12. Flexible Discriminants\\n-5 0 5-6 -4 -2 0 2 4 60\\n0\\n00\\n0000\\n0\\n0000\\n00\\n00\\n00000\\n00\\n00000\\n0\\n0000\\n000\\n00\\n0\\n00\\n00000\\n0\\n0000\\n0\\n0\\n0000\\n0\\n00000\\n0\\n000\\n00\\n00\\n000\\n000\\n0000\\n00\\n00\\n000\\n00\\n000\\n00\\n000\\n0\\n0000\\n0000\\n000\\n0\\n00\\n00\\n00\\n00\\n00\\n00\\n000\\n00000 000\\n0000\\n00\\n0\\n0000\\n0000\\n000\\n000\\n00 0\\n0\\n0000\\n0\\n00\\n0 000\\n0\\n000\\n00\\n0\\n0\\n000\\n00 00\\n000\\n00\\n0\\n000\\n00\\n0\\n00\\n00 0 0\\n000\\n0\\n00\\n0\\n000000\\n0\\n0 0\\n000\\n00\\n0 00\\n000 0\\n00\\n000\\n00\\n000\\n000\\n0\\n000\\n000\\n000\\n000\\n000\\n00\\n000\\n00\\n00\\n00\\n000\\n0\\n000\\n0000\\n0\\n00 00\\n0000\\n00\\n0\\n000\\n00\\n00\\n0\\n00\\n0\\n0000\\n0\\n000000\\n00\\n000\\n0\\n000\\n000\\n00\\n000\\n000\\n00\\n000\\n00000\\n00\\n0\\n0000\\n00\\n0\\n0\\n011\\n11\\n11\\n11\\n1\\n1111\\n111\\n111\\n11\\n1\\n11\\n11\\n111\\n11\\n111\\n11\\n111\\n11\\n1\\n111\\n1\\n11\\n111\\n1\\n111\\n111\\n111\\n11\\n11\\n1111\\n111\\n11\\n1\\n1\\n111\\n1\\n11\\n1111\\n1\\n111\\n111\\n111\\n11\\n1\\n11111\\n11\\n1\\n11\\n111\\n11111\\n111\\n1\\n11111\\n1\\n111\\n1111\\n11\\n11\\n11\\n11111\\n1111\\n11\\n1\\n11\\n11\\n1\\n1\\n1 1\\n11\\n1\\n111\\n111\\n11\\n1\\n1\\n111\\n1\\n11\\n111\\n111\\n11111\\n111\\n1\\n11111\\n111111\\n111\\n1\\n11\\n11\\n1111\\n111\\n11\\n11\\n11\\n111\\n11\\n1111111\\n1\\n11\\n111\\n111\\n1\\n11111111\\n1\\n111\\n111\\n1\\n22\\n22\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n2\\n22\\n2222\\n222\\n2\\n22\\n2\\n2\\n2\\n2222\\n2222\\n22222\\n22\\n2\\n22\\n22\\n2\\n22\\n222\\n222\\n2\\n2\\n22\\n22\\n2\\n222\\n2\\n2\\n22\\n22\\n2\\n2\\n22\\n22\\n222222\\n222\\n2\\n2\\n2222222\\n22222\\n222\\n222\\n2\\n22222\\n2\\n2\\n2222\\n222\\n22\\n222\\n222\\n2\\n22\\n2\\n2222\\n2\\n222\\n2222\\n22\\n222\\n22\\n222\\n2\\n2\\n222 22\\n2\\n22\\n22\\n22\\n2222\\n22\\n222\\n22\\n22\\n2222\\n333\\n3\\n33\\n3\\n3 3\\n333\\n333\\n333\\n33\\n33\\n3\\n33\\n3\\n3\\n333\\n33333\\n33\\n33\\n3\\n33333\\n33\\n3\\n3\\n333\\n3333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n33\\n3\\n333\\n3\\n3\\n333\\n33\\n333\\n333\\n333\\n3\\n3\\n333\\n33333\\n3\\n3333\\n33\\n3\\n3\\n33\\n3\\n33\\n3333\\n33\\n3333\\n33\\n33\\n333\\n333\\n333\\n33\\n3\\n333333\\n3\\n3\\n33 3\\n333\\n333\\n33\\n33\\n33\\n4\\n4444\\n4\\n4 44444\\n44\\n4\\n44\\n444\\n4\\n444\\n44\\n44\\n4\\n44\\n444\\n44\\n4\\n4\\n44\\n44\\n44\\n44\\n444\\n44444\\n44\\n444\\n4\\n44\\n44\\n4\\n44\\n4\\n44444\\n4\\n444\\n44\\n44\\n4 44\\n4\\n4\\n444\\n4\\n444\\n44\\n4444\\n4\\n4444\\n4444\\n4\\n444\\n44\\n44\\n444\\n44\\n444\\n44\\n44\\n4\\n4\\n44444\\n44\\n4\\n4\\n444\\n44 44\\n44\\n444\\n4\\n44\\n4\\n44\\n4\\n444\\n444444\\n444\\n4444\\n44\\n44\\n4\\n44\\n4\\n444\\n444\\n44\\n444\\n44\\n444\\n4455\\n55\\n5\\n55\\n555\\n5555\\n5\\n5 5\\n555\\n5\\n555\\n55\\n55\\n55\\n5\\n5555\\n555\\n55\\n5555\\n5555\\n5\\n55\\n55\\n55\\n5\\n555\\n55\\n55\\n55555\\n555\\n5 55\\n5\\n5555\\n555\\n5\\n555\\n5\\n555\\n555\\n55\\n555\\n55\\n5\\n555\\n55\\n55\\n555\\n555\\n55\\n55\\n555\\n55\\n5 555\\n55\\n5\\n5\\n5\\n55\\n5\\n555555\\n5\\n55\\n55\\n55\\n55\\n55\\n55\\n555\\n5\\n55\\n6\\n6666\\n66\\n66\\n6\\n66\\n66\\n666\\n6\\n66\\n66\\n6\\n666\\n666\\n6\\n6\\n6\\n666\\n6\\n66\\n66666\\n66\\n6\\n66\\n6\\n666\\n66\\n666\\n66\\n66\\n6\\n66\\n6\\n66\\n666\\n6\\n6\\n66\\n666\\n6\\n6666\\n6\\n666\\n66\\n66\\n666\\n66 6\\n6 66\\n66\\n6\\n666\\n666\\n6\\n6\\n66\\n666\\n666\\n6666\\n6\\n6 66\\n6\\n66\\n6\\n66\\n66\\n6666\\n666\\n66\\n6666\\n666\\n6666\\n66\\n666\\n666\\n66\\n666\\n6\\n6\\n6\\n7\\n7777\\n77\\n777\\n77\\n777\\n777\\n77777\\n777\\n7\\n777\\n77\\n77\\n7\\n777\\n777777\\n7\\n777\\n77\\n7 77\\n7\\n77\\n77\\n77777\\n7\\n77\\n77 7\\n777\\n77\\n7\\n77\\n777\\n77777\\n77\\n77\\n7\\n77777\\n777\\n77\\n7\\n77\\n77\\n777\\n7\\n7\\n7\\n77\\n77\\n77\\n77\\n77\\n7777\\n7\\n77\\n77\\n777\\n77\\n7777\\n77777\\n7\\n7\\n788\\n8\\n8\\n88\\n8888\\n88\\n8\\n8\\n888\\n88\\n8\\n888\\n88\\n88\\n8\\n888\\n8\\n888\\n88\\n88\\n888\\n88\\n888\\n88 888\\n8888\\n88\\n8888\\n88 88\\n88\\n88\\n8888\\n88 8\\n8\\n8 8\\n88\\n888\\n888\\n88\\n8\\n888\\n88\\n888\\n8\\n8\\n88\\n8888\\n888\\n888\\n88\\n8\\n888\\n88\\n88\\n8\\n8\\n88888\\n88\\n8\\n8888888 8\\n8\\n8\\n8\\n88\\n888 888\\n888\\n88\\n8\\n888\\n8\\n8 888\\n9999\\n999\\n999\\n99999\\n999\\n99\\n999\\n999\\n9\\n9\\n99\\n999\\n9\\n9\\n999\\n999\\n999\\n9999\\n9\\n99\\n999\\n99\\n999\\n99\\n9\\n9\\n9999\\n99\\n999\\n99\\n9\\n99\\n9\\n9\\n99\\n99\\n9\\n9\\n9\\n9\\n999\\n999\\n9\\n9\\n99\\n9\\n99\\n9\\n99\\n999\\n99\\n9999\\n99999\\n9\\n999\\n99\\n999\\n99\\n9\\n999\\n9\\n99\\n999\\n9\\n99\\n99\\n99\\n9\\n99\\n99\\n99\\n9\\n99\\n9\\n999\\n99\\n999\\n9\\n9999\\n9\\n99\\n999\\n9901\\n2\\n3\\n456\\n78\\n9\\nPDA: Discriminant Coordinate 1PDA: Discriminant Coordinate 2\\nFIGURE 12.12. The ﬁrst two penalized canonical variates, evaluated for the\\ntest data. The circles indicate the class centroids. The ﬁrst co ordinate contrasts\\nmainly 0’s and 1’s, while the second contrasts 6’s and 7/9’s.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='18e17550-37e8-4a23-b577-4d76f13040f5', embedding=None, metadata={'page_label': '468', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.7 Mixture Discriminant Analysis 449\\nexamples are discussed in more detail in Hastie et al. (1995), who also show\\nthat the regularization improves the classiﬁcation performance of LDA on\\nindependent test data by a factor of around 25% in the cases they tried.\\n12.7 Mixture Discriminant Analysis\\nLinear discriminant analysis can be viewed as a prototype classiﬁer. Each\\nclass is represented by its centroid, and we classify to the closest using an\\nappropriate metric. In many situations a single prototype is not suﬃcient\\nto represent inhomogeneous classes, and mixture models are more appro-\\npriate. In this section we review Gaussian mixture models and show how\\nthey can be generalized via the FDA and PDA methods discussed earlier.\\nA Gaussian mixture model for the kth class has density\\nP(X|G=k) =Rk∑\\nr=1πkrφ(X;θkr,Σ), (12.59)\\nwhere the mixing proportions πkrsum to one. This has Rkprototypes for\\nthekth class, and in our speciﬁcation, the same covariance matrix Σis\\nused as the metric throughout. Given such a model for each class, the class\\nposterior probabilities are given by\\nP(G=k|X=x) =∑Rk\\nr=1πkrφ(X;θkr,Σ)Πk∑K\\nℓ=1∑Rℓ\\nr=1πℓrφ(X;θℓr,Σ)Πℓ, (12.60)\\nwhere Π krepresent the class prior probabilities.\\nWe saw these calculations for the special case of two components in\\nChapter 8. As in LDA, we estimate the parameters by maximum likelihood,\\nusing the joint log-likelihood based on P(G,X):\\nK∑\\nk=1∑\\ngi=klog[Rk∑\\nr=1πkrφ(xi;θkr,Σ)Πk]\\n. (12.61)\\nThe sum within the log makes this a rather messy optimization problem\\nif tackled directly. The classical and natural method for computing the\\nmaximum-likelihood estimates (MLEs) for mixture distributions is the EM\\nalgorithm (Dempster et al., 1977), which is known to possess good conver -\\ngence properties. EM alternates between the two steps:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1e42c618-bf3f-4bdb-8d56-26cc38017ae7', embedding=None, metadata={'page_label': '469', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='450 12. Flexible Discriminants\\nE-step: Given the current parameters, compute the responsibility of sub-\\nclassckrwithin class kfor each of the class- kobservations ( gi=k):\\nW(ckr|xi,gi) =πkrφ(xi;θkr,Σ)∑Rk\\nℓ=1πkℓφ(xi;θkℓ,Σ). (12.62)\\nM-step: Compute the weighted MLEs for the parameters of each of the\\ncomponent Gaussians within each of the classes, using the weights\\nfrom the E-step.\\nIn the E-step, the algorithm apportions the unit weight of an observation\\nin class kto the various subclasses assigned to that class. If it is close to the\\ncentroid of a particular subclass, and far from the others, it will receive a\\nmass close to one for that subclass. On the other hand, observations halfway\\nbetween two subclasses will get approximately equal weight for both.\\nIn the M-step, an observation in class kis used Rktimes, to estimate the\\nparameters in each of the Rkcomponent densities, with a diﬀerent weight\\nfor each. The EM algorithm is studied in detail in Chapter 8. The algorithm\\nrequires initialization, which can have an impact, since mixture likelihoods\\nare generally multimodal. Our software (referenced in the Computational\\nConsiderations on page 455) allows several strategies; here we describe the\\ndefault. The user supplies the number Rkof subclasses per class. Within\\nclassk, ak-means clustering model, with multiple random starts, is ﬁtted\\nto the data. This partitions the observations into Rkdisjoint groups, from\\nwhich an initial weight matrix, consisting of zeros and ones, is created.\\nOur assumption of an equal component covariance matrix Σthroughout\\nbuys an additional simplicity; we can incorporate rank restrictions in the\\nmixture formulation just like in LDA. To understand this, we review a litt le-\\nknown fact about LDA. The rank- LLDA ﬁt (Section 4.3.3) is equivalent to\\nthe maximum-likelihood ﬁt of a Gaussian model,where the diﬀerent mean\\nvectors in each class are conﬁned to a rank- Lsubspace of IRp(Exercise 4.8).\\nWe can inherit this property for the mixture model, and maximize the log-\\nlikelihood (12.61) subject to rank constraints on allthe∑\\nkRkcentroids:\\nrank{θkℓ}=L.\\nAgain the EM algorithm is available, and the M-step turns out to be\\na weighted version of LDA, with R=∑K\\nk=1Rk“classes.” Furthermore,\\nwe can use optimal scoring as before to solve the weighted LDA problem,\\nwhich allows us to use a weighted version of FDA or PDA at this stage.\\nOne would expect, in addition to an increase in the number of “classes,” a\\nsimilar increase in the number of “observations” in the kth class by a factor\\nofRk. It turns out that this is not the case if linear operators are used for\\nthe optimal scoring regression. The enlarged indicator Ymatrix collapses\\nin this case to a blurred response matrix Z, which is intuitively pleasing.\\nFor example, suppose there are K= 3 classes, and Rk= 3 subclasses per\\nclass. Then Zmight be', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27b02aa9-601d-4652-a2f7-4a57cf73567a', embedding=None, metadata={'page_label': '470', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.7 Mixture Discriminant Analysis 451\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edc11c12c13c21c22c23c31c32c33\\ng1= 2 0 0 0 0 .3 0.5 0.2 0 0 0\\ng2= 1 0 .9 0.1 0.0 0 0 0 0 0 0\\ng3= 1 0 .1 0.8 0.1 0 0 0 0 0 0\\ng4= 3 0 0 0 0 0 0 0 .5 0.4 0.1\\ng5= 2 0 0 0 0 .7 0.1 0.2 0 0 0\\n......\\ngN= 3 0 0 0 0 0 0 0 .1 0.1 0.8\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,(12.63)\\nwhere the entries in a class- krow correspond to W(ckr|x,gi).\\nThe remaining steps are the same:\\nˆZ=SZ\\nZTˆZ=ΘDΘT\\nUpdate πs and Πs\\uf8fc\\n\\uf8fd\\n\\uf8feM-step of MDA .\\nThese simple modiﬁcations add considerable ﬂexibility to the mixture\\nmodel:\\n•The dimension reduction step in LDA, FDA or PDA is limited by\\nthe number of classes; in particular, for K= 2 classes no reduction is\\npossible. MDA substitutes subclasses for classes, and then allows us\\nto look at low-dimensional views of the subspace spanned by these\\nsubclass centroids. This subspace will often be an important one for\\ndiscrimination.\\n•By using FDA or PDA in the M-step, we can adapt even more to par-\\nticular situations. For example, we can ﬁt MDA models to digitized\\nanalog signals and images, with smoothness constraints built in.\\nFigure 12.13 compares FDA and MDA on the mixture example.\\n12.7.1 Example: Waveform Data\\nWe now illustrate some of these ideas on a popular simulated example,\\ntaken from Breiman et al. (1984, pages 49–55), and used in Hastie and\\nTibshirani (1996b) and elsewhere. It is a three-class problem with 21 vari-\\nables, and is considered to be a diﬃcult pattern recognition problem. The\\npredictors are deﬁned by\\nXj=Uh1(j) + (1 −U)h2(j) +ǫjClass 1 ,\\nXj=Uh1(j) + (1 −U)h3(j) +ǫjClass 2 , (12.64)\\nXj=Uh2(j) + (1 −U)h3(j) +ǫjClass 3 ,\\nwhere j= 1,2,... ,21,Uis uniform on (0 ,1),ǫjare standard normal vari-\\nates, and the hℓare the shifted triangular waveforms: h1(j) = max(6 −', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f6e6969-4051-47e0-ad49-12c5299ebe98', embedding=None, metadata={'page_label': '471', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='452 12. Flexible Discriminants\\nFDA / MARS - Degree 2\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.185\\nTest Error:       0.235\\nBayes Error:    0.210\\nMDA - 5 Subclasses per Class\\n.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••\\n••\\n•••\\n••••\\n••\\n•••\\n••\\n•\\nTraining Error: 0.17\\nTest Error:       0.22\\nBayes Error:    0.21\\nFIGURE 12.13. FDA and MDA on the mixture data. The upper plot uses\\nFDA with MARS as the regression procedure. The lower plot uses MDA with\\nﬁve mixture centers per class (indicated). The MDA solution is cl ose to Bayes\\noptimal, as might be expected given the data arise from mixture s of Gaussians.\\nThe broken purple curve in the background is the Bayes decisio n boundary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='86573c60-aab0-446a-8e24-3edeebc36ab9', embedding=None, metadata={'page_label': '472', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.7 Mixture Discriminant Analysis 453\\n1 1 1 1 1 1111111111\\n1\\n1\\n1\\n1\\n1\\n1 2 2 2 2 22222222222\\n2\\n2\\n2\\n2\\n2\\n2 3 3 3 3 3333333\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 4 4 4 4 4444444\\n4\\n4\\n4\\n4\\n4\\n44444 5 5 5 5 55555555555\\n5\\n5\\n5\\n5\\n5\\n5Class 1\\n11111111111\\n1\\n1\\n1\\n1\\n1\\n1 1 1 1 1 22222222222\\n2\\n2\\n2\\n2\\n2\\n2 2 2 2 2 33333333333\\n3\\n3\\n3\\n3\\n3\\n3 3 3 3 3 4444444\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4 4 4 4 4 5555555\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5 5 5 5 5Class 2\\n1111111\\n1\\n1\\n1\\n1\\n1\\n111111111 2222222\\n2\\n2\\n2\\n2\\n2\\n222222222 3333333\\n3\\n3333333\\n3\\n3\\n3\\n3\\n3\\n3 4444444\\n4\\n4\\n4\\n4\\n4\\n444444444 5555555\\n5\\n5\\n5\\n5\\n5\\n555555555Class 3\\nFIGURE 12.14. Some examples of the waveforms generated from model (12.64)\\nbefore the Gaussian noise is added.\\n|j−11|,0),h2(j) =h1(j−4) and h3(j) =h1(j+ 4). Figure 12.14 shows\\nsome example waveforms from each class.\\nTable 12.4 shows the results of MDA applied to the waveform data, as\\nwell as several other methods from this and other chapters. Each train-\\ning sample has 300 observations, and equal priors were used, so there are\\nroughly 100 observations in each class. We used test samples of size 500.\\nThe two MDA models are described in the caption.\\nFigure 12.15 shows the leading canonical variates for the penalized MDA\\nmodel, evaluated at the test data. As we might have guessed, the classes\\nappear to lie on the edges of a triangle. This is because the hj(i) are repre-\\nsented by three points in 21-space, thereby forming vertices of a triangle,\\nand each class is represented as a convex combination of a pair of vertices,\\nand hence lie on an edge. Also it is clear visually that all the information\\nlies in the ﬁrst two dimensions; the percentage of variance explained by the\\nﬁrst two coordinates is 99 .8%, and we would lose nothing by truncating the\\nsolution there. The Bayes risk for this problem has been estimated to be\\nabout 0 .14 (Breiman et al., 1984). MDA comes close to the optimal rate,\\nwhich is not surprising since the structure of the MDA model is similar to\\nthe generating model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='185a00cc-4f70-4659-b121-7158e0a888d2', embedding=None, metadata={'page_label': '473', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='454 12. Flexible Discriminants\\nTABLE 12.4. Results for waveform data. The values are averages over ten s im-\\nulations, with the standard error of the average in parentheses . The ﬁve entries\\nabove the line are taken from Hastie et al. (1994). The ﬁrst mode l below the line\\nis MDA with three subclasses per class. The next line is the same, except that the\\ndiscriminant coeﬃcients are penalized via a roughness penalty to e ﬀectively 4df.\\nThe third is the corresponding penalized LDA or PDA model.\\nTechnique Error Rates\\nTraining Test\\nLDA 0.121(0.006) 0.191(0.006)\\nQDA 0.039(0.004) 0.205(0.006)\\nCART 0.072(0.003) 0.289(0.004)\\nFDA/MARS (degree = 1) 0.100(0.006) 0.191(0.006)\\nFDA/MARS (degree = 2) 0.068(0.004) 0.215(0.002)\\nMDA (3 subclasses) 0.087(0.005) 0.169(0.006)\\nMDA (3 subclasses, penalized 4 df) 0.137(0.006) 0.157(0.005)\\nPDA (penalized 4 df) 0.150(0.005) 0.171(0.005)\\nBayes 0.140\\nDiscriminant Var 1Discriminant Var 2\\n-6 -4 -2 0 2 4 6-6 -4 -2 0 2 411\\n111\\n11\\n111\\n11\\n11\\n111\\n111\\n1\\n1\\n11\\n1\\n111\\n1111\\n111\\n11\\n11\\n11\\n11\\n11\\n111\\n11\\n111\\n11\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n111\\n111111\\n111\\n11\\n11\\n11\\n11\\n11\\n11\\n11\\n11\\n1\\n1111\\n1\\n11\\n111\\n111\\n11\\n11\\n111\\n1\\n11\\n1\\n11111\\n11\\n1\\n11111\\n1\\n11\\n11\\n111\\n11\\n1111\\n111\\n111\\n1\\n1\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n1111\\n11\\n11111\\n11\\n22\\n22\\n2\\n22222\\n2 22\\n2\\n222\\n22\\n2\\n222\\n22\\n2\\n222\\n22\\n222\\n22\\n22222\\n2\\n2222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n222\\n2\\n2\\n2\\n22\\n222\\n22\\n22\\n2\\n222\\n2\\n22\\n2\\n2\\n222\\n222\\n2\\n2\\n22\\n2\\n222\\n22\\n22\\n222\\n22\\n22\\n22\\n2\\n222222\\n22\\n2\\n22\\n2\\n2\\n22\\n222\\n2\\n22\\n22\\n22\\n2\\n2\\n2\\n22\\n2\\n22\\n22\\n2222\\n2\\n22\\n33\\n3 333\\n3333 33\\n33\\n3\\n3 3333\\n33\\n333\\n333\\n333\\n3\\n33\\n33\\n333\\n33\\n33\\n3333333\\n3\\n3\\n333\\n3333\\n333\\n33\\n333333\\n333\\n33\\n3\\n33\\n333\\n3\\n333 3\\n33\\n3\\n33\\n3\\n3\\n33\\n3\\n3\\n333\\n3\\n333 3\\n3 3\\n333\\n33\\n33\\n33\\n3\\n333\\n3\\n3\\n333\\n33\\n333\\n333\\n3\\n33\\n333\\n333\\n33\\n333\\n3\\n33\\n333\\n33 333\\n33\\n3\\n3 33\\n3\\n3\\n33\\n3\\n33333\\n333 Subclasses, Penalized 4 df\\nDiscriminant Var 3Discriminant Var 4\\n-2 -1 0 1 2-1.0 0.0 0.5 1.01\\n1\\n11\\n111\\n11\\n11\\n1\\n11\\n11\\n11\\n1\\n111\\n11\\n11\\n11\\n11\\n11\\n11 111\\n111\\n11\\n1\\n11\\n11\\n11\\n11\\n1\\n111\\n11\\n11111\\n11111\\n1\\n1111\\n1\\n11\\n1 1\\n11\\n1 111\\n11\\n1\\n11\\n1111\\n11\\n11\\n11\\n1\\n11\\n1\\n11\\n11\\n11\\n11\\n111\\n111\\n1\\n11\\n11\\n1\\n11\\n11\\n11\\n1\\n11111\\n1\\n111\\n11\\n111\\n1\\n1\\n1\\n111\\n11\\n11\\n11\\n111\\n1\\n1\\n111\\n1111\\n11\\n111\\n11\\n1111\\n22\\n22\\n22\\n222\\n22\\n222\\n22 2\\n22\\n22\\n22\\n22\\n22\\n22\\n22\\n222\\n22\\n2\\n22222\\n2\\n222\\n22\\n2\\n22222\\n22\\n22\\n22\\n22\\n2\\n2 22\\n222\\n2\\n222\\n22\\n2\\n22222\\n222\\n22\\n222\\n222\\n2222\\n222\\n22\\n2222\\n2\\n22\\n2\\n2\\n22\\n2\\n222\\n2\\n222\\n2\\n22\\n22\\n22\\n2\\n22\\n22 2\\n22\\n22\\n2\\n2\\n2\\n22\\n2222\\n2\\n22\\n222\\n222\\n3\\n333\\n3\\n33\\n3 33\\n3\\n33\\n33\\n33\\n333\\n3\\n33\\n33\\n3333\\n3\\n333\\n3\\n33\\n333\\n33\\n3\\n3\\n33\\n33\\n33\\n3\\n333\\n3\\n33333\\n3\\n3\\n33\\n33\\n33\\n3\\n3\\n3333\\n33\\n3333\\n3 3\\n33\\n3333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n333\\n3\\n33 3\\n33\\n3333\\n333\\n33\\n3\\n33\\n3\\n3\\n333\\n33\\n33\\n333\\n3333333\\n33\\n3333\\n3\\n33\\n333\\n3\\n33\\n333\\n3\\n33\\n3\\n33\\n3\\n333\\n33333 Subclasses, Penalized 4 df\\nFIGURE 12.15. Some two-dimensional views of the MDA model ﬁtted to a\\nsample of the waveform model. The points are independent test dat a, projected\\non to the leading two canonical coordinates (left panel), and the th ird and fourth\\n(right panel). The subclass centers are indicated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='289ac15e-24cf-4e1c-80c1-37691b7a03ed', embedding=None, metadata={'page_label': '474', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 455\\nComputational Considerations\\nWith Ntraining cases, ppredictors, and msupport vectors, the support\\nvector machine requires m3+mN+mpN operations, assuming m≈N.\\nThey do not scale well with N, although computational shortcuts are avail-\\nable (Platt, 1999). Since these are evolving rapidly, the reader is urged to\\nsearch the web for the latest technology.\\nLDA requires Np2+p3operations, as does PDA. The complexity of\\nFDA depends on the regression method used. Many techniques are linear\\ninN, such as additive models and MARS. General splines and kernel-based\\nregression methods will typically require N3operations.\\nSoftware is available for ﬁtting FDA, PDA and MDA models in the R\\npackagemda, which is also available in S-PLUS.\\nBibliographic Notes\\nThe theory behind support vector machines is due to Vapnik and is de-\\nscribed in Vapnik (1996). There is a burgeoning literature on SVMs; an\\nonline bibliography, created and maintained by Alex Smola and Bernhard\\nSch¨ olkopf, can be found at:\\nhttp://www.kernel-machines.org .\\nOur treatment is based on Wahba et al. (2000) and Evgeniou et al. (2000),\\nand the tutorial by Burges (Burges, 1998).\\nLinear discriminant analysis is due to Fisher (1936) and Rao (1973). The\\nconnection with optimal scoring dates back at least to Breiman and Ihaka\\n(1984), and in a simple form to Fisher (1936). There are strong connections\\nwith correspondence analysis (Greenacre, 1984). The description of ﬂexible,\\npenalized and mixture discriminant analysis is taken from Hastie et al.\\n(1994), Hastie et al. (1995) and Hastie and Tibshirani (1996b), and al l\\nthree are summarized in Hastie et al. (1998); see also Ripley (1996).\\nExercises\\nEx. 12.1 Show that the criteria (12.25) and (12.8) are equivalent.\\nEx. 12.2 Show that the solution to (12.29) is the same as the solution to\\n(12.25) for a particular kernel.\\nEx. 12.3 Consider a modiﬁcation to (12.43) where you do not penalize the\\nconstant. Formulate the problem, and characterize its solution.\\nEx. 12.4 Suppose you perform a reduced-subspace linear discriminant anal-\\nysis for a K-group problem. You compute the canonical variables of di-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='81f4c0d3-245a-4858-814a-a2b4e9aa6d10', embedding=None, metadata={'page_label': '475', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='456 12. Flexible Discriminants\\nmension L≤K−1 given by z=UTx, where Uis the p×Lmatrix of\\ndiscriminant coeﬃcients, and p > K is the dimension of x.\\n(a) If L=K−1 show that\\n∥z−¯zk∥2− ∥z−¯zk′∥2=∥x−¯xk∥2\\nW− ∥x−¯xk′∥2\\nW,\\nwhere ∥≤∥Wdenotes Mahalanobis distance with respect to the covari-\\nanceW.\\n(b) If L < K −1, show that the same expression on the left measures\\nthe diﬀerence in Mahalanobis squared distances for the distributions\\nprojected onto the subspace spanned by U.\\nEx. 12.5 The data in phoneme.subset , available from this book’s website\\nhttp://www-stat.stanford.edu/ElemStatLearn\\nconsists of digitized log-periodograms for phonemes uttered by 60 speakers,\\neach speaker having produced phonemes from each of ﬁve classes. It is\\nappropriate to plot each vector of 256 “features” against the frequencies\\n0–255.\\n(a) Produce a separate plot of all the phoneme curves against frequency\\nfor each class.\\n(b) You plan to use a nearest prototype classiﬁcation scheme to classify\\nthe curves into phoneme classes. In particular, you will use a K-means\\nclustering algorithm in each class ( kmeans() inR), and then classify\\nobservations to the class of the closest cluster center. The curves are\\nhigh-dimensional and you have a rather small sample-size-to-variables\\nratio. You decide to restrict all the prototypes to be smooth functions\\nof frequency. In particular, you decide to represent each prototype m\\nasm=Bθwhere Bis a 256 ×Jmatrix of natural spline basis\\nfunctions with Jknots uniformly chosen in (0 ,255) and boundary\\nknots at 0 and 255. Describe how to proceed analytically, and in\\nparticular, how to avoid costly high-dimensional ﬁtting procedures.\\n(Hint: It may help to restrict Bto be orthogonal.)\\n(c) Implement your procedure on the phoneme data, and try it out. Divide\\nthe data into a training set and a test set (50-50), making sure that\\nspeakers are not split across sets (why?). Use K= 1,3,5,7 centers\\nper class, and for each use J= 5,10,15 knots (taking care to start\\ntheK-means procedure at the same starting values for each value of\\nJ), and compare the results.\\nEx. 12.6 Suppose that the regression procedure used in FDA (Section 12.5.1)\\nis a linear expansion of basis functions hm(x), m= 1,... ,M . LetDπ=\\nYTY/Nbe the diagonal matrix of class proportions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8dc48446-78e9-4ef6-97cb-d44f68cdccdd', embedding=None, metadata={'page_label': '476', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 457\\n(a) Show that the optimal scoring problem (12.52) can be written in vector\\nnotation as\\nmin\\nθ,β∥Yθ−Hβ∥2, (12.65)\\nwhere θis a vector of Kreal numbers, and His the N×Mmatrix\\nof evaluations hj(xi).\\n(b) Suppose that the normalization on θisθTDπ1 = 0 and θTDπθ= 1.\\nInterpret these normalizations in terms of the original scored θ(gi).\\n(c) Show that, with this normalization, (12.65) can be partially optimized\\nw.r.t. β, and leads to\\nmax\\nθθTSθ, (12.66)\\nsubject to the normalization constraints, where Sis the projection\\noperator corresponding to the basis matrix H.\\n(d) Suppose that the hjinclude the constant function. Show that the\\nlargest eigenvalue of Sis 1.\\n(e) Let Θbe aK×Kmatrix of scores (in columns), and suppose the\\nnormalization is ΘTDπΘ=I. Show that the solution to (12.53) is\\ngiven by the complete set of eigenvectors of S; the ﬁrst eigenvector is\\ntrivial, and takes care of the centering of the scores. The remainder\\ncharacterize the optimal scoring solution.\\nEx. 12.7 Derive the solution to the penalized optimal scoring problem\\n(12.57).\\nEx. 12.8 Show that coeﬃcients βℓfound by optimal scoring are proportional\\nto the discriminant directions νℓfound by linear discriminant analysis.\\nEx. 12.9 LetˆY=XˆBbe the ﬁtted N×Kindicator response matrix after\\nlinear regression on the N×pmatrix X, where p > K . Consider the reduced\\nfeatures x∗\\ni=ˆBTxi. Show that LDA using x∗\\niis equivalent to LDA in the\\noriginal space.\\nEx. 12.10 Kernels and linear discriminant analysis . Suppose you wish to\\ncarry out a linear discriminant analysis (two classes) using a vector of\\ntransformations of the input variables h(x). Since h(x) is high-dimensional,\\nyou will use a regularized within-class covariance matrix Wh+γI. Show\\nthat the model can be estimated using only the inner products K(xi,xi′) =\\n⟨h(xi),h(xi′)⟩. Hence the kernel property of support vector machines is also\\nshared by regularized linear discriminant analysis.\\nEx. 12.11 The MDA procedure models each class as a mixture of Gaussians.\\nHence each mixture center belongs to one and only one class. A more\\ngeneral model allows each mixture center to be shared by all classes. We\\ntake the joint density of labels and features to be', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6f818526-fc33-40e3-8be1-8e2c64dc3e09', embedding=None, metadata={'page_label': '477', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='458 12. Flexible Discriminants\\nP(G,X) =R∑\\nr=1πrPr(G,X), (12.67)\\na mixture of joint densities. Furthermore we assume\\nPr(G,X) =Pr(G)φ(X;θr,Σ). (12.68)\\nThis model consists of regions centered at θr, and for each there is a class\\nproﬁle Pr(G). The posterior class distribution is given by\\nP(G=k|X=x) =∑R\\nr=1πrPr(G=k)φ(x;θr,Σ)\\n∑R\\nr=1πrφ(x;θr,Σ), (12.69)\\nwhere the denominator is the marginal distribution P(X).\\n(a) Show that this model (called MDA2) can be viewed as a generalization\\nof MDA since\\nP(X|G=k) =∑R\\nr=1πrPr(G=k)φ(x;θr,Σ)\\n∑R\\nr=1πrPr(G=k), (12.70)\\nwhere πrk=πrPr(G=k)/∑R\\nr=1πrPr(G=k) corresponds to the\\nmixing proportions for the kth class.\\n(b) Derive the EM algorithm for MDA2.\\n(c) Show that if the initial weight matrix is constructed as in MDA, in-\\nvolving separate k-means clustering in each class, then the algorithm\\nfor MDA2 is identical to the original MDA procedure.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b73babc8-eaf8-4625-b4d6-5137bfbd60e1', embedding=None, metadata={'page_label': '478', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 459\\nPrinter: Opaque this\\n13\\nPrototype Methods and\\nNearest-Neighbors\\n13.1 Introduction\\nIn this chapter we discuss some simple and essentially model-free methods\\nfor classiﬁcation and pattern recognition. Because they are highly unstruc-\\ntured, they typically are not useful for understanding the nature of the\\nrelationship between the features and class outcome. However, as black box\\nprediction engines, they can be very eﬀective, and are often among the best\\nperformers in real data problems. The nearest-neighbor technique can also\\nbe used in regression; this was touched on in Chapter 2 and works reason-\\nably well for low-dimensional problems. However, with high-dimensional\\nfeatures, the bias–variance tradeoﬀ does not work as favorably for nearest-\\nneighbor regression as it does for classiﬁcation.\\n13.2 Prototype Methods\\nThroughout this chapter, our training data consists of the Npairs ( x1,g1),\\n... ,(xn,gN) where giis a class label taking values in {1,2,... ,K }. Pro-\\ntotype methods represent the training data by a set of points in feature\\nspace. These prototypes are typically not examples from the training sam-\\nple, except in the case of 1-nearest-neighbor classiﬁcation discussed later.\\nEach prototype has an associated class label, and classiﬁcation of a query\\npoint xis made to the class of the closest prototype. “Closest” is usually\\ndeﬁned by Euclidean distance in the feature space, after each feature has', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='080b7ea3-5250-4f90-bd7c-de7fb8356fe9', embedding=None, metadata={'page_label': '479', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='460 13. Prototypes and Nearest-Neighbors\\nbeen standardized to have overall mean 0 and variance 1 in the training\\nsample. Euclidean distance is appropriate for quantitative features. We\\ndiscuss distance measures between qualitative and other kinds of feature\\nvalues in Chapter 14.\\nThese methods can be very eﬀective if the prototypes are well positioned\\nto capture the distribution of each class. Irregular class boundaries can be\\nrepresented, with enough prototypes in the right places in feature space.\\nThe main challenge is to ﬁgure out how many prototypes to use and where\\nto put them. Methods diﬀer according to the number and way in which\\nprototypes are selected.\\n13.2.1 K-means Clustering\\nK-means clustering is a method for ﬁnding clusters and cluster centers in a\\nset of unlabeled data. One chooses the desired number of cluster centers, say\\nR, and the K-means procedure iteratively moves the centers to minimize\\nthe total within cluster variance.1Given an initial set of centers, the K-\\nmeans algorithm alternates the two steps:\\n•for each center we identify the subset of training points (its cluster)\\nthat is closer to it than any other center;\\n•the means of each feature for the data points in each cluster are\\ncomputed, and this mean vector becomes the new center for that\\ncluster.\\nThese two steps are iterated until convergence. Typically the initial centers\\nareRrandomly chosen observations from the training data. Details of the\\nK-means procedure, as well as generalizations allowing for diﬀerent variable\\ntypes and more general distance measures, are given in Chapter 14.\\nTo use K-means clustering for classiﬁcation of labeled data, the steps\\nare:\\n•apply K-means clustering to the training data in each class sepa-\\nrately, using Rprototypes per class;\\n•assign a class label to each of the K×Rprototypes;\\n•classify a new feature xto the class of the closest prototype.\\nFigure 13.1 (upper panel) shows a simulated example with three classes\\nand two features. We used R= 5 prototypes per class, and show the clas-\\nsiﬁcation regions and the decision boundary. Notice that a number of the\\n1The “ K” inK-means refers to the number of cluster centers. Since we have already\\nreserved Kto denote the number of classes, we denote the number of clust ers by R.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='054c20a9-ff85-4b2c-9924-23cca1262637', embedding=None, metadata={'page_label': '480', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.2 Prototype Methods 461\\nK-means - 5 Prototypes  per Class\\n................................................................................................................................................................................................................................................................................................................................................................................................................................. ........................ ................................ ..................................... ............................................................................................................................................................................. .................................. ....................................... ......................................... ............................................ .............................................. .................................................. ......................... .............................. ........................ ................................... ......................... ..................................... .......................... ...................................... ............................. .................................. .................................. ....................... ....................................... ............. ............................................... ...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\n................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ........... ........................................ ............................................................. ........ ........................................... ........................................................ ..... ................................................................................................. .. .............................................................................................. . .......................................................................................... .... .................................................................................... ....... .............................................................................. .......... ............................................................................ ......... ............................................................................. ......... ............................................................................. ......... ............................................................................. ......... ............................................................................... ....... .................................................................................... ..... ........................................................................................ ... ............................................................................................ . ............................................................................................... . ................................................................................................ ... ................................................................................................ ..... ................................................................................................ ....... ............................................ ................................ .................... .... ....................................................................... ..................... .... ........................................................... ................. ............. .......................................... ............. ...................... ........................ .............. ....\\n................................................................................ ......................... ......................................... .................... .................................................... ................ ........................................................... ............... ............................................................. ............... ............................................................... ............... ............................................................... ............... ........................................................... ................ ....................................................... ................ .................................................... ................ ................................................... ............... ................................................. .............. .............................................. .............. ......................................... .................................................... ............................................... ............................................... ........... .......................................... .......... ................................................ .......... ...................................................... ......... .................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ .............................................................................................. ............................................................................................... ................................................................................................ ................................................................................................. ................................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n•\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nLVQ - 5 Prototypes per Class\\n......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ................................... ........................................ .............................................. .................................................. .................... .................................. ..................... ...................................... ..................... ......................................... ....................... ..................................... .............................. ............................ ..................................... .............. ............................................ .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\n................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... .... ............................................... ............................................................ .... ............................................... ........................................................... .... ............................................... .......................................................... ... ................................................ ........................................................ ... ................................................ ...................................................... ... ................................................ ..................................................... .. .................................................................................................... .. .................................................................................................. . ................................................................................................. . ................................................................................................ . ............................................................................................... ............................................................................................. ........................................................................................... . ......................................................................................... . ............................................................ .................... ......... ........................................... ........................ ..............\\n.......................... ................................. ............................... ....................... ................................................. ................ .......................................................... ............... ............................................................ ............... ............................................................. ............... .............................................................. ............... .............................................................. ............... ................................................................ .............. .................................................................. .............. ................................................................... .............. ................................................................ .............. ........................................................... ............... ..................................................... ................ ................................................ ............... ............................................. .............. ......................................... ................................................... ................................................ .................................................. ........... .............................................. ........... ...................................................... .......... ....................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ ..........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n••••\\n••\\n•••\\n•\\n• •\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nFIGURE 13.1. Simulated example with three classes and ﬁve prototypes per\\nclass. The data in each class are generated from a mixture of Gau ssians. In the\\nupper panel, the prototypes were found by applying the K-means clustering algo-\\nrithm separately in each class. In the lower panel, the LVQ alg orithm (starting\\nfrom the K-means solution) moves the prototypes away from the decision b ound-\\nary. The broken purple curve in the background is the Bayes dec ision boundary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8c7a6f1f-0729-4f6c-91e8-1c391a67a113', embedding=None, metadata={'page_label': '481', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='462 13. Prototypes and Nearest-Neighbors\\nAlgorithm 13.1 Learning Vector Quantization—LVQ.\\n1. Choose Rinitial prototypes for each class: m1(k),m2(k),... ,m R(k),\\nk= 1,2,... ,K , for example, by sampling Rtraining points at random\\nfrom each class.\\n2. Sample a training point xirandomly (with replacement), and let ( j,k)\\nindex the closest prototype mj(k) toxi.\\n(a) If gi=k(i.e., they are in the same class), move the prototype\\ntowards the training point:\\nmj(k)←mj(k) +ǫ(xi−mj(k)),\\nwhere ǫis the learning rate .\\n(b) If gi̸=k(i.e., they are in diﬀerent classes), move the prototype\\naway from the training point:\\nmj(k)←mj(k)−ǫ(xi−mj(k)).\\n3. Repeat step 2, decreasing the learning rate ǫwith each iteration to-\\nwards zero.\\nprototypes are near the class boundaries, leading to potential misclassiﬁca-\\ntion errors for points near these boundaries. This results from an obvious\\nshortcoming with this method: for each class, the other classes do not have\\na say in the positioning of the prototypes for that class. A better approach,\\ndiscussed next, uses all of the data to position all prototypes.\\n13.2.2 Learning Vector Quantization\\nIn this technique due to Kohonen (1989), prototypes are placed strategically\\nwith respect to the decision boundaries in an ad-hoc way. LVQ is an online\\nalgorithm—observations are processed one at a time.\\nThe idea is that the training points attract prototypes of the correct class,\\nand repel other prototypes. When the iterations settle down, prototypes\\nshould be close to the training points in their class. The learning rate ǫis\\ndecreased to zero with each iteration, following the guidelines for stochastic\\napproximation learning rates (Section 11.4.)\\nFigure 13.1 (lower panel) shows the result of LVQ, using the K-means\\nsolution as starting values. The prototypes have tended to move away from\\nthe decision boundaries, and away from prototypes of competing classes.\\nThe procedure just described is actually called LVQ1. Modiﬁcations\\n(LVQ2, LVQ3, etc.) have been proposed, that can sometimes improve per-\\nformance. A drawback of learning vector quantization methods is the fact', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='41132ec3-9589-4e07-9803-7f8823401508', embedding=None, metadata={'page_label': '482', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3k-Nearest-Neighbor Classiﬁers 463\\nthat they are deﬁned by algorithms, rather than optimization of some ﬁxed\\ncriteria; this makes it diﬃcult to understand their properties.\\n13.2.3 Gaussian Mixtures\\nThe Gaussian mixture model can also be thought of as a prototype method,\\nsimilar in spirit to K-means and LVQ. We discuss Gaussian mixtures in\\nsome detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms\\nof a Gaussian density, which has a centroid (as in K-means), and a covari-\\nance matrix. The comparison becomes crisper if we restrict the component\\nGaussians to have a scalar covariance matrix (Exercise 13.1). The two st eps\\nof the alternating EM algorithm are very similar to the two steps in K-\\nmeans:\\n•In the E-step, each observation is assigned a responsibility or weight\\nfor each cluster, based on the likelihood of each of the correspond-\\ning Gaussians. Observations close to the center of a cluster will most\\nlikely get weight 1 for that cluster, and weight 0 for every other clus-\\nter. Observations half-way between two clusters divide their weight\\naccordingly.\\n•In the M-step, each observation contributes to the weighted means\\n(and covariances) for every cluster.\\nAs a consequence, the Gaussian mixture model is often referred to as a soft\\nclustering method, while K-means is hard.\\nSimilarly, when Gaussian mixture models are used to represent the fea-\\nture density in each class, it produces smooth posterior probabilities ˆ p(x) =\\n{ˆp1(x),... ,ˆpK(x)}for classifying x(see (12.60) on page 449.) Often this\\nis interpreted as a soft classiﬁcation, while in fact the classiﬁcation rule i s\\nˆG(x) = arg max kˆpk(x). Figure 13.2 compares the results of K-means and\\nGaussian mixtures on the simulated mixture problem of Chapter 2. We\\nsee that although the decision boundaries are roughly similar, those for the\\nmixture model are smoother (although the prototypes are in approximately\\nthe same positions.) We also see that while both procedures devote a blue\\nprototype (incorrectly) to a region in the northwest, the Gaussian mixtur e\\nclassiﬁer can ultimately ignore this region, while K-means cannot. LVQ\\ngave very similar results to K-means on this example, and is not shown.\\n13.3 k-Nearest-Neighbor Classiﬁers\\nThese classiﬁers are memory-based , and require no model to be ﬁt. Given\\na query point x0, we ﬁnd the ktraining points x(r),r= 1,... ,k closest in\\ndistance to x0, and then classify using majority vote among the kneighbors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6ff40808-3fe5-4d1f-a838-d69dda7d769a', embedding=None, metadata={'page_label': '483', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='464 13. Prototypes and Nearest-Neighbors\\nK-means - 5 Prototypes per Class\\n... .. . . .. . . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no••\\n••\\n•••\\n••\\n•\\n•••\\n••\\n•••\\n••\\nTraining Error: 0.170\\nTest Error:       0.243\\nBayes Error:    0.210\\nGaussian Mixtures - 5 Subclasses per Class\\n.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••\\n••\\n•••\\n••••\\n••\\n•••\\n••\\n•\\nTraining Error: 0.17\\nTest Error:       0.22\\nBayes Error:    0.21\\nFIGURE 13.2. The upper panel shows the K-means classiﬁer applied to the\\nmixture data example. The decision boundary is piecewise linear . The lower panel\\nshows a Gaussian mixture model with a common covariance for all component\\nGaussians. The EM algorithm for the mixture model was started a t theK-means\\nsolution. The broken purple curve in the background is the Baye s decision\\nboundary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='922d40db-f8ba-44e3-a313-6fa9a1c95dba', embedding=None, metadata={'page_label': '484', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3k-Nearest-Neighbor Classiﬁers 465\\nTies are broken at random. For simplicity we will assume that the features\\nare real-valued, and we use Euclidean distance in feature space:\\nd(i)=||x(i)−x0||. (13.1)\\nTypically we ﬁrst standardize each of the features to have mean zero and\\nvariance 1, since it is possible that they are measured in diﬀerent units. In\\nChapter 14 we discuss distance measures appropriate for qualitative and\\nordinal features, and how to combine them for mixed data. Adaptively\\nchosen distance metrics are discussed later in this chapter.\\nDespite its simplicity, k-nearest-neighbors has been successful in a large\\nnumber of classiﬁcation problems, including handwritten digits, satellite\\nimage scenes and EKG patterns. It is often successful where each class\\nhas many possible prototypes, and the decision boundary is very irregular.\\nFigure 13.3 (upper panel) shows the decision boundary of a 15-nearest-\\nneighbor classiﬁer applied to the three-class simulated data. The decision\\nboundary is fairly smooth compared to the lower panel, where a 1-nearest-\\nneighbor classiﬁer was used. There is a close relationship between nearest-\\nneighbor and prototype methods: in 1-nearest-neighbor classiﬁcation, each\\ntraining point is a prototype.\\nFigure 13.4 shows the training, test and tenfold cross-validation errors\\nas a function of the neighborhood size, for the two-class mixture problem.\\nSince the tenfold CV errors are averages of ten numbers, we can estimate\\na standard error.\\nBecause it uses only the training point closest to the query point, the bias\\nof the 1-nearest-neighbor estimate is often low, but the variance is high.\\nA famous result of Cover and Hart (1967) shows that asymptotically the\\nerror rate of the 1-nearest-neighbor classiﬁer is never more than twice the\\nBayes rate. The rough idea of the proof is as follows (using squared-error\\nloss). We assume that the query point coincides with one of the training\\npoints, so that the bias is zero. This is true asymptotically if the dimensio n\\nof the feature space is ﬁxed and the training data ﬁlls up the space in a\\ndense fashion. Then the error of the Bayes rule is just the variance of a\\nBernoulli random variate (the target at the query point), while the error of\\n1-nearest-neighbor rule is twicethe variance of a Bernoulli random variate,\\none contribution each for the training and query targets.\\nWe now give more detail for misclassiﬁcation loss. At xletk∗be the\\ndominant class, and pk(x) the true conditional probability for class k. Then\\nBayes error = 1 −pk∗(x), (13.2)\\n1-nearest-neighbor error =K∑\\nk=1pk(x)(1−pk(x)), (13.3)\\n≥1−pk∗(x). (13.4)\\nThe asymptotic 1-nearest-neighbor error rate is that of a random rule; we\\npick both the classiﬁcation and the test point at random with probabili-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='152a7924-ed76-4d96-81c9-2d0a03933f6b', embedding=None, metadata={'page_label': '485', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='466 13. Prototypes and Nearest-Neighbors\\n15-Nearest Neighbors\\n. ........ ......... ..... ...... ......... ......................................................................................... ............................................................................................................................................................................................................................................................................ .............................................................................................................. .. .. . ...................................... .. . .... .... ......................................... ........... ....................................................... ......... ............................................................ ...... .......................................................................... ............................................................................... ...................... .................................................. ................................................................... ........................... ...................................... ........ .. .................................................................................... . .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\n................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................. . . ................................................... ............................................................. . .................................................. ............................................................ ................................................... ............................................................... .. ................................................. .................................................... ...... ... ................................................... ............................................... ..... ... .. ................................................. .............................................. .... .... ................................................... ............................................... ...... ................................................... ..................................................... .... ............................................... ........................................................ .... ............................................... ........................ ............................... ... .............................................................. .... . . ........................ ....... ...................................................... ........................ ................. ................................... ................. .............................. ............... ....... ... ........................................ ................................................................ .............................. . ...................... ......\\n.... .. . ..................... .............................. .................................................................................................. .......................................... .. ......... ............................................... ........... ..................................................... .............. ............................................................. .............. .................................................................. ........... .................................................................. . ............. .............................................................. ................ .......................................................... .. . . ............ .................... .................................... . . ............... .. ......... .. ................................... .................................................... ............................................... ................................................................................................................................ .................................................................................................... . ....................................................... . ........ ............................................................. ......................................................................... ................ ... ..................................................... ........................................................................... ............................................................................ .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................. ...........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n1-Nearest Neighbor\\n................. ................. ............ .............. .................. ....................... ... ................ ..... ............... ..... ............. .... ................ ......... ..... ..... ...... ................... ..... ...... ...... .... ....... ... .......... ... .... .. .... . ........ ............ ............ ...... ............ .......... ........ .............. ........ ........ ............. .......... ...... ............... ....... ............. ............. ......... .......................... ............. ......................... . ..... .. .. ............................. ....... .. . ...... .......................... ....... ............ .............. ................ ............... .............................. ............. ........... ............ ....................... ............. ........... ....................... .......................................... ...... ..................... ..................... ..... ......................... .......... ............... ........ ....... ... .... ....................... .... ... ................ ...................... . ........................ . ......... ..... .... .............. ............... .............. ...... ... .................................. ......... .......... ... .............................. ........... ...... .......... ............................. ......... .......... ...................................... ... ................................................... ...................................... ..................................... .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\n................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................ ................................... .............................................................. ....... ............................................ .............................................. ...... .... ......................................................................................... ........ ... ................................................ ............................................ ........ ............... .............................................................................. ........ .......... ............................................................................... ....... ........ ......................................................... .............. ......... ... .. .......................................................... ............ ....... ... ........ ...................................................... .......... ...... .... .......... .................................................. ...... .. ....... ................ .................................................... . .. .............. .............. ..................................................... ... ............ .... ... ........... ............................................. .. ............. ... .................. ....................................... .............. ..... ... ..... ..... ....... .......................................... ............. ...... . ....... ... ......... ....................................... ............... .. .......... ...... ..... ... .............................................. .................. .......... ........ . ............................................ ..... .......... ......... ............. .................................. ... ..... ......... ..................... ........................... . ... ... ........ ........................ ....................... ..... ... ....... ............ ............. ................ .. .... ..... ............. ... .... .................... .... ..... . ........... ..... ...... ...........\\n................................................. ............ ................. ................................................................. .................. ..................... ...................... ............................ ......................................... ................................................ .. ... .......... .......................... ...... . ...... ...................................... ...... . ..... ............ .................................... .. .............. .................................... .. . ................ ................................................ ... ....... .................................................. ...... .......... ........................................... ..... ........ ......... ........................................... ... . .. .. ... ................................... .. .. .. .. .................................. ........... .... . ...... . .... ............................................... ..... ..... ................................................... ...... . .................................................. .... .. . ......................................................... ..... ..... .................................................... ...... ............................................................ .................. ........................................................ ................... ......................................................... ................... .......................................................... ............................................................................... .............................................................................. .............................................................................. ................................................................................ .................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ........................................................................................... .........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\nFIGURE 13.3. k-nearest-neighbor classiﬁers applied to the simulation data o f\\nFigure 13.1. The broken purple curve in the background is the B ayes decision\\nboundary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c703a6b6-2bc9-4c9f-a1aa-b7e5a6c87308', embedding=None, metadata={'page_label': '486', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3k-Nearest-Neighbor Classiﬁers 467\\nNumber of NeighborsMisclassification Errors\\n0 5 10 15 20 25 300.0 0.05 0.10 0.15 0.20 0.25 0.30•\\n••••••• • ••• •••\\n•• ••••\\n••••• •• • ••\\n•••••• •••• ••••\\nTest Error\\n10-fold CV\\nTraining Error\\nBayes Error\\n7-Nearest Neighbors\\n.. .. . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.145\\nTest Error:       0.225\\nBayes Error:    0.210\\nFIGURE 13.4. k-nearest-neighbors on the two-class mixture data. The upper\\npanel shows the misclassiﬁcation errors as a function of neighbo rhood size. Stan-\\ndard error bars are included for 10-fold cross validation. The lower panel shows\\nthe decision boundary for 7-nearest-neighbors, which appears to be optimal for\\nminimizing test error. The broken purple curve in the backgrou nd is the Bayes\\ndecision boundary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c48bc2e3-4fb3-44e4-b9c0-16ab5e06f526', embedding=None, metadata={'page_label': '487', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='468 13. Prototypes and Nearest-Neighbors\\ntiespk(x), k= 1,... ,K . For K= 2 the 1-nearest-neighbor error rate is\\n2pk∗(x)(1−pk∗(x))≤2(1−pk∗(x)) (twice the Bayes error rate). More\\ngenerally, one can show (Exercise 13.3)\\nK∑\\nk=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K\\nK−1(1−pk∗(x))2.(13.5)\\nMany additional results of this kind have been derived; Ripley (1996) sum-\\nmarizes a number of them.\\nThis result can provide a rough idea about the best performance that\\nis possible in a given problem. For example, if the 1-nearest-neighbor rule\\nhas a 10% error rate, then asymptotically the Bayes error rate is at least\\n5%. The kicker here is the asymptotic part, which assumes the bias of the\\nnearest-neighbor rule is zero. In real problems the bias can be substantial.\\nThe adaptive nearest-neighbor rules, described later in this chapter, are an\\nattempt to alleviate this bias. For simple nearest-neighbors, the bias and\\nvariance characteristics can dictate the optimal number of near neighbors\\nfor a given problem. This is illustrated in the next example.\\n13.3.1 Example: A Comparative Study\\nWe tested the nearest-neighbors, K-means and LVQ classiﬁers on two sim-\\nulated problems. There are 10 independent features Xj, each uniformly\\ndistributed on [0 ,1]. The two-class 0-1 target variable is deﬁned as follows:\\nY=I(\\nX1>1\\n2)\\n; problem 1: “easy”,\\nY=I\\uf8eb\\n\\uf8edsign\\uf8f1\\n\\uf8f2\\n\\uf8f33∏\\nj=1(\\nXj−1\\n2)\\uf8fc\\n\\uf8fd\\n\\uf8fe>0\\uf8f6\\n\\uf8f8; problem 2: “diﬃcult.”(13.6)\\nHence in the ﬁrst problem the two classes are separated by the hyperplane\\nX1= 1/2; in the second problem, the two classes form a checkerboard\\npattern in the hypercube deﬁned by the ﬁrst three features. The Bayes\\nerror rate is zero in both problems. There were 100 training and 1000 test\\nobservations.\\nFigure 13.5 shows the mean and standard error of the misclassiﬁcation\\nerror for nearest-neighbors, K-means and LVQ over ten realizations, as\\nthe tuning parameters are varied. We see that K-means and LVQ give\\nnearly identical results. For the best choices of their tuning parameters,\\nK-means and LVQ outperform nearest-neighbors for the ﬁrst problem, and\\nthey perform similarly for the second problem. Notice that the best value\\nof each tuning parameter is clearly situation dependent. For example 25-\\nnearest-neighbors outperforms 1-nearest-neighbor by a factor of 70% in the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='99507fa7-61c8-4ed4-8edb-a68cce5153d1', embedding=None, metadata={'page_label': '488', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3k-Nearest-Neighbor Classiﬁers 469\\nNumber of NeighborsMisclassification Error\\n0 20 40 600.1 0.2 0.3 0.4 0.5Nearest Neighbors / Easy\\nNumber of Prototypes per ClassMisclassification Error\\n0 5 10 15 20 25 300.1 0.2 0.3 0.4 0.5K-means & LVQ / Easy\\nNumber of NeighborsMisclassification Error\\n0 20 40 600.40 0.45 0.50 0.55 0.60Nearest Neighbors / Difficult\\nNumber of Prototypes per ClassMisclassification Error\\n0 5 10 15 20 25 300.40 0.45 0.50 0.55 0.60K-means & LVQ / Difficult\\nFIGURE 13.5. Mean ±one standard error of misclassiﬁcation error for near-\\nest-neighbors, K-means (blue) and LVQ (red) over ten realizations for two sim-\\nulated problems: “easy” and “diﬃcult,” described in the text.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e328600-f82d-4ec3-98cf-331081373403', embedding=None, metadata={'page_label': '489', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='470 13. Prototypes and Nearest-Neighbors\\nSpectral Band 1 Spectral Band 2 Spectral Band 3\\nSpectral Band 4 Land Usage Predicted Land Usage\\nFIGURE 13.6. The ﬁrst four panels are LANDSAT images for an agricultural\\narea in four spectral bands, depicted by heatmap shading. The r emaining two\\npanels give the actual land usage (color coded) and the predicte d land usage using\\na ﬁve-nearest-neighbor rule described in the text.\\nﬁrst problem, while 1-nearest-neighbor is best in the second problem by a\\nfactor of 18%. These results underline the importance of using an objective,\\ndata-based method like cross-validation to estimate the best value of a\\ntuning parameter (see Figure 13.4 and Chapter 7).\\n13.3.2 Example: k-Nearest-Neighbors and Image Scene\\nClassiﬁcation\\nThe STATLOG project (Michie et al., 1994) used part of a LANDSAT\\nimage as a benchmark for classiﬁcation (82 ×100 pixels). Figure 13.6 shows\\nfour heat-map images, two in the visible spectrum and two in the infrared,\\nfor an area of agricultural land in Australia. Each pixel has a class label\\nfrom the 7-element set G={red soil, cotton, vegetation stubble, mixture,\\ngray soil, damp gray soil, very damp gray soil }, determined manually by\\nresearch assistants surveying the area. The lower middle panel shows the\\nactual land usage, shaded by diﬀerent colors to indicate the classes. The\\nobjective is to classify the land usage at a pixel, based on the information\\nin the four spectral bands.\\nFive-nearest-neighbors produced the predicted map shown in the bot-\\ntom right panel, and was computed as follows. For each pixel we extracted\\nan 8-neighbor feature map—the pixel itself and its 8 immediate neighbors', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f97faa94-9249-46f3-b4ba-8c45a91bb42a', embedding=None, metadata={'page_label': '490', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3k-Nearest-Neighbor Classiﬁers 471\\nN\\nN\\nN\\n N\\nX\\nN\\nN\\nN\\nN\\nFIGURE 13.7. A pixel and its 8-neighbor feature map.\\n(see Figure 13.7). This is done separately in the four spectral bands, giving\\n(1+8) ×4 = 36 input features per pixel. Then ﬁve-nearest-neighbors classi-\\nﬁcation was carried out in this 36-dimensional feature space. The resulting\\ntest error rate was about 9 .5% (see Figure 13.8). Of all the methods used\\nin the STATLOG project, including LVQ, CART, neural networks, linear\\ndiscriminant analysis and many others, k-nearest-neighbors performed best\\non this task. Hence it is likely that the decision boundaries in IR36are quite\\nirregular.\\n13.3.3 Invariant Metrics and Tangent Distance\\nIn some problems, the training features are invariant under certain natural\\ntransformations. The nearest-neighbor classiﬁer can exploit such invari-\\nances by incorporating them into the metric used to measure the distances\\nbetween objects. Here we give an example where this idea was used with\\ngreat success, and the resulting classiﬁer outperformed all others at the\\ntime of its development (Simard et al., 1993).\\nThe problem is handwritten digit recognition, as discussed is Chapter 1\\nand Section 11.7. The inputs are grayscale images with 16 ×16 = 256\\npixels; some examples are shown in Figure 13.9. At the top of Figure 13.1 0,\\na “3” is shown, in its actual orientation (middle) and rotated 7 .5◦and 15◦\\nin either direction. Such rotations can often occur in real handwriting, and\\nit is obvious to our eye that this “3” is still a “3” after small rotati ons.\\nHence we want our nearest-neighbor classiﬁer to consider these two “3”s\\nto be close together (similar). However the 256 grayscale pixel values for a\\nrotated “3” will look quite diﬀerent from those in the original image, a nd\\nhence the two objects can be far apart in Euclidean distance in IR256.\\nWe wish to remove the eﬀect of rotation in measuring distances between\\ntwo digits of the same class. Consider the set of pixel values consisting of\\nthe original “3” and its rotated versions. This is a one-dimensional curve in\\nIR256, depicted by the green curve passing through the “3” in Figure 13.10.\\nFigure 13.11 shows a stylized version of IR256, with two images indicated by\\nxiandxi′. These might be two diﬀerent “3”s, for example. Through each\\nimage we have drawn the curve of rotated versions of that image, called', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbc695a6-e052-42a7-8b24-07d8202bc2fa', embedding=None, metadata={'page_label': '491', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='472 13. Prototypes and Nearest-Neighbors\\nSTATLOG results\\nMethodTest Misclassification Error\\n2 4 6 8 10 12 140.0 0.05 0.10 0.15LVQRBFALLOC80CART NeuralNewID C4.5QDASMARTLogisticLDA\\nDANNK-NN\\nFIGURE 13.8. Test-error performance for a number of classiﬁers, as reported\\nby the STATLOG project. The entry DANN is a variant of k-nearest neighbors,\\nusing an adaptive metric (Section 13.4.2).\\nFIGURE 13.9. Examples of grayscale images of handwritten digits.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4cb08c5-73fe-4d27-80c5-767bc3b3cd27', embedding=None, metadata={'page_label': '492', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3k-Nearest-Neighbor Classiﬁers 473\\nTangent+ α.Transformations of 30 7.5 −15 −7.5\\n3\\nα=0 α=0.1 α=− 0.2 α=− 0.1 α=0.2\\nLinear equation for \\nimages above15\\nPixel space\\nFIGURE 13.10. The top row shows a “ 3” in its original orientation (middle)\\nand rotated versions of it. The green curve in the middle of the ﬁg ure depicts\\nthis set of rotated “ 3” in256-dimensional space. The red line is the tangent line\\nto the curve at the original image, with some “ 3”s on this tangent line, and its\\nequation shown at the bottom of the ﬁgure.\\ninvariance manifolds in this context. Now, rather than using the usual\\nEuclidean distance between the two images, we use the shortest distance\\nbetween the two curves. In other words, the distance between the two\\nimages is taken to be the shortest Euclidean distance between any rotated\\nversion of ﬁrst image, and any rotated version of the second image. This\\ndistance is called an invariant metric .\\nIn principle one could carry out 1-nearest-neighbor classiﬁcation using\\nthis invariant metric. However there are two problems with it. First, it is\\nvery diﬃcult to calculate for real images. Second, it allows large trans-\\nformations that can lead to poor performance. For example a “6” would\\nbe considered close to a “9” after a rotation of 180◦. We need to restrict\\nattention to small rotations.\\nThe use of tangent distance solves both of these problems. As shown in\\nFigure 13.10, we can approximate the invariance manifold of the image\\n“3” by its tangent at the original image. This tangent can be computed\\nby estimating the direction vector from small rotations of the image, or b y\\nmore sophisticated spatial smoothing methods (Exercise 13.4.) For large\\nrotations, the tangent image no longer looks like a “3,” so the problem\\nwith large transformations is alleviated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='554dcc42-aab3-46b3-8b14-ac1a5eb59e02', embedding=None, metadata={'page_label': '493', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='474 13. Prototypes and Nearest-Neighbors\\nTransformationsTransformations\\nxixi′ofxi\\nofxi′Tangent distance\\nEuclidean distance\\nbetween xiandxi′Distance between\\ntransformed\\nxiandxi′\\nFIGURE 13.11. Tangent distance computation for two images xiandxi′.\\nRather than using the Euclidean distance between xiandxi′, or the shortest\\ndistance between the two curves, we use the shortest distance b etween the two\\ntangent lines.\\nThe idea then is to compute the invariant tangent line for each training\\nimage. For a query image to be classiﬁed, we compute its invariant tangent\\nline, and ﬁnd the closest line to it among the lines in the training set. The\\nclass (digit) corresponding to this closest line is our predicted class for the\\nquery image. In Figure 13.11 the two tangent lines intersect, but this is only\\nbecause we have been forced to draw a two-dimensional representation of\\nthe actual 256-dimensional situation. In IR256the probability of two such\\nlines intersecting is eﬀectively zero.\\nNow a simpler way to achieve this invariance would be to add into the\\ntraining set a number of rotated versions of each training image, and then\\njust use a standard nearest-neighbor classiﬁer. This idea is called “hints” in\\nAbu-Mostafa (1995), and works well when the space of invariances is small.\\nSo far we have presented a simpliﬁed version of the problem. In addition to\\nrotation, there are six other types of transformations under which we would\\nlike our classiﬁer to be invariant. There are translation (two directio ns),\\nscaling (two directions), sheer, and character thickness. Hence the curves\\nand tangent lines in Figures 13.10 and 13.11 are actually 7-dimensional\\nmanifolds and hyperplanes. It is infeasible to add transformed versions\\nof each training image to capture all of these possibilities. The tangent\\nmanifolds provide an elegant way of capturing the invariances.\\nTable 13.1 shows the test misclassiﬁcation error for a problem with 7291\\ntraining images and 2007 test digits (the U.S. Postal Services database), for\\na carefully constructed neural network, and simple 1-nearest-neighbor and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f5fc828-1c03-483f-aedc-9196ab9d6d7a', embedding=None, metadata={'page_label': '494', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.4 Adaptive Nearest-Neighbor Methods 475\\nTABLE 13.1. Test error rates for the handwritten ZIP code problem.\\nMethod Error rate\\nNeural-net 0.049\\n1-nearest-neighbor/Euclidean distance 0.055\\n1-nearest-neighbor/tangent distance 0.026\\ntangent distance 1-nearest-neighbor rules. The tangent distance nearest-\\nneighbor classiﬁer works remarkably well, with test error rates near those\\nfor the human eye (this is a notoriously diﬃcult test set). In practice,\\nit turned out that nearest-neighbors are too slow for online classiﬁcation\\nin this application (see Section 13.5), and neural network classiﬁers were\\nsubsequently developed to mimic it.\\n13.4 Adaptive Nearest-Neighbor Methods\\nWhen nearest-neighbor classiﬁcation is carried out in a high-dimensional\\nfeature space, the nearest neighbors of a point can be very far away, causing\\nbias and degrading the performance of the rule.\\nTo quantify this, consider Ndata points uniformly distributed in the unit\\ncube [−1\\n2,1\\n2]p. LetRbe the radius of a 1-nearest-neighborhood centered at\\nthe origin. Then\\nmedian( R) =v−1/p\\np(\\n1−1\\n21/N)1/p\\n, (13.7)\\nwhere vprpis the volume of the sphere of radius rinpdimensions. Fig-\\nure 13.12 shows the median radius for various training sample sizes and\\ndimensions. We see that median radius quickly approaches 0 .5, the dis-\\ntance to the edge of the cube.\\nWhat can be done about this problem? Consider the two-class situation\\nin Figure 13.13. There are two features, and a nearest-neighborhood at\\na query point is depicted by the circular region. Implicit in near-neighbor\\nclassiﬁcation is the assumption that the class probabilities are roughly con-\\nstant in the neighborhood, and hence simple averages give good estimates.\\nHowever, in this example the class probabilities vary only in the horizontal\\ndirection. If we knew this, we would stretch the neighborhood in the verti-\\ncal direction, as shown by the tall rectangular region. This will reduce the\\nbias of our estimate and leave the variance the same.\\nIn general, this calls for adapting the metric used in nearest-neighbor\\nclassiﬁcation, so that the resulting neighborhoods stretch out in directions\\nfor which the class probabilities don’t change much. In high-dimensional\\nfeature space, the class probabilities might change only a low-dimensional\\nsubspace and hence there can be considerable advantage to adapting the\\nmetric.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fccce1ec-42af-42e3-bb2e-280631c13248', embedding=None, metadata={'page_label': '495', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='476 13. Prototypes and Nearest-Neighbors\\nDimensionMedian Radius\\n0 5 10 150.0 0.1 0.2 0.3 0.4 0.5 0.6N=100N=1,000\\nN=10,000\\nFIGURE 13.12. Median radius of a 1-nearest-neighborhood, for uniform data\\nwithNobservations in pdimensions.\\no\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noo\\no\\noo\\noo\\no\\n•5-Nearest Neighborhoods\\nFIGURE 13.13. The points are uniform in the cube, with the vertical line sepa-\\nrating class red and green. The vertical strip denotes the 5-nearest-neighbor region\\nusing only the horizontal coordinate to ﬁnd the nearest-neighbors fo r the target\\npoint (solid dot). The sphere shows the 5-nearest-neighbor region using both co-\\nordinates, and we see in this case it has extended into the class-re d region (and\\nis dominated by the wrong class in this instance).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89b3ea53-0ff1-4228-a32f-b956bf57edbc', embedding=None, metadata={'page_label': '496', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.4 Adaptive Nearest-Neighbor Methods 477\\nFriedman (1994a) proposed a method in which rectangular neighbor-\\nhoods are found adaptively by successively carving away edges of a box\\ncontaining the training data. Here we describe the discriminant adaptive\\nnearest-neighbor (DANN) rule of Hastie and Tibshirani (1996a). Earlier,\\nrelated proposals appear in Short and Fukunaga (1981) and Myles and\\nHand (1990).\\nAt each query point a neighborhood of say 50 points is formed, and the\\nclass distribution among the points is used to decide how to deform the\\nneighborhood—that is, to adapt the metric. The adapted metric is then\\nused in a nearest-neighbor rule at the query point. Thus at each query\\npoint a potentially diﬀerent metric is used.\\nIn Figure 13.13 it is clear that the neighborhood should be stretched in\\nthe direction orthogonal to line joining the class centroids. This direction\\nalso coincides with the linear discriminant boundary, and is the direction\\nin which the class probabilities change the least. In general this direction\\nof maximum change will not be orthogonal to the line joining the class cen-\\ntroids (see Figure 4.9 on page 116.) Assuming a local discriminant model,\\nthe information contained in the local within- and between-class covari-\\nance matrices is all that is needed to determine the optimal shape of the\\nneighborhood.\\nThediscriminant adaptive nearest-neighbor (DANN) metric at a query\\npoint x0is deﬁned by\\nD(x,x0) = (x−x0)TΣ(x−x0), (13.8)\\nwhere\\nΣ=W−1/2[W−1/2BW−1/2+ǫI]W−1/2\\n=W−1/2[B∗+ǫI]W−1/2. (13.9)\\nHereWis the pooled within-class covariance matrix∑K\\nk=1πkWkandB\\nis the between class covariance matrix∑K\\nk=1πk(¯xk−¯x)(¯xk−¯x)T, with\\nWandBcomputed using only the 50 nearest neighbors around x0. After\\ncomputation of the metric, it is used in a nearest-neighbor rule at x0.\\nThis complicated formula is actually quite simple in its operation. It ﬁrst\\nspheres the data with respect to W, and then stretches the neighborhood\\nin the zero-eigenvalue directions of B∗(the between-matrix for the sphered\\ndata ). This makes sense, since locally the observed class means do not dif-\\nfer in these directions. The ǫparameter rounds the neighborhood, from an\\ninﬁnite strip to an ellipsoid, to avoid using points far away from the quer y\\npoint. The value of ǫ= 1 seems to work well in general. Figure 13.14 shows\\nthe resulting neighborhoods for a problem where the classes form two con-\\ncentric circles. Notice how the neighborhoods stretch out orthogonally to\\nthe decision boundaries when both classes are present in the neighborhood.\\nIn the pure regions with only one class, the neighborhoods remain circular;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d2cf580f-3b56-4819-a550-7cac4e0ba53c', embedding=None, metadata={'page_label': '497', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='478 13. Prototypes and Nearest-Neighbors\\no\\noo ooo\\noo\\noo\\noo\\noo\\no\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\nooooo\\noo\\no\\nooo\\noo ooo\\noo\\no\\nooo\\nooo\\noo\\no oo\\nooo\\no\\nooo\\nooo\\noooooo\\no\\no\\noo\\noo\\nooooo\\noo\\no\\noooo\\noo\\no\\no\\no\\noo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\noo\\no\\nooo\\noo\\nooo\\nooo\\no\\noo\\noooo\\noo o\\noo\\noooo\\noo\\noooo\\noo\\no\\no\\noo\\no\\nooo\\nooo\\noo\\no\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\no\\noo\\no\\nooo\\no\\noooo\\noo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noooo\\no\\noo\\nooooo\\noo\\no\\noo\\nooo\\noo\\no o\\nooo\\nooo\\no\\noooo\\nooo\\noo\\no\\nooo\\noo\\noo\\noo\\noo\\noo\\no\\noo\\no o\\noooo\\noo\\noo\\noo\\noooo\\no\\nooo\\nooo\\noo\\noo oo\\no\\noo\\noooo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo o\\no ooo\\nooooo\\noo\\noo\\no\\no\\nooo\\noo\\nooo\\no\\noo\\nooo\\noo\\nooo\\noo\\noooo ooo\\noo\\noo\\no\\no\\noo\\nooo\\nFIGURE 13.14. Neighborhoods found by the DANN procedure, at various query\\npoints (centers of the crosses). There are two classes in the da ta, with one class\\nsurrounding the other. 50nearest-neighbors were used to estimate the local met-\\nrics. Shown are the resulting metrics used to form 15-nearest-neighborhoods.\\nin these cases the between matrix B= 0, and the Σin (13.8) is the identity\\nmatrix.\\n13.4.1 Example\\nHere we generate two-class data in ten dimensions, analogous to the two-\\ndimensional example of Figure 13.14. All ten predictors in class 1 are in-\\ndependent standard normal, conditioned on the radius being greater than\\n22.4 and less than 40, while the predictors in class 2 are independent stan-\\ndard normal without the restriction. There are 250 observations in each\\nclass. Hence the ﬁrst class almost completely surrounds the second class in\\nthe full ten-dimensional space.\\nIn this example there are no pure noise variables, the kind that a nearest-\\nneighbor subset selection rule might be able to weed out. At any given\\npoint in the feature space, the class discrimination occurs along only one\\ndirection. However, this direction changes as we move across the feature\\nspace and all variables are important somewhere in the space.\\nFigure 13.15 shows boxplots of the test error rates over ten realiza-\\ntions, for standard 5-nearest-neighbors, LVQ, and discriminant adaptive\\n5-nearest-neighbors. We used 50 prototypes per class for LVQ, to make\\nit comparable to 5 nearest-neighbors (since 250 /5 = 50). The adaptive\\nmetric signiﬁcantly reduces the error rate, compared to LVQ or standard\\nnearest-neighbors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='10299aa8-6d0c-44f2-b65a-eb467b5ccccc', embedding=None, metadata={'page_label': '498', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.4 Adaptive Nearest-Neighbor Methods 4790.0 0.1 0.2 0.3 0.4\\n5NN LVQ DANNTest Error\\nFIGURE 13.15. Ten-dimensional simulated example: boxplots of the test error\\nrates over ten realizations, for standard 5-nearest-neighbors, LVQ with 50centers,\\nand discriminant-adaptive 5-nearest-neighbors\\n13.4.2 Global Dimension Reduction for Nearest-Neighbors\\nThe discriminant-adaptive nearest-neighbor method carries out local di-\\nmension reduction—that is, dimension reduction separately at each query\\npoint. In many problems we can also beneﬁt from global dimension re-\\nduction, that is, apply a nearest-neighbor rule in some optimally chosen\\nsubspace of the original feature space. For example, suppose that the two\\nclasses form two nested spheres in four dimensions of feature space, and\\nthere are an additional six noise features whose distribution is independent\\nof class. Then we would like to discover the important four-dimensional\\nsubspace, and carry out nearest-neighbor classiﬁcation in that reduced sub-\\nspace. Hastie and Tibshirani (1996a) discuss a variation of the discriminan t-\\nadaptive nearest-neighbor method for this purpose. At each training point\\nxi, the between-centroids sum of squares matrix Biis computed, and then\\nthese matrices are averaged over all training points:\\n¯B=1\\nNN∑\\ni=1Bi. (13.10)\\nLete1,e2,... ,e pbe the eigenvectors of the matrix ¯B, ordered from largest\\nto smallest eigenvalue θk. Then these eigenvectors span the optimal sub-\\nspaces for global subspace reduction. The derivation is based on the fact\\nthat the best rank- Lapproximation to ¯B,¯B[L]=∑L\\nℓ=1θℓeℓeT\\nℓ, solves the\\nleast squares problem\\nmin\\nrank(M)=LN∑\\ni=1trace[(Bi−M)2]. (13.11)\\nSince each Bicontains information on (a) the local discriminant subspace,\\nand (b) the strength of discrimination in that subspace, (13.11) can be seen', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cd8633cf-c045-4f59-baa4-8db9278dbf91', embedding=None, metadata={'page_label': '499', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='480 13. Prototypes and Nearest-Neighbors\\nas a way of ﬁnding the best approximating subspace of dimension Lto a\\nseries of Nsubspaces by weighted least squares (Exercise 13.5.)\\nIn the four-dimensional sphere example mentioned above and examined\\nin Hastie and Tibshirani (1996a), four of the eigenvalues θℓturn out to be\\nlarge (having eigenvectors nearly spanning the interesting subspace), and\\nthe remaining six are near zero. Operationally, we project the data into\\nthe leading four-dimensional subspace, and then carry out nearest neighbor\\nclassiﬁcation. In the satellite image classiﬁcation example in Section 13. 3.2,\\nthe technique labeled DANNin Figure 13.8 used 5-nearest-neighbors in a\\nglobally reduced subspace. There are also connections of this technique\\nwith the sliced inverse regression proposal of Duan and Li (1991). These\\nauthors use similar ideas in the regression setting, but do global rather\\nthan local computations. They assume and exploit spherical symmetry of\\nthe feature distribution to estimate interesting subspaces.\\n13.5 Computational Considerations\\nOne drawback of nearest-neighbor rules in general is the computational\\nload, both in ﬁnding the neighbors and storing the entire training set. With\\nNobservations and ppredictors, nearest-neighbor classiﬁcation requires Np\\noperations to ﬁnd the neighbors per query point. There are fast algorithms\\nfor ﬁnding nearest-neighbors (Friedman et al., 1975; Friedman et al., 1977)\\nwhich can reduce this load somewhat. Hastie and Simard (1998) reduce\\nthe computations for tangent distance by developing analogs of K-means\\nclustering in the context of this invariant metric.\\nReducing the storage requirements is more diﬃcult, and various editing\\nandcondensing procedures have been proposed. The idea is to isolate a\\nsubset of the training set that suﬃces for nearest-neighbor predictions, and\\nthrow away the remaining training data. Intuitively, it seems important t o\\nkeep the training points that are near the decision boundaries and on the\\ncorrect side of those boundaries, while some points far from the boundaries\\ncould be discarded.\\nThemulti-edit algorithm of Devijver and Kittler (1982) divides the data\\ncyclically into training and test sets, computing a nearest neighbor rule on\\nthe training set and deleting test points that are misclassiﬁed. The idea is\\nto keep homogeneous clusters of training observations.\\nThecondensing procedure of Hart (1968) goes further, trying to keep\\nonly important exterior points of these clusters. Starting with a single ran-\\ndomly chosen observation as the training set, each additional data item is\\nprocessed one at a time, adding it to the training set only if it is misclas-\\nsiﬁed by a nearest-neighbor rule computed on the current training set.\\nThese procedures are surveyed in Dasarathy (1991) and Ripley (1996).\\nThey can also be applied to other learning procedures besides nearest-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8dd02688-645e-435d-b632-e8da2735dc94', embedding=None, metadata={'page_label': '500', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 481\\nneighbors. While such methods are sometimes useful, we have not had\\nmuch practical experience with them, nor have we found any systematic\\ncomparison of their performance in the literature.\\nBibliographic Notes\\nThe nearest-neighbor method goes back at least to Fix and Hodges (1951).\\nThe extensive literature on the topic is reviewed by Dasarathy (1991);\\nChapter 6 of Ripley (1996) contains a good summary. K-means cluster-\\ning is due to Lloyd (1957) and MacQueen (1967). Kohonen (1989) intro-\\nduced learning vector quantization. The tangent distance method is due to\\nSimard et al. (1993). Hastie and Tibshirani (1996a) proposed the discrim -\\ninant adaptive nearest-neighbor technique.\\nExercises\\nEx. 13.1 Consider a Gaussian mixture model where the covariance matrices\\nare assumed to be scalar: Σr=σI∀r= 1,... ,R , and σis a ﬁxed param-\\neter. Discuss the analogy between the K-means clustering algorithm and\\nthe EM algorithm for ﬁtting this mixture model in detail. Show that in the\\nlimitσ→0 the two methods coincide.\\nEx. 13.2 Derive formula (13.7) for the median radius of the 1-nearest-\\nneighborhood.\\nEx. 13.3 LetE∗be the error rate of the Bayes rule in a K-class problem,\\nwhere the true class probabilities are given by pk(x), k= 1,... ,K . As-\\nsuming the test point and training point have identical features x, prove\\n(13.5)\\nK∑\\nk=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K\\nK−1(1−pk∗(x))2.\\nwhere k∗= arg max kpk(x). Hence argue that the error rate of the 1-\\nnearest-neighbor rule converges in L1, as the size of the training set in-\\ncreases, to a value E1, bounded above by\\nE∗(\\n2−E∗K\\nK−1)\\n. (13.12)\\n[This statement of the theorem of Cover and Hart (1967) is taken from\\nChapter 6 of Ripley (1996), where a short proof is also given].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='88bd0916-710c-41e4-9797-018cf8bfcfff', embedding=None, metadata={'page_label': '501', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='482 13. Prototypes and Nearest-Neighbors\\nEx. 13.4 Consider an image to be a function F(x) : IR2↦→IR1over the two-\\ndimensional spatial domain (paper coordinates). Then F(c+x0+A(x−x0))\\nrepresents an aﬃne transformation of the image F, where Ais a 2 ×2\\nmatrix.\\n1. Decompose A(via Q-R) in such a way that parameters identifying\\nthe four aﬃne transformations (two scale, shear and rotation) are\\nclearly identiﬁed.\\n2. Using the chain rule, show that the derivative of F(c+x0+A(x−x0))\\nw.r.t. each of these parameters can be represented in terms of the two\\nspatial derivatives of F.\\n3. Using a two-dimensional kernel smoother (Chapter 6), describe how\\nto implement this procedure when the images are quantized to 16 ×16\\npixels.\\nEx. 13.5 LetBi,i= 1,2,... ,N be square p×ppositive semi-deﬁnite ma-\\ntrices and let ¯B= (1/N)∑Bi. Write the eigen-decomposition of ¯Bas∑p\\nℓ=1θℓeℓeT\\nℓwithθℓ≥θℓ−1≥ ≤≤≤ ≥ θ1. Show that the best rank- Lapprox-\\nimation for the Bi,\\nmin\\nrank(M)=LN∑\\ni=1trace[(Bi−M)2],\\nis given by ¯B[L]=∑L\\nℓ=1θℓeℓeT\\nℓ. (Hint: Write∑N\\ni=1trace[(Bi−M)2] as\\nN∑\\ni=1trace[(Bi−¯B)2] +N∑\\ni=1trace[(M−¯B)2]).\\nEx. 13.6 Here we consider the problem of shape averaging . In particular,\\nLi, i= 1,... ,M are each N×2 matrices of points in IR2, each sampled\\nfrom corresponding positions of handwritten (cursive) letters. We seek an\\naﬃne invariant average V, also N×2,VTV=I, of the Mletters Liwith\\nthe following property: Vminimizes\\nM∑\\nj=1min\\nAj∥Lj−VAj∥2.\\nCharacterize the solution.\\nThis solution can suﬀer if some of the letters are bigand dominate the\\naverage. An alternative approach is to minimize instead:\\nM∑\\nj=1min\\nAj\\ued79\\ued79LjA∗\\nj−V\\ued79\\ued792.\\nDerive the solution to this problem. How do the criteria diﬀer? Use the\\nSVD of the Ljto simplify the comparison of the two approaches.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ed41888-9e3f-47e1-bb6f-a2019d2ae036', embedding=None, metadata={'page_label': '502', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 483\\nEx. 13.7 Consider the application of nearest-neighbors to the “easy” and\\n“hard” problems in the left panel of Figure 13.5.\\n1. Replicate the results in the left panel of Figure 13.5.\\n2. Estimate the misclassiﬁcation errors using ﬁvefold cross-validation,\\nand compare the error rate curves to those in 1.\\n3. Consider an “AIC-like” penalization of the training set misclassiﬁca-\\ntion error. Speciﬁcally, add 2 t/Nto the training set misclassiﬁcation\\nerror, where tis the approximate number of parameters N/r,rbe-\\ning the number of nearest-neighbors. Compare plots of the resulting\\npenalized misclassiﬁcation error to those in 1 and 2. Which method\\ngives a better estimate of the optimal number of nearest-neighbors:\\ncross-validation or AIC?\\nEx. 13.8 Generate data in two classes, with two features. These features\\nare all independent Gaussian variates with standard deviation 1. Their\\nmean vectors are ( −1,−1) in class 1 and (1 ,1) in class 2. To each feature\\nvector apply a random rotation of angle θ,θchosen uniformly from 0 to\\n2π. Generate 50 observations from each class to form the training set, and\\n500 in each class as the test set. Apply four diﬀerent classiﬁers:\\n1. Nearest-neighbors.\\n2. Nearest-neighbors with hints: ten randomly rotated versions of each\\ndata point are added to the training set before applying nearest-\\nneighbors.\\n3. Invariant metric nearest-neighbors, using Euclidean distance invari-\\nant to rotations about the origin.\\n4. Tangent distance nearest-neighbors.\\nIn each case choose the number of neighbors by tenfold cross-validation.\\nCompare the results.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53b2cc91-229e-4f26-9e7c-c3ae765366c9', embedding=None, metadata={'page_label': '503', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='484 13. Prototypes and Nearest-Neighbors', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2ef42130-f68e-48f7-bf35-36c82503ee0b', embedding=None, metadata={'page_label': '504', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 485\\nPrinter: Opaque this\\n14\\nUnsupervised Learning\\n14.1 Introduction\\nThe previous chapters have been concerned with predicting the values\\nof one or more outputs or response variables Y= (Y1,... ,Y m) for a\\ngiven set of input or predictor variables XT= (X1,... ,X p). Denote by\\nxT\\ni= (xi1,... ,x ip) the inputs for the ith training case, and let yibe a\\nresponse measurement. The predictions are based on the training sample\\n(x1,y1),... ,(xN,yN) of previously solved cases, where the joint values of\\nall of the variables are known. This is called supervised learning or “learn-\\ning with a teacher.” Under this metaphor the “student” presents an an-\\nswer ˆyifor each xiin the training sample, and the supervisor or “teacher”\\nprovides either the correct answer and/or an error associated with the stu-\\ndent’s answer. This is usually characterized by some loss function L(y,ˆy),\\nfor example, L(y,ˆy) = (y−ˆy)2.\\nIf one supposes that ( X,Y) are random variables represented by some\\njoint probability density Pr( X,Y), then supervised learning can be formally\\ncharacterized as a density estimation problem where one is concerned with\\ndetermining properties of the conditional density Pr( Y|X). Usually the\\nproperties of interest are the “location” parameters θthat minimize the\\nexpected error at each x,\\nθ(x) = argmin\\nθEY|XL(Y,θ). (14.1)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6191f1cb-9fc0-46f0-b4cc-73c43e8739fe', embedding=None, metadata={'page_label': '505', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='486 14. Unsupervised Learning\\nConditioning one has\\nPr(X,Y) = Pr( Y|X)≤Pr(X),\\nwhere Pr( X) is the joint marginal density of the Xvalues alone. In su-\\npervised learning Pr( X) is typically of no direct concern. One is interested\\nmainly in the properties of the conditional density Pr( Y|X). Since Yis of-\\nten of low dimension (usually one), and only its location θ(x) is of interest,\\nthe problem is greatly simpliﬁed. As discussed in the previous chapters,\\nthere are many approaches for successfully addressing supervised learning\\nin a variety of contexts.\\nIn this chapter we address unsupervised learning or “learning without a\\nteacher.” In this case one has a set of Nobservations ( x1,x2,... ,x N) of a\\nrandom p-vector Xhaving joint density Pr( X). The goal is to directly infer\\nthe properties of this probability density without the help of a supervisor or\\nteacher providing correct answers or degree-of-error for each observation.\\nThe dimension of Xis sometimes much higher than in supervised learn-\\ning, and the properties of interest are often more complicated than simple\\nlocation estimates. These factors are somewhat mitigated by the fact that\\nXrepresents all of the variables under consideration; one is not required\\nto infer how the properties of Pr( X) change, conditioned on the changing\\nvalues of another set of variables.\\nIn low-dimensional problems (say p≤3), there are a variety of eﬀective\\nnonparametric methods for directly estimating the density Pr( X) itself at\\nallX-values, and representing it graphically (Silverman, 1986, e.g.). Owing\\nto the curse of dimensionality, these methods fail in high dimensions. One\\nmust settle for estimating rather crude global models, such as Gaussian\\nmixtures or various simple descriptive statistics that characterize Pr( X).\\nGenerally, these descriptive statistics attempt to characterize X-values,\\nor collections of such values, where Pr( X) is relatively large. Principal\\ncomponents, multidimensional scaling, self-organizing maps, and principal\\ncurves, for example, attempt to identify low-dimensional manifolds within\\ntheX-space that represent high data density. This provides information\\nabout the associations among the variables and whether or not they can be\\nconsidered as functions of a smaller set of “latent” variables. Cluster anal-\\nysis attempts to ﬁnd multiple convex regions of the X-space that contain\\nmodes of Pr( X). This can tell whether or not Pr( X) can be represented by\\na mixture of simpler densities representing distinct types or classes of ob-\\nservations. Mixture modeling has a similar goal. Association rules att empt\\nto construct simple descriptions (conjunctive rules) that describe regions\\nof high density in the special case of very high dimensional binary-valued\\ndata.\\nWith supervised learning there is a clear measure of success, or lack\\nthereof, that can be used to judge adequacy in particular situations and\\nto compare the eﬀectiveness of diﬀerent methods over various situations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a9099774-381f-4e8e-b0e3-2b28bcfc4580', embedding=None, metadata={'page_label': '506', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2 Association Rules 487\\nLack of success is directly measured by expected loss over the joint dis-\\ntribution Pr( X,Y). This can be estimated in a variety of ways including\\ncross-validation. In the context of unsupervised learning, there is no such\\ndirect measure of success. It is diﬃcult to ascertain the validity of inferences\\ndrawn from the output of most unsupervised learning algorithms. One must\\nresort to heuristic arguments not only for motivating the algorithms, as is\\noften the case in supervised learning as well, but also for judgments as to\\nthe quality of the results. This uncomfortable situation has led to heavy\\nproliferation of proposed methods, since eﬀectiveness is a matter of opinion\\nand cannot be veriﬁed directly.\\nIn this chapter we present those unsupervised learning techniques that\\nare among the most commonly used in practice, and additionally, a few\\nothers that are favored by the authors.\\n14.2 Association Rules\\nAssociation rule analysis has emerged as a popular tool for mining com-\\nmercial data bases. The goal is to ﬁnd joint values of the variables X=\\n(X1,X2,... ,X p) that appear most frequently in the data base. It is most\\noften applied to binary-valued data Xj∈ {0,1}, where it is referred to as\\n“market basket” analysis. In this context the observations are sales trans -\\nactions, such as those occurring at the checkout counter of a store. The\\nvariables represent all of the items sold in the store. For observation i, each\\nvariable Xjis assigned one of two values; xij= 1 if the jth item is pur-\\nchased as part of the transaction, whereas xij= 0 if it was not purchased.\\nThose variables that frequently have joint values of one represent items that\\nare frequently purchased together. This information can be quite useful for\\nstocking shelves, cross-marketing in sales promotions, catalog design, and\\nconsumer segmentation based on buying patterns.\\nMore generally, the basic goal of association rule analysis is to ﬁnd a\\ncollection of prototype X-values v1,... ,v Lfor the feature vector X, such\\nthat the probability density Pr( vl) evaluated at each of those values is rela-\\ntively large. In this general framework, the problem can be viewed as “mode\\nﬁnding” or “bump hunting.” As formulated, this problem is impossibly dif-\\nﬁcult. A natural estimator for each Pr( vl) is the fraction of observations\\nfor which X=vl. For problems that involve more than a small number\\nof variables, each of which can assume more than a small number of val-\\nues, the number of observations for which X=vlwill nearly always be too\\nsmall for reliable estimation. In order to have a tractable problem, both t he\\ngoals of the analysis and the generality of the data to which it is applied\\nmust be greatly simpliﬁed.\\nThe ﬁrst simpliﬁcation modiﬁes the goal. Instead of seeking values x\\nwhere Pr( x) is large, one seeks regions of the X-space with high probability', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d65e197-114e-4b71-ad8d-d352cfbc02e9', embedding=None, metadata={'page_label': '507', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='488 14. Unsupervised Learning\\ncontent relative to their size or support. Let Sjrepresent the set of all\\npossible values of the jth variable (its support ), and let sj⊆ Sjbe a subset\\nof these values. The modiﬁed goal can be stated as attempting to ﬁnd\\nsubsets of variable values s1,... ,s psuch that the probability of each of the\\nvariables simultaneously assuming a value within its respective subset,\\nPr\\uf8ee\\n\\uf8f0p⋂\\nj=1(Xj∈sj)\\uf8f9\\n\\uf8fb, (14.2)\\nis relatively large. The intersection of subsets ∩p\\nj=1(Xj∈sj) is called a\\nconjunctive rule . For quantitative variables the subsets sjare contiguous\\nintervals; for categorical variables the subsets are delineated explicitly. No te\\nthat if the subset sjis in fact the entire set of values sj=Sj, as is often\\nthe case, the variable Xjis said notto appear in the rule (14.2).\\n14.2.1 Market Basket Analysis\\nGeneral approaches to solving (14.2) are discussed in Section 14.2.5. These\\ncan be quite useful in many applications. However, they are not feasible\\nfor the very large ( p≈104,N≈108) commercial data bases to which\\nmarket basket analysis is often applied. Several further simpliﬁcations of\\n(14.2) are required. First, only two types of subsets are considered; either\\nsjconsists of a single value of Xj,sj=v0j, or it consists of the entire set\\nof values that Xjcan assume, sj=Sj. This simpliﬁes the problem (14.2)\\nto ﬁnding subsets of the integers J ⊂ { 1,... ,p }, and corresponding values\\nv0j, j∈ J, such that\\nPr\\uf8ee\\n\\uf8f0⋂\\nj∈J(Xj=v0j)\\uf8f9\\n\\uf8fb (14.3)\\nis large. Figure 14.1 illustrates this assumption.\\nOne can apply the technique of dummy variables to turn (14.3) into\\na problem involving only binary-valued variables. Here we assume that\\nthe support Sjis ﬁnite for each variable Xj. Speciﬁcally, a new set of\\nvariables Z1,... ,Z Kis created, one such variable for each of the values\\nvljattainable by each of the original variables X1,... ,X p. The number of\\ndummy variables Kis\\nK=p∑\\nj=1|Sj|,\\nwhere |Sj|is the number of distinct values attainable by Xj. Each dummy\\nvariable is assigned the value Zk= 1 if the variable with which it is as-\\nsociated takes on the corresponding value to which Zkis assigned, and\\nZk= 0 otherwise. This transforms (14.3) to ﬁnding a subset of the integers\\nK ⊂ { 1,... ,K }such that', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a0c9b63-6ef9-41d3-b8e4-104fcf66485c', embedding=None, metadata={'page_label': '508', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2 Association Rules 489\\nX1 X1 X1\\nX2X2X2\\nFIGURE 14.1. Simpliﬁcations for association rules. Here there are two inputs\\nX1andX2, taking four and six distinct values, respectively. The red squ ares\\nindicate areas of high density. To simplify the computations, w e assume that the\\nderived subset corresponds to either a single value of an input o r all values. With\\nthis assumption we could ﬁnd either the middle or right pattern, but not the left\\none.\\nPr[⋂\\nk∈K(Zk= 1)]\\n= Pr[∏\\nk∈KZk= 1]\\n(14.4)\\nis large. This is the standard formulation of the market basket problem.\\nThe set Kis called an “item set.” The number of variables Zkin the item\\nset is called its “size” (note that the size is no bigger than p). The estimated\\nvalue of (14.4) is taken to be the fraction of observations in the data bas e\\nfor which the conjunction in (14.4) is true:\\nˆPr[∏\\nk∈K(Zk= 1)]\\n=1\\nNN∑\\ni=1∏\\nk∈Kzik. (14.5)\\nHerezikis the value of Zkfor this ith case. This is called the “support” or\\n“prevalence” T(K) of the item set K. An observation ifor which∏\\nk∈Kzik=\\n1 is said to “contain” the item set K.\\nIn association rule mining a lower support bound tis speciﬁed, and one\\nseeksallitem sets Klthat can be formed from the variables Z1,... ,Z K\\nwith support in the data base greater than this lower bound t\\n{Kl|T(Kl)> t}. (14.6)\\n14.2.2 The Apriori Algorithm\\nThe solution to this problem (14.6) can be obtained with feasible compu-\\ntation for very large data bases provided the threshold tis adjusted so that\\n(14.6) consists of only a small fraction of all 2Kpossible item sets. The\\n“Apriori” algorithm (Agrawal et al., 1995) exploits several aspects o f the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='989a6ba1-4c48-4f60-9ed5-91b84abded0f', embedding=None, metadata={'page_label': '509', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='490 14. Unsupervised Learning\\ncurse of dimensionality to solve (14.6) with a small number of passes over\\nthe data. Speciﬁcally, for a given support threshold t:\\n•The cardinality |{K|T(K)> t}|is relatively small.\\n•Any item set Lconsisting of a subset of the items in Kmust have\\nsupport greater than or equal to that of K,L ⊆ K ⇒ T(L)≥T(K).\\nThe ﬁrst pass over the data computes the support of all single-item sets.\\nThose whose support is less than the threshold are discarded. The second\\npass computes the support of all item sets of size two that can be formed\\nfrom pairs of the single items surviving the ﬁrst pass. In other words, to\\ngenerate all frequent itemsets with |K|=m, we need to consider only\\ncandidates such that allof their mancestral item sets of size m−1 are\\nfrequent. Those size-two item sets with support less than the threshold are\\ndiscarded. Each successive pass over the data considers only those item\\nsets that can be formed by combining those that survived the previous\\npass with those retained from the ﬁrst pass. Passes over the data continue\\nuntil all candidate rules from the previous pass have support less than the\\nspeciﬁed threshold. The Apriori algorithm requires only one pass over the\\ndata for each value of |K|, which is crucial since we assume the data cannot\\nbe ﬁtted into a computer’s main memory. If the data are suﬃciently sparse\\n(or if the threshold tis high enough), then the process will terminate in\\nreasonable time even for huge data sets.\\nThere are many additional tricks that can be used as part of this strat-\\negy to increase speed and convergence (Agrawal et al., 1995). The Apriori\\nalgorithm represents one of the major advances in data mining technology.\\nEach high support item set K(14.6) returned by the Apriori algorithm is\\ncast into a set of “association rules.” The items Zk,k∈ K, are partitioned\\ninto two disjoint subsets, A∪B=K, and written\\nA⇒B. (14.7)\\nThe ﬁrst item subset Ais called the “antecedent” and the second Bthe\\n“consequent.” Association rules are deﬁned to have several properties based\\non the prevalence of the antecedent and consequent item sets in the data\\nbase. The “support” of the rule T(A⇒B) is the fraction of observations\\nin the union of the antecedent and consequent, which is just the support\\nof the item set Kfrom which they were derived. It can be viewed as an\\nestimate (14.5) of the probability of simultaneously observing both item\\nsets Pr( AandB) in a randomly selected market basket. The “conﬁdence”\\nor “predictability” C(A⇒B) of the rule is its support divided by the\\nsupport of the antecedent\\nC(A⇒B) =T(A⇒B)\\nT(A), (14.8)\\nwhich can be viewed as an estimate of Pr( B|A). The notation Pr( A), the\\nprobability of an item set Aoccurring in a basket, is an abbreviation for', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9cfb9064-485c-4536-9e1c-e5d8ab6e3c6d', embedding=None, metadata={'page_label': '510', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2 Association Rules 491\\nPr(∏\\nk∈AZk= 1). The “expected conﬁdence” is deﬁned as the support of\\nthe consequent T(B), which is an estimate of the unconditional probability\\nPr(B). Finally, the “lift” of the rule is deﬁned as the conﬁdence divided by\\nthe expected conﬁdence\\nL(A⇒B) =C(A⇒B)\\nT(B).\\nThis is an estimate of the association measure Pr( AandB)/Pr(A)Pr(B).\\nAs an example, suppose the item set K={peanut butter, jelly, bread }\\nand consider the rule {peanut butter, jelly } ⇒ {bread}. A support value\\nof 0.03 for this rule means that peanut butter ,jelly, andbread appeared\\ntogether in 3% of the market baskets. A conﬁdence of 0.82 for this rule im-\\nplies that when peanut butter andjelly were purchased, 82% of the time\\nbread was also purchased. If bread appeared in 43% of all market baskets\\nthen the rule {peanut butter, jelly } ⇒ {bread}would have a lift of 1 .95.\\nThe goal of this analysis is to produce association rules (14.7) with bot h\\nhigh values of support and conﬁdence (14.8). The Apriori algorithm returns\\nall item sets with high support as deﬁned by the support threshold t(14.6).\\nA conﬁdence threshold cis set, and all rules that can be formed from those\\nitem sets (14.6) with conﬁdence greater than this value\\n{A⇒B|C(A⇒B)> c} (14.9)\\nare reported. For each item set Kof size |K|there are 2|K|−1−1 rules of\\nthe form A⇒(K −A),A⊂ K. Agrawal et al. (1995) present a variant of\\nthe Apriori algorithm that can rapidly determine which rules survive the\\nconﬁdence threshold (14.9) from all possible rules that can be formed from\\nthe solution item sets (14.6).\\nThe output of the entire analysis is a collection of association rules (14.7 )\\nthat satisfy the constraints\\nT(A⇒B)> t and C(A⇒B)> c.\\nThese are generally stored in a data base that can be queried by the user.\\nTypical requests might be to display the rules in sorted order of conﬁdence,\\nlift or support. More speciﬁcally, one might request such a list conditioned\\non particular items in the antecedent or especially the consequent. For\\nexample, a request might be the following:\\nDisplay all transactions in which ice skates are the consequ ent\\nthat have conﬁdence over 80%and support of more than 2%.\\nThis could provide information on those items (antecedent) that predicate\\nsales of ice skates. Focusing on a particular consequent casts the problem\\ninto the framework of supervised learning.\\nAssociation rules have become a popular tool for analyzing very large\\ncommercial data bases in settings where market basket is relevant. That is', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f99443c4-b6e2-4011-a1b6-7016eccb5185', embedding=None, metadata={'page_label': '511', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='492 14. Unsupervised Learning\\nwhen the data can be cast in the form of a multidimensional contingency\\ntable. The output is in the form of conjunctive rules (14.4) that are easily\\nunderstood and interpreted. The Apriori algorithm allows this analysis to\\nbe applied to huge data bases, much larger that are amenable to other types\\nof analyses. Association rules are among data mining’s biggest successes.\\nBesides the restrictive form of the data to which they can be applied, as-\\nsociation rules have other limitations. Critical to computational feasibi lity\\nis the support threshold (14.6). The number of solution item sets, their size,\\nand the number of passes required over the data can grow exponentially\\nwith decreasing size of this lower bound. Thus, rules with high conﬁdence\\nor lift, but low support, will not be discovered. For example, a high conﬁ-\\ndence rule such as vodka ⇒caviar will not be uncovered owing to the low\\nsales volume of the consequent caviar .\\n14.2.3 Example: Market Basket Analysis\\nWe illustrate the use of Apriori on a moderately sized demographics data\\nbase. This data set consists of N= 9409 questionnaires ﬁlled out by shop-\\nping mall customers in the San Francisco Bay Area (Impact Resources, Inc.,\\nColumbus OH, 1987). Here we use answers to the ﬁrst 14 questions, relat-\\ning to demographics, for illustration. These questions are listed in Table\\n14.1. The data are seen to consist of a mixture of ordinal and (unordered)\\ncategorical variables, many of the latter having more than a few values.\\nThere are many missing values.\\nWe used a freeware implementation of the Apriori algorithm due to Chris-\\ntian Borgelt1. After removing observations with missing values, each ordinal\\npredictor was cut at its median and coded by two dummy variables; each\\ncategorical predictor with kcategories was coded by kdummy variables.\\nThis resulted in a 6876 ×50 matrix of 6876 observations on 50 dummy\\nvariables.\\nThe algorithm found a total of 6288 association rules, involving ≤5\\npredictors, with support of at least 10%. Understanding this large set of\\nrules is itself a challenging data analysis task. We will not attempt this here,\\nbut only illustrate in Figure 14.2 the relative frequency of each dummy\\nvariable in the data (top) and the association rules (bottom). Prevalent\\ncategories tend to appear more often in the rules, for example, the ﬁrst\\ncategory in language (English). However, others such as occupation are\\nunder-represented, with the exception of the ﬁrst and ﬁfth level.\\nHere are three examples of association rules found by the Apriori algo-\\nrithm:\\nAssociation rule 1: Support 25%, conﬁdence 99.7% and lift 1.03.\\n1Seehttp://fuzzy.cs.uni-magdeburg.de/ ∼borgelt.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb9a97b2-11e4-4088-b346-05089672620e', embedding=None, metadata={'page_label': '512', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2 Association Rules 493\\n0 10 20 30 40 500.0 0.02 0.04 0.06\\nAttributeRelative Frequency in Data\\nincomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic language\\n0 10 20 30 40 500.0 0.04 0.08 0.12\\nAttributeRelative Frequency in Association Rules\\nincomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic languageFIGURE 14.2. Market basket analysis: relative frequency of each dummy vari -\\nable (coding an input category) in the data (top), and the associ ation rules found\\nby the Apriori algorithm (bottom).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='112086d1-36e7-43ea-9deb-fc589955ed9b', embedding=None, metadata={'page_label': '513', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='494 14. Unsupervised Learning\\nTABLE 14.1. Inputs for the demographic data.\\nFeature Demographic # Values Type\\n1 Sex 2 Categorical\\n2 Marital status 5 Categorical\\n3 Age 7 Ordinal\\n4 Education 6 Ordinal\\n5 Occupation 9 Categorical\\n6 Income 9 Ordinal\\n7 Years in Bay Area 5 Ordinal\\n8 Dual incomes 3 Categorical\\n9 Number in household 9 Ordinal\\n10 Number of children 9 Ordinal\\n11 Householder status 3 Categorical\\n12 Type of home 5 Categorical\\n13 Ethnic classiﬁcation 8 Categorical\\n14 Language in home 3 Categorical\\n[number in household = 1\\nnumber of children = 0]\\n⇓\\nlanguage in home = English\\nAssociation rule 2: Support 13.4%, conﬁdence 80.8%, and lift 2.13.\\n\\uf8ee\\n\\uf8f0language in home = English\\nhouseholder status = own\\noccupation = {professional/managerial }\\uf8f9\\n\\uf8fb\\n⇓\\nincome ≥$40,000\\nAssociation rule 3: Support 26.5%, conﬁdence 82.8% and lift 2.15.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0language in home = English\\nincome <$40,000\\nmarital status = not married\\nnumber of children = 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n⇓\\neducation /∈ {college graduate, graduate study }', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cf1b0b0f-e22a-4108-b0f1-8cccb5095a95', embedding=None, metadata={'page_label': '514', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2 Association Rules 495\\nWe chose the ﬁrst and third rules based on their high support. The second\\nrule is an association rule with a high-income consequent, and could be\\nused to try to target high-income individuals.\\nAs stated above, we created dummy variables for each category of the\\ninput predictors, for example, Z1=I(income <$40,000) and Z2=\\nI(income ≥$40,000) for below and above the median income. If we were\\ninterested only in ﬁnding associations with the high-income category, we\\nwould include Z2but not Z1. This is often the case in actual market basket\\nproblems, where we are interested in ﬁnding associations with the presence\\nof a relatively rare item, but not associations with its absence.\\n14.2.4 Unsupervised as Supervised Learning\\nHere we discuss a technique for transforming the density estimation prob-\\nlem into one of supervised function approximation. This forms the basis\\nfor the generalized association rules described in the next section.\\nLetg(x) be the unknown data probability density to be estimated, and\\ng0(x) be a speciﬁed probability density function used for reference. For ex-\\nample, g0(x) might be the uniform density over the range of the variables.\\nOther possibilities are discussed below. The data set x1,x2,... ,x Nis pre-\\nsumed to be an i.i.d.random sample drawn from g(x). A sample of size N0\\ncan be drawn from g0(x) using Monte Carlo methods. Pooling these two\\ndata sets, and assigning mass w=N0/(N+N0) to those drawn from g(x),\\nandw0=N/(N+N0) to those drawn from g0(x), results in a random\\nsample drawn from the mixture density ( g(x) +g0(x))/2. If one assigns\\nthe value Y= 1 to each sample point drawn from g(x) and Y= 0 those\\ndrawn from g0(x), then\\nθ(x) =E(Y|x) =g(x)\\ng(x) +g0(x)\\n=g(x)/g0(x)\\n1 +g(x)/g0(x)(14.10)\\ncan be estimated by supervised learning using the combined sample\\n(y1,x1),(y2,x2),... ,(yN+N0,xN+N0) (14.11)\\nas training data. The resulting estimate ˆ θ(x) can be inverted to provide an\\nestimate for g(x)\\nˆg(x) =g0(x)ˆθ(x)\\n1−ˆθ(x). (14.12)\\nGeneralized versions of logistic regression (Section 4.4) are especially wel l\\nsuited for this application since the log-odds,\\nf(x) = logg(x)\\ng0(x), (14.13)\\nare estimated directly. In this case one has', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='517a1180-6138-4a40-8ae0-bbec8b8c629f', embedding=None, metadata={'page_label': '515', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='496 14. Unsupervised Learning\\n-1 0 1 2-2 0 2 4 6••\\n••••\\n••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•\\n••••\\n••••\\n••\\n•••\\n••\\n••\\n••\\n•\\n••••\\n•••\\n•\\n••\\n••••••\\n••••\\n• •\\n••\\n••••\\n••••\\n•\\n•••\\n•••••\\n•\\n• • •\\n••\\n••\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••••\\n••••\\n•••••\\n•••\\n••••••••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n• •\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•••••\\n•\\n-1 0 1 2-2 0 2 4 6••\\n••••\\n••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•\\n••••\\n••••\\n••\\n•••\\n••\\n••\\n••\\n•\\n••••\\n•••\\n•\\n••\\n••••••\\n••••\\n• •\\n••\\n••••\\n••••\\n•\\n•••\\n•••••\\n•\\n• • •\\n••\\n••\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••••\\n••••\\n•••••\\n•••\\n••••••••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n• •\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•••••\\n••\\n• ••\\n•\\n•••\\n••\\n•\\n••••\\n••\\n••••\\n•\\n••\\n••••\\n•\\n••\\n••\\n••\\n••\\n•••••• •\\n••••\\n•\\n•••\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••••\\n•\\n•••••\\n•\\n•• ••\\n••\\n••\\n••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n•••\\n•\\n••••\\n•\\n••\\n••\\n••\\n•••\\n•\\n•• •••\\n•••••\\n••\\n•••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•\\n••\\n••\\n••\\n••••\\n•••\\n•••\\n•\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n• •\\n••\\n••••\\n••\\n• •\\nX1 X1\\nX2X2\\nFIGURE 14.3. Density estimation via classiﬁcation. (Left panel:) Training set\\nof200data points. (Right panel:) Training set plus 200reference data points,\\ngenerated uniformly over the rectangle containing the training data . The training\\nsample was labeled as class 1, and the reference sample class 0, and a semipara-\\nmetric logistic regression model was ﬁt to the data. Some contou rs for ˆg(x)are\\nshown.\\nˆg(x) =g0(x)eˆf(x). (14.14)\\nAn example is shown in Figure 14.3. We generated a training set of size\\n200 shown in the left panel. The right panel shows the reference data (blue)\\ngenerated uniformly over the rectangle containing the training data. The\\ntraining sample was labeled as class 1, and the reference sample class 0,\\nand a logistic regression model, using a tensor product of natural splines\\n(Section 5.2.1), was ﬁt to the data. Some probability contours of ˆ θ(x) are\\nshown in the right panel; these are also the contours of the density estimate\\nˆg(x), since ˆ g(x) = ˆθ(x)/(1−ˆθ(x)), is a monotone function. The contours\\nroughly capture the data density.\\nIn principle any reference density can be used for g0(x) in (14.14). In\\npractice the accuracy of the estimate ˆ g(x) can depend greatly on partic-\\nular choices. Good choices will depend on the data density g(x) and the\\nprocedure used to estimate (14.10) or (14.13). If accuracy is the goal, g0(x)\\nshould be chosen so that the resulting functions θ(x) orf(x) are approx-\\nimated easily by the method being used. However, accuracy is not always\\nthe primary goal. Both θ(x) and f(x) are monotonic functions of the den-\\nsity ratio g(x)/g0(x). They can thus be viewed as “contrast” statistics that\\nprovide information concerning departures of the data density g(x) from\\nthe chosen reference density g0(x). Therefore, in data analytic settings, a\\nchoice for g0(x) is dictated by types of departures that are deemed most\\ninteresting in the context of the speciﬁc problem at hand. For example, if\\ndepartures from uniformity are of interest, g0(x) might be the a uniform\\ndensity over the range of the variables. If departures from joint normality', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16f6c857-4ac2-4379-b261-a8444351a066', embedding=None, metadata={'page_label': '516', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2 Association Rules 497\\nare of interest, a good choice for g0(x) would be a Gaussian distribution\\nwith the same mean vector and covariance matrix as the data. Departures\\nfrom independence could be investigated by using\\ng0(x) =p∏\\nj=1gj(xj), (14.15)\\nwhere gj(xj) is the marginal data density of Xj, thejth coordinate of X.\\nA sample from this independent density (14.15) is easily generated from the\\ndata itself by applying a diﬀerent random permutation to the data values\\nof each of the variables.\\nAs discussed above, unsupervised learning is concerned with revealing\\nproperties of the data density g(x). Each technique focuses on a particu-\\nlar property or set of properties. Although this approach of transforming\\nthe problem to one of supervised learning (14.10)–(14.14) seems to have\\nbeen part of the statistics folklore for some time, it does not appear to\\nhave had much impact despite its potential to bring well-developed su-\\npervised learning methodology to bear on unsupervised learning problems.\\nOne reason may be that the problem must be enlarged with a simulated\\ndata set generated by Monte Carlo techniques. Since the size of this data\\nset should be at least as large as the data sample N0≥N, the compu-\\ntation and memory requirements of the estimation procedure are at least\\ndoubled. Also, substantial computation may be required to generate the\\nMonte Carlo sample itself. Although perhaps a deterrent in the past, these\\nincreased computational requirements are becoming much less of a burden\\nas increased resources become routinely available. We illustrate the use of\\nsupervising learning methods for unsupervised learning in the next section.\\n14.2.5 Generalized Association Rules\\nThe more general problem (14.2) of ﬁnding high-density regions in the data\\nspace can be addressed using the supervised learning approach described\\nabove. Although not applicable to the huge data bases for which market\\nbasket analysis is feasible, useful information can be obtained from mod-\\nerately sized data sets. The problem (14.2) can be formulated as ﬁnding\\nsubsets of the integers J ⊂ { 1,2,... ,p }and corresponding value subsets\\nsj, j∈ Jfor the corresponding variables Xj, such that\\nˆPr\\uf8eb\\n\\uf8ed⋂\\nj∈J(Xj∈sj)\\uf8f6\\n\\uf8f8=1\\nNN∑\\ni=1I\\uf8eb\\n\\uf8ed⋂\\nj∈J(xij∈sj)\\uf8f6\\n\\uf8f8 (14.16)\\nis large. Following the nomenclature of association rule analysis, {(Xj∈\\nsj)}j∈Jwill be called a “generalized” item set. The subsets sjcorrespond-\\ning to quantitative variables are taken to be contiguous intervals wit hin', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='afeda31b-7df3-4844-ace2-c370041f9b72', embedding=None, metadata={'page_label': '517', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='498 14. Unsupervised Learning\\ntheir range of values, and subsets for categorical variables can involve more\\nthan a single value. The ambitious nature of this formulation precludes a\\nthorough search for all generalized item sets with support (14.16) greater\\nthan a speciﬁed minimum threshold, as was possible in the more restric-\\ntive setting of market basket analysis. Heuristic search methods must be\\nemployed, and the most one can hope for is to ﬁnd a useful collection of\\nsuch generalized item sets.\\nBoth market basket analysis (14.5) and the generalized formulation (14.1 6)\\nimplicitly reference the uniform probability distribution. One seeks item\\nsets that are more frequent than would be expected if all joint data values\\n(x1,x2,... ,x N) were uniformly distributed. This favors the discovery of\\nitem sets whose marginal constituents ( Xj∈sj) areindividually frequent,\\nthat is, the quantity\\n1\\nNN∑\\ni=1I(xij∈sj) (14.17)\\nis large. Conjunctions of frequent subsets (14.17) will tend to appear more\\noften among item sets of high support (14.16) than conjunctions of margin-\\nally less frequent subsets. This is why the rule vodka ⇒caviar is not likely\\nto be discovered in spite of a high association (lift); neither item has high\\nmarginal support, so that their joint support is especially small. Reference\\nto the uniform distribution can cause highly frequent item sets with low\\nassociations among their constituents to dominate the collection of highest\\nsupport item sets.\\nHighly frequent subsets sjare formed as disjunctions of the most fre-\\nquent Xj-values. Using the product of the variable marginal data densities\\n(14.15) as a reference distribution removes the preference for highly fre-\\nquent values of the individual variables in the discovered item sets. This is\\nbecause the density ratio g(x)/g0(x) is uniform if there are no associations\\namong the variables (complete independence), regardless of the frequency\\ndistribution of the individual variable values. Rules like vodka ⇒caviar\\nwould have a chance to emerge. It is not clear however, how to incorporate\\nreference distributions other than the uniform into the Apriori algorithm.\\nAs explained in Section 14.2.4, it is straightforward to generate a sampl e\\nfrom the product density (14.15), given the original data set.\\nAfter choosing a reference distribution, and drawing a sample from it\\nas in (14.11), one has a supervised learning problem with a binary-valued\\noutput variable Y∈ {0,1}. The goal is to use this training data to ﬁnd\\nregions\\nR=⋂\\nj∈J(Xj∈sj) (14.18)\\nfor which the target function θ(x) =E(Y|x) is relatively large. In addition,\\none might wish to require that the datasupport of these regions', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c56c85a5-1f3b-40a6-a6cc-d27560f35f26', embedding=None, metadata={'page_label': '518', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2 Association Rules 499\\nT(R) =∫\\nx∈Rg(x)dx (14.19)\\nnot be too small.\\n14.2.6 Choice of Supervised Learning Method\\nThe regions (14.18) are deﬁned by conjunctive rules. Hence supervised\\nmethods that learn such rules would be most appropriate in this context.\\nThe terminal nodes of a CART decision tree are deﬁned by rules precisely\\nof the form (14.18). Applying CART to the pooled data (14.11) will pro-\\nduce a decision tree that attempts to model the target (14.10) over the\\nentire data space by a disjoint set of regions (terminal nodes). Each region\\nis deﬁned by a rule of the form (14.18). Those terminal nodes twith high\\naverage y-values\\n¯yt= ave( yi|xi∈t)\\nare candidates for high-support generalized item sets (14.16). The actual\\n(data) support is given by\\nT(R) = ¯yt≤Nt\\nN+N0,\\nwhere Ntis the number of (pooled) observations within the region repre-\\nsented by the terminal node. By examining the resulting decision tree, one\\nmight discover interesting generalized item sets of relatively high-support.\\nThese can then be partitioned into antecedents and consequents in a search\\nfor generalized association rules of high conﬁdence and/or lift.\\nAnother natural learning method for this purpose is the patient rule\\ninduction method PRIM described in Section 9.3. PRIM also produces\\nrules precisely of the form (14.18), but it is especially designed for ﬁnding\\nhigh-support regions that maximize the average target (14.10) value within\\nthem, rather than trying to model the target function over the entire data\\nspace. It also provides more control over the support/average-target-value\\ntradeoﬀ.\\nExercise 14.3 addresses an issue that arises with either of these methods\\nwhen we generate random data from the product of the marginal distribu-\\ntions.\\n14.2.7 Example: Market Basket Analysis (Continued)\\nWe illustrate the use of PRIM on the demographics data of Table 14.1.\\nThree of the high-support generalized item sets emerging from the PRIM\\nanalysis were the following:\\nItem set 1: Support= 24%.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5426bfa3-0d76-4df9-9a59-b6a13198e8ab', embedding=None, metadata={'page_label': '519', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='500 14. Unsupervised Learning\\n\\uf8ee\\n\\uf8f0marital status = married\\nhouseholder status = own\\ntype of home ̸=apartment\\uf8f9\\n\\uf8fb\\nItem set 2: Support= 24%.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0age≤24\\nmarital status ∈ {living together-not married, single }\\noccupation /∈ {professional, homemaker, retired }\\nhouseholder status ∈ {rent, live with family }\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nItem set 3: Support= 15%.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0householder status = rent\\ntype of home ̸=house\\nnumber in household ≤2\\nnumber of children = 0\\noccupation /∈ {homemaker, student, unemployed }\\nincome ∈[$20,000 ,$150,000]\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nGeneralized association rules derived from these item sets with conﬁdence\\n(14.8) greater than 95% are the following:\\nAssociation rule 1: Support 25%, conﬁdence 99.7% and lift 1.35.\\n[\\nmarital status = married\\nhouseholder status = own]\\n⇓\\ntype of home ̸=apartment\\nAssociation rule 2: Support 25%, conﬁdence 98.7% and lift 1.97.\\n\\uf8ee\\n\\uf8f0age≤24\\noccupation /∈ {professional, homemaker, retired }\\nhouseholder status ∈ {rent, live with family }\\uf8f9\\n\\uf8fb\\n⇓\\nmarital status ∈ {single, living together-not married }\\nAssociation rule 3: Support 25%, conﬁdence 95.9% and lift 2.61.\\n[householder status = own\\ntype of home ̸=apartment]\\n⇓\\nmarital status = married', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='22e05a94-50ab-44af-abe6-853c2d405bee', embedding=None, metadata={'page_label': '520', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 501\\nAssociation rule 4: Support 15%, conﬁdence 95.4% and lift 1.50.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0householder status = rent\\ntype of home ̸=house\\nnumber in household ≤2\\noccupation /∈ {homemaker, student, unemployed }\\nincome ∈[$20,000 ,$150,000]\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⇓\\nnumber of children = 0\\nThere are no great surprises among these particular rules. For the most\\npart they verify intuition. In other contexts where there is less prior in-\\nformation available, unexpected results have a greater chance to emerge.\\nThese results do illustrate the type of information generalized associatio n\\nrules can provide, and that the supervised learning approach, coupled with\\na ruled induction method such as CART or PRIM, can uncover item sets\\nexhibiting high associations among their constituents.\\nHow do these generalized association rules compare to those found earlier\\nby the Apriori algorithm? Since the Apriori procedure gives thousands of\\nrules, it is diﬃcult to compare them. However some general points can be\\nmade. The Apriori algorithm is exhaustive—it ﬁnds allrules with support\\ngreater than a speciﬁed amount. In contrast, PRIM is a greedy algorithm\\nand is not guaranteed to give an “optimal” set of rules. On the other hand,\\nthe Apriori algorithm can deal only with dummy variables and hence could\\nnot ﬁnd some of the above rules. For example, since type of home is a\\ncategorical input, with a dummy variable for each level, Apriori could not\\nﬁnd a rule involving the set\\ntype of home ̸=apartment .\\nTo ﬁnd this set, we would have to code a dummy variable for apartment\\nversus the other categories of type of home. It will not generally be feasible\\nto precode all such potentially interesting comparisons.\\n14.3 Cluster Analysis\\nCluster analysis, also called data segmentation, has a variety of goals. All\\nrelate to grouping or segmenting a collection of objects into subsets or\\n“clusters,” such that those within each cluster are more closely related to\\none another than objects assigned to diﬀerent clusters. An object can be\\ndescribed by a set of measurements, or by its relation to other objects.\\nIn addition, the goal is sometimes to arrange the clusters into a natural\\nhierarchy. This involves successively grouping the clusters themselves so', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a60db171-7128-4efc-b649-c571b8c2b30e', embedding=None, metadata={'page_label': '521', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='502 14. Unsupervised Learning\\n• •••\\n••••\\n• •••\\n••\\n•••••• • ••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n••••\\n••\\n••••\\n•••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n•••\\n•••••\\n••••••\\n•\\n••••\\n•••••\\n•••••\\n•••••\\n••••\\n•• •\\n•••••\\n•••\\n••\\n•••\\n••\\n•••\\n••••\\n•••\\n••• ••\\nX1X2\\nFIGURE 14.4. Simulated data in the plane, clustered into three classes (repr e-\\nsented by orange, blue and green) by the K-means clustering algorithm\\nthat at each level of the hierarchy, clusters within the same group are more\\nsimilar to each other than those in diﬀerent groups.\\nCluster analysis is also used to form descriptive statistics to ascertain\\nwhether or not the data consists of a set distinct subgroups, each group\\nrepresenting objects with substantially diﬀerent properties. This latter goa l\\nrequires an assessment of the degree of diﬀerence between the objects as-\\nsigned to the respective clusters.\\nCentral to all of the goals of cluster analysis is the notion of the degree of\\nsimilarity (or dissimilarity) between the individual objects being clustered.\\nA clustering method attempts to group the objects based on the deﬁnition\\nof similarity supplied to it. This can only come from subject matter consid-\\nerations. The situation is somewhat similar to the speciﬁcation of a loss or\\ncost function in prediction problems (supervised learning). There the cost\\nassociated with an inaccurate prediction depends on considerations outside\\nthe data.\\nFigure 14.4 shows some simulated data clustered into three groups via\\nthe popular K-means algorithm. In this case two of the clusters are not\\nwell separated, so that “segmentation” more accurately describes the part\\nof this process than “clustering.” K-means clustering starts with guesses\\nfor the three cluster centers. Then it alternates the following steps until\\nconvergence:\\n•for each data point, the closest cluster center (in Euclidean distance)\\nis identiﬁed;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c3851e7d-cc6e-4e63-a458-c04cf0caf44c', embedding=None, metadata={'page_label': '522', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 503\\n•each cluster center is replaced by the coordinate-wise average of all\\ndata points that are closest to it.\\nWe describe K-means clustering in more detail later, including the prob-\\nlem of how to choose the number of clusters (three in this example). K-\\nmeans clustering is a top-down procedure, while other cluster approaches\\nthat we discuss are bottom-up . Fundamental to all clustering techniques is\\nthe choice of distance or dissimilarity measure between two objects. We\\nﬁrst discuss distance measures before describing a variety of algorithms for\\nclustering.\\n14.3.1 Proximity Matrices\\nSometimes the data is represented directly in terms of the proximity (alike-\\nness or aﬃnity) between pairs of objects. These can be either similarities or\\ndissimilarities (diﬀerence or lack of aﬃnity). For example, in social science\\nexperiments, participants are asked to judge by how much certain objects\\ndiﬀer from one another. Dissimilarities can then be computed by averaging\\nover the collection of such judgments. This type of data can be represented\\nby an N×Nmatrix D, where Nis the number of objects, and each element\\ndii′records the proximity between the ith and i′th objects. This matrix is\\nthen provided as input to the clustering algorithm.\\nMost algorithms presume a matrix of dissimilarities with nonnegative\\nentries and zero diagonal elements: dii= 0, i= 1,2,... ,N. If the original\\ndata were collected as similarities, a suitable monotone-decreasing function\\ncan be used to convert them to dissimilarities. Also, most algorithms as -\\nsume symmetric dissimilarity matrices, so if the original matrix Dis not\\nsymmetric it must be replaced by ( D+DT)/2. Subjectively judged dissimi-\\nlarities are seldom distances in the strict sense, since the triangle inequality\\ndii′≤dik+di′k, for all k∈ {1,... ,N }does not hold. Thus, some algorithms\\nthat assume distances cannot be used with such data.\\n14.3.2 Dissimilarities Based on Attributes\\nMost often we have measurements xijfori= 1,2,... ,N , on variables\\nj= 1,2,... ,p (also called attributes ). Since most of the popular clustering\\nalgorithms take a dissimilarity matrix as their input, we must ﬁrst const ruct\\npairwise dissimilarities between the observations. In the most common cas e,\\nwe deﬁne a dissimilarity dj(xij,xi′j) between values of the jth attribute,\\nand then deﬁne\\nD(xi,xi′) =p∑\\nj=1dj(xij,xi′j) (14.20)\\nas the dissimilarity between objects iandi′. By far the most common\\nchoice is squared distance', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca70d283-4b86-4404-b436-d286bc663e8f', embedding=None, metadata={'page_label': '523', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='504 14. Unsupervised Learning\\ndj(xij,xi′j) = (xij−xi′j)2. (14.21)\\nHowever, other choices are possible, and can lead to potentially diﬀerent\\nresults. For nonquantitative attributes (e.g., categorical data), squared dis-\\ntance may not be appropriate. In addition, it is sometimes desirable to\\nweigh attributes diﬀerently rather than giving them equal weight as in\\n(14.20).\\nWe ﬁrst discuss alternatives in terms of the attribute type:\\nQuantitative variables. Measurements of this type of variable or attribute\\nare represented by continuous real-valued numbers. It is natural to\\ndeﬁne the “error” between them as a monotone-increasing function\\nof their absolute diﬀerence\\nd(xi,xi′) =l(|xi−xi′|).\\nBesides squared-error loss ( xi−xi′)2, a common choice is the identity\\n(absolute error). The former places more emphasis on larger diﬀer-\\nences than smaller ones. Alternatively, clustering can be based on the\\ncorrelation\\nρ(xi,xi′) =∑\\nj(xij−¯xi)(xi′j−¯xi′)√∑\\nj(xij−¯xi)2∑\\nj(xi′j−¯xi′)2, (14.22)\\nwith ¯xi=∑\\njxij/p. Note that this is averaged over variables , not ob-\\nservations. If the observations are ﬁrst standardized, then∑\\nj(xij−\\nxi′j)2∝2(1−ρ(xi,xi′)). Hence clustering based on correlation (simi-\\nlarity) is equivalent to that based on squared distance (dissimilarity).\\nOrdinal variables. The values of this type of variable are often represented\\nas contiguous integers, and the realizable values are considered to be\\nan ordered set. Examples are academic grades (A, B, C, D, F), degree\\nof preference (can’t stand, dislike, OK, like, terriﬁc). Rank data are a\\nspecial kind of ordinal data. Error measures for ordinal variables are\\ngenerally deﬁned by replacing their Moriginal values with\\ni−1/2\\nM, i= 1,... ,M (14.23)\\nin the prescribed order of their original values. They are then treated\\nas quantitative variables on this scale.\\nCategorical variables. With unordered categorical (also called nominal)\\nvariables, the degree-of-diﬀerence between pairs of values must be\\ndelineated explicitly. If the variable assumes Mdistinct values, these\\ncan be arranged in a symmetric M×Mmatrix with elements Lrr′=\\nLr′r,Lrr= 0,Lrr′≥0. The most common choice is Lrr′= 1 for all\\nr̸=r′, while unequal losses can be used to emphasize some errors\\nmore than others.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a04c4d04-551c-45c3-8e2f-00102ac36ff5', embedding=None, metadata={'page_label': '524', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 505\\n14.3.3 Object Dissimilarity\\nNext we deﬁne a procedure for combining the p-individual attribute dissim-\\nilarities dj(xij,xi′j), j= 1,2,... ,p into a single overall measure of dissim-\\nilarity D(xi,xi′) between two objects or observations ( xi,xi′) possessing\\nthe respective attribute values. This is nearly always done by means of a\\nweighted average (convex combination)\\nD(xi,xi′) =p∑\\nj=1wj≤dj(xij,xi′j);p∑\\nj=1wj= 1. (14.24)\\nHerewjis a weight assigned to the jth attribute regulating the relative\\ninﬂuence of that variable in determining the overall dissimilarity between\\nobjects. This choice should be based on subject matter considerations.\\nIt is important to realize that setting the weight wjto the same value\\nfor each variable (say, wj= 1∀j) does notnecessarily give all attributes\\nequal inﬂuence. The inﬂuence of the jth attribute Xjon object dissimilarity\\nD(xi,xi′) (14.24) depends upon its relative contribution to the average\\nobject dissimilarity measure over all pairs of observations in the data set\\n¯D=1\\nN2N∑\\ni=1N∑\\ni′=1D(xi,xi′) =p∑\\nj=1wj≤¯dj,\\nwith\\n¯dj=1\\nN2N∑\\ni=1N∑\\ni′=1dj(xij,xi′j) (14.25)\\nbeing the average dissimilarity on the jth attribute. Thus, the relative in-\\nﬂuence of the jth variable is wj≤¯dj, and setting wj∼1/¯djwould give all\\nattributes equal inﬂuence in characterizing overall dissimilarity between ob-\\njects. For example, with pquantitative variables and squared-error distance\\nused for each coordinate, then (14.24) becomes the (weighted) squared Eu-\\nclidean distance\\nDI(xi,xi′) =p∑\\nj=1wj≤(xij−xi′j)2(14.26)\\nbetween pairs of points in an IRp, with the quantitative variables as axes.\\nIn this case (14.25) becomes\\n¯dj=1\\nN2N∑\\ni=1N∑\\ni′=1(xij−xi′j)2= 2≤varj, (14.27)\\nwhere var jis the sample estimate of Var( Xj). Thus, the relative impor-\\ntance of each such variable is proportional to its variance over the data', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='95cf4ac9-01ef-46c6-bf23-cde1c5f95be4', embedding=None, metadata={'page_label': '525', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='506 14. Unsupervised Learning\\n-6 -4 -2 0 2 4-6 -4 -2 0 2 4•••\\n••\\n•••\\n••••\\n••\\n••••••\\n•\\n••\\n••\\n••\\n•\\n••••••••\\n••••••••\\n••\\n•••• •••\\n••••\\n•••\\n•••• •••\\n••\\n••••\\n••••\\n••\\n•••••\\n•••\\n•••\\n•••••\\n•••\\n••\\n-2 -1 0 1 2-2 -1 0 1 2••\\n••••\\n•\\n••••\\n•\\n••\\n•\\n•\\n••••\\n•\\n•••\\n•••\\n••\\n•••\\n•••\\n•••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n•\\n•••••\\n•••\\n•\\n•••\\n••\\n••••••••\\n••\\n••\\n••\\nX1 X1\\nX2X2\\nFIGURE 14.5. Simulated data: on the left, K-means clustering (with K=2) has\\nbeen applied to the raw data. The two colors indicate the clust er memberships. On\\nthe right, the features were ﬁrst standardized before cluster ing. This is equivalent\\nto using feature weights 1/[2≤var(Xj)]. The standardization has obscured the two\\nwell-separated groups. Note that each plot uses the same unit s in the horizontal\\nand vertical axes.\\nset. In general, setting wj= 1/¯djfor all attributes, irrespective of type,\\nwill cause each one of them to equally inﬂuence the overall dissimilarity\\nbetween pairs of objects ( xi,xi′). Although this may seem reasonable, and\\nis often recommended, it can be highly counterproductive. If the goal is to\\nsegment the data into groups of similar objects, all attributes may not con-\\ntribute equally to the (problem-dependent) notion of dissimilarity between\\nobjects. Some attribute value diﬀerences may reﬂect greater actual object\\ndissimilarity in the context of the problem domain.\\nIf the goal is to discover natural groupings in the data, some attributes\\nmay exhibit more of a grouping tendency than others. Variables that are\\nmore relevant in separating the groups should be assigned a higher inﬂu-\\nence in deﬁning object dissimilarity. Giving all attributes equal inﬂuence\\nin this case will tend to obscure the groups to the point where a clustering\\nalgorithm cannot uncover them. Figure 14.5 shows an example.\\nAlthough simple generic prescriptions for choosing the individual at-\\ntribute dissimilarities dj(xij,xi′j) and their weights wjcan be comforting,\\nthere is no substitute for careful thought in the context of each individ-\\nual problem. Specifying an appropriate dissimilarity measure is far more\\nimportant in obtaining success with clustering than choice of clustering\\nalgorithm. This aspect of the problem is emphasized less in the cluster-\\ning literature than the algorithms themselves, since it depends on domain\\nknowledge speciﬁcs and is less amenable to general research.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a72a749e-6b08-47a7-8334-08a8091be781', embedding=None, metadata={'page_label': '526', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 507\\nFinally, often observations have missing values in one or more of the\\nattributes. The most common method of incorporating missing values in\\ndissimilarity calculations (14.24) is to omit each observation pair xij,xi′j\\nhaving at least one value missing, when computing the dissimilarity be-\\ntween observations xiandx′\\ni. This method can fail in the circumstance\\nwhen both observations have no measured values in common. In this case\\nboth observations could be deleted from the analysis. Alternatively, the\\nmissing values could be imputed using the mean or median of each attribute\\nover the nonmissing data. For categorical variables, one could consider the\\nvalue “missing” as just another categorical value, if it were reasonable to\\nconsider two objects as being similar if they both have missing values on\\nthe same variables.\\n14.3.4 Clustering Algorithms\\nThe goal of cluster analysis is to partition the observations into groups\\n(“clusters”) so that the pairwise dissimilarities between those assigned t o\\nthe same cluster tend to be smaller than those in diﬀerent clusters. Clus-\\ntering algorithms fall into three distinct types: combinatorial algorit hms,\\nmixture modeling, and mode seeking.\\nCombinatorial algorithms work directly on the observed data with no\\ndirect reference to an underlying probability model. Mixture modeling sup-\\nposes that the data is an i.i.dsample from some population described by a\\nprobability density function. This density function is characterized by a pa-\\nrameterized model taken to be a mixture of component density functions;\\neach component density describes one of the clusters. This model is then ﬁt\\nto the data by maximum likelihood or corresponding Bayesian approaches.\\nMode seekers (“bump hunters”) take a nonparametric perspective, attempt-\\ning to directly estimate distinct modes of the probability density function.\\nObservations “closest” to each respective mode then deﬁne the individual\\nclusters.\\nMixture modeling is described in Section 6.8. The PRIM algorithm, dis-\\ncussed in Sections 9.3 and 14.2.5, is an example of mode seeking or “bump\\nhunting.” We discuss combinatorial algorithms next.\\n14.3.5 Combinatorial Algorithms\\nThe most popular clustering algorithms directly assign each observation\\nto a group or cluster without regard to a probability model describing the\\ndata. Each observation is uniquely labeled by an integer i∈ {1,≤ ≤ ≤,N}.\\nA prespeciﬁed number of clusters K < N is postulated, and each one is\\nlabeled by an integer k∈ {1,... ,K }. Each observation is assigned to one\\nand only one cluster. These assignments can be characterized by a many-\\nto-one mapping, or encoder k=C(i), that assigns the ith observation to\\nthekth cluster. One seeks the particular encoder C∗(i) that achieves the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9f937c96-da86-496f-b7dd-6d7d4df27abe', embedding=None, metadata={'page_label': '527', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='508 14. Unsupervised Learning\\nrequired goal (details below), based on the dissimilarities d(xi,xi′) between\\nevery pair of observations. These are speciﬁed by the user as described\\nabove. Generally, the encoder C(i) is explicitly delineated by giving its\\nvalue (cluster assignment) for each observation i. Thus, the “parameters”\\nof the procedure are the individual cluster assignments for each of the N\\nobservations. These are adjusted so as to minimize a “loss” function that\\ncharacterizes the degree to which the clustering goal is notmet.\\nOne approach is to directly specify a mathematical loss function and\\nattempt to minimize it through some combinatorial optimization algorit hm.\\nSince the goal is to assign close points to the same cluster, a natural loss\\n(or “energy”) function would be\\nW(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)=kd(xi,xi′). (14.28)\\nThis criterion characterizes the extent to which observations assigned to\\nthe same cluster tend to be close to one another. It is sometimes referred\\nto as the “within cluster” point scatter since\\nT=1\\n2N∑\\ni=1N∑\\ni′=1dii′=1\\n2K∑\\nk=1∑\\nC(i)=k\\uf8eb\\n\\uf8ed∑\\nC(i′)=kdii′+∑\\nC(i′)̸=kdii′\\uf8f6\\n\\uf8f8,\\nor\\nT=W(C) +B(C),\\nwhere dii′=d(xi,xi′). Here Tis thetotalpoint scatter, which is a constant\\ngiven the data, independent of cluster assignment. The quantity\\nB(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)̸=kdii′ (14.29)\\nis the between-cluster point scatter. This will tend to be large when obser-\\nvations assigned to diﬀerent clusters are far apart. Thus one has\\nW(C) =T−B(C)\\nand minimizing W(C) is equivalent to maximizing B(C).\\nCluster analysis by combinatorial optimization is straightforward in prin-\\nciple. One simply minimizes Wor equivalently maximizes Bover all pos-\\nsible assignments of the Ndata points to Kclusters. Unfortunately, such\\noptimization by complete enumeration is feasible only for very small data\\nsets. The number of distinct assignments is (Jain and Dubes, 1988)\\nS(N,K) =1\\nK!K∑\\nk=1(−1)K−k(K\\nk)\\nkN. (14.30)\\nFor example, S(10,4) = 34 ,105 which is quite feasible. But, S(N,K) grows\\nvery rapidly with increasing values of its arguments. Already S(19,4)≃', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='71f6f12f-26aa-495d-90fe-34d327eecf4c', embedding=None, metadata={'page_label': '528', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 509\\n1010, and most clustering problems involve much larger data sets than\\nN= 19. For this reason, practical clustering algorithms are able to examine\\nonly a very small fraction of all possible encoders k=C(i). The goal is to\\nidentify a small subset that is likely to contain the optimal one, or at least\\na good suboptimal partition.\\nSuch feasible strategies are based on iterative greedy descent. An initial\\npartition is speciﬁed. At each iterative step, the cluster assignments are\\nchanged in such a way that the value of the criterion is improved from\\nits previous value. Clustering algorithms of this type diﬀer in their pre-\\nscriptions for modifying the cluster assignments at each iteration. When\\nthe prescription is unable to provide an improvement, the algorithm ter-\\nminates with the current assignments as its solution. Since the assignment\\nof observations to clusters at any iteration is a perturbation of that for the\\nprevious iteration, only a very small fraction of all possible assignmen ts\\n(14.30) are examined. However, these algorithms converge to localoptima\\nwhich may be highly suboptimal when compared to the global optimum.\\n14.3.6 K-means\\nTheK-means algorithm is one of the most popular iterative descent clus-\\ntering methods. It is intended for situations in which all variables are of\\nthe quantitative type, and squared Euclidean distance\\nd(xi,xi′) =p∑\\nj=1(xij−xi′j)2=||xi−xi′||2\\nis chosen as the dissimilarity measure. Note that weighted Euclidean dis-\\ntance can be used by redeﬁning the xijvalues (Exercise 14.1).\\nThe within-point scatter (14.28) can be written as\\nW(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)=k||xi−xi′||2\\n=K∑\\nk=1Nk∑\\nC(i)=k||xi−¯xk||2, (14.31)\\nwhere ¯ xk= (¯x1k,... ,¯xpk) is the mean vector associated with the kth clus-\\nter, and Nk=∑N\\ni=1I(C(i) =k). Thus, the criterion is minimized by\\nassigning the Nobservations to the Kclusters in such a way that within\\neach cluster the average dissimilarity of the observations from the cluster\\nmean, as deﬁned by the points in that cluster, is minimized.\\nAn iterative descent algorithm for solving', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c829b2ac-b49b-4acb-9b24-1980bcca17e9', embedding=None, metadata={'page_label': '529', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='510 14. Unsupervised Learning\\nAlgorithm 14.1 K-means Clustering.\\n1. For a given cluster assignment C, the total cluster variance (14.33) is\\nminimized with respect to {m1,... ,m K}yielding the means of the\\ncurrently assigned clusters (14.32).\\n2. Given a current set of means {m1,... ,m K}, (14.33) is minimized by\\nassigning each observation to the closest (current) cluster mean. That\\nis,\\nC(i) = argmin\\n1≤k≤K||xi−mk||2. (14.34)\\n3. Steps 1 and 2 are iterated until the assignments do not change.\\nC∗= min\\nCK∑\\nk=1Nk∑\\nC(i)=k||xi−¯xk||2\\ncan be obtained by noting that for any set of observations S\\n¯xS= argmin\\nm∑\\ni∈S||xi−m||2. (14.32)\\nHence we can obtain C∗by solving the enlarged optimization problem\\nmin\\nC,{mk}K\\n1K∑\\nk=1Nk∑\\nC(i)=k||xi−mk||2. (14.33)\\nThis can be minimized by an alternating optimization procedure given in\\nAlgorithm 14.1.\\nEach of steps 1 and 2 reduces the value of the criterion (14.33), so that\\nconvergence is assured. However, the result may represent a suboptimal\\nlocal minimum. The algorithm of Hartigan and Wong (1979) goes further,\\nand ensures that there is no single switch of an observation from one group\\nto another group that will decrease the objective. In addition, one should\\nstart the algorithm with many diﬀerent random choices for the starting\\nmeans, and choose the solution having smallest value of the objective func-\\ntion.\\nFigure 14.6 shows some of the K-means iterations for the simulated data\\nof Figure 14.4. The centroids are depicted by “O”s. The straight lines show\\nthe partitioning of points, each sector being the set of points closest to\\neach centroid. This partitioning is called the Voronoi tessellation . After 20\\niterations the procedure has converged.\\n14.3.7 Gaussian Mixtures as Soft K-means Clustering\\nTheK-means clustering procedure is closely related to the EM algorithm\\nfor estimating a certain Gaussian mixture model. (Sections 6.8 and 8.5.1).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='583b86e4-6132-43bf-aceb-8acad9d0ffd5', embedding=None, metadata={'page_label': '530', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 511\\n-4 -2 0 2 4 6-2 0 2 4 6Initial Centroids\\n••••\\n••••••••\\n••\\n•••••••••\\n••••\\n••\\n••\\n••\\n•\\n•••••\\n•••\\n••\\n•••\\n••\\n••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••••\\n•••••\\n••••••••\\n•••\\n••••••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n••\\n•••\\n•••••\\n•••\\n•••\\n••••\\n••••••••\\n••\\n•••••••••\\n••••\\n••\\n••\\n••\\n•\\n•••••\\n•••\\n••\\n•••\\n••\\n••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••••\\n•••••\\n••••••••\\n•••\\n••••••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n••\\n•••\\n•••••\\n•••\\n•••Initial Partition\\n••••\\n••••••••\\n••\\n•••••••••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••\\n••\\n••••••\\n•••\\n••••\\n••\\n••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n•••••Iteration Number  2\\n•••\\n•••\\n••••\\n••••••••\\n••\\n•••••••••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n••••\\n••\\n••••\\n•••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n•••••\\n•••••\\n•••••••••\\n•••\\n•••••\\n•••\\n••\\n•••\\n••\\n•••\\n••••\\n•••\\n•••••Iteration Number  20\\n•••\\n•••\\nFIGURE 14.6. Successive iterations of the K-means clustering algorithm for\\nthe simulated data of Figure 14.4.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9f5b3dbc-4a21-4a46-9be5-4540d3923cb4', embedding=None, metadata={'page_label': '531', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='512 14. Unsupervised Learning\\n• •Responsibilities\\n0.0 0.2 0.4 0.6 0.8 1.0\\n• •Responsibilities\\n0.0 0.2 0.4 0.6 0.8 1.0σ= 1.0 σ= 1.0\\nσ= 0.2 σ= 0.2\\nFIGURE 14.7. (Left panels:) two Gaussian densities g0(x) and g1(x)(blue and\\norange) on the real line, and a single data point (green dot) at x= 0.5. The colored\\nsquares are plotted at x=−1.0andx= 1.0, the means of each density. (Right\\npanels:) the relative densities g0(x)/(g0(x) +g1(x))andg1(x)/(g0(x) +g1(x)),\\ncalled the “responsibilities” of each cluster, for this data point. In the top panels,\\nthe Gaussian standard deviation σ= 1.0; in the bottom panels σ= 0.2. The\\nEM algorithm uses these responsibilities to make a “soft” ass ignment of each\\ndata point to each of the two clusters. When σis fairly large, the responsibilities\\ncan be near 0.5(they are 0.36and0.64 in the top right panel). As σ→0, the\\nresponsibilities →1, for the cluster center closest to the target point, and 0for\\nall other clusters. This “hard” assignment is seen in the botto m right panel.\\nThe E-step of the EM algorithm assigns “responsibilities” for each data\\npoint based in its relative density under each mixture component, while\\nthe M-step recomputes the component density parameters based on the\\ncurrent responsibilities. Suppose we specify Kmixture components, each\\nwith a Gaussian density having scalar covariance matrix σ2I. Then the\\nrelative density under each mixture component is a monotone function of\\nthe Euclidean distance between the data point and the mixture center.\\nHence in this setup EM is a “soft” version of K-means clustering, making\\nprobabilistic (rather than deterministic) assignments of points to cluster\\ncenters. As the variance σ2→0, these probabilities become 0 and 1, and\\nthe two methods coincide. Details are given in Exercise 14.2. Figure 14.7\\nillustrates this result for two clusters on the real line.\\n14.3.8 Example: Human Tumor Microarray Data\\nWe apply K-means clustering to the human tumor microarray data de-\\nscribed in Chapter 1. This is an example of high-dimensional clustering.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a0b0d6d2-49b6-4eb2-95ef-4cd9ad251431', embedding=None, metadata={'page_label': '532', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 513\\nNumber of Clusters KSum of Squares\\n2 4 6 8 10160000 200000 240000•\\n•\\n•\\n•••••••\\nFIGURE 14.8. Total within-cluster sum of squares for K-means clustering ap-\\nplied to the human tumor microarray data.\\nTABLE 14.2. Human tumor data: number of cancer cases of each type, in each\\nof the three clusters from K-means clustering.\\nCluster Breast CNS Colon K562 Leukemia MCF7\\n1 3 5 0 0 0 0\\n2 2 0 0 2 6 2\\n3 2 0 7 0 0 0\\nCluster Melanoma NSCLC Ovarian Prostate Renal Unknown\\n1 1 7 6 2 9 1\\n2 7 2 0 0 0 0\\n3 0 0 0 0 0 0\\nThe data are a 6830 ×64 matrix of real numbers, each representing an\\nexpression measurement for a gene (row) and sample (column). Here we\\ncluster the samples, each of which is a vector of length 6830, correspond-\\ning to expression values for the 6830 genes. Each sample has a label such\\nasbreast (for breast cancer), melanoma , and so on; we don’t use these la-\\nbels in the clustering, but will examine posthoc which labels fall into which\\nclusters.\\nWe applied K-means clustering with Krunning from 1 to 10, and com-\\nputed the total within-sum of squares for each clustering, shown in Fig-\\nure 14.8. Typically one looks for a kink in the sum of squares curve (or its\\nlogarithm) to locate the optimal number of clusters (see Section 14.3.11).\\nHere there is no clear indication: for illustration we chose K= 3 giving the\\nthree clusters shown in Table 14.2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38b972e4-ac91-40df-ad84-e83d58e0a383', embedding=None, metadata={'page_label': '533', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='514 14. Unsupervised Learning\\nFIGURE 14.9. Sir Ronald A. Fisher ( 1890−1962) was one of the founders\\nof modern day statistics, to whom we owe maximum-likelihood, suﬃciency, and\\nmany other fundamental concepts. The image on the left is a 1024×1024grayscale\\nimage at 8bits per pixel. The center image is the result of 2×2block VQ, using\\n200code vectors, with a compression rate of 1.9bits/pixel. The right image uses\\nonly four code vectors, with a compression rate of 0.50bits/pixel\\nWe see that the procedure is successful at grouping together samples of\\nthe same cancer. In fact, the two breast cancers in the second cluster were\\nlater found to be misdiagnosed and were melanomas that had metastasized.\\nHowever, K-means clustering has shortcomings in this application. For one,\\nit does not give a linear ordering of objects within a cluster: we have simply\\nlisted them in alphabetic order above. Secondly, as the number of clusters\\nKis changed, the cluster memberships can change in arbitrary ways. That\\nis, with say four clusters, the clusters need not be nested within the three\\nclusters above. For these reasons, hierarchical clustering (described later),\\nis probably preferable for this application.\\n14.3.9 Vector Quantization\\nTheK-means clustering algorithm represents a key tool in the apparently\\nunrelated area of image and signal compression, particularly in vector quan-\\ntization or VQ (Gersho and Gray, 1992). The left image in Figure 14.92is a\\ndigitized photograph of a famous statistician, Sir Ronald Fisher. It consist s\\nof 1024 ×1024 pixels, where each pixel is a grayscale value ranging from 0\\nto 255, and hence requires 8 bits of storage per pixel. The entire image oc-\\ncupies 1 megabyte of storage. The center image is a VQ-compressed version\\nof the left panel, and requires 0 .239 of the storage (at some loss in quality).\\nThe right image is compressed even more, and requires only 0 .0625 of the\\nstorage (at a considerable loss in quality).\\nThe version of VQ implemented here ﬁrst breaks the image into small\\nblocks, in this case 2 ×2 blocks of pixels. Each of the 512 ×512 blocks of four\\n2This example was prepared by Maya Gupta.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a8b87d79-c3e7-4d04-84c7-55b8ce6be07b', embedding=None, metadata={'page_label': '534', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 515\\nnumbers is regarded as a vector in IR4. AK-means clustering algorithm\\n(also known as Lloyd’s algorithm in this context) is run in this space.\\nThe center image uses K= 200, while the right image K= 4. Each of\\nthe 512 ×512 pixel blocks (or points) is approximated by its closest cluster\\ncentroid, known as a codeword. The clustering process is called the encoding\\nstep, and the collection of centroids is called the codebook .\\nTo represent the approximated image, we need to supply for each block\\nthe identity of the codebook entry that approximates it. This will require\\nlog2(K) bits per block. We also need to supply the codebook itself, which\\nisK×4 real numbers (typically negligible). Overall, the storage for the\\ncompressed image amounts to log2(K)/(4≤8) of the original (0 .239 for\\nK= 200, 0 .063 for K= 4). This is typically expressed as a ratein bits\\nper pixel: log2(K)/4, which are 1 .91 and 0 .50, respectively. The process\\nof constructing the approximate image from the centroids is called the\\ndecoding step.\\nWhy do we expect VQ to work at all? The reason is that for typical\\neveryday images like photographs, many of the blocks look the same. In\\nthis case there are many almost pure white blocks, and similarly pure gray\\nblocks of various shades. These require only one block each to represent\\nthem, and then multiple pointers to that block.\\nWhat we have described is known as lossycompression, since our im-\\nages are degraded versions of the original. The degradation or distortion is\\nusually measured in terms of mean squared error. In this case D= 0.89\\nforK= 200 and D= 16.95 for K= 4. More generally a rate/distortion\\ncurve would be used to assess the tradeoﬀ. One can also perform lossless\\ncompression using block clustering, and still capitalize on the repeated pat-\\nterns. If you took the original image and losslessly compressed it, the bes t\\nyou would do is 4.48 bits per pixel.\\nWe claimed above that log2(K) bits were needed to identify each of the K\\ncodewords in the codebook. This uses a ﬁxed-length code, and is ineﬃcient\\nif some codewords occur many more times than others in the image. Using\\nShannon coding theory, we know that in general a variable length code\\nwill do better, and the rate then becomes −∑K\\nℓ=1pℓlog2(pℓ)/4. The term\\nin the numerator is the entropy of the distribution pℓof the codewords\\nin the image. Using variable length coding our rates come down to 1 .42\\nand 0.39, respectively. Finally, there are many generalizations of VQ that\\nhave been developed: for example, tree-structured VQ ﬁnds the centroids\\nwith a top-down, 2-means style algorithm, as alluded to in Section 14.3.12.\\nThis allows successive reﬁnement of the compression. Further details may\\nbe found in Gersho and Gray (1992).\\n14.3.10 K-medoids\\nAs discussed above, the K-means algorithm is appropriate when the dis-\\nsimilarity measure is taken to be squared Euclidean distance D(xi,xi′)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2ea698d-ff88-403b-872a-1eb2edd95800', embedding=None, metadata={'page_label': '535', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='516 14. Unsupervised Learning\\nAlgorithm 14.2 K-medoids Clustering.\\n1. For a given cluster assignment Cﬁnd the observation in the cluster\\nminimizing total distance to other points in that cluster:\\ni∗\\nk= argmin\\n{i:C(i)=k}∑\\nC(i′)=kD(xi,xi′). (14.35)\\nThen mk=xi∗\\nk, k= 1,2,... ,K are the current estimates of the\\ncluster centers.\\n2. Given a current set of cluster centers {m1,... ,m K}, minimize the to-\\ntal error by assigning each observation to the closest (current) cluster\\ncenter:\\nC(i) = argmin\\n1≤k≤KD(xi,mk). (14.36)\\n3. Iterate steps 1 and 2 until the assignments do not change.\\n(14.112). This requires all of the variables to be of the quantitative t ype. In\\naddition, using squared Euclidean distance places the highest inﬂuence on\\nthe largest distances. This causes the procedure to lack robustness against\\noutliers that produce very large distances. These restrictions can be re-\\nmoved at the expense of computation.\\nThe only part of the K-means algorithm that assumes squared Eu-\\nclidean distance is the minimization step (14.32); the cluster representatives\\n{m1,... ,m K}in (14.33) are taken to be the means of the currently assigned\\nclusters. The algorithm can be generalized for use with arbitrarily deﬁned\\ndissimilarities D(xi,xi′) by replacing this step by an explicit optimization\\nwith respect to {m1,... ,m K}in (14.33). In the most common form, cen-\\nters for each cluster are restricted to be one of the observations assigned\\nto the cluster, as summarized in Algorithm 14.2. This algorithm assumes\\nattribute data, but the approach can also be applied to data described\\nonlyby proximity matrices (Section 14.3.1). There is no need to explicitly\\ncompute cluster centers; rather we just keep track of the indices i∗\\nk.\\nSolving (14.32) for each provisional cluster krequires an amount of com-\\nputation proportional to the number of observations assigned to it, whereas\\nfor solving (14.35) the computation increases to O(N2\\nk). Given a set of clus-\\nter “centers,” {i1,... ,i K}, obtaining the new assignments\\nC(i) = argmin\\n1≤k≤Kdii∗\\nk(14.37)\\nrequires computation proportional to K≤Nas before. Thus, K-medoids is\\nfar more computationally intensive than K-means.\\nAlternating between (14.35) and (14.37) represents a particular heuristic\\nsearch strategy for trying to solve', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='332eb591-7ea0-4beb-b3a0-e3dc9652ff55', embedding=None, metadata={'page_label': '536', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 517\\nTABLE 14.3. Data from a political science survey: values are average pair wise\\ndissimilarities of countries from a questionnaire given to pol itical science students.\\nBEL BRA CHI CUB EGY FRA IND ISR USA USS YUG\\nBRA 5.58\\nCHI 7.00 6.50\\nCUB 7.08 7.00 3.83\\nEGY 4.83 5.08 8.17 5.83\\nFRA 2.17 5.75 6.67 6.92 4.92\\nIND 6.42 5.00 5.58 6.00 4.67 6.42\\nISR 3.42 5.50 6.42 6.42 5.00 3.92 6.17\\nUSA 2.50 4.92 6.25 7.33 4.50 2.25 6.33 2.75\\nUSS 6.08 6.67 4.25 2.67 6.00 6.17 6.17 6.92 6.17\\nYUG 5.25 6.83 4.50 3.75 5.75 5.42 6.08 5.83 6.67 3.67\\nZAI 4.75 3.00 6.08 6.67 5.00 5.58 4.83 6.17 5.67 6.50 6.92\\nmin\\nC,{ik}K\\n1K∑\\nk=1∑\\nC(i)=kdiik. (14.38)\\nKaufman and Rousseeuw (1990) propose an alternative strategy for directly\\nsolving (14.38) that provisionally exchanges each center ikwith an obser-\\nvation that is not currently a center, selecting the exchange that produces\\nthe greatest reduction in the value of the criterion (14.38). This is repeated\\nuntil no advantageous exchanges can be found. Massart et al. (1983) derive\\na branch-and-bound combinatorial method that ﬁnds the global minimum\\nof (14.38) that is practical only for very small data sets.\\nExample: Country Dissimilarities\\nThis example, taken from Kaufman and Rousseeuw (1990), comes from a\\nstudy in which political science students were asked to provide pairwise dis-\\nsimilarity measures for 12 countries: Belgium, Brazil, Chile, Cuba, Egypt,\\nFrance, India, Israel, United States, Union of Soviet Socialist Republics,\\nYugoslavia and Zaire. The average dissimilarity scores are given in Ta -\\nble 14.3. We applied 3-medoid clustering to these dissimilarities. Note that\\nK-means clustering could not be applied because we have only distances\\nrather than raw observations. The left panel of Figure 14.10 shows the\\ndissimilarities reordered and blocked according to the 3-medoid clustering.\\nThe right panel is a two-dimensional multidimensional scaling plot, with\\nthe 3-medoid clusters assignments indicated by colors (multidimensional\\nscaling is discussed in Section 14.8.) Both plots show three well-separated\\nclusters, but the MDS display indicates that “Egypt” falls about halfway\\nbetween two clusters.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='007adadf-e7be-4aaa-9e36-ddd1e194907b', embedding=None, metadata={'page_label': '537', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='518 14. Unsupervised Learning\\nCHICUBUSSYUGBRAINDZAIBELEGYFRAISR\\nCUBUSSYUGBRAINDZAIBELEGYFRAISRUSA\\nReordered Dissimilarity Matrix First MDS CoordinateSecond MDS Coordinate\\n-2 0 2 4-2 -1 0 1 2 3CHI\\nCUB\\nUSS\\nYUGBRAIND ZAI\\nBELEGY\\nFRAISRUSA\\nFIGURE 14.10. Survey of country dissimilarities. (Left panel:) dissimilari ties\\nreordered and blocked according to 3-medoid clustering. Heat map is coded from\\nmost similar (dark red) to least similar (bright red). (Righ t panel:) two-dimen-\\nsional multidimensional scaling plot, with 3-medoid clusters indicated by diﬀerent\\ncolors.\\n14.3.11 Practical Issues\\nIn order to apply K-means or K-medoids one must select the number of\\nclusters K∗and an initialization. The latter can be deﬁned by specifying\\nan initial set of centers {m1,... ,m K}or{i1,... ,i K}or an initial encoder\\nC(i). Usually specifying the centers is more convenient. Suggestions range\\nfrom simple random selection to a deliberate strategy based on forward\\nstepwise assignment. At each step a new center ikis chosen to minimize\\nthe criterion (14.33) or (14.38), given the centers i1,... ,i k−1chosen at the\\nprevious steps. This continues for Ksteps, thereby producing Kinitial\\ncenters with which to begin the optimization algorithm.\\nA choice for the number of clusters Kdepends on the goal. For data\\nsegmentation Kis usually deﬁned as part of the problem. For example,\\na company may employ Ksales people, and the goal is to partition a\\ncustomer database into Ksegments, one for each sales person, such that the\\ncustomers assigned to each one are as similar as possible. Often, however,\\ncluster analysis is used to provide a descriptive statistic for ascertaining t he\\nextent to which the observations comprising the data base fall into natural\\ndistinct groupings. Here the number of such groups K∗is unknown and\\none requires that it, as well as the groupings themselves, be estimated from\\nthe data.\\nData-based methods for estimating K∗typically examine the within-\\ncluster dissimilarity WKas a function of the number of clusters K. Separate\\nsolutions are obtained for K∈ {1,2,... ,K max}. The corresponding values', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='30a3d653-5bae-44a0-aa2c-62f0769b78af', embedding=None, metadata={'page_label': '538', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 519\\n{W1,W2,... ,W Kmax}generally decrease with increasing K. This will be\\nthe case even when the criterion is evaluated on an independent test set,\\nsince a large number of cluster centers will tend to ﬁll the feature space\\ndensely and thus will be close to all data points. Thus cross-validation\\ntechniques, so useful for model selection in supervised learning, cannot be\\nutilized in this context.\\nThe intuition underlying the approach is that if there are actually K∗\\ndistinct groupings of the observations (as deﬁned by the dissimilarity mea-\\nsure), then for K < K∗the clusters returned by the algorithm will each\\ncontain a subset of the true underlying groups. That is, the solution will\\nnot assign observations in the same naturally occurring group to diﬀerent\\nestimated clusters. To the extent that this is the case, the solution criterion\\nvalue will tend to decrease substantially with each successive increase in the\\nnumber of speciﬁed clusters, WK+1≪WK, as the natural groups are suc-\\ncessively assigned to separate clusters. For K > K∗, one of the estimated\\nclusters must partition at least one of the natural groups into two sub-\\ngroups. This will tend to provide a smaller decrease in the criterion as Kis\\nfurther increased. Splitting a natural group, within which the observations\\nare all quite close to each other, reduces the criterion less than partitioning\\nthe union of two well-separated groups into their proper constituents.\\nTo the extent this scenario is realized, there will be a sharp decrease in\\nsuccessive diﬀerences in criterion value, WK−WK+1, atK=K∗. That\\nis,{WK−WK+1|K < K∗} ≫ { WK−WK+1|K≥K∗}. An estimate\\nˆK∗forK∗is then obtained by identifying a “kink” in the plot of WKas a\\nfunction of K. As with other aspects of clustering procedures, this approach\\nis somewhat heuristic.\\nThe recently proposed Gap statistic (Tibshirani et al., 2001b) compares\\nthe curve log WKto the curve obtained from data uniformly distributed\\nover a rectangle containing the data. It estimates the optimal number of\\nclusters to be the place where the gap between the two curves is largest.\\nEssentially this is an automatic way of locating the aforementioned “ki nk.”\\nIt also works reasonably well when the data fall into a single cluster, and\\nin that case will tend to estimate the optimal number of clusters to be one.\\nThis is the scenario where most other competing methods fail.\\nFigure 14.11 shows the result of the Gap statistic applied to simulated\\ndata of Figure 14.4. The left panel shows log WKfork= 1,2,... ,8 clusters\\n(green curve) and the expected value of log WKover 20 simulations from\\nuniform data (blue curve). The right panel shows the gap curve, which is the\\nexpected curve minus the observed curve. Shown also are error bars of half-\\nwidth s′\\nK=sK√\\n1 + 1/20, where sKis the standard deviation of log WK\\nover the 20 simulations. The Gap curve is maximized at K= 2 clusters. If\\nG(K) is the Gap curve at Kclusters, the formal rule for estimating K∗is\\nK∗= argmin\\nK{K|G(K)≥G(K+ 1)−s′\\nK+1}. (14.39)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5b6a35e-f73e-48db-8f7d-9424686b3707', embedding=None, metadata={'page_label': '539', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='520 14. Unsupervised Learning\\nNumber of Clusters2 4 6 8-3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0•\\n••\\n••\\n••••\\n•\\n•\\n•\\n•\\n•••\\nNumber of ClustersGap\\n2 4 6 8-0.5 0.0 0.5 1.0••\\n•••••\\n•logWK\\nFIGURE 14.11. (Left panel): observed (green) and expected (blue) values of\\nlogWKfor the simulated data of Figure 14.4. Both curves have been t ranslated\\nto equal zero at one cluster. (Right panel): Gap curve, equal to the diﬀerence\\nbetween the observed and expected values of logWK. The Gap estimate K∗is the\\nsmallest Kproducing a gap within one standard deviation of the gap at K+ 1;\\nhereK∗= 2.\\nThis gives K∗= 2, which looks reasonable from Figure 14.4.\\n14.3.12 Hierarchical Clustering\\nThe results of applying K-means or K-medoids clustering algorithms de-\\npend on the choice for the number of clusters to be searched and a starting\\nconﬁguration assignment. In contrast, hierarchical clustering methods do\\nnot require such speciﬁcations. Instead, they require the user to specify a\\nmeasure of dissimilarity between (disjoint) groups of observations, based\\non the pairwise dissimilarities among the observations in the two groups.\\nAs the name suggests, they produce hierarchical representations in which\\nthe clusters at each level of the hierarchy are created by merging clusters\\nat the next lower level. At the lowest level, each cluster contains a single\\nobservation. At the highest level there is only one cluster containing all of\\nthe data.\\nStrategies for hierarchical clustering divide into two basic paradigms: ag-\\nglomerative (bottom-up) and divisive (top-down). Agglomerative strategies\\nstart at the bottom and at each level recursively merge a selected pair of\\nclusters into a single cluster. This produces a grouping at the next higher\\nlevel with one less cluster. The pair chosen for merging consist of the two\\ngroups with the smallest intergroup dissimilarity. Divisive methods s tart\\nat the top and at each level recursively split one of the existing clusters at', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5272e727-37ab-4248-941e-ce4556b87f9a', embedding=None, metadata={'page_label': '540', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 521\\nthat level into two new clusters. The split is chosen to produce two new\\ngroups with the largest between-group dissimilarity. With both paradigms\\nthere are N−1 levels in the hierarchy.\\nEach level of the hierarchy represents a particular grouping of the data\\ninto disjoint clusters of observations. The entire hierarchy represents an\\nordered sequence of such groupings. It is up to the user to decide which\\nlevel (if any) actually represents a “natural” clustering in the sense that\\nobservations within each of its groups are suﬃciently more similar to eac h\\nother than to observations assigned to diﬀerent groups at that level. The\\nGap statistic described earlier can be used for this purpose.\\nRecursive binary splitting/agglomeration can be represented by a rooted\\nbinary tree. The nodes of the trees represent groups. The root node repre-\\nsents the entire data set. The Nterminal nodes each represent one of the\\nindividual observations (singleton clusters). Each nonterminal node (“par-\\nent”) has two daughter nodes. For divisive clustering the two daughters\\nrepresent the two groups resulting from the split of the parent; for agglom-\\nerative clustering the daughters represent the two groups that were merged\\nto form the parent.\\nAll agglomerative and some divisive methods (when viewed bottom-up)\\npossess a monotonicity property. That is, the dissimilarity between merged\\nclusters is monotone increasing with the level of the merger. Thus the\\nbinary tree can be plotted so that the height of each node is proportional\\nto the value of the intergroup dissimilarity between its two daughters. The\\nterminal nodes representing individual observations are all plotted at zero\\nheight. This type of graphical display is called a dendrogram .\\nA dendrogram provides a highly interpretable complete description of\\nthe hierarchical clustering in a graphical format. This is one of the main\\nreasons for the popularity of hierarchical clustering methods.\\nFor the microarray data, Figure 14.12 shows the dendrogram resulting\\nfrom agglomerative clustering with average linkage; agglomerative cl uster-\\ning and this example are discussed in more detail later in this chapter.\\nCutting the dendrogram horizontally at a particular height partitions the\\ndata into disjoint clusters represented by the vertical lines that intersect\\nit. These are the clusters that would be produced by terminating the pro-\\ncedure when the optimal intergroup dissimilarity exceeds that threshold\\ncut value. Groups that merge at high values, relative to the merger values\\nof the subgroups contained within them lower in the tree, are candidates\\nfor natural clusters. Note that this may occur at several diﬀerent levels,\\nindicating a clustering hierarchy: that is, clusters nested within clusters.\\nSuch a dendrogram is often viewed as a graphical summary of the data\\nitself, rather than a description of the results of the algorithm. However,\\nsuch interpretations should be treated with caution. First, diﬀerent hierar-\\nchical methods (see below), as well as small changes in the data, can lead\\nto quite diﬀerent dendrograms. Also, such a summary will be valid only to\\nthe extent that the pairwise observation dissimilarities possess the hierar-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='afee48d3-c2bd-4f5b-822e-ff5bcc55d536', embedding=None, metadata={'page_label': '541', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='522 14. Unsupervised Learning\\nCNSCNSCNSRENAL\\nBREASTCNSCNS\\nBREASTNSCLC\\nNSCLCRENAL\\nRENALRENALRENAL\\nRENALRENALRENALBREAST\\nNSCLCRENAL\\nUNKNOWN\\nOVARIAN\\nMELANOMA\\nPROSTATEOVARIANOVARIAN\\nOVARIANOVARIAN\\nOVARIAN\\nPROSTATENSCLCNSCLCNSCLCLEUKEMIAK562B-reproK562A-reproLEUKEMIA\\nLEUKEMIALEUKEMIALEUKEMIALEUKEMIA\\nCOLONCOLON\\nCOLON\\nCOLONCOLONCOLON\\nCOLONMCF7A-repro\\nBREAST\\nMCF7D-reproBREASTNSCLC\\nNSCLCNSCLCMELANOMA\\nBREASTBREAST\\nMELANOMA\\nMELANOMA\\nMELANOMAMELANOMAMELANOMA\\nMELANOMA\\nFIGURE 14.12. Dendrogram from agglomerative hierarchical clustering with\\naverage linkage to the human tumor microarray data.\\nchical structure produced by the algorithm. Hierarchical methods impose\\nhierarchical structure whether or not such structure actually exists in the\\ndata.\\nThe extent to which the hierarchical structure produced by a dendro-\\ngram actually represents the data itself can be judged by the cophenetic\\ncorrelation coeﬃcient . This is the correlation between the N(N−1)/2 pair-\\nwise observation dissimilarities dii′input to the algorithm and their corre-\\nsponding cophenetic dissimilarities Cii′derived from the dendrogram. The\\ncophenetic dissimilarity Cii′between two observations ( i,i′) is the inter-\\ngroup dissimilarity at which observations iandi′are ﬁrst joined together\\nin the same cluster.\\nThe cophenetic dissimilarity is a very restrictive dissimilarity measure.\\nFirst, the Cii′over the observations must contain many ties, since only N−1\\nof the total N(N−1)/2 values can be distinct. Also these dissimilarities\\nobey the ultrametric inequality\\nCii′≤max{Cik,Ci′k} (14.40)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00109711-4d89-4642-a858-ad4fc2697a3d', embedding=None, metadata={'page_label': '542', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 523\\nfor any three observations ( i,i′,k). As a geometric example, suppose the\\ndata were represented as points in a Euclidean coordinate system. In order\\nfor the set of interpoint distances over the data to conform to (14.40), the\\ntriangles formed by all triples of points must be isosceles triangles wit h the\\nunequal length no longer than the length of the two equal sides (Jain and\\nDubes, 1988). Therefore it is unrealistic to expect general dissimilarities\\nover arbitrary data sets to closely resemble their corresponding cophenetic\\ndissimilarities as calculated from a dendrogram, especially if there are not\\nmany tied values. Thus the dendrogram should be viewed mainly as a de-\\nscription of the clustering structure of the data as imposed by the particular\\nalgorithm employed.\\nAgglomerative Clustering\\nAgglomerative clustering algorithms begin with every observation repre-\\nsenting a singleton cluster. At each of the N−1 steps the closest two (least\\ndissimilar) clusters are merged into a single cluster, producing one less clus-\\nter at the next higher level. Therefore, a measure of dissimilarity between\\ntwo clusters (groups of observations) must be deﬁned.\\nLetGandHrepresent two such groups. The dissimilarity d(G,H) be-\\ntween GandHis computed from the set of pairwise observation dissim-\\nilarities dii′where one member of the pair iis inGand the other i′is\\ninH.Single linkage (SL) agglomerative clustering takes the intergroup\\ndissimilarity to be that of the closest (least dissimilar) pair\\ndSL(G,H) = min\\ni∈G\\ni′∈Hdii′. (14.41)\\nThis is also often called the nearest-neighbor technique. Complete linkage\\n(CL) agglomerative clustering ( furthest-neighbor technique) takes the in-\\ntergroup dissimilarity to be that of the furthest (most dissimilar) pai r\\ndCL(G,H) = max\\ni∈G\\ni′∈Hdii′. (14.42)\\nGroup average (GA) clustering uses the average dissimilarity between the\\ngroups\\ndGA(G,H) =1\\nNGNH∑\\ni∈G∑\\ni′∈Hdii′ (14.43)\\nwhere NGandNHare the respective number of observations in each group.\\nAlthough there have been many other proposals for deﬁning intergroup\\ndissimilarity in the context of agglomerative clustering, the above thr ee are\\nthe ones most commonly used. Figure 14.13 shows examples of all three.\\nIf the data dissimilarities {dii′}exhibit a strong clustering tendency, with\\neach of the clusters being compact and well separated from others, then all\\nthree methods produce similar results. Clusters are compact if all of the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='77e7cc6b-dcd4-42f8-9ddb-46c607120de5', embedding=None, metadata={'page_label': '543', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='524 14. Unsupervised Learning\\nAverage Linkage Complete Linkage Single Linkage\\nFIGURE 14.13. Dendrograms from agglomerative hierarchical clustering of h u-\\nman tumor microarray data.\\nobservations within them are relatively close together (small dissimilar ities)\\nas compared with observations in diﬀerent clusters. To the extent this is\\nnot the case, results will diﬀer.\\nSingle linkage (14.41) only requires that a single dissimilarity dii′,i∈G\\nandi′∈H, be small for two groups GandHto be considered close\\ntogether, irrespective of the other observation dissimilarities between the\\ngroups. It will therefore have a tendency to combine, at relatively low\\nthresholds, observations linked by a series of close intermediate observa-\\ntions. This phenomenon, referred to as chaining , is often considered a de-\\nfect of the method. The clusters produced by single linkage can violate the\\n“compactness” property that all observations within each cluster tend to\\nbe similar to one another, based on the supplied observation dissimilari-\\nties{dii′}. If we deﬁne the diameter DGof a group of observations as the\\nlargest dissimilarity among its members\\nDG= max\\ni∈G\\ni′∈Gdii′, (14.44)\\nthen single linkage can produce clusters with very large diameters.\\nComplete linkage (14.42) represents the opposite extreme. Two groups\\nGandHare considered close only if all of the observations in their union\\nare relatively similar. It will tend to produce compact clusters with small\\ndiameters (14.44). However, it can produce clusters that violate the “close-\\nness” property. That is, observations assigned to a cluster can be much', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0879e0d1-9c83-4930-b242-739935e26fa3', embedding=None, metadata={'page_label': '544', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 525\\ncloser to members of other clusters than they are to some members of their\\nown cluster.\\nGroup average clustering (14.43) represents a compromise between the\\ntwo extremes of single and complete linkage. It attempts to produce rel-\\natively compact clusters that are relatively far apart. However, its results\\ndepend on the numerical scale on which the observation dissimilarities dii′\\nare measured. Applying a monotone strictly increasing transformation h(≤)\\nto the dii′,hii′=h(dii′), can change the result produced by (14.43). In\\ncontrast, (14.41) and (14.42) depend only on the ordering of the dii′and\\nare thus invariant to such monotone transformations. This invariance is\\noften used as an argument in favor of single or complete linkage over group\\naverage methods.\\nOne can argue that group average clustering has a statistical consis-\\ntency property violated by single and complete linkage. Assume we have\\nattribute-value data XT= (X1,... ,X p) and that each cluster kis a ran-\\ndom sample from some population joint density pk(x). The complete data\\nset is a random sample from a mixture of Ksuch densities. The group\\naverage dissimilarity dGA(G,H) (14.43) is an estimate of\\n∫ ∫\\nd(x,x′)pG(x)pH(x′)dx dx′, (14.45)\\nwhere d(x,x′) is the dissimilarity between points xandx′in the space\\nof attribute values. As the sample size Napproaches inﬁnity dGA(G,H)\\n(14.43) approaches (14.45), which is a characteristic of the relationshi p\\nbetween the two densities pG(x) and pH(x). For single linkage, dSL(G,H)\\n(14.41) approaches zero as N→ ∞ independent of pG(x) and pH(x). For\\ncomplete linkage, dCL(G,H) (14.42) becomes inﬁnite as N→ ∞, again\\nindependent of the two densities. Thus, it is not clear what aspects of the\\npopulation distribution are being estimated by dSL(G,H) and dCL(G,H).\\nExample: Human Cancer Microarray Data (Continued)\\nThe left panel of Figure 14.13 shows the dendrogram resulting from average\\nlinkage agglomerative clustering of the samples (columns) of the microarra y\\ndata. The middle and right panels show the result using complete and single\\nlinkage. Average and complete linkage gave similar results, while single\\nlinkage produced unbalanced groups with long thin clusters. We focus on\\nthe average linkage clustering.\\nLikeK-means clustering, hierarchical clustering is successful at clustering\\nsimple cancers together. However it has other nice features. By cutting oﬀ\\nthe dendrogram at various heights, diﬀerent numbers of clusters emerge,\\nand the sets of clusters are nested within one another. Secondly, it gives\\nsome partial ordering information about the samples. In Figure 14.14, w e\\nhave arranged the genes (rows) and samples (columns) of the expression\\nmatrix in orderings derived from hierarchical clustering.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b474c30-9117-4637-a628-a3d6bb5a29d7', embedding=None, metadata={'page_label': '545', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='526 14. Unsupervised Learning\\nNote that if we ﬂip the orientation of the branches of a dendrogram at any\\nmerge, the resulting dendrogram is still consistent with the series of hierar-\\nchical clustering operations. Hence to determine an ordering of the leaves,\\nwe must add a constraint. To produce the row ordering of Figure 14.14,\\nwe have used the default rule in S-PLUS: at each merge, the subtree with\\nthe tighter cluster is placed to the left (toward the bottom in the rotated\\ndendrogram in the ﬁgure.) Individual genes are the tightest clusters possi-\\nble, and merges involving two individual genes place them in order by their\\nobservation number. The same rule was used for the columns. Many other\\nrules are possible—for example, ordering by a multidimensional scaling of\\nthe genes; see Section 14.8.\\nThe two-way rearrangement of Figure14.14 produces an informative pic-\\nture of the genes and samples. This picture is more informative than the\\nrandomly ordered rows and columns of Figure 1.3 of Chapter 1. Further-\\nmore, the dendrograms themselves are useful, as biologists can, for example,\\ninterpret the gene clusters in terms of biological processes.\\nDivisive Clustering\\nDivisive clustering algorithms begin with the entire data set as a single\\ncluster, and recursively divide one of the existing clusters into two daugh-\\nter clusters at each iteration in a top-down fashion. This approach has not\\nbeen studied nearly as extensively as agglomerative methods in the cluster-\\ning literature. It has been explored somewhat in the engineering literature\\n(Gersho and Gray, 1992) in the context of compression. In the clustering\\nsetting, a potential advantage of divisive over agglomerative methods can\\noccur when interest is focused on partitioning the data into a relatively\\nsmall number of clusters.\\nThe divisive paradigm can be employed by recursively applying any of\\nthe combinatorial methods such as K-means (Section 14.3.6) or K-medoids\\n(Section 14.3.10), with K= 2, to perform the splits at each iteration. How-\\never, such an approach would depend on the starting conﬁguration speciﬁed\\nat each step. In addition, it would not necessarily produce a splitting se-\\nquence that possesses the monotonicity property required for dendrogram\\nrepresentation.\\nA divisive algorithm that avoids these problems was proposed by Mac-\\nnaughton Smith et al. (1965). It begins by placing all observations in a\\nsingle cluster G. It then chooses that observation whose average dissimi-\\nlarity from all the other observations is largest. This observation for ms the\\nﬁrst member of a second cluster H. At each successive step that observation\\ninGwhose average distance from those in H, minus that for the remaining\\nobservations in Gis largest, is transferred to H. This continues until the\\ncorresponding diﬀerence in averages becomes negative. That is, there are\\nno longer any observations in Gthat are, on average, closer to those in\\nH. The result is a split of the original cluster into two daughter clusters,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a6240dae-c4a2-458d-b39f-8a6240aad33f', embedding=None, metadata={'page_label': '546', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Cluster Analysis 527\\nFIGURE 14.14. DNA microarray data: average linkage hierarchical clusteri ng\\nhas been applied independently to the rows (genes) and columns (sam ples), de-\\ntermining the ordering of the rows and columns (see text). The color s range from\\nbright green (negative, under-expressed) to bright red (posit ive, over-expressed).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb9730d1-d6b3-4f5f-af4f-dbed1b04cf90', embedding=None, metadata={'page_label': '547', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='528 14. Unsupervised Learning\\nthe observations transferred to H, and those remaining in G. These two\\nclusters represent the second level of the hierarchy. Each successive level\\nis produced by applying this splitting procedure to one of the clusters at\\nthe previous level. Kaufman and Rousseeuw (1990) suggest choosing the\\ncluster at each level with the largest diameter (14.44) for splitting. An al -\\nternative would be to choose the one with the largest average dissimilar ity\\namong its members\\n¯dG=1\\nNG∑\\ni∈G∑\\ni′∈Gdii′.\\nThe recursive splitting continues until all clusters either become singletons\\nor all members of each one have zero dissimilarity from one another.\\n14.4 Self-Organizing Maps\\nThis method can be viewed as a constrained version of K-means clustering,\\nin which the prototypes are encouraged to lie in a one- or two-dimensional\\nmanifold in the feature space. The resulting manifold is also referred to\\nas aconstrained topological map , since the original high-dimensional obser-\\nvations can be mapped down onto the two-dimensional coordinate system.\\nThe original SOM algorithm was online—observations are processed one at\\na time—and later a batch version was proposed. The technique also bears\\na close relationship to principal curves and surfaces , which are discussed in\\nthe next section.\\nWe consider a SOM with a two-dimensional rectangular grid of Kproto-\\ntypes mj∈IRp(other choices, such as hexagonal grids, can also be used).\\nEach of the Kprototypes are parametrized with respect to an integer\\ncoordinate pair ℓj∈ Q1× Q2. Here Q1={1,2,... ,q 1}, similarly Q2, and\\nK=q1≤q2. Themjare initialized, for example, to lie in the two-dimensional\\nprincipal component plane of the data (next section). We can think of the\\nprototypes as “buttons,” “sewn” on the principal component plane in a\\nregular pattern. The SOM procedure tries to bend the plane so that the\\nbuttons approximate the data points as well as possible. Once the model is\\nﬁt, the observations can be mapped down onto the two-dimensional grid.\\nThe observations xiare processed one at a time. We ﬁnd the closest\\nprototype mjtoxiin Euclidean distance in IRp, and then for all neighbors\\nmkofmj, move mktoward xivia the update\\nmk←mk+α(xi−mk). (14.46)\\nThe “neighbors” of mjare deﬁned to be all mksuch that the distance\\nbetween ℓjandℓkis small. The simplest approach uses Euclidean distance,\\nand “small” is determined by a threshold r. This neighborhood always\\nincludes the closest prototype mjitself.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c193950-60d6-4069-9711-2cb64fa21d56', embedding=None, metadata={'page_label': '548', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.4 Self-Organizing Maps 529\\nNotice that distance is deﬁned in the space Q1×Q2of integer topological\\ncoordinates of the prototypes, rather than in the feature space IRp. The\\neﬀect of the update (14.46) is to move the prototypes closer to the data,\\nbut also to maintain a smooth two-dimensional spatial relationship between\\nthe prototypes.\\nThe performance of the SOM algorithm depends on the learning rate\\nαand the distance threshold r. Typically αis decreased from say 1 .0 to\\n0.0 over a few thousand iterations (one per observation). Similarly ris\\ndecreased linearly from starting value Rto 1 over a few thousand iterations.\\nWe illustrate a method for choosing Rin the example below.\\nWe have described the simplest version of the SOM. More sophisticated\\nversions modify the update step according to distance:\\nmk←mk+αh(∥ℓj−ℓk∥)(xi−mk), (14.47)\\nwhere the neighborhood function hgives more weight to prototypes mkwith\\nindices ℓkcloser to ℓjthan to those further away.\\nIf we take the distance rsmall enough so that each neighborhood contains\\nonly one point, then the spatial connection between prototypes is lost. In\\nthat case one can show that the SOM algorithm is an online version of\\nK-means clustering, and eventually stabilizes at one of the local minima\\nfound by K-means. Since the SOM is a constrained version of K-means\\nclustering, it is important to check whether the constraint is reasonable\\nin any given problem. One can do this by computing the reconstruction\\nerror∥x−mj∥2, summed over observations, for both methods. This will\\nnecessarily be smaller for K-means, but should not be much smaller if the\\nSOM is a reasonable approximation.\\nAs an illustrative example, we generated 90 data points in three dimen-\\nsions, near the surface of a half sphere of radius 1. The points were in each\\nof three clusters—red, green, and blue—located near (0 ,1,0), (0,0,1) and\\n(1,0,0). The data are shown in Figure 14.15\\nBy design, the red cluster was much tighter than the green or blue ones.\\n(Full details of the data generation are given in Exercise 14.5.) A 5 ×5 grid\\nof prototypes was used, with initial grid size R= 2; this meant that about\\na third of the prototypes were initially in each neighborhood. We did a\\ntotal of 40 passes through the dataset of 90 observations, and let randα\\ndecrease linearly over the 3600 iterations.\\nIn Figure 14.16 the prototypes are indicated by circles, and the points\\nthat project to each prototype are plotted randomly within the correspond-\\ning circle. The left panel shows the initial conﬁguration, while the right\\npanel shows the ﬁnal one. The algorithm has succeeded in separating the\\nclusters; however, the separation of the red cluster indicates that the man-\\nifold has folded back on itself (see Figure 14.17). Since the distances in the\\ntwo-dimensional display are not used, there is little indication in the SOM\\nprojection that the red cluster is tighter than the others.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1a2cde22-864e-457d-8937-322135d56e50', embedding=None, metadata={'page_label': '549', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='530 14. Unsupervised Learning\\n−1−0.500.511.5\\n−1−0.500.511.5−1−0.500.511.5\\nFIGURE 14.15. Simulated data in three classes, near the surface of a half–\\nsphere.\\n•••\\n••\\n•••\\n••\\n•••\\n•••••\\n••••••••\\n•••\\n•••\\n••\\n•\\n••\\n••••\\n••••\\n•• ••• ••\\n••\\n••• •\\n••••\\n•••••\\n••\\n••••\\n••• •\\n••\\n•\\n•••\\n••\\n•••\\n••\\n1 2 3 4 512345\\n••••\\n••\\n•\\n••\\n••••••\\n•\\n••\\n••••\\n•\\n•••••\\n••\\n• • • •••\\n••\\n•\\n••••\\n•••••\\n•\\n•••\\n•• •• ••\\n• •\\n•••\\n••••\\n•••\\n•••\\n••\\n••\\n•• •\\n•••\\n• • ••\\n•••\\n1 2 3 4 512345\\nFIGURE 14.16. Self-organizing map applied to half-sphere data example. Left\\npanel is the initial conﬁguration, right panel the ﬁnal one. The 5×5grid of\\nprototypes are indicated by circles, and the points that projec t to each prototype\\nare plotted randomly within the corresponding circle.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fd5edca3-0039-449a-9764-78d3376b389e', embedding=None, metadata={'page_label': '550', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.4 Self-Organizing Maps 531\\nFIGURE 14.17. Wiremesh representation of the ﬁtted SOM model in I R3. The\\nlines represent the horizontal and vertical edges of the topolog ical lattice. The\\ndouble lines indicate that the surface was folded diagonally ba ck on itself in order\\nto model the red points. The cluster members have been jittere d to indicate their\\ncolor, and the purple points are the node centers.\\nFigure 14.18 shows the reconstruction error, equal to the total sum of\\nsquares of each data point around its prototype. For comparison we carried\\nout aK-means clustering with 25 centroids, and indicate its reconstruction\\nerror by the horizontal line on the graph. We see that the SOM signiﬁcantly\\ndecreases the error, nearly to the level of the K-means solution. This pro-\\nvides evidence that the two-dimensional constraint used by the SOM is\\nreasonable for this particular dataset.\\nIn the batch version of the SOM, we update each mjvia\\nmj=∑wkxk∑wk. (14.48)\\nThe sum is over points xkthat mapped (i.e., were closest to) neighbors mk\\nofmj. The weight function may be rectangular, that is, equal to 1 for the\\nneighbors of mk, or may decrease smoothly with distance ∥ℓk−ℓj∥as before.\\nIf the neighborhood size is chosen small enough so that it consists only\\nofmk, with rectangular weights, this reduces to the K-means clustering\\nprocedure described earlier. It can also be thought of as a discrete version\\nof principal curves and surfaces, described in Section 14.5.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b51af619-9aa2-4848-b7d0-402ce8da13e8', embedding=None, metadata={'page_label': '551', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='532 14. Unsupervised Learning\\nIterationReconstruction Error\\n0 500 1000 1500 2000 25000 10 20 30 40 50••\\n•\\n•••\\n••\\n••\\n••••\\n•••••••••••••••••••••••••••••••••••••\\nFIGURE 14.18. Half-sphere data: reconstruction error for the SOM as a func-\\ntion of iteration. Error for k-means clustering is indicated by the horizontal line.\\nExample: Document Organization and Retrieval\\nDocument retrieval has gained importance with the rapid development of\\nthe Internet and the Web, and SOMs have proved to be useful for organiz-\\ning and indexing large corpora. This example is taken from the WEBSOM\\nhomepage http://websom.hut.fi/ (Kohonen et al., 2000). Figure 14.19 rep-\\nresents a SOM ﬁt to 12,088 newsgroup comp.ai.neural-nets articles. The\\nlabels are generated automatically by the WEBSOM software and provide\\na guide as to the typical content of a node.\\nIn applications such as this, the documents have to be reprocessed in\\norder to create a feature vector. A term-document matrix is created, where\\neach row represents a single document. The entries in each row are the\\nrelative frequency of each of a predeﬁned set of terms. These terms could\\nbe a large set of dictionary entries (50,000 words), or an even larger set\\nof bigrams (word pairs), or subsets of these. These matrices are typically\\nvery sparse, and so often some preprocessing is done to reduce the number\\nof features (columns). Sometimes the SVD (next section) is used to reduce\\nthe matrix; Kohonen et al. (2000) use a randomized variant thereof. These\\nreduced vectors are then the input to the SOM.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91b7972c-8705-4c44-84a1-c2c5359fd555', embedding=None, metadata={'page_label': '552', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.4 Self-Organizing Maps 533\\nFIGURE 14.19. Heatmap representation of the SOM model ﬁt to a corpus\\nof12,088 newsgroup comp.ai.neural-nets contributions (courtesy WEBSOM\\nhomepage). The lighter areas indicate higher-density areas. Populated nodes are\\nautomatically labeled according to typical content.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9f31314b-fb65-4233-9a9e-50db0d0333f1', embedding=None, metadata={'page_label': '553', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='534 14. Unsupervised Learning\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n• ••\\n••\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n• ••\\n•v1v1v1v1v1v1v1v1\\nui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1\\nxixixixixixixixi\\nFIGURE 14.20. The ﬁrst linear principal component of a set of data. The line\\nminimizes the total squared distance from each point to its orth ogonal projection\\nonto the line.\\nIn this application the authors have developed a “zoom” feature, which\\nallows one to interact with the map in order to get more detail. The ﬁnal\\nlevel of zooming retrieves the actual news articles, which can then be read.\\n14.5 Principal Components, Curves and Surfaces\\nPrincipal components are discussed in Sections 3.4.1, where they shed light\\non the shrinkage mechanism of ridge regression. Principal components are\\na sequence of projections of the data, mutually uncorrelated and ordered\\nin variance. In the next section we present principal components as linear\\nmanifolds approximating a set of Npoints xi∈IRp. We then present\\nsome nonlinear generalizations in Section 14.5.2. Other recent proposals\\nfor nonlinear approximating manifolds are discussed in Section 14.9.\\n14.5.1 Principal Components\\nThe principal components of a set of data in IRpprovide a sequence of best\\nlinear approximations to that data, of all ranks q≤p.\\nDenote the observations by x1,x2,... ,x N, and consider the rank- qlinear\\nmodel for representing them', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d2cfd5c1-ec86-49de-aceb-aaf92ea25597', embedding=None, metadata={'page_label': '554', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 535\\nf(λ) =θ+Vqλ, (14.49)\\nwhere θis a location vector in IRp,Vqis ap×qmatrix with qorthogonal\\nunit vectors as columns, and λis aqvector of parameters. This is the\\nparametric representation of an aﬃne hyperplane of rank q. Figures 14.20\\nand 14.21 illustrate for q= 1 and q= 2, respectively. Fitting such a model\\nto the data by least squares amounts to minimizing the reconstruction error\\nmin\\nθ,{λi},VqN∑\\ni=1∥xi−θ−Vqλi∥2. (14.50)\\nWe can partially optimize for θand the λi(Exercise 14.7) to obtain\\nˆθ= ¯x, (14.51)\\nˆλi=VT\\nq(xi−¯x). (14.52)\\nThis leaves us to ﬁnd the orthogonal matrix Vq:\\nmin\\nVqN∑\\ni=1||(xi−¯x)−VqVT\\nq(xi−¯x)||2. (14.53)\\nFor convenience we assume that ¯ x= 0 (otherwise we simply replace the\\nobservations by their centered versions ˜ xi=xi−¯x). The p×pmatrix\\nHq=VqVT\\nqis aprojection matrix , and maps each point xionto its rank-\\nqreconstruction Hqxi, the orthogonal projection of xionto the subspace\\nspanned by the columns of Vq. The solution can be expressed as follows.\\nStack the (centered) observations into the rows of an N×pmatrix X. We\\nconstruct the singular value decomposition ofX:\\nX=UDVT. (14.54)\\nThis is a standard decomposition in numerical analysis, and many algo-\\nrithms exist for its computation (Golub and Van Loan, 1983, for example).\\nHereUis anN×porthogonal matrix ( UTU=Ip) whose columns ujare\\ncalled the left singular vectors ;Vis ap×porthogonal matrix ( VTV=Ip)\\nwith columns vjcalled the right singular vectors , andDis ap×pdiagonal\\nmatrix, with diagonal elements d1≥d2≥ ≤≤≤ ≥ dp≥0 known as the sin-\\ngular values . For each rank q, the solution Vqto (14.53) consists of the ﬁrst\\nqcolumns of V. The columns of UDare called the principal components\\nofX(see Section 3.5.1). The Noptimal ˆλiin (14.52) are given by the ﬁrst\\nqprincipal components (the Nrows of the N×qmatrix UqDq).\\nThe one-dimensional principal component line in IR2is illustrated in Fig-\\nure 14.20. For each data point xi, there is a closest point on the line, given\\nbyui1d1v1. Here v1is the direction of the line and ˆλi=ui1d1measures\\ndistance along the line from the origin. Similarly Figure 14.21 shows the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab077da5-20d9-4f48-ad83-909417b0f1d9', embedding=None, metadata={'page_label': '555', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='536 14. Unsupervised Learning\\nFirst principal componentSecond principal component\\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0•\\n•••\\n•••\\n•\\n••\\n•••\\n••••\\n•\\n•••\\n••••••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n••\\nFIGURE 14.21. The best rank-two linear approximation to the half-sphere data .\\nThe right panel shows the projected points with coordinates giv en byU2D2, the\\nﬁrst two principal components of the data.\\ntwo-dimensional principal component surface ﬁt to the half-sphere data\\n(left panel). The right panel shows the projection of the data onto the\\nﬁrst two principal components. This projection was the basis for the initial\\nconﬁguration for the SOM method shown earlier. The procedure is quite\\nsuccessful at separating the clusters. Since the half-sphere is nonlinear, a\\nnonlinear projection will do a better job, and this is the topic of the next\\nsection.\\nPrincipal components have many other nice properties, for example, the\\nlinear combination Xv1has the highest variance among all linear com-\\nbinations of the features; Xv2has the highest variance among all linear\\ncombinations satisfying v2orthogonal to v1, and so on.\\nExample: Handwritten Digits\\nPrincipal components are a useful tool for dimension reduction and com-\\npression. We illustrate this feature on the handwritten digits data described\\nin Chapter 1. Figure 14.22 shows a sample of 130 handwritten 3’s, each a\\ndigitized 16 ×16 grayscale image, from a total of 658 such 3’s. We see\\nconsiderable variation in writing styles, character thickness and orienta-\\ntion. We consider these images as points xiin IR256, and compute their\\nprincipal components via the SVD (14.54).\\nFigure 14.23 shows the ﬁrst two principal components of these data. For\\neach of these ﬁrst two principal components ui1andui2, we computed the\\n5%, 25%, 50%, 75% and 95% quantile points, and used them to deﬁne\\nthe rectangular grid superimposed on the plot. The circled points indicate', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20a329cf-b966-4525-ac43-c735279519e9', embedding=None, metadata={'page_label': '556', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 537\\nFIGURE 14.22. A sample of 130handwritten 3’s shows a variety of writing\\nstyles.\\nthose images close to the vertices of the grid, where the distance measure\\nfocuses mainly on these projected coordinates, but gives some weight to the\\ncomponents in the orthogonal subspace. The right plot shows the images\\ncorresponding to these circled points. This allows us to visualize the nature\\nof the ﬁrst two principal components. We see that the v1(horizontal move-\\nment) mainly accounts for the lengthening of the lower tail of the three,\\nwhile v2(vertical movement) accounts for character thickness. In terms of\\nthe parametrized model (14.49), this two-component model has the form\\nˆf(λ) = ¯ x+λ1v1+λ2v2\\n= +λ1≤ +λ2≤ . (14.55)\\nHere we have displayed the ﬁrst two principal component directions, v1\\nandv2, as images. Although there are a possible 256 principal components,\\napproximately 50 account for 90% of the variation in the threes, 12 ac-\\ncount for 63%. Figure 14.24 compares the singular values to those obtained\\nfor equivalent uncorrelated data, obtained by randomly scrambling each\\ncolumn of X. The pixels in a digitized image are inherently correlated,\\nand since these are all the same digit the correlations are even stronger.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='338da158-0176-4885-a6c5-66d79c5803ab', embedding=None, metadata={'page_label': '557', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='538 14. Unsupervised Learning\\nFirst Principal ComponentSecond Principal Component\\n-6 -4 -2 0 2 4 6 8-5 0 5••\\n••\\n••\\n••\\n• ••••\\n••\\n•••\\n••\\n••••\\n••\\n•\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•\\n• •\\n••\\n•••\\n••\\n••••\\n•••\\n•••••\\n••\\n•\\n••\\n••\\n•\\n•••\\n•\\n••\\n••••\\n•\\n•\\n•\\n••••••\\n•\\n•••\\n•••\\n•\\n•••••\\n•\\n•••\\n•\\n••\\n••\\n••\\n••\\n•••\\n•••\\n•\\n•••\\n••\\n••••\\n•\\n•\\n••\\n••\\n••\\n••\\n•\\n•••\\n•\\n••\\n•\\n•• •\\n••\\n••\\n•••\\n••\\n•••\\n•\\n•••••\\n••\\n•\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•••••\\n•\\n••\\n••\\n•\\n••\\n••\\n•\\n•• ••\\n••••\\n••\\n••\\n•••\\n••\\n•\\n•••\\n•••\\n•\\n•\\n•\\n•• •\\n•\\n•••\\n• •• ••\\n••••\\n•\\n•\\n••\\n•• •\\n•\\n•\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n• •••\\n•\\n•\\n•••\\n••••\\n•••\\n••\\n••••\\n••\\n•\\n• ••\\n•••\\n•••\\n••\\n••••\\n••••\\n••\\n• ••\\n••\\n•••••\\n•\\n•\\n••••\\n•\\n••••\\n•\\n••\\n••\\n•\\n• •••\\n••\\n••••\\n•\\n•••\\n•\\n•\\n••\\n••••\\n••••\\n•••\\n•\\n•••\\n•\\n•••\\n••\\n•• •\\n•\\n•\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n••\\n•••\\n•••\\n•\\n•••\\n•\\n•••\\n•••••\\n••••\\n•• •\\n•\\n•••\\n•\\n••\\n••\\n•••\\n•\\n••\\n•• ••\\n••\\n••\\n••••\\n••\\n•\\n•••\\n••\\n•\\n•\\n•••\\n•\\n•\\n••\\n••\\n••\\n••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n•\\n••••\\n•\\n••\\n•••\\n•••\\n•••• ••\\n••\\n••••\\n••\\n•••\\n••••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•\\n•••\\n••\\n••\\n••••\\n•\\n•\\n•••••\\n•••\\n••\\n••••\\n•••••\\n••\\n•••\\n••\\n••\\n••••\\n•\\n••\\n•• •••\\n•••\\n••\\n•\\n••••\\n••• ••\\n•\\n••\\n•••\\n•••\\n•••\\n•\\n•\\n•••\\n•••••\\n••\\n•\\n••\\n••\\n••\\n•\\n••••••\\n••\\n•\\n••\\n•\\n••\\nO O O OOO O OO\\nOOO\\nOO OO O O OOO O OOO\\nFIGURE 14.23. (Left panel:) the ﬁrst two principal components of the hand-\\nwritten threes. The circled points are the closest projected images to the vertices\\nof a grid, deﬁned by the marginal quantiles of the principal compone nts. (Right\\npanel:) The images corresponding to the circled points. These sh ow the nature of\\nthe ﬁrst two principal components.\\nDimensionSingular Values\\n0 50 100 150 200 2500 20 40 60 80•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•••\\n•\\n••\\n••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Real Trace\\n•Randomized Trace\\nFIGURE 14.24. The256singular values for the digitized threes, compared to\\nthose for a randomized version of the data (each column of Xwas scrambled).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2cc6ec88-c731-490c-b900-1dfac5736fd4', embedding=None, metadata={'page_label': '558', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 539\\nA relatively small subset of the principal components serve as excellent\\nlower-dimensional features for representing the high-dimensional data.\\nExample: Procrustes Transformations and Shape Averaging\\nFIGURE 14.25. (Left panel:) Two diﬀerent digitized handwritten Ss, each rep-\\nresented by 96 corresponding points in I R2. The green S has been deliberately\\nrotated and translated for visual eﬀect. (Right panel:) A Procr ustes transforma-\\ntion applies a translation and rotation to best match up the two set of points.\\nFigure 14.25 represents two sets of points, the orange and green, in the\\nsame plot. In this instance these points represent two digitized versions\\nof a handwritten S, extracted from the signature of a subject “Suresh.”\\nFigure 14.26 shows the entire signatures from which these were extracted\\n(third and fourth panels). The signatures are recorded dynamically using\\ntouch-screen devices, familiar sights in modern supermarkets. There are\\nN= 96 points representing each S, which we denote by the N×2 matrices\\nX1andX2. There is a correspondence between the points—the ith rows\\nofX1andX2are meant to represent the same positions along the two S’s.\\nIn the language of morphometrics, these points represent landmarks on\\nthe two objects. How one ﬁnds such corresponding landmarks is in general\\ndiﬃcult and subject speciﬁc. In this particular case we used dynamic time\\nwarping of the speed signal along each signature (Hastie et al., 1992), but\\nwill not go into details here.\\nIn the right panel we have applied a translation and rotation to the green\\npoints so as best to match the orange—a so-called Procrustes3transforma-\\ntion (Mardia et al., 1979, for example).\\nConsider the problem\\nmin\\nθ,R||X2−(X1R+1θT)||F, (14.56)\\n3Procrustes was an African bandit in Greek mythology, who str etched or squashed\\nhis visitors to ﬁt his iron bed (eventually killing them).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6361ff2d-68a8-415f-b8a3-e8c4c2df79ea', embedding=None, metadata={'page_label': '559', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='540 14. Unsupervised Learning\\nwithX1andX2bothN×pmatrices of corresponding points, Ran or-\\nthonormal p×pmatrix4, and θap-vector of location coordinates. Here\\n||X||2\\nF= trace( XTX) is the squared Frobenius matrix norm.\\nLet ¯x1and ¯x2be the column mean vectors of the matrices, and ˜X1and\\n˜X2be the versions of these matrices with the means removed. Consider\\nthe SVD ˜XT\\n1˜X2=UDVT. Then the solution to (14.56) is given by (Exer-\\ncise 14.8)\\nˆR=UVT\\nˆθ= ¯x2−ˆR¯x1,(14.57)\\nand the minimal distances is referred to as the Procrustes distance . From\\nthe form of the solution, we can center each matrix at its column centroid,\\nand then ignore location completely. Hereafter we assume this is the case.\\nTheProcrustes distance with scaling solves a slightly more general\\nproblem,\\nmin\\nβ,R||X2−βX1R||F, (14.58)\\nwhere β >0 is a positive scalar. The solution for Ris as before, with\\nˆβ= trace( D)/||X1||2\\nF.\\nRelated to Procrustes distance is the Procrustes average of a collection\\nofLshapes, which solves the problem\\nmin\\n{Rℓ}L\\n1,ML∑\\nℓ=1||XℓRℓ−M||2\\nF; (14.59)\\nthat is, ﬁnd the shape Mclosest in average squared Procrustes distance to\\nall the shapes. This is solved by a simple alternating algorithm:\\n0. Initialize M=X1(for example).\\n1. Solve the LProcrustes rotation problems with Mﬁxed, yielding\\nX′\\nℓ←XˆRℓ.\\n2. Let M←1\\nL∑L\\nℓ=1X′\\nℓ.\\nSteps 1. and 2. are repeated until the criterion (14.59) converges.\\nFigure 14.26 shows a simple example with three shapes. Note that we can\\nonly expect a solution up to a rotation; alternatively, we can impose a\\nconstraint, such as that Mbe upper-triangular, to force uniqueness. One\\ncan easily incorporate scaling in the deﬁnition (14.59); see Exercise 14.9.\\nMost generally we can deﬁne the aﬃne-invariant average of a set of\\nshapes via\\n4To simplify matters, we consider only orthogonal matrices w hich include reﬂections\\nas well as rotations [the O(p) group]; although reﬂections are unlikely here, these meth ods\\ncan be restricted further to allow only rotations [ SO(p) group].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5536a2d4-2915-4b1b-af7d-9b797f09f91d', embedding=None, metadata={'page_label': '560', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 541\\nFIGURE 14.26. The Procrustes average of three versions of the leading S in\\nSuresh’s signatures. The left panel shows the preshape average, with each of the\\nshapes X′\\nℓin preshape space superimposed. The right three panels map th e pre-\\nshapeMseparately to match each of the original S’s.\\nmin\\n{Aℓ}L\\n1,ML∑\\nℓ=1||XℓAℓ−M||2\\nF, (14.60)\\nwhere the Aℓare any p×pnonsingular matrices. Here we require a stan-\\ndardization, such as MTM=I, to avoid a trivial solution. The solution is\\nattractive, and can be computed without iteration (Exercise 14.10):\\n1. Let Hℓ=Xℓ(XT\\nℓXℓ)−1Xℓbe the rank- pprojection matrix deﬁned\\nbyXℓ.\\n2.Mis the N×p matrix formed from the plargest eigenvectors of ¯H=\\n1\\nL∑L\\nℓ=1Hℓ.\\n14.5.2 Principal Curves and Surfaces\\nPrincipal curves generalize the principal component line, providing a smooth\\none-dimensional curved approximation to a set of data points in IRp. A prin-\\ncipal surface is more general, providing a curved manifold approximation\\nof dimension 2 or more.\\nWe will ﬁrst deﬁne principal curves for random variables X∈IRp, and\\nthen move to the ﬁnite data case. Let f(λ) be a parameterized smooth\\ncurve in IRp. Hence f(λ) is a vector function with pcoordinates, each a\\nsmooth function of the single parameter λ. The parameter λcan be chosen,\\nfor example, to be arc-length along the curve from some ﬁxed origin. For\\neach data value x, letλf(x) deﬁne the closest point on the curve to x. Then\\nf(λ) is called a principal curve for the distribution of the random vector\\nXif\\nf(λ) = E( X|λf(X) =λ). (14.61)\\nThis says f(λ) is the average of all data points that project to it, that is, the\\npoints for which it is “responsible.” This is also known as a self-consistency\\nproperty. Although in practice, continuous multivariate distributes have\\ninﬁnitely many principal curves (Duchamp and Stuetzle, 1996), we are', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c391ab27-ceac-462e-b1a8-56916304d066', embedding=None, metadata={'page_label': '561', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='542 14. Unsupervised Learning\\n....\\n•••\\n•••\\n••••\\n•• ••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n•\\n••••\\n••••\\n•••\\n••••\\n••••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n•\\n••••\\n•.....\\nf(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)]\\nFIGURE 14.27. The principal curve of a set of data. Each point on the curve\\nis the average of all data points that project there.\\ninterested mainly in the smooth ones. A principal curve is illustrated in\\nFigure 14.27.\\nPrincipal points are an interesting related concept. Consider a set of k\\nprototypes and for each point xin the support of a distribution, identify\\nthe closest prototype, that is, the prototype that is responsible for it. T his\\ninduces a partition of the feature space into so-called Voronoi regions. The\\nset of kpoints that minimize the expected distance from Xto its prototype\\nare called the principal points of the distribution. Each principal point is\\nself-consistent, in that it equals the mean of Xin its Voronoi region. For\\nexample, with k= 1, the principal point of a circular normal distribution is\\nthe mean vector; with k= 2 they are a pair of points symmetrically placed\\non a ray through the mean vector. Principal points are the distributional\\nanalogs of centroids found by K-means clustering. Principal curves can be\\nviewed as k=∞principal points, but constrained to lie on a smooth curve,\\nin a similar way that a SOM constrains K-means cluster centers to fall on\\na smooth manifold.\\nTo ﬁnd a principal curve f(λ) of a distribution, we consider its coordinate\\nfunctions f(λ) = [f1(λ),f2(λ),... ,f p(λ)] and let XT= (X1,X2,... ,X p).\\nConsider the following alternating steps:\\n(a) ˆfj(λ)←E(Xj|λ(X) =λ);j= 1,2,... ,p,\\n(b) ˆλf(x)←argminλ′||x−ˆf(λ′)||2.(14.62)\\nThe ﬁrst equation ﬁxes λand enforces the self-consistency requirement\\n(14.61). The second equation ﬁxes the curve and ﬁnds the closest point on', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66f5df36-6cdb-4930-9c8c-c78bbe796d62', embedding=None, metadata={'page_label': '562', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 543\\n-0.1 0.0 0.1 0.2-0.2 -0.1 0.0 0.1 0.2••••\\n•••\\n•\\n••\\n•••\\n•••••\\n•••\\n••••••\\n••\\n• ••\\n••\\n•••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n• •••\\n•\\n•••\\n•••\\n•\\n••\\n•\\n•••••\\n••••\\n••••\\n•••\\n••\\nλ1λ2\\nFIGURE 14.28. Principal surface ﬁt to half-sphere data. (Left panel:) ﬁtted\\ntwo-dimensional surface. (Right panel:) projections of data po ints onto the sur-\\nface, resulting in coordinates ˆλ1,ˆλ2.\\nthe curve to each data point. With ﬁnite data, the principal curve algorithm\\nstarts with the linear principal component, and iterates the two steps in\\n(14.62) until convergence. A scatterplot smoother is used to estimate the\\nconditional expectations in step (a) by smoothing each Xjas a function of\\nthe arc-length ˆλ(X), and the projection in (b) is done for each of the ob-\\nserved data points. Proving convergence in general is diﬃcult, but one can\\nshow that if a linear least squares ﬁt is used for the scatterplot smoothing,\\nthen the procedure converges to the ﬁrst linear principal component, and\\nis equivalent to the power method for ﬁnding the largest eigenvector of a\\nmatrix.\\nPrincipal surfaces have exactly the same form as principal curves, but\\nare of higher dimension. The mostly commonly used is the two-dimensional\\nprincipal surface, with coordinate functions\\nf(λ1,λ2) = [f1(λ1,λ2),... ,f p(λ1,λ2)].\\nThe estimates in step (a) above are obtained from two-dimensional surface\\nsmoothers. Principal surfaces of dimension greater than two are rarely used,\\nsince the visualization aspect is less attractive, as is smoothing in high\\ndimensions.\\nFigure 14.28 shows the result of a principal surface ﬁt to the half-sphere\\ndata. Plotted are the data points as a function of the estimated nonlinear\\ncoordinates ˆλ1(xi),ˆλ2(xi). The class separation is evident.\\nPrincipal surfaces are very similar to self-organizing maps. If we use a\\nkernel surface smoother to estimate each coordinate function fj(λ1,λ2),\\nthis has the same form as the batch version of SOMs (14.48). The SOM\\nweights wkare just the weights in the kernel. There is a diﬀerence, however:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2212a49d-c2a5-40e4-a3dd-aff1fcedf19f', embedding=None, metadata={'page_label': '563', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='544 14. Unsupervised Learning\\nthe principal surface estimates a separate prototype f(λ1(xi),λ2(xi)) for\\neach data point xi, while the SOM shares a smaller number of prototypes\\nfor all data points. As a result, the SOM and principal surface will agree\\nonly as the number of SOM prototypes grows very large.\\nThere also is a conceptual diﬀerence between the two. Principal sur-\\nfaces provide a smooth parameterization of the entire manifold in terms\\nof its coordinate functions, while SOMs are discrete and produce only the\\nestimated prototypes for approximating the data. The smooth parameter-\\nization in principal surfaces preserves distance locally: in Figure 14.28 it\\nreveals that the red cluster is tighter than the green or blue clusters. In\\nsimple examples the estimates coordinate functions themselves can be in-\\nformative: see Exercise14.13.\\n14.5.3 Spectral Clustering\\nTraditional clustering methods like K-means use a spherical or elliptical\\nmetric to group data points. Hence they will not work well when the clus-\\nters are non-convex, such as the concentric circles in the top left panel of\\nFigure 14.29. Spectral clustering is a generalization of standard clustering\\nmethods, and is designed for these situations. It has close connections with\\nthe local multidimensional-scaling techniques (Section 14.9) that generalize\\nMDS.\\nThe starting point is a N×Nmatrix of pairwise similarities sii′≥0 be-\\ntween all observation pairs. We represent the observations in an undirected\\nsimilarity graph G=⟨V, E⟩. The Nvertices virepresent the observations,\\nand pairs of vertices are connected by an edge if their similarity is positive\\n(or exceeds some threshold). The edges are weighted by the sii′. Clustering\\nis now rephrased as a graph-partition problem, where we identify connected\\ncomponents with clusters. We wish to partition the graph, such that edges\\nbetween diﬀerent groups have low weight, and within a group have high\\nweight. The idea in spectral clustering is to construct similarity graphs that\\nrepresent the local neighborhood relationships between observations.\\nTo make things more concrete, consider a set of Npoints xi∈IRp, and let\\ndii′be the Euclidean distance between xiandxi′. We will use as similarity\\nmatrix the radial-kernel gram matrix; that is, sii′= exp( −d2\\nii′/c), where\\nc >0 is a scale parameter.\\nThere are many ways to deﬁne a similarity matrix and its associated\\nsimilarity graph that reﬂect local behavior. The most popular is the mutual\\nK-nearest-neighbor graph . Deﬁne NKto be the symmetric set of nearby\\npairs of points; speciﬁcally a pair ( i,i′) is in NKif point iis among the\\nK-nearest neighbors of i′, or vice-versa. Then we connect all symmetric\\nnearest neighbors, and give them edge weight wii′=sii′; otherwise the\\nedge weight is zero. Equivalently we set to zero all the pairwise similarit ies\\nnot in NK, and draw the graph for this modiﬁed similarity matrix.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e199f3f-1328-446a-acb0-dee9167fee3d', embedding=None, metadata={'page_label': '564', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 545\\nAlternatively, a fully connected graph includes all pairwise edges with\\nweights wii′=sii′, and the local behavior is controlled by the scale param-\\neterc.\\nThe matrix of edge weights W={wii′}from a similarity graph is called\\ntheadjacency matrix . The degree of vertex iisgi=∑\\ni′wii′, the sum of\\nthe weights of the edges connected to it. Let Gbe a diagonal matrix with\\ndiagonal elements gi.\\nFinally, the graph Laplacian is deﬁned by\\nL=G−W (14.63)\\nThis is called the unnormalized graph Laplacian ; a number of normalized\\nversions have been proposed—these standardize the Laplacian with respect\\nto the node degrees gi, for example, ˜L=I−G−1W.\\nSpectral clustering ﬁnds the meigenvectors ZN×mcorresponding to the\\nmsmallest eigenvalues of L(ignoring the trivial constant eigenvector).\\nUsing a standard method like K-means, we then cluster the rows of Zto\\nyield a clustering of the original data points.\\nAn example is presented in Figure 14.29. The top left panel shows 450\\nsimulated data points in three circular clusters indicated by the colors. K-\\nmeans clustering would clearly have diﬃculty identifying the outer clusters.\\nWe applied spectral clustering using a 10-nearest neighbor similarity graph,\\nand display the eigenvector corresponding to the second and third smallest\\neigenvalue of the graph Laplacian in the lower left. The 15 smallest eigen-\\nvalues are shown in the top right panel. The two eigenvectors shown have\\nidentiﬁed the three clusters, and a scatterplot of the rows of the eigenvector\\nmatrix Yin the bottom right clearly separates the clusters. A procedure\\nsuch as K-means clustering applied to these transformed points would eas-\\nily identify the three groups.\\nWhy does spectral clustering work? For any vector fwe have\\nfTLf=N∑\\ni=1gif2\\ni−N∑\\ni=1N∑\\ni′=1fifi′wii′\\n=1\\n2N∑\\ni=1N∑\\ni′=1wii′(fi−fi′)2. (14.64)\\nFormula 14.64 suggests that a small value of fTLfwill be achieved if pairs\\nof points with large adjacencies have coordinates fiandfi′close together.\\nSince1TL1= 0 for any graph, the constant vector is a trivial eigenvector\\nwith eigenvalue zero. Not so obvious is the fact that if the graph is con-\\nnected5, it is the onlyzero eigenvector (Exercise 14.21). Generalizing this\\nargument, it is easy to show that for a graph with mconnected components,\\n5A graph is connected if any two nodes can be reached via a path o f connected nodes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3af038e9-537f-46e3-9a1a-cadb0ba6c95a', embedding=None, metadata={'page_label': '565', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='546 14. Unsupervised Learning\\n−4 −2 0 2 4−4 −2 0 2 4\\nx1x2\\n0.0 0.1 0.2 0.3 0.4 0.5\\nNumberEigenvalue\\n1 3 5 10 15\\n0 100 200 300 400Eigenvectors\\nIndex2nd Smallest 3rd Smallest\\n−0.05  0.05 −0.05  0.05\\n−0.04 −0.02 0.00 0.02−0.06 −0.02 0.02 0.06\\nSecond Smallest EigenvectorThird Smallest EigenvectorSpectral Clustering\\nFIGURE 14.29. Toy example illustrating spectral clustering. Data in top left are\\n450points falling in three concentric clusters of 150points each. The points are\\nuniformly distributed in angle, with radius 1,2.8and5in the three groups, and\\nGaussian noise with standard deviation 0.25added to each point. Using a k= 10\\nnearest-neighbor similarity graph, the eigenvector correspo nding to the second and\\nthird smallest eigenvalues of Lare shown in the bottom left; the smallest eigen-\\nvector is constant. The data points are colored in the same way as in the top left.\\nThe 15 smallest eigenvalues are shown in the top right panel. Th e coordinates of\\nthe 2nd and 3rd eigenvectors (the 450rows of Z) are plotted in the bottom right\\npanel. Spectral clustering does standard (e.g., K-means) clustering of these points\\nand will easily recover the three original clusters.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7273a50a-879e-4069-a8b3-a4d07160cfad', embedding=None, metadata={'page_label': '566', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 547\\nthe nodes can be reordered so that Lis block diagonal with a block for each\\nconnected component. Then Lhasmeigenvectors of eigenvalue zero, and\\nthe eigenspace of eigenvalue zero is spanned by the indicator vectors of the\\nconnected components. In practice one has strong and weak connections,\\nso zero eigenvalues are approximated by small eigenvalues.\\nSpectral clustering is an interesting approach for ﬁnding non-convex clus-\\nters. When a normalized graph Laplacian is used, there is another way to\\nview this method. Deﬁning P=G−1W, we consider a random walk on\\nthe graph with transition probability matrix P. Then spectral clustering\\nyields groups of nodes such that the random walk seldom transitions from\\none group to another.\\nThere are a number of issues that one must deal with in applying spec-\\ntral clustering in practice. We must choose the type of similarity graph—eg.\\nfully connected or nearest neighbors, and associated parameters such as the\\nnumber of nearest of neighbors kor the scale parameter of the kernel c. We\\nmust also choose the number of eigenvectors to extract from Land ﬁnally,\\nas with all clustering methods, the number of clusters. In the toy example\\nof Figure 14.29 we obtained good results for k∈[5,200], the value 200 cor-\\nresponding to a fully connected graph. With k <5 the results deteriorated.\\nLooking at the top-right panel of Figure 14.29, we see no strong separation\\nbetween the smallest three eigenvalues and the rest. Hence it is not clear\\nhow many eigenvectors to select.\\n14.5.4 Kernel Principal Components\\nSpectral clustering is related to kernel principal components , a non-linear\\nversion of linear principal components. Standard linear principal compo-\\nnents (PCA) are obtained from the eigenvectors of the covariance matrix,\\nand give directions in which the data have maximal variance. Kernel PCA\\n(Sch¨ olkopf et al., 1999) expand the scope of PCA, mimicking what we would\\nobtain if we were to expand the features by non-linear transformations, and\\nthen apply PCA in this transformed feature space.\\nWe show in Section 18.5.2 that the principal components variables Zof\\na data matrix Xcan be computed from the inner-product (gram) matrix\\nK=XXT. In detail, we compute the eigen-decomposition of the double-\\ncentered version of the gram matrix\\n˜K= (I−M)K(I−M) =UD2UT, (14.65)\\nwithM=11T/N, and then Z=UD. Exercise 18.15 shows how to com-\\npute the projections of new observations in this space.\\nKernel PCA simply mimics this procedure, interpreting the kernel ma-\\ntrixK={K(xi,xi′)}as an inner-product matrix of the implicit fea-\\ntures⟨φ(xi),φ(xi′)⟩and ﬁnding its eigenvectors. The elements of the mth\\ncomponent zm(mth column of Z) can be written (up to centering) as\\nzim=∑N\\nj=1αjmK(xi,xj), where αjm=ujm/dm(Exercise 14.16).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='87749864-6a26-4da4-b596-f4137fe2a066', embedding=None, metadata={'page_label': '567', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='548 14. Unsupervised Learning\\nWe can gain more insight into kernel PCA by viewing the zmas sam-\\nple evaluations of principal component functions gm∈ H K, with HKthe\\nreproducing kernel Hilbert space generated by K(see Section 5.8.1). The\\nﬁrst principal component function g1solves\\nmax\\ng1∈HKVarTg1(X) subject to ||g1||HK= 1 (14.66)\\nHere Var Trefers to the sample variance over training data T. The norm\\nconstraint ||g1||HK= 1 controls the size and roughness of the function g1,\\nas dictated by the kernel K. As in the regression case it can be shown that\\nthe solution to (14.66) is ﬁnite dimensional with representation g1(x) =∑N\\nj=1cjK(x,xj). Exercise 14.17 shows that the solution is deﬁned by ˆ cj=\\nαj1, j= 1,... ,N above. The second principal component function is de-\\nﬁned in a similar way, with the additional constraint that ⟨g1,g2⟩HK= 0,\\nand so on.6\\nSch¨ olkopf et al. (1999) demonstrate the use of kernel principal compo-\\nnents as features for handwritten-digit classiﬁcation, and show that they\\ncan improve the performance of a classiﬁer when these are used instead of\\nlinear principal components.\\nNote that if we use the radial kernel\\nK(x,x′) = exp( −∥x−x′∥2/c), (14.67)\\nthen the kernel matrix Khas the same form as the similarity matrix Sin\\nspectral clustering. The matrix of edge weights Wis a localized version of\\nK, setting to zero all similarities for pairs of points that are not nearest\\nneighbors.\\nKernel PCA ﬁnds the eigenvectors corresponding to the largest eigenval-\\nues of ˜K; this is equivalent to ﬁnding the eigenvectors corresponding to the\\nsmallest eigenvalues of\\nI−˜K. (14.68)\\nThis is almost the same as the Laplacian (14.63), the diﬀerences being the\\ncentering of ˜Kand the fact that Ghas the degrees of the nodes along the\\ndiagonal.\\nFigure 14.30 examines the performance of kernel principal components\\nin the toy example of Figure 14.29. In the upper left panel we used the ra-\\ndial kernel with c= 2, the same value that was used in spectral clustering.\\nThis does not separate the groups, but with c= 10 (upper right panel), the\\nﬁrst component separates the groups well. In the lower-left panel we ap-\\nplied kernel PCA using the nearest-neighbor radial kernel Wfrom spectral\\nclustering. In the lower right panel we use the kernel matrix itself as the\\n6This section beneﬁted from helpful discussions with Jonath an Taylor.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='96040bae-39cf-4c9f-99cc-06b6c3280531', embedding=None, metadata={'page_label': '568', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 549\\n−0.10 −0.06 −0.02 0.02−0.10 −0.05 0.00 0.05 0.10\\nFirst Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=2)\\n−0.06 −0.02 0.02 0.06−0.05 0.00 0.05\\nFirst Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=10)\\n0.00 0.05 0.10 0.15−0.2 −0.1 0.0 0.1 0.2\\nFirst Largest EigenvectorSecond Largest EigenvectorNN Radial Kernel (c=2)\\n−0.05 0.00 0.05 0.10 0.15−0.10 0.00 0.05 0.10 0.15\\nSecond Smallest EigenvectorThird Smallest EigenvectorRadial Kernel Laplacian (c=2)\\nFIGURE 14.30. Kernel principal components applied to the toy example of Fig-\\nure 14.29, using diﬀerent kernels. (Top left:) Radial kernel (14 .67) with c= 2.\\n(Top right:) Radial kernel with c= 10. (Bottom left): Nearest neighbor radial ker-\\nnelWfrom spectral clustering. (Bottom right:) Spectral clusteri ng with Laplacian\\nconstructed from the radial kernel.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f53d7177-00c7-4962-9016-4b5a3bb64112', embedding=None, metadata={'page_label': '569', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='550 14. Unsupervised Learning\\nsimilarity matrix for constructing the Laplacian (14.63) in spectral cl uster-\\ning. In neither case do the projections separate the two groups. Adjusting\\ncdid not help either.\\nIn this toy example, we see that kernel PCA is quite sensitive to the scale\\nand nature of the kernel. We also see that the nearest-neighbor truncation\\nof the kernel is important for the success of spectral clustering.\\n14.5.5 Sparse Principal Components\\nWe often interpret principal components by examining the direction vectors\\nvj, also known as loadings , to see which variables play a role. We did this\\nwith the image loadings in (14.55). Often this interpretation is made eas ier\\nif the loadings are sparse. In this section we brieﬂy discuss some methods\\nfor deriving principal components with sparse loadings. They are all based\\non lasso ( L1) penalties.\\nWe start with an N×pdata matrix X, with centered columns. The\\nproposed methods focus on either the maximum-variance property of prin-\\ncipal components, or the minimum reconstruction error. The SCoTLASS\\nprocedure of Joliﬀe et al. (2003) takes the ﬁrst approach, by solving\\nmaxvT(XTX)v,subject to∑p\\nj=1|vj| ≤t,vTv= 1. (14.69)\\nThe absolute-value constraint encourages some of the loadings to be zero\\nand hence vto be sparse. Further sparse principal components are found\\nin the same way, by forcing the kth component to be orthogonal to the\\nﬁrstk−1 components. Unfortunately this problem is not convex and the\\ncomputations are diﬃcult.\\nZou et al. (2006) start instead with the regression/reconstruction prop-\\nerty of PCA, similar to the approach in Section 14.5.1. Let xibe the ith row\\nofX. For a single component, their sparse principal component technique\\nsolves\\nmin\\nθ,vN∑\\ni=1||xi−θvTxi||2\\n2+λ||v||2\\n2+λ1||v||1 (14.70)\\nsubject to ||θ||2= 1.\\nLets examine this formulation in more detail.\\n•If both λandλ1are zero and N > p , it is easy to show that v=θ\\nand is the largest principal component direction.\\n•When p≫Nthe solution is not necessarily unique unless λ >0. For\\nanyλ >0 and λ1= 0 the solution for vis proportional to the largest\\nprincipal component direction.\\n•The second penalty on vencourages sparseness of the loadings.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b36e497a-1460-4b7a-8ed1-22bf9f9135d9', embedding=None, metadata={'page_label': '570', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 Principal Components, Curves and Surfaces 551\\nWalking Speed\\nVerbal Fluency\\nPrincipal Components Sparse Principal Components\\nFIGURE 14.31. Standard and sparse principal components from a study of\\nthe corpus callosum variation. The shape variations correspo nding to signiﬁcant\\nprincipal components (red curves) are overlaid on the mean CC sh ape (black\\ncurves).\\nFor multiple components, the sparse principal components procedures\\nminimizes\\nN∑\\ni=1||xi−ΘVTxi||2+λK∑\\nk=1||vk||2\\n2+K∑\\nk=1λ1k||vk||1, (14.71)\\nsubject to ΘTΘ=IK. Here Vis ap×Kmatrix with columns vkandΘ\\nis also p×K.\\nCriterion (14.71) is not jointly convex in VandΘ, but it is convex in\\neach parameter with the other parameter ﬁxed7. Minimization over Vwith\\nΘﬁxed is equivalent to Kelastic net problems (Section 18.4) and can be\\ndone eﬃciently. On the other hand, minimization over ΘwithVﬁxed is a\\nversion of the Procrustes problem (14.56), and is solved by a simple SVD\\ncalculation (Exercise 14.12). These steps are alternated until convergence.\\nFigure 14.31 shows an example of sparse principal components analysis\\nusing (14.71), taken from Sj¨ ostrand et al. (2007). Here the shape of the\\nmid-sagittal cross-section of the corpus callosum (CC) is related to various\\nclinical parameters in a study involving 569 elderly persons8. In this exam-\\n7Note that the usual principal component criterion, for exam ple (14.50), is not jointly\\nconvex in the parameters either. Nevertheless, the solutio n is well deﬁned and an eﬃcient\\nalgorithm is available.\\n8We thank Rasmus Larsen and Karl Sj¨ ostrand for suggesting th is application, and\\nsupplying us with the postscript ﬁgures reproduced here.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='836da7e4-ac06-4ae3-9ecc-780e29202205', embedding=None, metadata={'page_label': '571', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='552 14. Unsupervised Learning\\nFIGURE 14.32. An example of a mid-saggital brain slice, with the corpus col-\\nlosum annotated with landmarks.\\nple PCA is applied to shape data, and is a popular tool in morphometrics.\\nFor such applications, a number of landmarks are identiﬁed along the cir-\\ncumference of the shape; an example is given in Figure 14.32. These are\\naligned by Procrustes analysis to allow for rotations, and in this case s cal-\\ning as well (see Section 14.5.1). The features used for PCA are the sequence\\nof coordinate pairs for each landmark, unpacked into a single vector.\\nIn this analysis, both standard and sparse principal components were\\ncomputed, and components that were signiﬁcantly associated with various\\nclinical parameters were identiﬁed. In the ﬁgure, the shape variations cor-\\nresponding to signiﬁcant principal components (red curves) are overlaid on\\nthe mean CC shape (black curves). Low walking speed relates to CCs that\\nare thinner (displaying atrophy) in regions connecting the motor control\\nand cognitive centers of the brain. Low verbal ﬂuency relates to CCs that\\nare thinner in regions connecting auditory/visual/cognitive centers. The\\nsparse principal components procedure gives a more parsimonious, and po-\\ntentially more informative picture of the important diﬀerences.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bb8861cd-2c97-4e22-b247-fd01204bdeaa', embedding=None, metadata={'page_label': '572', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.6 Non-negative Matrix Factorization 553\\n14.6 Non-negative Matrix Factorization\\nNon-negative matrix factorization (Lee and Seung, 1999) is a recent al-\\nternative approach to principal components analysis, in which the data\\nand components are assumed to be non-negative. It is useful for modeling\\nnon-negative data such as images.\\nTheN×pdata matrix Xis approximated by\\nX≈WH (14.72)\\nwhere WisN×randHisr×p,r≤max(N,p). We assume that\\nxij,wik,hkj≥0.\\nThe matrices WandHare found by maximizing\\nL(W,H) =N∑\\ni=1p∑\\nj=1[xijlog(WH)ij−(WH)ij]. (14.73)\\nThis is the log-likelihood from a model in which xijhas a Poisson dis-\\ntribution with mean ( WH)ij—quite reasonable for positive data.\\nThe following alternating algorithm (Lee and Seung, 2001) converges to\\na local maximum of L(W,H):\\nwik←wik∑p\\nj=1hkjxij/(WH)ij∑p\\nj=1hkj\\nhkj←hkj∑N\\ni=1wikxij/(WH)ij∑N\\ni=1wik(14.74)\\nThis algorithm can be derived as a minorization procedure for maximizing\\nL(W,H) (Exercise 14.23) and is also related to the iterative-proportional-\\nscaling algorithm for log-linear models (Exercise 14.24).\\nFigure 14.33 shows an example taken from Lee and Seung (1999)9, com-\\nparing non-negative matrix factorization (NMF), vector quantization (VQ,\\nequivalent to k-means clustering) and principal components analysis (PCA).\\nThe three learning methods were applied to a database of N= 2,429 fa-\\ncial images, each consisting of 19 ×19 pixels, resulting in a 2 ,429×381\\nmatrix X. As shown in the 7 ×7 array of montages (each a 19 ×19 image),\\neach method has learned a set of r= 49 basis images. Positive values are\\nillustrated with black pixels and negative values with red pixels. A par-\\nticular instance of a face, shown at top right, is approximated by a linear\\nsuperposition of basis images. The coeﬃcients of the linear superposition\\nare shown next to each montage, in a 7 ×7 array10, and the resulting su-\\nperpositions are shown to the right of the equality sign. The authors poin t\\n9We thank Sebastian Seung for providing this image.\\n10These 7 ×7 arrangements allow for a compact display, and have no struc tural\\nsigniﬁcance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4c7d788-c2f1-43f6-9c55-03fba1e943c0', embedding=None, metadata={'page_label': '573', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='554 14. Unsupervised Learning\\nout that unlike VQ and PCA, NMF learns to represent faces with a set of\\nbasis images resembling parts of faces.\\nDonoho and Stodden (2004) point out a potentially serious problem with\\nnon-negative matrix factorization. Even in situations where X=WHholds\\nexactly, the decomposition may not be unique. Figure 14.34 illustrates the\\nproblem. The data points lie in p= 2 dimensions, and there is “open space”\\nbetween the data and the coordinate axes. We can choose the basis vectors\\nh1andh2anywhere in this open space, and represent each data point\\nexactly with a nonnegative linear combination of these vectors. This non-\\nuniqueness means that the solution found by the above algorithm depends\\non the starting values, and it would seem to hamper the interpretability of\\nthe factorization. Despite this interpretational drawback, the non-negative\\nmatrix factorization and its applications has attracted a lot of interest .\\n14.6.1 Archetypal Analysis\\nThis method, due to Cutler and Breiman (1994), approximates data points\\nby prototypes that are themselves linear combinations of data points. In\\nthis sense it has a similar ﬂavor to K-means clustering. However, rather\\nthan approximating each data point by a single nearby prototype, archety-\\npal analysis approximates each data point by a convex combination of a\\ncollection of prototypes. The use of a convex combination forces the proto-\\ntypes to lie on the convex hull of the data cloud. In this sense, the prototypes\\nare “pure,”, or “archetypal.”\\nAs in (14.72), the N×pdata matrix Xis modeled as\\nX≈WH (14.75)\\nwhereWisN×randHisr×p. We assume that wik≥0 and∑r\\nk=1wik=\\n1∀i. Hence the Ndata points (rows of X) inp-dimensional space are\\nrepresented by convex combinations of the rarchetypes (rows of H). We\\nalso assume that\\nH=BX (14.76)\\nwhere Bisr×Nwithbki≥0 and∑N\\ni=1bki= 1∀k. Thus the archetypes\\nthemselves are convex combinations of the data points. Using both (14.75)\\nand (14.76) we minimize\\nJ(W,B) = ||X−WH||2\\n=||X−WBX ||2(14.77)\\nover the weights WandB. This function is minimized in an alternating\\nfashion, with each separate minimization involving a convex optimizatio n.\\nThe overall problem is not convex however, and so the algorithm converges\\nto a local minimum of the criterion.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7dcb6ddf-d509-44ff-9983-86758d233f55', embedding=None, metadata={'page_label': '574', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.6 Non-negative Matrix Factorization 555\\nVQ\\n×\\r =NMF\\n= ×\\r\\nPCA\\n= ×\\r\\nOriginal\\nFIGURE 14.33. Non-negative matrix factorization (NMF), vector quantizatio n\\n(VQ, equivalent to k-means clustering) and principal components analysis (PCA)\\napplied to a database of facial images. Details are given in t he text. Unlike VQ\\nand PCA, NMF learns to represent faces with a set of basis images r esembling\\nparts of faces.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b51f2d2-48d4-4ed0-a796-df8a0251cfe3', embedding=None, metadata={'page_label': '575', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='556 14. Unsupervised Learning\\nh1\\nh2\\nFIGURE 14.34. Non-uniqueness of the non-negative matrix factorization.\\nThere are 11 data points in two dimensions. Any choice of the basis vectors h1\\nandh2in the open space between the coordinate axes and data, gives an e xact\\nreconstruction of the data.\\nFigure 14.35 shows an example with simulated data in two dimensions.\\nThe top panel displays the results of archetypal analysis, while the bottom\\npanel shows the results from K-means clustering. In order to best recon-\\nstruct the data from convex combinations of the prototypes, it pays to\\nlocate the prototypes on the convex hull of the data. This is seen in the top\\npanels of Figure 14.35 and is the case in general, as proven by Cutler and\\nBreiman (1994). K-means clustering, shown in the bottom panels, chooses\\nprototypes in the middle of the data cloud.\\nWe can think of K-means clustering as a special case of the archetypal\\nmodel, in which each row of Whas a single one and the rest of the entries\\nare zero.\\nNotice also that the archetypal model (14.75) has the same general form\\nas the non-negative matrix factorization model (14.72). However, the two\\nmodels are applied in diﬀerent settings, and have somewhat diﬀerent goals.\\nNon-negative matrix factorization aims to approximate the columns of the\\ndata matrix X, and the main output of interest are the columns of W\\nrepresenting the primary non-negative components in the data. Archetypal\\nanalysis focuses instead on the approximation of the rows of Xusing the\\nrows of H, which represent the archetypal data points. Non-negative matrix\\nfactorization also assumes that r≤p. With r=p, we can get an exact\\nreconstruction simply choosing Wto be the data Xwith columns scaled\\nso that they sum to 1. In contrast, archetypal analysis requires r≤N,\\nbut allows r > p . In Figure 14.35, for example, p= 2,N= 50 while\\nr= 2,4 or 8. The additional constraint (14.76) implies that the archetypal\\napproximation will not be perfect, even if r > p.\\nFigure 14.36 shows the results of archetypal analysis applied to the\\ndatabase of 3’s displayed in Figure 14.22. The three rows in Figure 14.36\\nare the resulting archetypes from three runs, specifying two, three and four', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d2b89809-8428-42c2-9769-62dbb6687319', embedding=None, metadata={'page_label': '576', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.7 Independent Component Analysisand Exploratory Projection Pursu it 557\\n2 Prototypes 4 Prototypes 8 Prototypes\\nFIGURE 14.35. Archetypal analysis (top panels) and K-means clustering (bot-\\ntom panels) applied to 50data points drawn from a bivariate Gaussian distribu-\\ntion. The colored points show the positions of the prototypes in each case.\\narchetypes, respectively. As expected, the algorithm has produced extreme\\n3’s both in size and shape.\\n14.7 Independent Component Analysis and\\nExploratory Projection Pursuit\\nMultivariate data are often viewed as multiple indirect measurements aris-\\ning from an underlying source, which typically cannot be directly measured.\\nExamples include the following:\\n•Educational and psychological tests use the answers to questionnaires\\nto measure the underlying intelligence and other mental abilities of\\nsubjects.\\n•EEG brain scans measure the neuronal activity in various parts of\\nthe brain indirectly via electromagnetic signals recorded at sensors\\nplaced at various positions on the head.\\n•The trading prices of stocks change constantly over time, and reﬂect\\nvarious unmeasured factors such as market conﬁdence, external in-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13047111-a545-4af8-97d7-242f0d791645', embedding=None, metadata={'page_label': '577', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='558 14. Unsupervised Learning\\nFIGURE 14.36. Archetypal analysis applied to the database of digitized 3’s . The\\nrows in the ﬁgure show the resulting archetypes from three runs, specifying two,\\nthree and four archetypes, respectively.\\nﬂuences, and other driving forces that may be hard to identify or\\nmeasure.\\nFactor analysis is a classical technique developed in the statistical liter-\\nature that aims to identify these latent sources. Factor analysis models\\nare typically wed to Gaussian distributions, which has to some extent hin-\\ndered their usefulness. More recently, independent component analysis has\\nemerged as a strong competitor to factor analysis, and as we will see, relies\\non the non-Gaussian nature of the underlying sources for its success.\\n14.7.1 Latent Variables and Factor Analysis\\nThe singular-value decomposition X=UDVT(14.54) has a latent variable\\nrepresentation. Writing S=√\\nNUandAT=DVT/√\\nN, we have X=\\nSAT, and hence each of the columns of Xis a linear combination of the\\ncolumns of S. Now since Uis orthogonal, and assuming as before that the\\ncolumns of X(and hence U) each have mean zero, this implies that the\\ncolumns of Shave zero mean, are uncorrelated and have unit variance. In\\nterms of random variables, we can interpret the SVD, or the corresponding\\nprincipal component analysis (PCA) as an estimate of a latent variable\\nmodel', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1aa9f978-4fe1-4843-80a9-f6e0ea36ed65', embedding=None, metadata={'page_label': '578', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.7 Independent Component Analysis and Exploratory Projection Pursu it 559\\nX1=a11S1+a12S2+≤≤≤+a1pSp\\nX2=a21S1+s22S2+≤≤≤+a2pSp\\n......\\nXp=ap1S1+sp2S2+≤≤≤+appSp,(14.78)\\nor simply X=AS. The correlated Xjare each represented as a linear\\nexpansion in the uncorrelated, unit variance variables Sℓ. This is not too\\nsatisfactory, though, because given any orthogonal p×pmatrix R, we can\\nwrite\\nX=AS\\n=ARTRS\\n=A∗S∗, (14.79)\\nand Cov( S∗) =RCov(S)RT=I. Hence there are many such decom-\\npositions, and it is therefore impossible to identify any particular lat ent\\nvariables as unique underlying sources. The SVD decomposition does have\\nthe property that any rank q < p truncated decomposition approximates\\nXin an optimal way.\\nThe classical factor analysis model, developed primarily by researchers in\\npsychometrics, alleviates these problems to some extent; see, for example,\\nMardia et al. (1979). With q < p, a factor analysis model has the form\\nX1=a11S1+≤≤≤+a1qSq+ε1\\nX2=a21S1+≤≤≤+a2qSq+ε2\\n......\\nXp=ap1S1+≤≤≤+apqSq+εp,(14.80)\\norX=AS+ε. Here Sis a vector of q < p underlying latent variables or\\nfactors, Ais ap×qmatrix of factor loadings , and the εjare uncorrelated\\nzero-mean disturbances. The idea is that the latent variables Sℓare com-\\nmon sources of variation amongst the Xj, and account for their correlation\\nstructure, while the uncorrelated εjare unique to each Xjand pick up the\\nremaining unaccounted variation. Typically the Sjand the εjare modeled\\nas Gaussian random variables, and the model is ﬁt by maximum likelihood.\\nThe parameters all reside in the covariance matrix\\nΣ=AAT+Dε, (14.81)\\nwhere Dε= diag[Var( ε1),... ,Var(εp)]. The Sjbeing Gaussian and un-\\ncorrelated makes them statistically independent random variables. Thus a\\nbattery of educational test scores would be thought to be driven by the\\nindependent underlying factors such as intelligence ,drive and so on. The\\ncolumns of Aare referred to as the factor loadings , and are used to name\\nand interpret the factors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1f8bc34-ce1f-4574-9ccc-57a3428e8ce5', embedding=None, metadata={'page_label': '579', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='560 14. Unsupervised Learning\\nUnfortunately the identiﬁability issue (14.79) remains, since AandART\\nare equivalent in (14.81) for any q×qorthogonal R. This leaves a certain\\nsubjectivity in the use of factor analysis, since the user can search for ro-\\ntated versions of the factors that are more easily interpretable. This aspect\\nhas left many analysts skeptical of factor analysis, and may account for it s\\nlack of popularity in contemporary statistics. Although we will not g o into\\ndetails here, the SVD plays a key role in the estimation of (14.81). For ex-\\nample, if the Var( εj) are all assumed to be equal, the leading qcomponents\\nof the SVD identify the subspace determined by A.\\nBecause of the separate disturbances εjfor each Xj, factor analysis can\\nbe seen to be modeling the correlation structure of the Xjrather than the\\ncovariance structure. This can be easily seen by standardizing the covari-\\nance structure in (14.81) (Exercise 14.14). This is an important disti nction\\nbetween factor analysis and PCA, although not central to the discussion\\nhere. Exercise 14.15 discusses a simple example where the solutions from\\nfactor analysis and PCA diﬀer dramatically because of this distinction.\\n14.7.2 Independent Component Analysis\\nThe independent component analysis (ICA) model has exactly the same\\nform as (14.78), except the Siare assumed to be statistically indepen-\\ndentrather than uncorrelated. Intuitively, lack of correlation determines\\nthe second-degree cross-moments (covariances) of a multivariate distribu-\\ntion, while in general statistical independence determines all of the cross-\\nmoments. These extra moment conditions allow us to identify the elements\\nofAuniquely. Since the multivariate Gaussian distribution is determined\\nby its second moments alone, it is the exception, and any Gaussian inde-\\npendent components can be determined only up to a rotation, as before.\\nHence identiﬁability problems in (14.78) and (14.80) can be avoided if we\\nassume that the Siare independent and non-Gaussian .\\nHere we will discuss the full p-component model as in (14.78), where the\\nSℓare independent with unit variance; ICA versions of the factor analysis\\nmodel (14.80) exist as well. Our treatment is based on the survey article\\nby Hyv¨ arinen and Oja (2000).\\nWe wish to recover the mixing matrix AinX=AS. Without loss\\nof generality, we can assume that Xhas already been whitened to have\\nCov(X) =I; this is typically achieved via the SVD described above. This\\nin turn implies that Ais orthogonal, since Salso has covariance I. So\\nsolving the ICA problem amounts to ﬁnding an orthogonal Asuch that\\nthe components of the vector random variable S=ATXare independent\\n(and non-Gaussian).\\nFigure 14.37 shows the power of ICA in separating two mixed signals.\\nThis is an example of the classical cocktail party problem , where diﬀer-\\nent microphones Xjpick up mixtures of diﬀerent independent sources Sℓ\\n(music, speech from diﬀerent speakers, etc.). ICA is able to perform blind', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e465ea51-5ea5-41e7-ac92-a9444f5d2035', embedding=None, metadata={'page_label': '580', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.7 Independent Component Analysis and Exploratory Projection Pursu it 561\\nSource Signals Measured Signals\\nPCA Solution ICA Solution\\nFIGURE 14.37. Illustration of ICA vs. PCA on artiﬁcial time-series data. Th e\\nupper left panel shows the two source signals, measured at 1000uniformly spaced\\ntime points. The upper right panel shows the observed mixed signa ls. The lower\\ntwo panels show the principal components and independent component sol utions.\\nsource separation , by exploiting the independence and non-Gaussianity of\\nthe original sources.\\nMany of the popular approaches to ICA are based on entropy. The dif-\\nferential entropy Hof a random variable Ywith density g(y) is given by\\nH(Y) =−∫\\ng(y)logg(y)dy. (14.82)\\nA well-known result in information theory says that among all random\\nvariables with equal variance, Gaussian variables have the maximum en-\\ntropy. Finally, the mutual information I(Y) between the components of the\\nrandom vector Yis a natural measure of dependence:\\nI(Y) =p∑\\nj=1H(Yj)−H(Y). (14.83)\\nThe quantity I(Y) is called the Kullback–Leibler distance between the\\ndensity g(y) ofYand its independence version∏p\\nj=1gj(yj), where gj(yj)\\nis the marginal density of Yj. Now if Xhas covariance I, and Y=ATX\\nwithAorthogonal, then it is easy to show that\\nI(Y) =p∑\\nj=1H(Yj)−H(X)−log|detA| (14.84)\\n=p∑\\nj=1H(Yj)−H(X). (14.85)\\nFinding an Ato minimize I(Y) =I(ATX) looks for the orthogonal trans-\\nformation that leads to the most independence between its components. In', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9fa87ca3-32a1-429a-96d1-d7baa3b7cc3b', embedding=None, metadata={'page_label': '581', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='562 14. Unsupervised Learning\\n*\\n**\\n**\\n**\\n**\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n*\\n*\\n**\\n** ***\\n**\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n*****\\n**\\n**\\n*\\n**\\n**\\n*\\n**\\n*\\n***\\n*****\\n**\\n* **\\n*\\n***\\n* **\\n***\\n** **\\n**\\n****\\n*\\n******\\n***\\n*\\n**\\n*\\n* ***\\n***\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n** *\\n**\\n***\\n****\\n** *\\n**\\n*\\n**\\n**\\n*\\n**\\n**\\n*\\n* **\\n***\\n***\\n******\\n***\\n**\\n**\\n******\\n*\\n*****\\n***\\n**\\n*\\n*****\\n**\\n***\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n*\\n**\\n****\\n***\\n*\\n*\\n**\\n***\\n**\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n*\\n**\\n**\\n*\\n*\\n*\\n****\\n**\\n***\\n**\\n**\\n***\\n*\\n***\\n****\\n**\\n**\\n** *\\n***\\n**\\n* ***\\n***\\n***\\n*\\n**\\n**\\n****\\n**\\n*\\n*\\n***\\n*\\n***\\n**\\n**\\n**\\n***\\n* **\\n**\\n**\\n*\\n***\\n*\\n*\\n***\\n*\\n***\\n**\\n****\\n*\\n***\\n*\\n**\\n*\\n**\\n****\\n**\\n*\\n***\\n*****\\n**\\n**\\n** **\\n**\\n***\\n*** **\\n***\\n***\\n***\\n***\\n**\\n**\\n**\\n*\\n** **\\n*\\n**\\n**\\n**\\n**\\n*\\n**\\n*\\n***\\n***\\n***\\n**Source S\\n**\\n*\\n**\\n**\\n****\\n**\\n*\\n*****\\n****\\n*\\n**\\n***\\n***\\n**\\n***\\n*\\n**\\n*\\n**\\n****\\n*\\n**\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n***\\n****\\n****\\n*\\n**\\n*\\n**\\n*****\\n*\\n*\\n****\\n***\\n*\\n*\\n***\\n**\\n****\\n**\\n***\\n***\\n*\\n*\\n***\\n*\\n**\\n*\\n**\\n**\\n***\\n***\\n***\\n****\\n****\\n**\\n***\\n**\\n**\\n**\\n***\\n***\\n****\\n***\\n**\\n**\\n* **\\n*\\n****\\n***\\n***\\n*\\n***\\n*\\n**\\n***\\n***\\n**\\n**\\n**\\n****\\n**\\n*\\n****\\n*\\n*\\n**\\n**\\n**\\n***\\n*\\n***\\n**\\n**\\n****\\n*\\n*\\n***\\n****\\n*\\n***\\n****\\n*\\n***\\n**\\n**\\n*\\n**\\n*\\n***\\n**\\n**\\n*****\\n*\\n***\\n*\\n****\\n******\\n*\\n**\\n**\\n**\\n*\\n*\\n**\\n****\\n***\\n**\\n****\\n***\\n**\\n**\\n****\\n*\\n***\\n***\\n***\\n**\\n*\\n***\\n***\\n***\\n*\\n***\\n****\\n*\\n***\\n**\\n******\\n*\\n*****\\n**\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n***\\n***\\n*\\n**\\n**\\n**\\n***\\n*\\n******\\n****\\n*\\n**\\n***\\n****\\n*\\n**\\n**\\n***\\n* *\\n**\\n***\\n****\\n******\\n**\\n***\\n*\\n**\\n**\\n***\\n****\\n*\\n**\\n**\\n**\\n**\\n***\\n****\\n*\\n**\\n*\\n**\\n**Data X\\n*\\n***\\n***\\n*\\n*\\n***\\n***\\n**\\n**\\n***\\n**\\n*** ***\\n*\\n**\\n* **\\n***\\n*\\n***\\n**\\n***\\n****\\n**\\n**\\n***\\n***\\n**\\n***\\n*\\n****\\n****\\n*\\n** *\\n***\\n**\\n**\\n*****\\n*\\n***\\n***\\n***\\n***\\n***\\n**\\n**\\n*\\n*\\n*****\\n*\\n*\\n****\\n**\\n*\\n*\\n**\\n*\\n**\\n****\\n***\\n***\\n*\\n***\\n*\\n**\\n**\\n****\\n**\\n**\\n**\\n**\\n*** **\\n*\\n***\\n**\\n***\\n**\\n***\\n*\\n**\\n*\\n**\\n* *\\n*\\n**\\n**\\n**\\n**\\n**\\n***\\n***\\n****\\n****\\n*\\n***\\n***\\n**\\n**\\n*\\n**\\n*\\n***\\n**\\n**\\n*\\n*\\n***\\n*\\n** *\\n***\\n***\\n**\\n**\\n*\\n***\\n**\\n**\\n*\\n**\\n**\\n* *\\n**\\n** *\\n***\\n**\\n**\\n*\\n***\\n**\\n*\\n****\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*******\\n** *\\n**\\n***\\n***\\n***\\n**\\n**\\n**\\n*\\n* **\\n**\\n***\\n*\\n***\\n*\\n*******\\n**\\n**\\n**\\n**\\n**\\n*\\n*\\n** *\\n*\\n*****\\n**\\n**\\n*\\n**\\n***\\n**\\n***\\n**\\n**\\n** ***\\n****\\n*\\n****\\n*\\n*\\n***\\n**\\n*\\n****\\n*\\n***\\n**\\n***\\n****\\n*\\n*\\n*\\n***\\n**\\n*****\\n***\\n*****\\n*\\n***\\n**\\n*\\n**\\n*\\n*****\\n**\\n*\\n*****\\n**\\n*\\n*\\n***\\n***\\n***\\n***\\n***\\n**PCA Solution\\n*\\n**\\n**\\n**\\n**\\n**\\n* *\\n*\\n*\\n**\\n**\\n**\\n*\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n*\\n*\\n**\\n*****\\n**\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n*** **\\n**\\n**\\n*\\n**\\n**\\n*\\n**\\n*\\n***\\n**** *\\n**\\n*\\n**\\n*\\n***\\n* **\\n***\\n** **\\n**\\n****\\n*\\n** ****\\n***\\n*\\n**\\n*\\n** **\\n***\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*\\n*\\n***\\n** *\\n**\\n***\\n****\\n***\\n**\\n*\\n**\\n**\\n*\\n**\\n**\\n*\\n* **\\n***\\n***\\n*** ***\\n***\\n**\\n**\\n******\\n*\\n** ***\\n***\\n***\\n*****\\n**\\n***\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n*\\n**\\n* ***\\n***\\n*\\n*\\n**\\n***\\n**\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n*\\n**\\n**\\n*\\n*\\n*\\n****\\n**\\n***\\n**\\n**\\n***\\n*\\n***\\n****\\n**\\n**\\n** *\\n***\\n* *\\n****\\n***\\n***\\n*\\n**\\n**\\n*** *\\n**\\n*\\n*\\n***\\n*\\n***\\n**\\n**\\n**\\n***\\n* **\\n**\\n**\\n*\\n***\\n*\\n* ***\\n*\\n*****\\n****\\n*\\n***\\n*\\n**\\n*\\n**\\n****\\n**\\n*\\n***\\n*\\n****\\n**\\n**\\n****\\n**\\n***\\n* ** **\\n***\\n***\\n** *\\n*\\n**\\n**\\n**\\n**\\n*\\n****\\n*\\n**\\n**\\n**\\n**\\n*\\n**\\n*\\n***\\n***\\n***\\n**ICA Solution\\nFIGURE 14.38. Mixtures of independent uniform random variables. The upper\\nleft panel shows 500realizations from the two independent uniform sources, the\\nupper right panel their mixed versions. The lower two panels show the PCA and\\nICA solutions, respectively.\\nlight of (14.84) this is equivalent to minimizing the sum of the entropies of\\nthe separate components of Y, which in turn amounts to maximizing their\\ndepartures from Gaussianity.\\nFor convenience, rather than using the entropy H(Yj), Hyv¨ arinen and\\nOja (2000) use the negentropy measure J(Yj) deﬁned by\\nJ(Yj) =H(Zj)−H(Yj), (14.86)\\nwhere Zjis a Gaussian random variable with the same variance as Yj. Ne-\\ngentropy is non-negative, and measures the departure of Yjfrom Gaussian-\\nity. They propose simple approximations to negentropy which can be com-\\nputed and optimized on data. The ICA solutions shown in Figures 14.37–\\n14.39 use the approximation\\nJ(Yj)≈[EG(Yj)−EG(Zj)]2, (14.87)\\nwhere G(u) =1\\nalog cosh( au) for 1 ≤a≤2. When applied to a sample\\nofxi, the expectations are replaced by data averages. This is one of the\\noptions in the FastICA software provided by these authors. More classical\\n(and less robust) measures are based on fourth moments, and hence look for\\ndepartures from the Gaussian via kurtosis. See Hyv¨ arinen and Oja (2000)\\nfor more details. In Section 14.7.4 we describe their approximate Newton\\nalgorithm for ﬁnding the optimal directions.\\nIn summary then, ICA applied to multivariate data looks for a sequence\\nof orthogonal projections such that the projected data look as far from', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='265e085c-0647-48d9-8007-2d5180212f99', embedding=None, metadata={'page_label': '582', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.7 Independent Component Analysis and Exploratory Projection Pursu it 563\\nComponent\\n 1oooooo o\\nooo\\noo\\noo\\noo\\no\\noooooo\\nooo\\noooo ooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooooooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\noooooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\noooo ooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooooooooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\nooooo o\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noooooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\nooooooooooo\\nooooooo\\noo\\noo\\noo\\nooo oo\\no\\nooooo\\noo\\noooo\\noooooooo\\nooo\\noo\\noo\\noo\\no\\noooooo\\nooo\\nooooooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\noooo o\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooo oooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\noooooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\nooooooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\noo o\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooo o ooooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\nooooo o\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noo oooo\\no oo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\noo ooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\noooooooo\\nooo\\noo\\noo\\noo\\no\\noooooo\\noo o\\nooooooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\noooooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\nooooo o oooo\\noo\\no\\noooooooo\\nooo\\noooo oooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\noooooooo ooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooo oooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\nooo ooo\\nooooo\\noooooooo\\nooo oooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\noooo ooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\nooooo o oooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\noooooo\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\no ooo\\no\\nooo\\noo oooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\nooooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\noooooooo\\nooo\\noo\\noo\\noo\\no\\noooooo\\noo o\\nooooooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\nooo\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooo oooooo\\noooo oo\\noooooo\\nooo ooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\nooo ooooooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\noooooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\nooooooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\no ooo\\noo o\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooooooooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\no ooo\\noooooo\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noo oooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\noo o\\nooooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\no\\noo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noooo\\noo\\noooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooo\\noo\\noooo\\nooo\\nooooo\\noo\\no\\noo\\noooooo\\no\\noo\\noo oo\\no\\noo\\noooooo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\no\\noo\\noo\\noo\\noo\\nooo\\noooo\\nooo\\noo\\nooooo\\no\\noooo\\noo\\nooo\\nooo\\no\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooooo oooo\\noo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\no\\noo oo\\noooo\\noooo\\nooo\\noo\\no\\nooo\\nooo\\no\\no\\no\\nooo\\no\\nooo\\nooooo\\noooo\\noo\\noooo o\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\noooo\\noooo\\nooo\\noo\\noooo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\noooo\\nooooo\\noo\\noooooo\\no\\nooooo\\nooooo\\nooooo\\noooo\\noo\\noooo\\noooo\\no\\no\\noooo oooooo\\nooo\\no\\noooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noo\\no\\noo\\no\\noo\\nooo\\nooo\\noooo\\noooo\\nooooo\\noooo\\nooo\\no\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noooo\\noo\\no\\nooo\\noo\\no\\noooo\\noo\\noo\\noo\\noo\\noo\\noo\\noo\\noooo\\no\\noooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\noooooo\\noo\\nooo\\noooo\\no\\nooooooo\\noo\\noo\\no\\nooo\\noo\\noooooo\\no\\no\\nooooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooooooo\\noooo\\nooo\\noo ooo\\nooo\\nooo\\noooo\\nooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooooo\\noo\\no\\noo\\noo\\nooo\\noooooo\\noo\\no\\noo\\nooo\\nComponent\\n 2\\noooo\\nooooo\\nooo\\noooo\\nooooooo\\noo\\noooo\\noooo\\noooo\\noooo\\nooooooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noo oooo o\\noo\\noooooooooo\\nooo\\no\\noo\\nooooooooooooooo\\no o\\nooo\\nooooo\\noooo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\no\\no oooooooooooo\\noooo\\no\\nooo\\noo\\nooooo oo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooooo\\noooo\\no oooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo ooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noooooo\\nooooo\\nooooo\\noooooo\\noooo\\noooooooo\\no\\noo oooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\nooooo\\noooo oooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\noooooo\\noooooo\\nooo\\nooo\\noo ooooooooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noo oooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\nooo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooo oooo\\no\\no\\noo\\noooo oo\\nooooo\\nooo\\noooo\\nooooooo\\noo\\noooo\\noooo\\noooo\\noooo\\noooo ooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noo ooooo\\noo\\nooooo ooooo\\nooo\\no\\noo\\noo ooooooooooooo\\no o\\noo o\\nooooo\\noooo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\no\\nooooooooooooo\\noooo\\no\\nooo\\noo\\nooooooo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooo oo\\noooo\\nooooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo ooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noo o ooo\\nooooo\\nooo oo\\noooooo\\noooo\\noooooooo\\no\\noooooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\no oooo\\noooooooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\noooooo\\noooooo\\nooo\\nooo\\nooooooooooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noooooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\nooo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooooooo\\no\\no\\noo\\noooo oo\\nooooo\\nooo\\noooo\\nooooooo\\no o\\noooo\\noooo\\noooo\\noooo\\noooo ooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noooo ooo\\noo\\noooooooooo\\nooo\\no\\noo\\noo ooooooooooooo\\no o\\noo o\\nooooo\\noooo\\nooo\\noo oooo\\no\\noo\\noo\\nooo\\no\\nooooo oooooooo\\noooo\\no\\nooo\\noo\\nooooooo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\noooooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noooooo\\nooooo\\nooo oo\\noooooo\\noooo\\noooooooo\\no\\noooooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noo oo o\\noooooooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\nooo ooo\\noooooo\\nooo\\nooo\\nooooooooooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noooooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\no oo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooooooo\\no\\no\\noo\\noo\\no\\noooooo\\nooo\\nooo\\no\\nooo\\nooo\\no\\nooo oooo\\noo\\nooo\\nooooo\\noo\\noo\\no\\noooooo\\no\\nooo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooo\\noo\\nooooooo\\no o\\noo\\no\\nooo\\noo\\noo\\noo\\no\\nooo\\nooooo\\noo\\noo\\nooo\\no\\nooo\\nooo o\\nooooo\\noo\\no\\noo\\noo\\no\\nooo\\no\\nooo\\noo\\noooo\\nooo ooo\\nooo\\no\\noo\\noo\\nooo\\nooo o\\noo\\noo\\no\\nooo\\no\\noo\\noooooo\\nooo\\noo\\nooo\\no\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\no\\no\\noooo\\nooo\\noooo\\noo\\nooo\\noo\\no\\nooooo\\nooo\\noo\\no\\no\\noo\\noo\\no\\noo\\noo\\noooooooo\\nooooo\\nooooo\\no\\noooo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\noooo\\noo\\nooo\\no\\nooo\\noo\\noooooo\\noo\\noooo\\noo\\nooo\\noooo\\nooooo\\noooo\\no\\noooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\no\\no\\noooo\\noo\\nooo\\no\\noo\\noo\\no\\noooooo\\no\\no\\nooooo\\nooo\\noooo\\no\\no\\nooo\\nooo\\noooooooooo\\noo\\nooo\\noo\\noooooo\\nooo\\noooo\\noo\\nooo\\nooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\nooo\\no\\nooo\\no\\noo oo\\noo\\nooo\\nooo\\no\\noo\\nooo\\nooooo\\noo\\nooo\\no\\noooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\no\\nooooo\\noo\\noo\\nooo\\noo\\nooo\\nooo\\nooooo\\no\\nooooooo\\noooo\\nooo\\nooooo\\noo\\noo\\nooooooo\\noo o\\noo\\no\\nooo\\noo o\\noo\\no\\noo\\noooooo\\noo\\no\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\no\\noooo\\noooooo\\nooo\\nooo\\no\\nooo\\nooo\\no\\nooooooo\\noo\\noo o\\nooooo\\noo\\noo\\no\\nooooooo\\nooo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\no\\nooo\\noo\\noo\\noo\\no\\nooo\\noo ooo\\noo\\noo\\nooo\\no\\nooo\\nooo o\\no oooo\\noo\\no\\noo\\noo\\no\\nooo\\no\\nooo\\noo\\noooo\\noooooo\\nooo\\no\\noo\\noo\\nooo\\noooo\\noo\\noo\\no\\nooo\\no\\noo\\noooooo\\nooo\\noo\\nooo\\no\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\no\\no\\noooo\\nooo\\noooo\\noo\\nooo\\noo\\no\\nooooo\\nooo\\noo\\no\\no\\noo\\noo\\no\\noo\\noo\\noooooo oo\\nooooo\\nooooo\\no\\noooo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\noooo\\noo\\nooo\\no\\nooo\\noo\\noooooo\\noo\\noooo\\noo\\nooo\\noooo\\nooooo\\noooo\\no\\no ooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\noooo\\noo\\nooo\\no\\noo\\noo\\no\\noooooo\\no\\no\\nooo oo\\nooo\\noooo\\no\\no\\nooo\\nooo\\noooooooooo\\noo\\nooo\\noo\\noooooo\\nooo\\noooo\\noo\\nooo\\nooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\nooo\\no\\nooo\\no\\noooo\\noo\\nooo\\nooo\\no\\noo\\nooo\\nooooo\\noo\\nooo\\no\\noooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\no\\nooooo\\noo\\noo\\nooo\\no o\\nooo\\nooo\\nooooo\\no\\nooooooo\\noooo\\nooo\\nooooo\\noo\\noo\\nooooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\noo\\noooooo\\noo\\no\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\no\\noooComponent\\n 3oo\\nooooooooo\\noooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\noooo\\nooooooo\\nooo\\nooooo\\nooo\\nooooo\\no ooooooo\\nooo\\no\\noo\\no o\\nooo\\noo\\nooooo\\no ooo\\noooooo\\nooo\\noooooooo\\noo\\nooo\\nooooo\\noooooooooo\\noo\\noooooooo\\noo\\nooooo o ooooooooo\\nooooo\\nooooo\\nooo\\nooo\\noo\\nooooooo\\nooooooo\\no\\no\\noo\\noooo\\noo\\noooo\\noo\\noo\\nooooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noooooo\\nooo oo\\noooo\\noo\\nooooo\\no\\noo\\noooooooo\\noo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooooooooo\\noo\\noooo\\noooo\\nooo\\noooo\\nooo\\noo\\noooooo\\no\\noooo\\noooo\\noooooo o\\nooo\\noooooo\\nooo\\noooo\\no\\noo\\noo\\noo\\noooooooooo\\no\\noooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\nooo\\noooooo\\nooo\\noooooooo\\noooooo\\no\\noooo\\noooooooooooo\\noo\\noo\\noooooooo o\\noo\\nooo\\noooooooo\\noooooooo\\noooo\\noo\\noooooooo\\noo\\noo\\noooooo\\noooooooo\\noooo\\nooo\\noooooooo\\noooooo\\no\\noo\\no ooo\\noooo\\no\\nooooo\\noooo\\noooo\\nooooo\\noooooooo\\noo\\nooo ooo\\nooooooo\\nooooooo\\nooooo\\noooo\\nooooooooo\\noooo oo\\noooo\\noooo\\noooooooo\\nooo o\\nooo\\no\\nooo\\noo\\nooooooo\\nooooooooo\\noooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\noooo\\nooooooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\nooo\\no\\noo\\noo\\nooo\\noo\\nooooo\\noooo\\noooooo\\nooo\\noooooooo\\noo\\nooo\\nooooo\\noooooooooo\\noo\\noooooooo\\noo\\nooooooooooo o ooo\\nooooo\\nooo oo\\nooo\\nooo\\noo\\nooooo oo\\nooooooo\\no\\no\\noo\\noooo\\noo\\noooo\\noo\\noo\\nooooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noooo\\noo\\nooooo\\no\\noo\\noooooooo\\noo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooooooooo\\noo\\noooo\\noooo\\nooo\\noooo\\nooo\\noo\\noooooo\\no\\noooo\\noooo\\nooooooo\\nooo\\noooooo\\nooo\\noooo\\no\\noo\\noo\\noo\\noooooooooo\\no\\noooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\nooo\\noooooo\\nooo\\noooooooo\\noooooo\\no\\noooo\\nooooo o oooooo\\noo\\noo\\nooooo oooo\\noo\\nooo\\noooooooo\\noooooooo\\noooo\\noo\\noooooooo\\noo\\noo\\noooooo\\noooooooo\\noooo\\nooo\\noooooooo\\noooooo\\no\\noo\\no ooo\\noooo\\no\\nooooo\\noooo\\noooo\\nooooo\\noooooooo\\noo\\noooooo\\nooooooo\\nooooooo\\nooooo\\noooo\\noooo oo ooo\\noooo oo\\noooo\\noooo\\noooooooo\\noo oo\\nooo\\no\\nooo\\noo\\nooooo\\noooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\noooo\\noooooo\\noooo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\nooooo\\no\\noo\\noooo\\nooo\\noo\\no\\nooo\\nooooooooooo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\noo o\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\nooooooooooo\\nooo\\no\\noo\\no\\noooo\\no\\nooooooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooo oo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\no oooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noooooo\\nooo\\noo\\noo o\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\no oooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\noo\\nooooo\\noooo oo\\noooo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\noooo\\nooo\\noo\\no\\no oo\\noooo ooooo oo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\nooooooooooo\\nooo\\no\\noo\\no\\noooo\\no\\noooo\\nooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooooo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\nooooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noooooo\\nooo\\noo\\nooo\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\nooo ooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\nooooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\noo o\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\noooo\\noooooo\\no ooo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\nooooo\\no\\noo\\noooo\\nooo\\noo\\no\\nooo\\nooooooooooo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\noooo ooooooo\\nooo\\no\\noo\\no\\noooo\\no\\nooooooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooooo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\nooooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noo oooo\\nooo\\noo\\noo o\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\noComponent\\n 4ooo\\nooo\\nooo\\no\\noooo oooooo\\noooooo\\noooooo\\noo\\noo\\nooo\\noo\\noo ooo\\no\\nooo\\noo\\nooooo\\no\\nooo\\no\\noooo\\no\\no\\noooooo ooo\\nooooooo\\no oooo\\nooooo\\no\\noooooo\\noo\\noooooooo\\no\\nooo\\noo\\noooo\\nooo\\noo\\nooo\\noooooo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\noo\\noooooo o\\nooo\\nooooo\\noo\\no ooo\\no\\noo\\nooooooo\\nooo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\nooooooooo\\nooooooo\\no\\nooooo\\noo\\noooo\\noooo\\noo\\no\\nooooooo\\noo\\nooooooooooo\\no\\noooooo\\noo\\noooo\\nooo o\\nooooooo\\nooo\\nooo\\no\\noooo\\noooo\\nooo\\noooooooooooo\\nooooooooooo\\noooo\\nooooo\\nooo\\no\\no\\no\\nooooo\\noooooooooo\\noooooo\\noo o\\no\\noooo\\noooooo\\no\\noooooooooo\\noo\\noooooooooo\\nooooooo\\nooo oo\\noo\\no\\noooo\\nooo\\nooo oo\\nooooooo\\no\\nooo\\noo\\no\\no\\nooooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\noo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooooo\\nooo\\noo\\noo\\noo\\noooooooo\\nooo\\nooooooo\\nooo\\nooooooooooo\\noooo\\noooo\\no\\noooooooo\\noo\\noooooo\\no\\noo\\nooooooo\\nooo\\nooooooooo\\nooo\\noo\\no\\noooooo\\noooo\\noooooo\\noo\\noo\\noo oo o\\no\\no\\noo oooooo\\nooo\\no\\noo\\noo\\no\\noooo\\no\\noooo\\nooooo\\noo\\no\\noo\\noo\\noo\\no\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\no ooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\noooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\nooooooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noo oo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooo oooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooooo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\no\\nooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\nooooooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\no ooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\nooo ooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooooooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\nooo o\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\no oo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\no ooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooo oo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\noooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noooo ooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\nooooo ooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooo oooo oo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\no\\nooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noooo ooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooo oo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noooooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooo\\noo oooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noo\\noo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noooo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooComponent\\n 5PCA ComponentsICA Components\\nFIGURE 14.39. A comparison of the ﬁrst ﬁve ICA components computed using\\nFastICA (above diagonal) with the ﬁrst ﬁve PCA components(below diagonal) .\\nEach component is standardized to have unit variance.\\nGaussian as possible. With pre-whitened data, this amounts to looking for\\ncomponents that are as independent as possible.\\nICA starts from essentially a factor analysis solution, and looks fo r rota-\\ntions that lead to independent components. From this point of view, ICA is\\njust another factor rotation method, along with the traditional “varimax ”\\nand “quartimax” methods used in psychometrics.\\nExample: Handwritten Digits\\nWe revisit the handwritten threes analyzed by PCA in Section 14.5.1. Fig-\\nure 14.39 compares the ﬁrst ﬁve (standardized) principal components with\\nthe ﬁrst ﬁve ICA components, all shown in the same standardized units.\\nNote that each plot is a two-dimensional projection from a 256-dimensional', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1e1cf73-311c-41fe-a81d-ae9d2b0d10a2', embedding=None, metadata={'page_label': '583', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='564 14. Unsupervised Learning\\nMean ICA 1 ICA 2 ICA 3 ICA 4 ICA 5\\nFIGURE 14.40. The highlighted digits from Figure 14.39. By comparing with\\nthe mean digits, we see the nature of the ICA component.\\nspace. While the PCA components all appear to have joint Gaussian distri-\\nbutions, the ICA components have long-tailed distributions. This is not too\\nsurprising, since PCA focuses on variance, while ICA speciﬁcally looks for\\nnon-Gaussian distributions. All the components have been standardized,\\nso we do not see the decreasing variances of the principal components.\\nFor each ICA component we have highlighted two of the extreme digits,\\nas well as a pair of central digits and displayed them in Figure 14.40.\\nThis illustrates the nature of each of the components. For example, ICA\\ncomponent ﬁve picks up the long sweeping tailed threes.\\nExample: EEG Time Courses\\nICA has become an important tool in the study of brain dynamics—the\\nexample we present here uses ICA to untangle the components of signals\\nin multi-channel electroencephalographic (EEG) data (Onton and Makeig,\\n2006).\\nSubjects wear a cap embedded with a lattice of 100 EEG electrodes,\\nwhich record brain activity at diﬀerent locations on the scalp. Figure 14.4111\\n(top panel) shows 15 seconds of output from a subset of nine of these elec-\\ntrodes from a subject performing a standard “two-back” learning task over\\na 30 minute period. The subject is presented with a letter (B, H, J, C, F, or\\nK) at roughly 1500-ms intervals, and responds by pressing one of two but-\\ntons to indicate whether the letter presented is the same or diﬀerent from\\nthat presented two steps back. Depending on the answer, the subject earns\\nor loses points, and occasionally earns bonus or loses penalty points. The\\ntime-course data show spatial correlation in the EEG signals—the signals\\nof nearby sensors look very similar.\\nThe key assumption here is that signals recorded at each scalp electrode\\nare a mixture of independent potentials arising from diﬀerent cortical ac-\\n11Reprinted from Progress in Brain Research , Vol. 159, Julie Onton and Scott Makeig,\\n“Information based modeling of event-related brain dynami cs,” Page 106 , Copyright\\n(2006), with permission from Elsevier. We thank Julie Onton and Scott Makeig for\\nsupplying an electronic version of the image.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='70dead99-8434-4dcf-9ecf-cbe071786dbb', embedding=None, metadata={'page_label': '584', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.7 Independent Component Analysis and Exploratory Projection Pursu it 565\\ntivities, as well as non-cortical artifact domains; see the reference for a\\ndetailed overview of ICA in this domain.\\nThe lower part of Figure 14.41 shows a selection of ICA components.\\nThe colored images represent the estimated unmixing coeﬃcient vectors ˆ aj\\nas heatmap images superimposed on the scalp, indicating the location of\\nactivity. The corresponding time-courses show the activity of the learned\\nICA components.\\nFor example, the subject blinked after each performance feedback signal\\n(colored vertical lines), which accounts for the location and artifact signa l\\nin IC1 and IC3. IC12 is an artifact associated with the cardiac pulse. IC4\\nand IC7 account for frontal theta-band activities, and appear after a stretch\\nof correct performance. See Onton and Makeig (2006) for a more detailed\\ndiscussion of this example, and the use of ICA in EEG modeling.\\n14.7.3 Exploratory Projection Pursuit\\nFriedman and Tukey (1974) proposed exploratory projection pursuit, a\\ngraphical exploration technique for visualizing high-dimensional data. Their\\nview was that most low (one- or two-dimensional) projections of high-\\ndimensional data look Gaussian. Interesting structure, such as clusters or\\nlong tails, would be revealed by non-Gaussian projections. They proposed\\na number of projection indices for optimization, each focusing on a diﬀer-\\nent departure from Gaussianity. Since their initial proposal, a variety of\\nimprovements have been suggested (Huber, 1985; Friedman, 1987), and a\\nvariety of indices, including entropy, are implemented in the interactive\\ngraphics package Xgobi (Swayne et al., 1991, now called GGobi). These\\nprojection indices are exactly of the same form as J(Yj) above, where\\nYj=aT\\njX, a normalized linear combination of the components of X. In\\nfact, some of the approximations and substitutions for cross-entropy coin-\\ncide with indices proposed for projection pursuit. Typically with projection\\npursuit, the directions ajare not constrained to be orthogonal. Friedman\\n(1987) transforms the data to look Gaussian in the chosen projection, and\\nthen searches for subsequent directions. Despite their diﬀerent origins, ICA\\nand exploratory projection pursuit are quite similar, at least in the repre-\\nsentation described here.\\n14.7.4 A Direct Approach to ICA\\nIndependent components have by deﬁnition a joint product density\\nfS(s) =p∏\\nj=1fj(sj), (14.88)\\nso here we present an approach that estimates this density directly us-\\ning generalized additive models (Section 9.1). Full details can be found in', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2cdb25e1-3e52-4183-a3a7-5a1a6a049cca', embedding=None, metadata={'page_label': '585', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='566 14. Unsupervised Learning\\nFIGURE 14.41. Fifteen seconds of EEG data (of 1917seconds) at nine (of\\n100) scalp channels (top panel), as well as nine ICA components (lower pa nel).\\nWhile nearby electrodes record nearly identical mixtures of bra in and non-brain\\nactivity, ICA components are temporally distinct. The colored scalps represent the\\nICA unmixing coeﬃcients ˆajas a heatmap, showing brain or scalp location of the\\nsource.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='03da5e9c-05bc-4d23-8ecf-086ad3e6fb5e', embedding=None, metadata={'page_label': '586', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.7 Independent Component Analysis and Exploratory Projection Pursu it 567\\nHastie and Tibshirani (2003), and the method is implemented in the R\\npackageProDenICA , available from CRAN.\\nIn the spirit of representing departures from Gaussianity, we represent\\neachfjas\\nfj(sj) =φ(sj)egj(sj), (14.89)\\natilted Gaussian density. Here φis the standard Gaussian density, and\\ngjsatisﬁes the normalization conditions required of a density. Assuming\\nas before that Xis pre-whitened, the log-likelihood for the observed data\\nX=ASis\\nℓ(A,{gj}p\\n1;X) =N∑\\ni=1p∑\\nj=1[\\nlogφj(aT\\njxi) +gj(aT\\njxi)\\n], (14.90)\\nwhich we wish to maximize subject to the constraints that Ais orthogonal\\nand that the gjresult in densities in (14.89). Without imposing any further\\nrestrictions on gj, the model (14.90) is over-parametrized, so we instead\\nmaximize a regularized version\\np∑\\nj=1[\\n1\\nNN∑\\ni=1[\\nlogφ(aT\\njxi) +gj(aT\\njxi)]\\n−∫\\nφ(t)egj(t)dt−λj∫\\n{g′′′\\nj(t)}2(t)dt]\\n.\\n(14.91)\\nWe have subtracted two penalty terms (for each j) in (14.91), inspired by\\nSilverman (1986, Section 5.4.4):\\n•The ﬁrst enforces the density constraint∫\\nφ(t)eˆgj(t)dt= 1 on any\\nsolution ˆ gj.\\n•The second is a roughness penalty, which guarantees that the solution\\nˆgjis a quartic-spline with knots at the observed values of sij=aT\\njxi.\\nIt can further be shown that the solution densities ˆfj=φeˆgjeach have\\nmean zero and variance one (Exercise 14.18). As we increase λj, these\\nsolutions approach the standard Gaussian φ.\\nAlgorithm 14.3 Product Density ICA Algorithm: ProDenICA\\n1. Initialize A(random Gaussian matrix followed by orthogonalization).\\n2. Alternate until convergence of A:\\n(a) Given A, optimize (14.91) w.r.t. gj(separately for each j).\\n(b) Given gj, j= 1,... ,p , perform one step of a ﬁxed point algo-\\nrithm towards ﬁnding the optimal A.\\nWe ﬁt the functions gjand directions ajby optimizing (14.91) in an\\nalternating fashion, as described in Algorithm 14.3.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8bc299b2-b8a7-4375-9545-dbfd24af1549', embedding=None, metadata={'page_label': '587', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='568 14. Unsupervised Learning\\nStep 2(a) amounts to a semi-parametric density estimation, which can\\nbe solved using a novel application of generalized additive models. For\\nconvenience we extract one of the pseparate problems,\\n1\\nNN∑\\ni=1[logφ(si) +g(si)]−∫\\nφ(t)eg(t)dt−λ∫\\n{g′′′(t)}2(t)dt. (14.92)\\nAlthough the second integral in (14.92) leads to a smoothing spline, the\\nﬁrst integral is problematic, and requires an approximation. We construct\\na ﬁne grid of Lvalues s∗\\nℓin increments ∆ covering the observed values si,\\nand count the number of siin the resulting bins:\\ny∗\\nℓ=#si∈(s∗\\nℓ−∆/2,s∗\\nℓ+ ∆/2)\\nN. (14.93)\\nTypically we pick Lto be 1000, which is more than adequate. We can then\\napproximate (14.92) by\\nL∑\\nℓ=1{\\ny∗\\ni[log(φ(s∗\\nℓ)) +g(s∗\\nℓ)]−∆φ(s∗\\nℓ)eg(s∗\\nℓ)}\\n−λ∫\\ng′′′2(s)ds. (14.94)\\nThis last expression can be seen to be proportional to a penalized Poisson\\nlog-likelihood with response y∗\\nℓ/∆ and penalty parameter λ/∆, and mean\\nθ(s) =φ(s)eg(s). This is a generalized additive spline model (Hastie and\\nTibshirani, 1990; Efron and Tibshirani, 1996), with an oﬀset term log φ(s),\\nand can be ﬁt using a Newton algorithm in O(L) operations. Although\\na quartic spline is called for, we ﬁnd in practice that a cubic spline is\\nadequate. We have ptuning parameters λjto set; in practice we make\\nthem all the same, and specify the amount of smoothing via the eﬀective\\ndegrees-of-freedom df( λ). Our software uses 5df as a default value.\\nStep 2(b) in Algorithm 14.3 requires optimizing (14.92) with respect to\\nA, holding the ˆ gjﬁxed. Only the ﬁrst terms in the sum involve A, and\\nsinceAis orthogonal, the collection of terms involving φdo not depend on\\nA(Exercise 14.19). Hence we need to maximize\\nC(A) =1\\nNp∑\\nj=1N∑\\ni=1ˆgj(aT\\njxi) (14.95)\\n=p∑\\nj=1Cj(aj)\\nC(A) is a log-likelihood ratio between the ﬁtted density and a Gaussian,\\nand can be seen as an estimate of negentropy (14.86), with each ˆ gja con-\\ntrast function as in (14.87). The ﬁxed point update in step 2(b) is a modiﬁed\\nNewton step (Exercise 14.20)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbce6e34-1511-441a-8c55-02ca1abccf82', embedding=None, metadata={'page_label': '588', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.7 Independent Component Analysis and Exploratory Projection Pursu it 569\\n1. For each jupdate\\naj←E{\\nXˆg′\\nj(aT\\njX)−E[ˆg′′\\nj(aT\\njX)]aj}\\n, (14.96)\\nwhere E represents expectation w.r.t the sample xi. Since ˆ gjis a ﬁtted\\nquartic (or cubic) spline, the ﬁrst and second derivatives are readily\\navailable.\\n2. Orthogonalize Ausing the symmetric square-root transformation\\n(AAT)−1\\n2A. IfA=UDVTis the SVD of A, it is easy to show that\\nthis leads to the update A←UVT.\\nOurProDenICA algorithm works as well as FastICA on the artiﬁcial time\\nseries data of Figure 14.37, the mixture of uniforms data of Figure 14.38 ,\\nand the digit data in Figure 14.39.\\nExample: Simulations\\na b c\\nd e f\\ng h i\\nj k l\\nm n o\\np q r\\nDistributionAmari Distance from True A\\na b c d e f g h i j k l m n o p q r0.01 0.02 0.05 0.10 0.20 0.50FastICA\\nKernelICA\\nProdDenICA\\nFIGURE 14.42. The left panel shows 18distributions used for comparisons.\\nThese include the “t”, uniform, exponential, mixtures of exponential s, symmetric\\nand asymmetric Gaussian mixtures. The right panel shows (on the log scale)\\nthe average Amari metric for each method and each distributio n, based on 30\\nsimulations in I R2for each distribution.\\nFigure 14.42 shows the results of a simulation comparing ProDenICA to\\nFastICA , and another semi-parametric competitor KernelICA (Bach and\\nJordan, 2002). The left panel shows the 18 distributions used as a basis\\nof comparison. For each distribution, we generated a pair of independent\\ncomponents ( N= 1024), and a random mixing matrix in IR2with condition\\nnumber between 1 and 2. We used our R implementations of FastICA , using\\nthe negentropy criterion (14.87), and ProDenICA . ForKernelICA we used', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='05dfb12f-e5ff-45bb-af66-47d84140f585', embedding=None, metadata={'page_label': '589', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='570 14. Unsupervised Learning\\nthe authors MATLAB code.12Since the search criteria are nonconvex, we\\nused ﬁve random starts for each method. Each of the algorithms delivers\\nan orthogonal mixing matrix A(the data were pre-whitened ), which is\\navailable for comparison with the generating orthogonalized mixing matri x\\nA0. We used the Amari metric (Bach and Jordan, 2002) as a measure of\\nthe closeness of the two frames:\\nd(A0,A) =1\\n2pp∑\\ni=1(∑p\\nj=1|rij|\\nmax j|rij|−1)\\n+1\\n2pp∑\\nj=1(∑p\\ni=1|rij|\\nmax i|rij|−1)\\n,(14.97)\\nwhere rij= (AoA−1)ij. The right panel in Figure 14.42 compares the\\naverages (on the log scale) of the Amari metric between the truth and the\\nestimated mixing matrices. ProDenICA is competitive with FastICA and\\nKernelICA in all situations, and dominates most of the mixture simulations.\\n14.8 Multidimensional Scaling\\nBoth self-organizing maps and principal curves and surfaces map data\\npoints in IRpto a lower-dimensional manifold. Multidimensional scaling\\n(MDS) has a similar goal, but approaches the problem in a somewhat dif-\\nferent way.\\nWe start with observations x1,x2,... ,x N∈IRp, and let dijbe the dis-\\ntance between observations iandj. Often we choose Euclidean distance\\ndij=||xi−xj||, but other distances may be used. Further, in some ap-\\nplications we may not even have available the data points xi, but only\\nhave some dissimilarity measure dij(see Section 14.3.10). For example, in\\na wine tasting experiment, dijmight be a measure of how diﬀerent a sub-\\nject judged wines iandj, and the subject provides such a measure for all\\npairs of wines i,j. MDS requires only the dissimilarities dij, in contrast to\\nthe SOM and principal curves and surfaces which need the data points xi.\\nMultidimensional scaling seeks values z1,z2,... ,z N∈IRkto minimize\\nthe so-called stress function13\\nSM(z1,z2,... ,z N) =∑\\ni̸=i′(dii′− ||zi−zi′||)2. (14.98)\\nThis is known as least squares orKruskal–Shephard scaling. The idea is\\nto ﬁnd a lower-dimensional representation of the data that preserves the\\npairwise distances as well as possible. Notice that the approximation is\\n12Francis Bach kindly supplied this code, and helped us set up th e simulations.\\n13Some authors deﬁne stress as the square-root of SM; since it does not aﬀect the\\noptimization, we leave it squared to make comparisons with o ther criteria simpler.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb628ec7-2ac5-40a9-8f68-61fd51ccf928', embedding=None, metadata={'page_label': '590', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.8 Multidimensional Scaling 571\\nin terms of the distances rather than squared distances (which results in\\nslightly messier algebra). A gradient descent algorithm is used to minimize\\nSM.\\nA variation on least squares scaling is the so-called Sammon mapping\\nwhich minimizes\\nSSm(z1,z2,... ,z N) =∑\\ni̸=i′(dii′− ||zi−zi′||)2\\ndii′. (14.99)\\nHere more emphasis is put on preserving smaller pairwise distances.\\nInclassical scaling , we instead start with similarities sii′: often we use\\nthe centered inner product sii′=⟨xi−¯x,xi′−¯x⟩. The problem then is to\\nminimize\\nSC(z1,z2,... ,z N) =∑\\ni,i′(sii′− ⟨zi−¯z,zi′−¯z⟩)2(14.100)\\noverz1,z2,... ,z N∈IRk. This is attractive because there is an explicit\\nsolution in terms of eigenvectors: see Exercise 14.11. If we have distances\\nrather than inner-products, we can convert them to centered inner-products\\nif the distances are Euclidean ;14see (18.31) on page 671 in Chapter 18.\\nIf the similarities are in fact centered inner-products, classical scaling is\\nexactly equivalent to principal components, an inherently linear dimension-\\nreduction technique. Classical scaling is not equivalent to least squares\\nscaling; the loss functions are diﬀerent, and the mapping can be nonlinear.\\nLeast squares and classical scaling are referred to as metric scaling meth-\\nods, in the sense that the actual dissimilarities or similarities are appro x-\\nimated. Shephard–Kruskal nonmetric scaling eﬀectively uses only ranks.\\nNonmetric scaling seeks to minimize the stress function\\nSNM(z1,z2,... ,z N) =∑\\ni̸=i′[\\n||zi−zi′|| −θ(dii′)]2\\n∑\\ni̸=i′||zi−zi′||2(14.101)\\nover the ziand an arbitrary increasing function θ. With θﬁxed, we min-\\nimize over ziby gradient descent. With the ziﬁxed, the method of iso-\\ntonic regression is used to ﬁnd the best monotonic approximation θ(dii′)\\nto||zi−zi′||. These steps are iterated until the solutions stabilize.\\nLike the self-organizing map and principal surfaces, multidimensional\\nscaling represents high-dimensional data in a low-dimensional coordinate\\nsystem. Principal surfaces and SOMs go a step further, and approximate\\nthe original data by a low-dimensional manifold, parametrized in the low\\ndimensional coordinate system. In a principal surface and SOM, points\\n14AnN×Ndistance matrix is Euclidean if the entries represent pairw ise Euclidean\\ndistances between Npoints in some dimensional space.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8f854b4c-6cfc-4268-869a-33340408572e', embedding=None, metadata={'page_label': '591', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='572 14. Unsupervised Learning\\nFirst MDS CoordinateSecond MDS Coordinate\\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•\\n•••\\n•••\\n•\\n••\\n•••\\n••••\\n•\\n•••\\n••••••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n••\\nFIGURE 14.43. First two coordinates for half-sphere data, from classical m ulti-\\ndimensional scaling.\\nclose together in the original feature space should map close together on\\nthe manifold, but points far apart in feature space might also map close\\ntogether. This is less likely in multidimensional scaling since it explicitly\\ntries to preserve all pairwise distances.\\nFigure 14.43 shows the ﬁrst two MDS coordinates from classical scaling\\nfor the half-sphere example. There is clear separation of the clusters, and\\nthe tighter nature of the red cluster is apparent.\\n14.9 Nonlinear Dimension Reduction and Local\\nMultidimensional Scaling\\nSeveral methods have been recently proposed for nonlinear dimension re-\\nduction, similar in spirit to principal surfaces. The idea is that the data lie\\nclose to an intrinsically low-dimensional nonlinear manifold embedded in a\\nhigh-dimensional space. These methods can be thought of as “ﬂattening”\\nthe manifold, and hence reducing the data to a set of low-dimensional co-\\nordinates that represent their relative positions in the manifold. They are\\nuseful for problems where signal-to-noise ratio is very high (e.g., physical\\nsystems), and are probably not as useful for observational data with lower\\nsignal-to-noise ratios.\\nThe basic goal is illustrated in the left panel of Figure 14.44. The data\\nlie near a parabola with substantial curvature. Classical MDS does not pre-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fee0b863-627f-4fb2-8c22-e5bda8cd2b9b', embedding=None, metadata={'page_label': '592', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling 573\\n−5 0 5−15 −10 −5 0Classical MDS\\n−5 0 5−15 −10 −5 0Local MDS\\nx1 x1\\nx2x2\\nFIGURE 14.44. The orange points show data lying on a parabola, while the blue\\npoints shows multidimensional scaling representations in one dime nsion. Classical\\nmultidimensional scaling (left panel) does not preserve the orde ring of the points\\nalong the curve, because it judges points on opposite ends of the curve to be close\\ntogether. In contrast, local multidimensional scaling (right p anel) does a good job\\nof preserving the ordering of the points along the curve.\\nserve the ordering of the points along the curve, because it judges points\\non opposite ends of the curve to be close together. The right panel shows\\nthe results of local multi-dimensional scaling , one of the three methods for\\nnon-linear multi-dimensional scaling that we discuss below. These meth-\\nods use only the coordinates of the points in pdimensions, and have no\\nother information about the manifold. Local MDS has done a good job of\\npreserving the ordering of the points along the curve.\\nWe now brieﬂy describe three new approaches to nonlinear dimension\\nreduction and manifold mapping.\\nIsometric feature mapping (ISOMAP) (Tenenbaum et al., 2000) con-\\nstructs a graph to approximate the geodesic distance between points along\\nthe manifold. Speciﬁcally, for each data point we ﬁnd its neighbors—points\\nwithin some small Euclidean distance of that point. We construct a graph\\nwith an edge between any two neighboring points. The geodesic distance\\nbetween any two points is then approximated by the shortest path be-\\ntween points on the graph. Finally, classical scaling is applied to the graph\\ndistances, to produce a low-dimensional mapping.\\nLocal linear embedding (Roweis and Saul, 2000) takes a very diﬀerent ap-\\nproach, trying to preserve the local aﬃne structure of the high-dimensional\\ndata. Each data point is approximated by a linear combination of neigh-\\nboring points. Then a lower dimensional representation is constructed that', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9d76cb8a-3c46-4a0c-98a4-b8fb98bd02e5', embedding=None, metadata={'page_label': '593', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='574 14. Unsupervised Learning\\nbest preserves these local approximations. The details are interesting, so\\nwe give them here.\\n1. For each data point xiinpdimensions, we ﬁnd its K-nearest neigh-\\nborsN(i) in Euclidean distance.\\n2. We approximate each point by an aﬃne mixture of the points in its\\nneighborhood:\\nmin\\nWik||xi−∑\\nk∈N(i)wikxk||2(14.102)\\nover weights wiksatisfying wik= 0, k /∈ N(i),∑N\\nk=1wik= 1.wik\\nis the contribution of point kto the reconstruction of point i. Note\\nthat for a hope of a unique solution, we must have K < p .\\n3. Finally, we ﬁnd points yiin a space of dimension d < p to minimize\\nN∑\\ni=1||yi−N∑\\nk=1wikyk||2(14.103)\\nwithwikﬁxed.\\nIn step 3, we minimize\\ntr[(Y−WY)T(Y−WY)] = tr[ YT(I−W)T(I−W)Y] (14.104)\\nwhere WisN×N;YisN×d, for some small d < p . The solutions ˆY\\nare the trailing eigenvectors of M= (I−W)T(I−W). Since 1is a trivial\\neigenvector with eigenvalue 0, we discard it and keep the next d. This has\\nthe side eﬀect that 1TY= 0, and hence the embedding coordinates are\\nmean centered.\\nLocal MDS (Chen and Buja, 2008) takes the simplest and arguably the\\nmost direct approach. We deﬁne Nto be the symmetric set of nearby pairs\\nof points; speciﬁcally a pair ( i,i′) is in Nif point iis among the K-nearest\\nneighbors of i′, or vice-versa. Then we construct the stress function\\nSL(z1,z2,... ,z N) =∑\\n(i,i′)∈N(dii′− ||zi−zi′||)2\\n+∑\\n(i,i′)/∈Nw≤(D− ||zi−zi′||)2.(14.105)\\nHereDis some large constant and wis a weight. The idea is that points\\nthat are not neighbors are considered to be very far apart; such pairs are\\ngiven a small weight wso that they don’t dominate the overall stress func-\\ntion. To simplify the expression, we take w∼1/D, and let D→ ∞ .\\nExpanding (14.105), this gives', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='12e0a5bd-e893-4ca8-aa6d-ca299c27ded2', embedding=None, metadata={'page_label': '594', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling 575\\nFIGURE 14.45. Images of faces mapped into the embedding space described by\\nthe ﬁrst two coordinates of LLE. Next to the circled points, repre sentative faces\\nare shown in diﬀerent parts of the space. The images at the bott om of the plot\\ncorrespond to points along the top right path (linked by solid line ), and illustrate\\none particular mode of variability in pose and expression.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2496da6b-e5d9-4bab-b772-98de4e3a066e', embedding=None, metadata={'page_label': '595', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='576 14. Unsupervised Learning\\nSL(z1,z2,... ,z N) =∑\\n(i,i′)∈N(dii′− ||zi−zi′||)2−τ∑\\n(i,i′)/∈N||zi−zi′||,\\n(14.106)\\nwhere τ= 2wD. The ﬁrst term in (14.106) tries to preserve local structure\\nin the data, while the second term encourages the representations zi,zi′\\nfor pairs ( i,i′) that are non-neighbors to be farther apart. Local MDS\\nminimizes the stress function (14.106) over zi, for ﬁxed values of the number\\nof neighbors Kand the tuning parameter τ.\\nThe right panel of Figure 14.44 shows the result of local MDS, using k= 2\\nneighbors and τ= 0.01. We used coordinate descent with multiple starting\\nvalues to ﬁnd a good minimum of the (nonconvex) stress function (14.106).\\nThe ordering of the points along the curve has been largely preserved,\\nFigure 14.45 shows a more interesting application of one of these meth-\\nods (LLE)15. The data consist of 1965 photographs, digitized as 20 ×28\\ngrayscale images. The result of the ﬁrst two-coordinates of LLE are shown\\nand reveal some variability in pose and expression. Similar pictures were\\nproduced by local MDS.\\nIn experiments reported in Chen and Buja (2008), local MDS shows su-\\nperior performance, as compared to ISOMAP and LLE. They also demon-\\nstrate the usefulness of local MDS for graph layout. There are also close\\nconnections between the methods discussed here, spectral clustering (Sec-\\ntion 14.5.3) and kernel PCA (Section 14.5.4).\\n14.10 The Google PageRank Algorithm\\nIn this section we give a brief description of the original PageRank algo-\\nrithm used by the Google search engine, an interesting recent application\\nof unsupervised learning methods.\\nWe suppose that we have Nweb pages and wish to rank them in terms\\nof importance. For example, the Npages might all contain a string match\\nto “statistical learning” and we might wish to rank the pages in terms of\\ntheir likely relevance to a websurfer.\\nThePageRank algorithm considers a webpage to be important if many\\nother webpages point to it. However the linking webpages that point to a\\ngiven page are not treated equally: the algorithm also takes into account\\nboth the importance ( PageRank ) of the linking pages and the number of\\noutgoing links that they have. Linking pages with higher PageRank are\\ngiven more weight, while pages with more outgoing links are given less\\nweight. These ideas lead to a recursive deﬁnition for PageRank , detailed\\nnext.\\n15Sam Roweis and Lawrence Saul kindly provided this ﬁgure.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7cb2ab52-32c3-4648-b55c-74fa9efef7ec', embedding=None, metadata={'page_label': '596', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.10 The Google PageRank Algorithm 577\\nLetLij= 1 if page jpoints to page i, and zero otherwise. Let cj=∑N\\ni=1Lijequal the number of pages pointed to by page j(number of out-\\nlinks). Then the Google PageRanks piare deﬁned by the recursive rela-\\ntionship\\npi= (1−d) +dN∑\\nj=1(Lij\\ncj)\\npj (14.107)\\nwhere dis a positive constant (apparently set to 0.85).\\nThe idea is that the importance of page iis the sum of the importances of\\npages that point to that page. The sums are weighted by 1 /cj, that is, each\\npage distributes a total vote of 1 to other pages. The constant densures\\nthat each page gets a PageRank of at least 1 −d. In matrix notation\\np= (1−d)e+d≤LD−1\\ncp (14.108)\\nwhere eis a vector of Nones and Dc= diag( c) is a diagonal matrix with\\ndiagonal elements cj. Introducing the normalization eTp=N(i.e., the\\naverage PageRank is 1), we can write (14.108) as\\np=[\\n(1−d)eeT/N+dLD−1\\nc]\\np\\n=Ap (14.109)\\nwhere the matrix Ais the expression in square braces.\\nExploiting a connection with Markov chains (see below), it can be shown\\nthat the matrix Ahas a real eigenvalue equal to one, and one is its largest\\neigenvalue. This means that we can ﬁnd ˆpby the power method: starting\\nwith some p=p0we iterate\\npk←Apk−1;pk←Npk\\neTpk. (14.110)\\nThe ﬁxed points ˆpare the desired PageRanks .\\nIn the original paper of Page et al. (1998), the authors considered PageR-\\nankas a model of user behavior, where a random web surfer clicks on links\\nat random, without regard to content. The surfer does a random walk on\\nthe web, choosing among available outgoing links at random. The factor\\n1−dis the probability that he does not click on a link, but jumps instead\\nto a random webpage.\\nSome descriptions of PageRank have (1 −d)/Nas the ﬁrst term in def-\\ninition (14.107), which would better coincide with the random surfer in-\\nterpretation. Then the page rank solution (divided by N) is the stationary\\ndistribution of an irreducible, aperiodic Markov chain over the Nwebpages.\\nDeﬁnition (14.107) also corresponds to an irreducible, aperiodic Markov\\nchain, with diﬀerent transition probabilities than those from he (1 −d)/N\\nversion. Viewing PageRank as a Markov chain makes clear why the matrix\\nAhas a maximal real eigenvalue of 1. Since Ahas positive entries with', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4fc09872-9d15-4455-8cdb-344e766869c0', embedding=None, metadata={'page_label': '597', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='578 14. Unsupervised Learning\\nPage 2\\nPage 3Page 4Page 1\\nFIGURE 14.46. PageRank algorithm: example of a small network\\neach column summing to one, Markov chain theory tells us that it has a\\nunique eigenvector with eigenvalue one, corresponding to the stationary\\ndistribution of the chain (Bremaud, 1999).\\nA small network is shown for illustration in Figure 14.46. The link ma trix\\nis\\nL=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed0 0 1 0\\n1 0 0 0\\n1 1 0 1\\n0 0 0 0\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8(14.111)\\nand the number of outlinks is c= (2,1,1,1).\\nThePageRank solution is ˆp= (1.49,0.78,1.58,0.15). Notice that page 4\\nhas no incoming links, and hence gets the minimum PageRank of 0.15.\\nBibliographic Notes\\nThere are many books on clustering, including Hartigan (1975), Gordon\\n(1999) and Kaufman and Rousseeuw (1990). K-means clustering goes back\\nat least to Lloyd (1957), Forgy (1965), Jancey (1966) and MacQueen (1967 ).\\nApplications in engineering, especially in image compression via vector\\nquantization, can be found in Gersho and Gray (1992). The k-medoid pro-\\ncedure is described in Kaufman and Rousseeuw (1990). Association rules\\nare outlined in Agrawal et al. (1995). The self-organizing map was propos ed\\nby Kohonen (1989) and Kohonen (1990); Kohonen et al. (2000) give a more\\nrecent account. Principal components analysis and multidimensional scal-\\ning are described in standard books on multivariate analysis, for exampl e,\\nMardia et al. (1979). Buja et al. (2008) have implemented a powerful en-\\nvironment called Ggvis for multidimensional scaling, and the user manual', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac97f835-ea84-4855-8d5c-bf2d8dc3d7fe', embedding=None, metadata={'page_label': '598', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 579\\ncontains a lucid overview of the subject. Figures 14.17, 14.21 (left panel)\\nand 14.28 (left panel) were produced in Xgobi, a multidimensional data\\nvisualization package by the same authors. GGobi is a more recent im-\\nplementation (Cook and Swayne, 2007). Goodall (1991) gives a technical\\noverview of Procrustes methods in statistics, and Ramsay and Silverman\\n(1997) discuss the shape registration problem. Principal curves and surfaces\\nwere proposed in Hastie (1984) and Hastie and Stuetzle (1989). The idea of\\nprincipal points was formulated in Flury (1990), Tarpey and Flury (1996 )\\ngive an exposition of the general concept of self-consistency. An excellent\\ntutorial on spectral clustering can be found in von Luxburg (2007); this was\\nthe main source for Section 14.5.3. Luxborg credits Donath and Hoﬀman\\n(1973) and Fiedler (1973) with the earliest work on the subject. A history\\nof spectral clustering my be found in Spielman and Teng (1996). Indepen-\\ndent component analysis was proposed by Comon (1994), with subsequent\\ndevelopments by Bell and Sejnowski (1995); our treatment in Section 14.7\\nis based on Hyv¨ arinen and Oja (2000). Projection pursuit was proposed by\\nFriedman and Tukey (1974), and is discussed in detail in Huber (1985). A\\ndynamic projection pursuit algorithm is implemented in GGobi.\\nExercises\\nEx. 14.1 Weights for clustering . Show that weighted Euclidean distance\\nd(w)\\ne(xi,xi′) =∑p\\nl=1wl(xil−xi′l)2\\n∑p\\nl=1wl\\nsatisﬁes\\nd(w)\\ne(xi,xi′) =de(zi,zi′) =p∑\\nl=1(zil−zi′l)2, (14.112)\\nwhere\\nzil=xil≤(wl∑p\\nl=1wl)1/2\\n. (14.113)\\nThus weighted Euclidean distance based on xis equivalent to unweighted\\nEuclidean distance based on z.\\nEx. 14.2 Consider a mixture model density in p-dimensional feature space,\\ng(x) =K∑\\nk=1πkgk(x), (14.114)\\nwhere gk=N(θk,L≤σ2) and πk≥0∀kwith∑\\nkπk= 1. Here {θk,πk},k=\\n1,... ,K andσ2are unknown parameters.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59641b16-d944-416c-a596-43ba2269ef2b', embedding=None, metadata={'page_label': '599', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='580 14. Unsupervised Learning\\nSuppose we have data x1,x2,... ,x N∼g(x) and we wish to ﬁt the mix-\\nture model.\\n1. Write down the log-likelihood of the data\\n2. Derive an EM algorithm for computing the maximum likelihood es-\\ntimates (see Section 8.1).\\n3. Show that if σhas a known value in the mixture model and we take\\nσ→0, then in a sense this EM algorithm coincides with K-means\\nclustering.\\nEx. 14.3 In Section 14.2.6 we discuss the use of CART or PRIM for con-\\nstructing generalized association rules. Show that a problem occurs with ei-\\nther of these methods when we generate the random data from the product-\\nmarginal distribution; i.e., by randomly permuting the values for each of\\nthe variables. Propose ways to overcome this problem.\\nEx. 14.4 Cluster the demographic data of Table 14.1 using a classiﬁcation\\ntree. Speciﬁcally, generate a reference sample of the same size of the train-\\ning set, by randomly permuting the values within each feature. Build a\\nclassiﬁcation tree to the training sample (class 1) and the reference sample\\n(class 0) and describe the terminal nodes having highest estimated class 1\\nprobability. Compare the results to the PRIM results near Table 14.1 and\\nalso to the results of K-means clustering applied to the same data.\\nEx. 14.5 Generate data with three features, with 30 data points in each of\\nthree classes as follows:\\nθ1=U(−π/8, π/8)\\nφ1=U(0,2π)\\nx1= sin( θ1)cos(φ1) +W11\\ny1= sin( θ1)sin(φ1) +W12\\nz1= cos( θ1) +W13\\nθ2=U(π/2−π/4, π/2 +π/4)\\nφ2=U(−π/4, π/4)\\nx2= sin( θ2)cos(φ2) +W21\\ny2= sin( θ2)sin(φ2) +W22\\nz2= cos( θ2) +W23\\nθ3=U(π/2−π/4, π/2 +π/4)\\nφ3=U(π/2−π/4, π/2 +π/4)\\nx3= sin( θ3)cos(φ3) +W31\\ny3= sin( θ3)sin(φ3) +W32\\nz3= cos( θ3) +W33\\nHereU(a,b) indicates a uniform variate on the range [ a,b] and Wjkare\\nindependent normal variates with standard deviation 0 .6. Hence the data', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68647b9a-f39b-46f1-b2a8-39e09ca07a60', embedding=None, metadata={'page_label': '600', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 581\\nlie near the surface of a sphere in three clusters centered at (1 ,0,0), (0,1,0)\\nand (0 ,0,1).\\nWrite a program to ﬁt a SOM to these data, using the learning rates\\ngiven in the text. Carry out a K-means clustering of the same data, and\\ncompare the results to those in the text.\\nEx. 14.6 Write programs to implement K-means clustering and a self-\\norganizing map (SOM), with the prototype lying on a two-dimensional\\ngrid. Apply them to the columns of the human tumor microarray data, us-\\ningK= 2,5,10,20 centroids for both. Demonstrate that as the size of the\\nSOM neighborhood is taken to be smaller and smaller, the SOM solution\\nbecomes more similar to the K-means solution.\\nEx. 14.7 Derive (14.51) and (14.52) in Section 14.5.1. Show that ˆ θis not\\nunique, and characterize the family of equivalent solutions.\\nEx. 14.8 Derive the solution (14.57) to the Procrustes problem (14.56).\\nDerive also the solution to the Procrustes problem with scaling (14.58).\\nEx. 14.9 Write an algorithm to solve\\nmin\\n{βℓ,Rℓ}L\\n1,ML∑\\nℓ=1||XℓRℓ−M||2\\nF. (14.115)\\nApply it to the three S’s, and compare the results to those shown in Fig-\\nure 14.26.\\nEx. 14.10 Derive the solution to the aﬃne-invariant average problem (14.60).\\nApply it to the three S’s, and compare the results to those computed in\\nExercise 14.9.\\nEx. 14.11 Classical multidimensional scaling. LetSbe the centered in-\\nner product matrix with elements ⟨xi−¯x,xj−¯x⟩. Let λ1> λ2>≤≤≤>\\nλkbe the klargest eigenvalues of S, with associated eigenvectors Ek=\\n(e1,e2,... ,ek). Let Dkbe a diagonal matrix with diagonal entries√λ1,√λ2,... ,√λk. Show that the solutions zito the classical scaling problem\\n(14.100) are the rowsofEkDk.\\nEx. 14.12 Consider the sparse PCA criterion (14.71).\\n1. Show that with Θﬁxed, solving for Vamounts to Kseparate elastic-\\nnet regression problems, with responses the Kelements of ΘTxi.\\n2. Show that with Vﬁxed, solving for Θamounts to a reduced-rank\\nversion of the Procrustes problem, which reduces to\\nmax\\nΘtrace(ΘTM) subject to ΘTΘ=IK, (14.116)\\nwhere MandΘare both p×KwithK≤p. IfM=UDQTis the\\nSVD of M, show that the optimal Θ=UQT.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='18187765-6f3d-453d-b7d2-0457d15e238d', embedding=None, metadata={'page_label': '601', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='582 14. Unsupervised Learning\\nEx. 14.13 Generate 200 data points with three features, lying close to a\\nhelix. In detail, deﬁne X1= cos( s) + 0.1≤Z1,X2= sin( s) + 0.1≤Z2,X3=\\ns+ 0.1≤Z3where stakes on 200 equally spaced values between 0 and 2 π,\\nandZ1,Z2,Z3are independent and have standard Gaussian distributions.\\n(a) Fit a principal curve to the data and plot the estimated coordinate\\nfunctions. Compare them to the underlying functions cos( s),sin(s)\\nands.\\n(b) Fit a self-organizing map to the same data, and see if you can discover\\nthe helical shape of the original point cloud.\\nEx. 14.14 Pre- and post-multiply equation (14.81) by a diagonal matrix\\ncontaining the inverse variances of the Xj. Hence obtain an equivalent\\ndecomposition for the correlation matrix, in the sense that a simple scali ng\\nis applied to the matrix A.\\nEx. 14.15 Generate 200 observations of three variates X1,X2,X3according\\nto\\nX1∼Z1\\nX2=X1+ 0.001≤Z2\\nX3= 10 ≤Z3 (14.117)\\nwhere Z1,Z2,Z3are independent standard normal variates. Compute the\\nleading principal component and factor analysis directions. Hence show\\nthat the leading principal component aligns itself in the maximal variance\\ndirection X3, while the leading factor essentially ignores the uncorrelated\\ncomponent X3, and picks up the correlated component X2+X1(Geoﬀrey\\nHinton, personal communication).\\nEx. 14.16 Consider the kernel principal component procedure outlined in\\nSection 14.5.4. Argue that the number Mof principal components is equal\\nto the rank of K, which is the number of non-zero elements in D. Show\\nthat the mth component zm(mth column of Z) can be written (up to\\ncentering) as zim=∑N\\nj=1αjmK(xi,xj), where αjm=ujm/dm. Show that\\nthe mapping of a new observation x0to the mth component is given by\\nz0m=∑N\\nj=1αjmK(x0,xj).\\nEx. 14.17 Show that with g1(x) =∑N\\nj=1cjK(x,xj), the solution to (14.66)\\nis given by ˆ cj=uj1/d1, where u1is the ﬁrst column of Uin (14.65), and\\nd1the ﬁrst diagonal element of D. Show that the second and subsequent\\nprincipal component functions are deﬁned in a similar manner ( hint: see\\nSection 5.8.1.)\\nEx. 14.18 Consider the regularized log-likelihood for the density estimation\\nproblem arising in ICA,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e419cfe2-97c3-4a9d-86a7-f690d26e80dc', embedding=None, metadata={'page_label': '602', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 583\\n1\\nNN∑\\ni=1[logφ(si) +g(si)]−∫\\nφ(t)eg(t)dt−λ∫\\n{g′′′(t)}2(t)dt.(14.118)\\nThe solution ˆ gis a quartic smoothing spline, and can be written as ˆ g(s) =\\nˆq(s) + ˆq⊥(s), where qis a quadratic function (in the null space of the\\npenalty). Let q(s) =θ0+θ1s+θ2s2. By examining the stationarity condi-\\ntions for ˆθk, k= 1,2,3, show that the solution ˆf=φeˆgis a density, and\\nhas mean zero and variance one. If we used a second-derivative penalty∫\\n{g′′(t)}2(t)dtinstead, what simple modiﬁcation could we make to the\\nproblem to maintain the three moment conditions?\\nEx. 14.19 IfAisp×porthogonal, show that the ﬁrst term in (14.92) on\\npage 568\\np∑\\nj=1N∑\\ni=1logφ(aT\\njxi),\\nwithajthejth column of A, does not depend on A.\\nEx. 14.20 Fixed point algorithm for ICA (Hyv¨ arinen et al., 2001). Consider\\nmaximizing C(a) =E{g(aTX)}with respect to a, with ||a||= 1 and\\nCov(X) =I. Use a Lagrange multiplier to enforce the norm constraint,\\nand write down the ﬁrst two derivatives of the modiﬁed criterion. Use the\\napproximation\\nE{XXTg′′(aTX)} ≈E{XXT}E{g′′(aTX)}\\nto show that the Newton update can be written as the ﬁxed-point update\\n(14.96).\\nEx. 14.21 Consider an undirected graph with non-negative edge weights\\nwii′and graph Laplacian L. Suppose there are mconnected components\\nA1,A2,... ,A min the graph. Show that there are meigenvectors of Lcorre-\\nsponding to eigenvalue zero, and the indicator vectors of these components\\nIA1,IA2,... ,I Amspan the zero eigenspace.\\nEx. 14.22\\n(a) Show that deﬁnition (14.108) implies that the sum of the PageRanks\\npiisN, the number of web pages.\\n(b) Write a program to compute the PageRank solutions by the power\\nmethod using formulation (14.107). Apply it to the network of Fig-\\nure 14.47.\\nEx. 14.23 Algorithm for non-negative matrix factorization (Wu and Lange,\\n2007). A function g(x,y) to said to minorize a function f(x) if', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='668149e9-052a-4683-bb53-6e4a99aeff2f', embedding=None, metadata={'page_label': '603', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='584 14. Unsupervised Learning\\nPage 2Page 1\\nPage 3\\nPage 4\\nPage 5Page 6\\nFIGURE 14.47. Example of a small network.\\ng(x,y)≤f(x), g(x,x) =f(x) (14.119)\\nfor all x,yin the domain. This is useful for maximizing f(x) since it is easy\\nto show that f(x) is nondecreasing under the update\\nxs+1= argmaxxg(x,xs) (14.120)\\nThere are analogous deﬁnitions for majorization , for minimizing a function\\nf(x). The resulting algorithms are known as MMalgorithms, for “minorize-\\nmaximize” or “majorize-minimize” (Lange, 2004). It also can be shown\\nthat the EM algorithm (8.5) is an example of an MM algorithm: see Sec-\\ntion 8.5.3 and Exercise 8.2 for details.\\n(a) Consider maximization of the function L(W,H) in (14.73), written\\nhere without the matrix notation\\nL(W,H) =N∑\\ni=1p∑\\nj=1[\\nxijlog(r∑\\nk=1wikhkj)\\n−r∑\\nk=1wikhkj]\\n.\\nUsing the concavity of log( x), show that for any set of rvalues yk≥0\\nand 0 ≤ck≤1 with∑r\\nk=1ck= 1,\\nlog(r∑\\nk=1yk)\\n≥r∑\\nk=1cklog(yk/ck)\\nHence\\nlog(r∑\\nk=1wikhkj)\\n≥r∑\\nk=1as\\nikj\\nbs\\nijlog(\\nbs\\nij\\nas\\nikjwikhkj)\\n,\\nwhere\\nas\\nikj=ws\\nikhs\\nkjandbs\\nij=r∑\\nk=1ws\\nikhs\\nkj,\\nandsindicates the current iteration.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b03ddd2e-fded-4c61-8c89-ca632bfe62a0', embedding=None, metadata={'page_label': '604', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 585\\n(b) Hence show that, ignoring constants, the function\\ng(W,H|Ws,Hs) =N∑\\ni=1p∑\\nj=1r∑\\nk=1uijas\\nikj\\nbs\\nij(\\nlogwik+ loghkj)\\n−N∑\\ni=1p∑\\nj=1r∑\\nk=1wikhkj\\nminorizes L(W,H).\\n(c) Set the partial derivatives of g(W,H|Ws,Hs) to zero and hence\\nderive the updating steps (14.74).\\nEx. 14.24 Consider the non-negative matrix factorization (14.72) in the\\nrank one case ( r= 1).\\n(a) Show that the updates (14.74) reduce to\\nwi←wi∑p\\nj=1xij∑p\\nj=1wihj\\nhj←hj∑N\\ni=1xij∑N\\ni=1wihj(14.121)\\nwhere wi=wi1,hj=h1j. This is an example of the iterative pro-\\nportional scaling procedure, applied to the independence model for a\\ntwo-way contingency table (Fienberg, 1977, for example).\\n(b) Show that the ﬁnal iterates have the explicit form\\nwi=c≤∑p\\nj=1xij∑N\\ni=1∑p\\nj=1xij, h k=1\\nc≤∑N\\ni=1xik∑N\\ni=1∑p\\nj=1xij(14.122)\\nfor any constant c >0. These are equivalent to the usual row and\\ncolumn estimates for a two-way independence model.\\nEx. 14.25 Fit a non-negative matrix factorization model to the collection\\nof two’s in the digits database. Use 25 basis elements, and compare with a\\n24- component (plus mean) PCA model. In both cases display the Wand\\nHmatrices as in Figure 14.33.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='629cfc39-ed37-4068-a7a2-506d53ed94cb', embedding=None, metadata={'page_label': '605', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='586 14. Unsupervised Learning', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c825d2ad-4136-47ff-a261-a67a5299a9e5', embedding=None, metadata={'page_label': '606', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 587\\nPrinter: Opaque this\\n15\\nRandom Forests\\n15.1 Introduction\\nBagging or bootstrap aggregation (section 8.7) is a technique for reducing\\nthe variance of an estimated prediction function. Bagging seems to work\\nespecially well for high-variance, low-bias procedures, such as trees. For\\nregression, we simply ﬁt the same regression tree many times to bootstra p-\\nsampled versions of the training data, and average the result. For classiﬁ-\\ncation, a committee of trees each cast a vote for the predicted class.\\nBoosting in Chapter 10 was initially proposed as a committee method as\\nwell, although unlike bagging, the committee of weak learners evolves over\\ntime, and the members cast a weighted vote. Boosting appears to dominate\\nbagging on most problems, and became the preferred choice.\\nRandom forests (Breiman, 2001) is a substantial modiﬁcation of bagging\\nthat builds a large collection of de-correlated trees, and then averages them.\\nOn many problems the performance of random forests is very similar to\\nboosting, and they are simpler to train and tune. As a consequence, random\\nforests are popular, and are implemented in a variety of packages.\\n15.2 Deﬁnition of Random Forests\\nThe essential idea in bagging (Section 8.7) is to average many noisy but\\napproximately unbiased models, and hence reduce the variance. Trees are\\nideal candidates for bagging, since they can capture complex interaction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44490cfb-59ec-4460-99e1-74282e0143d5', embedding=None, metadata={'page_label': '607', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='588 15. Random Forests\\nAlgorithm 15.1 Random Forest for Regression or Classiﬁcation.\\n1. For b= 1 to B:\\n(a) Draw a bootstrap sample Z∗of size Nfrom the training data.\\n(b) Grow a random-forest tree Tbto the bootstrapped data, by re-\\ncursively repeating the following steps for each terminal node of\\nthe tree, until the minimum node size nminis reached.\\ni. Select mvariables at random from the pvariables.\\nii. Pick the best variable/split-point among the m.\\niii. Split the node into two daughter nodes.\\n2. Output the ensemble of trees {Tb}B\\n1.\\nTo make a prediction at a new point x:\\nRegression: ˆfB\\nrf(x) =1\\nB∑B\\nb=1Tb(x).\\nClassiﬁcation: LetˆCb(x) be the class prediction of the bth random-forest\\ntree. Then ˆCB\\nrf(x) =majority vote {ˆCb(x)}B\\n1.\\nstructures in the data, and if grown suﬃciently deep, have relatively low\\nbias. Since trees are notoriously noisy, they beneﬁt greatly from the averag-\\ning. Moreover, since each tree generated in bagging is identically distributed\\n(i.d.), the expectation of an average of Bsuch trees is the same as the ex-\\npectation of any one of them. This means the bias of bagged trees is the\\nsame as that of the individual trees, and the only hope of improvement is\\nthrough variance reduction. This is in contrast to boosting, where the trees\\nare grown in an adaptive way to remove bias, and hence are not i.d.\\nAn average of Bi.i.d. random variables, each with variance σ2, has vari-\\nance1\\nBσ2. If the variables are simply i.d. (identically distributed, but not\\nnecessarily independent) with positive pairwise correlation ρ, the variance\\nof the average is (Exercise 15.1)\\nρσ2+1−ρ\\nBσ2. (15.1)\\nAsBincreases, the second term disappears, but the ﬁrst remains, and\\nhence the size of the correlation of pairs of bagged trees limits the beneﬁts\\nof averaging. The idea in random forests (Algorithm 15.1) is to improve\\nthe variance reduction of bagging by reducing the correlation between the\\ntrees, without increasing the variance too much. This is achieved in the\\ntree-growing process through random selection of the input variables.\\nSpeciﬁcally, when growing a tree on a bootstrapped dataset:\\nBefore each split, select m≤pof the input variables at random\\nas candidates for splitting.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e0cf27b7-fdce-48cd-92e7-43301c4c82d3', embedding=None, metadata={'page_label': '608', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.2 Deﬁnition of Random Forests 589\\nTypically values for mare√por even as low as 1.\\nAfter Bsuch trees {T(x;Θb)}B\\n1are grown, the random forest (regression)\\npredictor is\\nˆfB\\nrf(x) =1\\nBB∑\\nb=1T(x;Θb). (15.2)\\nAs in Section 10.9 (page 356), Θ bcharacterizes the bth random forest tree in\\nterms of split variables, cutpoints at each node, and terminal-node values.\\nIntuitively, reducing mwill reduce the correlation between any pair of trees\\nin the ensemble, and hence by (15.1) reduce the variance of the average.\\n0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060 0.065 0.070Spam Data\\nNumber of TreesTest ErrorBagging\\nRandom Forest\\nGradient Boosting (5 Node)\\nFIGURE 15.1. Bagging, random forest, and gradient boosting, applied to the\\nspam data. For boosting, 5-node trees were used, and the number of trees were\\nchosen by 10-fold cross-validation ( 2500trees). Each “step” in the ﬁgure corre-\\nsponds to a change in a single misclassiﬁcation (in a test set of 1536).\\nNot all estimators can be improved by shaking up the data like this.\\nIt seems that highly nonlinear estimators, such as trees, beneﬁt the most.\\nFor bootstrapped trees, ρis typically small (0 .05 or lower is typical; see\\nFigure 15.9), while σ2is not much larger than the variance for the original\\ntree. On the other hand, bagging does not change linear estimates, such\\nas the sample mean (hence its variance either); the pairwise correlation\\nbetween bootstrapped means is about 50% (Exercise 15.4).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e2d3fcb6-e755-423c-b1d1-e969dcab2fd2', embedding=None, metadata={'page_label': '609', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='590 15. Random Forests\\nRandom forests are popular. Leo Breiman’s1collaborator Adele Cutler\\nmaintains a random forest website2where the software is freely available,\\nwith more than 3000 downloads reported by 2002. There is a randomForest\\npackage in R, maintained by Andy Liaw, available from the CRANwebsite.\\nThe authors make grand claims about the success of random forests:\\n“most accurate,” “most interpretable,” and the like. In our experience ran-\\ndom forests do remarkably well, with very little tuning required. A ran-\\ndom forest classiﬁer achieves 4 .88% misclassiﬁcation error on the spamtest\\ndata, which compares well with all other methods, and is not signiﬁcantly\\nworse than gradient boosting at 4 .5%. Bagging achieves 5 .4% which is\\nsigniﬁcantly worse than either (using the McNemar test outlined in Ex-\\nercise 10.6), so it appears on this example the additional randomization\\nhelps.\\nRF−1 RF−3 Bagging GBM−1 GBM−60.00 0.05 0.10 0.15Nested SpheresTest Misclassification Error\\nBayes Error\\nFIGURE 15.2. The results of 50simulations from the “nested spheres” model in\\nI R10. The Bayes decision boundary is the surface of a sphere (addit ive). “RF-3”\\nrefers to a random forest with m= 3, and “GBM-6” a gradient boosted model\\nwith interaction order six; similarly for “RF-1” and “GBM-1.” Th e training sets\\nwere of size 2000, and the test sets 10,000.\\nFigure 15.1 shows the test-error progression on 2500 trees for the three\\nmethods. In this case there is some evidence that gradient boosting has\\nstarted to overﬁt, although 10-fold cross-validation chose all 2500 tr ees.\\n1Sadly, Leo Breiman died in July, 2005.\\n2http://www.math.usu.edu/ ∼adele/forests/', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='69f7beb0-0d98-450f-a231-2920ba652010', embedding=None, metadata={'page_label': '610', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.2 Deﬁnition of Random Forests 591\\n0 200 400 600 800 10000.32 0.34 0.36 0.38 0.40 0.42 0.44California Housing Data\\nNumber of TreesTest Average Absolute ErrorRF m=2\\nRF m=6\\nGBM depth=4\\nGBM depth=6\\nFIGURE 15.3. Random forests compared to gradient boosting on the California\\nhousing data. The curves represent mean absolute error on the t est data as a\\nfunction of the number of trees in the models. Two random forests are shown, with\\nm= 2andm= 6. The two gradient boosted models use a shrinkage parameter\\nν= 0.05in (10.41), and have interaction depths of 4and6. The boosted models\\noutperform random forests.\\nFigure 15.2 shows the results of a simulation3comparing random forests\\nto gradient boosting on the nested spheres problem [Equation (10.2) in\\nChapter 10]. Boosting easily outperforms random forests here. Notice that\\nsmaller mis better here, although part of the reason could be that the true\\ndecision boundary is additive.\\nFigure 15.3 compares random forests to boosting (with shrinkage) in a\\nregression problem, using the California housing data (Section 10.14.1).\\nTwo strong features that emerge are\\n•Random forests stabilize at about 200 trees, while at 1000 trees boost-\\ning continues to improve. Boosting is slowed down by the shrinkage,\\nas well as the fact that the trees are much smaller.\\n•Boosting outperforms random forests here. At 1000 terms, the weaker\\nboosting model (GBM depth 4) has a smaller error than the stronger\\n3Details: The random forests were ﬁt using the R package randomForest 4.5-11 ,\\nwith 500 trees. The gradient boosting models were ﬁt using R p ackagegbm 1.5 , with\\nshrinkage parameter set to 0.05, and 2000 trees.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a0fb87f7-69b2-4a81-bd91-3a84ce4c27d0', embedding=None, metadata={'page_label': '611', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='592 15. Random Forests\\n0 500 1000 1500 2000 25000.045 0.055 0.065 0.075\\nNumber of TreesMisclassification ErrorOOB Error\\nTest Error\\nFIGURE 15.4. ooberror computed on the spamtraining data, compared to the\\ntest error computed on the test set.\\nrandom forest (RF m= 6); a Wilcoxon test on the mean diﬀerences\\nin absolute errors has a p-value of 0 .007. For larger mthe random\\nforests performed no better.\\n15.3 Details of Random Forests\\nWe have glossed over the distinction between random forests for classiﬁca-\\ntion versus regression. When used for classiﬁcation, a random forest obtains\\na class vote from each tree, and then classiﬁes using majority vote (see Sec-\\ntion 8.7 on bagging for a similar discussion). When used for regression, the\\npredictions from each tree at a target point xare simply averaged, as in\\n(15.2). In addition, the inventors make the following recommendations:\\n•For classiﬁcation, the default value for mis⌊√p⌋and the minimum\\nnode size is one.\\n•For regression, the default value for mis⌊p/3⌋and the minimum\\nnode size is ﬁve.\\nIn practice the best values for these parameters will depend on the problem,\\nand they should be treated as tuning parameters. In Figure 15.3 the m= 6\\nperforms much better than the default value ⌊8/3⌋= 2.\\n15.3.1 Out of Bag Samples\\nAn important feature of random forests is its use of out-of-bag (oob) sam-\\nples:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='10666d8e-331d-42ec-ad5d-93a4ae4f673d', embedding=None, metadata={'page_label': '612', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.3 Details of Random Forests 593\\nFor each observation zi= (xi,yi), construct its random forest\\npredictor by averaging onlythose trees corresponding to boot-\\nstrap samples in which zidid not appear.\\nAnooberror estimate is almost identical to that obtained by N-fold cross-\\nvalidation; see Exercise 15.2. Hence unlike many other nonlinear estimators,\\nrandom forests can be ﬁt in one sequence, with cross-validation being per-\\nformed along the way. Once the ooberror stabilizes, the training can be\\nterminated.\\nFigure 15.4 shows the oobmisclassiﬁcation error for the spamdata, com-\\npared to the test error. Although 2500 trees are averaged here, it appears\\nfrom the plot that about 200 would be suﬃcient.\\n15.3.2 Variable Importance\\nVariable importance plots can be constructed for random forests in exactly\\nthe same way as they were for gradient-boosted models (Section 10.13).\\nAt each split in each tree, the improvement in the split-criterion is the\\nimportance measure attributed to the splitting variable, and is accumulated\\nover all the trees in the forest separately for each variable. The left plot\\nof Figure 15.5 shows the variable importances computed in this way for\\nthespamdata; compare with the corresponding Figure 10.6 on page 354 for\\ngradient boosting. Boosting ignores some variables completely, while the\\nrandom forest does not. The candidate split-variable selection increases\\nthe chance that any single variable gets included in a random forest, while\\nno such selection occurs with boosting.\\nRandom forests also use the oobsamples to construct a diﬀerent variable-\\nimportance measure, apparently to measure the prediction strength of each\\nvariable. When the bth tree is grown, the oobsamples are passed down\\nthe tree, and the prediction accuracy is recorded. Then the values for the\\njth variable are randomly permuted in the oobsamples, and the accuracy\\nis again computed. The decrease in accuracy as a result of this permuting\\nis averaged over all trees, and is used as a measure of the importance of\\nvariable jin the random forest. These are expressed as a percent of the\\nmaximum in the right plot in Figure 15.5. Although the rankings of the\\ntwo methods are similar, the importances in the right plot are more uni-\\nform over the variables. The randomization eﬀectively voids the eﬀect of\\na variable, much like setting a coeﬃcient to zero in a linear model (Exer-\\ncise 15.7). This does not measure the eﬀect on prediction were this variable\\nnot available, because if the model was reﬁtted without the variable, other\\nvariables could be used as surrogates.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d084ec22-25f7-475a-a575-28624637828a', embedding=None, metadata={'page_label': '613', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='594 15. Random Forests\\n!$removefreeCAPAVEyourCAPMAXhpCAPTOTmoneyouryougeorge000eduhplbusiness1999internet(willallemailrereceiveovermail;650meetinglabsorderaddresspmpeoplemake#creditfontdatatechnology85[labtelnetreportoriginalprojectconferencedirect415857addresses3dcspartstableGini\\n0 20 40 60 80 100\\nVariable Importance!remove$CAPAVEhpfreeCAPMAXedugeorgeCAPTOTyourour1999reyouhplbusiness000meetingmoney(willinternet650pmreceiveoveremail;fontmailtechnologyorderalllabs[85addressoriginallabtelnetpeopleprojectdatacreditconference857#415makecsreportdirectaddresses3dpartstableRandomization\\n0 20 40 60 80 100\\nVariable Importance\\nFIGURE 15.5. Variable importance plots for a classiﬁcation random forest\\ngrown on the spamdata. The left plot bases the importance on the Gini split-\\nting index, as in gradient boosting. The rankings compare well with t he rankings\\nproduced by gradient boosting (Figure 10.6 on page 354). The ri ght plot uses oob\\nrandomization to compute variable importances, and tends to spr ead the impor-\\ntances more uniformly.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b38fc89c-5f22-48e6-806f-4669ea10f9b3', embedding=None, metadata={'page_label': '614', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.3 Details of Random Forests 595\\nProximity Plot\\n12\\n34\\n5\\n6Random Forest Classifier\\n123456\\nDimension 1Dimension 2\\nX1X2\\nFIGURE 15.6. (Left): Proximity plot for a random forest classiﬁer grown to\\nthe mixture data. (Right): Decision boundary and training data fo r random forest\\non mixture data. Six points have been identiﬁed in each plot.\\n15.3.3 Proximity Plots\\nOne of the advertised outputs of a random forest is a proximity plot . Fig-\\nure 15.6 shows a proximity plot for the mixture data deﬁned in Section 2.3.3\\nin Chapter 2. In growing a random forest, an N×Nproximity matrix is\\naccumulated for the training data. For every tree, any pair of oobobser-\\nvations sharing a terminal node has their proximity increased by one. This\\nproximity matrix is then represented in two dimensions using multidimen-\\nsional scaling (Section 14.8). The idea is that even though the data may be\\nhigh-dimensional, involving mixed variables, etc., the proximity plot gives\\nan indication of which observations are eﬀectively close together in the eyes\\nof the random forest classiﬁer.\\nProximity plots for random forests often look very similar, irrespect ive of\\nthe data, which casts doubt on their utility. They tend to have a star shape,\\none arm per class, which is more pronounced the better the classiﬁcation\\nperformance.\\nSince the mixture data are two-dimensional, we can map points from the\\nproximity plot to the original coordinates, and get a better understanding of\\nwhat they represent. It seems that points in pure regions class-wise map to\\nthe extremities of the star, while points nearer the decision boundaries map\\nnearer the center. This is not surprising when we consider the construction\\nof the proximity matrices. Neighboring points in pure regions will often\\nend up sharing a bucket, since when a terminal node is pure, it is no longer', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c79d9b5a-b665-4309-9929-2754a24dcf6e', embedding=None, metadata={'page_label': '615', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='596 15. Random Forests\\nsplit by a random forest tree-growing algorithm. On the other hand, pairs\\nof points that are close but belong to diﬀerent classes will sometimes share\\na terminal node, but not always.\\n15.3.4 Random Forests and Overﬁtting\\nWhen the number of variables is large, but the fraction of relevant variables\\nsmall, random forests are likely to perform poorly with small m. At each\\nsplit the chance can be small that the relevant variables will be selected.\\nFigure 15.7 shows the results of a simulation that supports this claim. De-\\ntails are given in the ﬁgure caption and Exercise 15.3. At the top of each\\npair we see the hyper-geometric probability that a relevant variable will be\\nselected at any split by a random forest tree (in this simulation, the relevant\\nvariables are all equal in stature). As this probability gets small, the ga p\\nbetween boosting and random forests increases. When the number of rele-\\nvant variables increases, the performance of random forests is surprisingly\\nrobust to an increase in the number of noise variables. For example, with 6\\nrelevant and 100 noise variables, the probability of a relevant variable bei ng\\nselected at any split is 0.46, assuming m=√\\n(6 + 100) ≈10. According to\\nFigure 15.7, this does not hurt the performance of random forests compared\\nwith boosting. This robustness is largely due to the relative insensitivity of\\nmisclassiﬁcation cost to the bias and variance of the probability estimates\\nin each tree. We consider random forests for regression in the next section.\\nAnother claim is that random forests “cannot overﬁt” the data. It is\\ncertainly true that increasing Bdoes not cause the random forest sequence\\nto overﬁt; like bagging, the random forest estimate (15.2) approximates t he\\nexpectation\\nˆfrf(x) = E ΘT(x;Θ) = lim\\nB→∞ˆf(x)B\\nrf (15.3)\\nwith an average over Brealizations of Θ. The distribution of Θ here is con-\\nditional on the training data. However, this limit can overﬁt the data ; the\\naverage of fully grown trees can result in too rich a model, and incur unnec-\\nessary variance. Segal (2004) demonstrates small gains in performance by\\ncontrolling the depths of the individual trees grown in random forests. Our\\nexperience is that using full-grown trees seldom costs much, and results in\\none less tuning parameter.\\nFigure 15.8 shows the modest eﬀect of depth control in a simple regression\\nexample. Classiﬁers are less sensitive to variance, and this eﬀect of over-\\nﬁtting is seldom seen with random-forest classiﬁcation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3c27043a-8e18-4c35-96be-c624264dffcd', embedding=None, metadata={'page_label': '616', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.4 Analysis of Random Forests 597Test Misclassification Error\\n0.10 0.15 0.20 0.25 0.30Bayes Error\\n(2, 5) (2, 25) (2, 50) (2, 100) (2, 150)\\nNumber of (Relevant, Noise) Variables0.52 0.34 0.25 0.19 0.15\\nRandom Forest\\nGradient Boosting\\nFIGURE 15.7. A comparison of random forests and gradient boosting on prob-\\nlems with increasing numbers of noise variables. In each case the true decision\\nboundary depends on two variables, and an increasing number of noise variables\\nare included. Random forests uses its default value m=√p. At the top of each\\npair is the probability that one of the relevant variables is ch osen at any split.\\nThe results are based on 50simulations for each pair, with a training sample of\\n300, and a test sample of 500.\\n15.4 Analysis of Random Forests\\nIn this section we analyze the mechanisms at play with the additional\\nrandomization employed by random forests. For this discussion we focus\\non regression and squared error loss, since this gets at the main points,\\nand bias and variance are more complex with 0–1 loss (see Section 7.3.1).\\nFurthermore, even in the case of a classiﬁcation problem, we can consider\\nthe random-forest average as an estimate of the class posterior probabilit ies,\\nfor which bias and variance are appropriate descriptors.\\n15.4.1 Variance and the De-Correlation Eﬀect\\nThe limiting form ( B→ ∞) of the random forest regression estimator is\\nˆfrf(x) = E Θ|ZT(x;Θ(Z)), (15.4)\\nwhere we have made explicit the dependence on the training data Z. Here\\nwe consider estimation at a single target point x. From (15.1) we see that', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5e2215df-15a4-4a12-aaf4-29dc70b2b505', embedding=None, metadata={'page_label': '617', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='598 15. Random Forests\\n50 30 20 10 51.00 1.05 1.10\\nMinimum Node SizeMean Squared Test ErrorShallow Deep\\nFIGURE 15.8. The eﬀect of tree size on the error in random forest regres-\\nsion. In this example, the true surface was additive in two of th e12variables,\\nplus additive unit-variance Gaussian noise. Tree depth is contr olled here by the\\nminimum node size; the smaller the minimum node size, the deeper t he trees.\\nVarˆfrf(x) =ρ(x)σ2(x). (15.5)\\nHere\\n•ρ(x) is the sampling correlation between any pair of trees used in the\\naveraging:\\nρ(x) = corr[ T(x;Θ1(Z)),T(x;Θ2(Z))], (15.6)\\nwhere Θ 1(Z) and Θ 2(Z) are a randomly drawn pair of random forest\\ntrees grown to the randomly sampled Z;\\n•σ2(x) is the sampling variance of any single randomly drawn tree,\\nσ2(x) = Var T(x;Θ(Z)). (15.7)\\nIt is easy to confuse ρ(x) with the average correlation between ﬁtted trees\\nin agiven random-forest ensemble; that is, think of the ﬁtted trees as N-\\nvectors, and compute the average pairwise correlation between these vec-\\ntors, conditioned on the data. This is notthe case; this conditional corre-\\nlation is not directly relevant in the averaging process, and the dependence\\nonxinρ(x) warns us of the distinction. Rather, ρ(x) is the theoretical\\ncorrelation between a pair of random-forest trees evaluated at x, induced\\nby repeatedly making training sample draws Zfrom the population, and\\nthen drawing a pair of random forest trees. In statistical jargon, this is the\\ncorrelation induced by the sampling distribution ofZand Θ.\\nMore precisely, the variability averaged over in the calculations in (15.6)\\nand (15.7) is both', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c505c82-0ecd-4d6f-b364-4b5a92eb5f8a', embedding=None, metadata={'page_label': '618', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.4 Analysis of Random Forests 599\\n•conditional on Z: due to the bootstrap sampling and feature sampling\\nat each split, and\\n•a result of the sampling variability of Zitself.\\nIn fact, the conditional covariance of a pair of tree ﬁts at xis zero, because\\nthe bootstrap and feature sampling is i.i.d; see Exercise 15.5.\\n1 4 7 13 19 25 31 37 43 490.00 0.02 0.04 0.06 0.08\\nNumber of Randomly Selected Splitting Variables mCorrelation between Trees\\nFIGURE 15.9. Correlations between pairs of trees drawn by a random-forest\\nregression algorithm, as a function of m. The boxplots represent the correlations\\nat600randomly chosen prediction points x.\\nThe following demonstrations are based on a simulation model\\nY=1√\\n5050∑\\nj=1Xj+ε, (15.8)\\nwith all the Xjandεiid Gaussian. We use 500 training sets of size 100, and\\na single set of test locations of size 600. Since regression trees are nonlinear\\ninZ, the patterns we see below will diﬀer somewhat depending on the\\nstructure of the model.\\nFigure 15.9 shows how the correlation (15.6) between pairs of trees de-\\ncreases as mdecreases: pairs of tree predictions at xfor diﬀerent training\\nsetsZare likely to be less similar if they do not use the same splitting\\nvariables.\\nIn the left panel of Figure 15.10 we consider the variances of single tree\\npredictors, Var T(x;Θ(Z)) (averaged over 600 prediction points xdrawn\\nrandomly from our simulation model). This is the total variance, and can be', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9193e3a2-a8f2-41de-a6d1-197f6fcdbe14', embedding=None, metadata={'page_label': '619', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='600 15. Random Forests\\ndecomposed into two parts using standard conditional variance arguments\\n(see Exercise 15.5):\\nVarΘ,ZT(x;Θ(Z)) = Var ZEΘ|ZT(x;Θ(Z)) + E ZVarΘ|ZT(x;Θ(Z))\\nTotal Variance = Var Zˆfrf(x) + within- ZVariance\\n(15.9)\\nThe second term is the within- Zvariance—a result of the randomization,\\nwhich increases as mdecreases. The ﬁrst term is in fact the sampling vari-\\nance of the random forest ensemble (shown in the right panel), which de-\\ncreases as mdecreases. The variance of the individual trees does not change\\nappreciably over much of the range of m, hence in light of (15.5), the vari-\\nance of the ensemble is dramatically lower than this tree variance.\\n0 10 20 30 40 501.80 1.85 1.90 1.95Single Tree\\nmVariance\\nWithin Z\\nTotal\\n0 10 20 30 40 500.65 0.70 0.75 0.80 0.85Random Forest Ensemble\\nmMean Squared Error and Squared Bias\\nVariance\\nMean Squared Error\\nSquared Bias\\nVariance\\n0.0 0.05 0.10 0.15 0.20\\nFIGURE 15.10. Simulation results. The left panel shows the average variance o f\\na single random forest tree, as a function of m. “Within Z” refers to the average\\nwithin-sample contribution to the variance, resulting from the bootstrap sampling\\nand split-variable sampling (15.9). “Total” includes the samp ling variability of\\nZ. The horizontal line is the average variance of a single fully gro wn tree (with-\\nout bootstrap sampling). The right panel shows the average mea n-squared error,\\nsquared bias and variance of the ensemble, as a function of m. Note that the\\nvariance axis is on the right (same scale, diﬀerent level). The h orizontal line is\\nthe average squared-bias of a fully grown tree.\\n15.4.2 Bias\\nAs in bagging, the bias of a random forest is the same as the bias of any\\nof the individual sampled trees T(x;Θ(Z)):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b66f94c9-4848-41c0-ad0c-f39abeea1564', embedding=None, metadata={'page_label': '620', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.4 Analysis of Random Forests 601\\nBias(x) = θ(x)−EZˆfrf(x)\\n=θ(x)−EZEΘ|ZT(x;Θ(Z)). (15.10)\\nThis is also typically greater (in absolute terms) than the bias of an un-\\npruned tree grown to Z, since the randomization and reduced sample space\\nimpose restrictions. Hence the improvements in prediction obtained by bag-\\nging or random forests are solely a result of variance reduction .\\nAny discussion of bias depends on the unknown true function. Fig-\\nure 15.10 (right panel) shows the squared bias for our additive model simu-\\nlation (estimated from the 500 realizations). Although for diﬀerent model s\\nthe shape and rate of the bias curves may diﬀer, the general trend is that\\nasmdecreases, the bias increases. Shown in the ﬁgure is the mean-squared\\nerror, and we see a classical bias-variance trade-oﬀ in the choice of m. For\\nallmthe squared bias of the random forest is greater than that for a single\\ntree (horizontal line).\\nThese patterns suggest a similarity with ridge regression (Section 3.4.1) .\\nRidge regression is useful (in linear models) when one has a large number\\nof variables with similarly sized coeﬃcients; ridge shrinks their coeﬃcients\\ntoward zero, and those of strongly correlated variables toward each other.\\nAlthough the size of the training sample might not permit all the variables\\nto be in the model, this regularization via ridge stabilizes the model and al-\\nlows all the variables to have their say (albeit diminished). Random forests\\nwith small mperform a similar averaging. Each of the relevant variables\\nget their turn to be the primary split, and the ensemble averaging reduces\\nthe contribution of any individual variable. Since this simulation exam-\\nple (15.8) is based on a linear model in all the variables, ridge regression\\nachieves a lower mean-squared error (about 0 .45 with df( λopt)≈29).\\n15.4.3 Adaptive Nearest Neighbors\\nThe random forest classiﬁer has much in common with the k-nearest neigh-\\nbor classiﬁer (Section 13.3); in fact a weighted version thereof. Since each\\ntree is grown to maximal size, for a particular Θ∗,T(x;Θ∗(Z)) is the re-\\nsponse value for one of the training samples4. The tree-growing algorithm\\nﬁnds an “optimal” path to that observation, choosing the most informative\\npredictors from those at its disposal. The averaging process assigns weight s\\nto these training responses, which ultimately vote for the prediction. Hence\\nvia the random-forest voting mechanism, those observations close to the\\ntarget point get assigned weights—an equivalent kernel—which combine to\\nform the classiﬁcation decision.\\nFigure 15.11 demonstrates the similarity between the decision boundary\\nof 3-nearest neighbors and random forests on the mixture data.\\n4We gloss over the fact that pure nodes are not split further, a nd hence there can be\\nmore than one observation in a terminal node', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8cb2113a-a880-48b4-935c-936a89a0ac36', embedding=None, metadata={'page_label': '621', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='602 15. Random Forests\\nRandom Forest Classifier\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo oooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.000\\nTest Error:       0.238\\nBayes Error:    0.2103−Nearest Neighbors\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo oooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.130\\nTest Error:       0.242\\nBayes Error:    0.210\\nFIGURE 15.11. Random forests versus 3-NN on the mixture data. The axis-ori-\\nented nature of the individual trees in a random forest lead to dec ision regions\\nwith an axis-oriented ﬂavor.\\nBibliographic Notes\\nRandom forests as described here were introduced by Breiman (2001), al-\\nthough many of the ideas had cropped up earlier in the literature in dif-\\nferent forms. Notably Ho (1995) introduced the term “random forest,” and\\nused a consensus of trees grown in random subspaces of the features. The\\nidea of using stochastic perturbation and averaging to avoid overﬁtting was\\nintroduced by Kleinberg (1990), and later in Kleinberg (1996). Amit and\\nGeman (1997) used randomized trees grown on image features for image\\nclassiﬁcation problems. Breiman (1996a) introduced bagging, a precursor\\nto his version of random forests. Dietterich (2000b) also proposed an im -\\nprovement on bagging using additional randomization. His approach was\\nto rank the top 20 candidate splits at each node, and then select from the\\nlist at random. He showed through simulations and real examples that this\\nadditional randomization improved over the performance of bagging. Fried-\\nman and Hall (2007) showed that sub-sampling (without replacement) is\\nan eﬀective alternative to bagging. They showed that growing and aver-\\naging trees on samples of size N/2 is approximately equivalent (in terms\\nbias/variance considerations) to bagging, while using smaller fractions of\\nNreduces the variance even further (through decorrelation).\\nThere are several free software implementations of random forests. In\\nthis chapter we used the randomForest package in R, maintained by Andy\\nLiaw, available from the CRANwebsite. This allows both split-variable se-\\nlection, as well as sub-sampling. Adele Cutler maintains a random forest\\nwebsitehttp://www.math.usu.edu/ ∼adele/forests/ where (as of Au-\\ngust 2008) the software written by Leo Breiman and Adele Cutler is freely', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6aafa94-0b66-4cf8-a7c5-a60840c19870', embedding=None, metadata={'page_label': '622', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 603\\navailable. Their code, and the name “random forests”, is exclusively li-\\ncensed to Salford Systems for commercial release. The Wekamachine learn-\\ning archive http://www.cs.waikato.ac.nz/ml/weka/ at Waikato Univer-\\nsity, New Zealand, oﬀers a free javaimplementation of random forests.\\nExercises\\nEx. 15.1 Derive the variance formula (15.1). This appears to fail if ρis\\nnegative; diagnose the problem in this case.\\nEx. 15.2 Show that as the number of bootstrap samples Bgets large, the\\nooberror estimate for a random forest approaches its N-fold CV error\\nestimate, and that in the limit, the identity is exact.\\nEx. 15.3 Consider the simulation model used in Figure 15.7 (Mease and\\nWyner, 2008). Binary observations are generated with probabilities\\nPr(Y= 1|X) =q+ (1−2q)≤1\\uf8ee\\n\\uf8f0J∑\\nj=1Xj> J/2\\uf8f9\\n\\uf8fb, (15.11)\\nwhere X∼U[0,1]p, 0≤q≤1\\n2, and J≤pis some predeﬁned (even)\\nnumber. Describe this probability surface, and give the Bayes error rate.\\nEx. 15.4 Suppose xi, i= 1,... ,N are iid ( θ,σ2). Let ¯ x∗\\n1and ¯x∗\\n2be two\\nbootstrap realizations of the sample mean. Show that the sampling cor-\\nrelation corr(¯ x∗\\n1,¯x∗\\n2) =n\\n2n−1≈50%. Along the way, derive var(¯ x∗\\n1) and\\nthe variance of the bagged mean ¯ xbag. Here ¯ xis alinear statistic; bagging\\nproduces no reduction in variance for linear statistics.\\nEx. 15.5 Show that the sampling correlation between a pair of random-\\nforest trees at a point xis given by\\nρ(x) =VarZ[EΘ|ZT(x;Θ(Z))]\\nVarZ[EΘ|ZT(x;Θ(Z))] + E ZVarΘ|Z[T(x,Θ(Z)]. (15.12)\\nThe term in the numerator is Var Z[ˆfrf(x)], and the second term in the\\ndenominator is the expected conditional variance due to the randomization\\nin random forests.\\nEx. 15.6 Fit a series of random-forest classiﬁers to the spamdata, to explore\\nthe sensitivity to the parameter m. Plot both the ooberror as well as the\\ntest error against a suitably chosen range of values for m.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='122f0190-d550-4dc8-a7e9-f2d6aebf0f71', embedding=None, metadata={'page_label': '623', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='604 15. Random Forests\\nEx. 15.7 Suppose we ﬁt a linear regression model to Nobservations with\\nresponse yiand predictors xi1,... ,x ip. Assume that all variables are stan-\\ndardized to have mean zero and standard deviation one. Let RSSbe the\\nmean-squared residual on the training data, and ˆβthe estimated coeﬃcient.\\nDenote by RSS∗\\njthe mean-squared residual on the training data using the\\nsame ˆβ, but with the Nvalues for the jth variable randomly permuted\\nbefore the predictions are calculated. Show that\\nEP[RSS∗\\nj−RSS] = 2ˆβ2\\nj, (15.13)\\nwhere E Pdenotes expectation with respect to the permutation distribution.\\nArgue that this is approximately true when the evaluations are done using\\nan independent test set.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1cad7443-682b-4a29-8a6d-6fdc39eb69f4', embedding=None, metadata={'page_label': '624', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 605\\nPrinter: Opaque this\\n16\\nEnsemble Learning\\n16.1 Introduction\\nThe idea of ensemble learning is to build a prediction model by combining\\nthe strengths of a collection of simpler base models. We have already seen\\na number of examples that fall into this category.\\nBagging in Section 8.7 and random forests in Chapter 15 are ensemble\\nmethods for classiﬁcation, where a committee of trees each cast a vote for\\nthe predicted class. Boosting in Chapter 10 was initially proposed as a\\ncommittee method as well, although unlike random forests, the committee\\nofweak learners evolves over time, and the members cast a weighted vote.\\nStacking (Section 8.8) is a novel approach to combining the strengths of\\na number of ﬁtted models. In fact one could characterize any dictionary\\nmethod, such as regression splines, as an ensemble method, with the basis\\nfunctions serving the role of weak learners.\\nBayesian methods for nonparametric regression can also be viewed as\\nensemble methods: a large number of candidate models are averaged with\\nrespect to the posterior distribution of their parameter settings (e.g. (Neal\\nand Zhang, 2006)).\\nEnsemble learning can be broken down into two tasks: developing a pop-\\nulation of base learners from the training data, and then combining them\\nto form the composite predictor. In this chapter we discuss boosting tech-\\nnology that goes a step further; it builds an ensemble model by conducting\\na regularized and supervised search in a high-dimensional space of weak\\nlearners.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0499dbdb-70fb-465b-9988-1f21290f2863', embedding=None, metadata={'page_label': '625', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='606 16. Ensemble Learning\\nAn early example of a learning ensemble is a method designed for multi-\\nclass classiﬁcation using error-correcting output codes (Dietterich and Bakiri,\\n1995, ECOC). Consider the 10-class digit classiﬁcation problem, and the\\ncoding matrix Cgiven in Table 16.1.\\nTABLE 16.1. Part of a 15-bit error-correcting coding matrix Cfor the 10-class\\ndigit classiﬁcation problem. Each column deﬁnes a two-class cl assiﬁcation prob-\\nlem.\\nDigit C1C2C3C4C5C6· · · C15\\n0 1 1 0 0 0 0 · · · 1\\n1 0 0 1 1 1 1 · · · 0\\n2 1 0 0 1 0 0 · · · 1\\n.....................· · ·...\\n8 1 1 0 1 0 1 · · · 1\\n9 0 1 1 1 0 0 · · · 0\\nNote that the ℓth column of the coding matrix Cℓdeﬁnes a two-class\\nvariable that merges all the original classes into two groups. The method\\nworks as follows:\\n1. Learn a separate classiﬁer for each of the L= 15 two class problems\\ndeﬁned by the columns of the coding matrix.\\n2. At a test point x, let ˆpℓ(x) be the predicted probability of a one for\\ntheℓth response.\\n3. Deﬁne δk(x) =∑L\\nℓ=1|Ckℓ−ˆpℓ(x)|, the discriminant function for the\\nkth class, where Ckℓis the entry for row kand column ℓin Table 16.1.\\nEach row of Cis a binary code for representing that class. The rows have\\nmore bits than is necessary, and the idea is that the redundant “error-\\ncorrecting” bits allow for some inaccuracies, and can improve performance.\\nIn fact, the full code matrix Cabove has a minimum Hamming distance1\\nof 7 between any pair of rows. Note that even the indicator response coding\\n(Section 4.2) is redundant, since 10 classes require only ⌈log210 = 4 bits for\\ntheir unique representation. Dietterich and Bakiri (1995) showed impressive\\nimprovements in performance for a variety of multiclass problems when\\nclassiﬁcation trees were used as the base classiﬁer.\\nJames and Hastie (1998) analyzed the ECOC approach, and showed\\nthat random code assignment worked as well as the optimally constructed\\nerror-correcting codes. They also argued that the main beneﬁt of the coding\\nwas in variance reduction (as in bagging and random forests), because the\\ndiﬀerent coded problems resulted in diﬀerent trees, and the decoding step\\n(3) above has a similar eﬀect as averaging.\\n1The Hamming distance between two vectors is the number of mis matches between\\ncorresponding entries.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='876a025f-4dc2-4973-99e1-a83d96a0e2ba', embedding=None, metadata={'page_label': '626', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.2 Boosting and Regularization Paths 607\\n16.2 Boosting and Regularization Paths\\nIn Section 10.12.2 of the ﬁrst edition of this book, we suggested an analogy\\nbetween the sequence of models produced by a gradient boosting algorithm\\nand regularized model ﬁtting in high-dimensional feature spaces. This was\\nprimarily motivated by observing the close connection between a boosted\\nversion of linear regression and the lasso (Section 3.4.2). These connec-\\ntions have been pursued by us and others, and here we present our current\\nthinking in this area. We start with the original motivation, which ﬁts m ore\\nnaturally in this chapter on ensemble learning.\\n16.2.1 Penalized Regression\\nIntuition for the success of the shrinkage strategy (10.41) of gradient bo ost-\\ning (page 364 in Chapter 10) can be obtained by drawing analogies with\\npenalized linear regression with a large basis expansion. Consider the dic-\\ntionary of all possible J-terminal node regression trees T={Tk}that could\\nbe realized on the training data as basis functions in IRp. The linear model\\nis\\nf(x) =K∑\\nk=1αkTk(x), (16.1)\\nwhere K= card( T). Suppose the coeﬃcients are to be estimated by least\\nsquares. Since the number of such trees is likely to be much larger than\\neven the largest training data sets, some form of regularization is required.\\nLet ˆα(λ) solve\\nmin\\nα\\uf8f1\\n\\uf8f2\\n\\uf8f3N∑\\ni=1(\\nyi−K∑\\nk=1αkTk(xi))2\\n+λ≤J(α)\\uf8fc\\n\\uf8fd\\n\\uf8fe, (16.2)\\nJ(α) is a function of the coeﬃcients that generally penalizes larger values.\\nExamples are\\nJ(α) =K∑\\nk=1|αk|2ridge regression , (16.3)\\nJ(α) =K∑\\nk=1|αk|lasso, (16.4)\\n(16.5)\\nboth covered in Section 3.4. As discussed there, the solution to the lasso\\nproblem with moderate to large λtends to be sparse; many of the ˆ αk(λ) =\\n0. That is, only a small fraction of all possible trees enter the model (16. 1).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='72c1f3d6-bd6d-4bea-b65e-fe0b3bf09632', embedding=None, metadata={'page_label': '627', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='608 16. Ensemble Learning\\nAlgorithm 16.1 Forward Stagewise Linear Regression.\\n1. Initialize ˇ αk= 0, k= 1,... ,K . Setε >0 to some small constant,\\nandMlarge.\\n2. For m= 1 to M:\\n(a) (β∗,k∗) = arg min β,k∑N\\ni=1(\\nyi−∑K\\nl=1ˇαlTl(xi)−βTk(xi))2\\n.\\n(b) ˇαk∗←ˇαk∗+ε≤sign(β∗).\\n3. Output fM(x) =∑K\\nk=1ˇαkTk(x).\\nThis seems reasonable since it is likely that only a small fraction of all po s-\\nsible trees will be relevant in approximating any particular target function.\\nHowever, the relevant subset will be diﬀerent for diﬀerent targets. Those\\ncoeﬃcients that are not set to zero are shrunk by the lasso in that their\\nabsolute values are smaller than their corresponding least squares values2:\\n|ˆαk(λ)|<|ˆαk(0)|. Asλincreases, the coeﬃcients all shrink, each one\\nultimately becoming zero.\\nOwing to the very large number of basis functions Tk, directly solving\\n(16.2) with the lasso penalty (16.4) is not possible. However, a feasibl e\\nforward stagewise strategy exists that closely approximates the eﬀect of\\nthe lasso, and is very similar to boosting and the forward stagewise A lgo-\\nrithm 10.2. Algorithm 16.1 gives the details. Although phrased in terms\\nof tree basis functions Tk, the algorithm can be used with any set of ba-\\nsis functions. Initially all coeﬃcients are zero in line 1; this corresponds\\ntoλ=∞in (16.2). At each successive step, the tree Tk∗is selected that\\nbest ﬁts the current residuals in line 2(a). Its corresponding coeﬃcient ˇ αk∗\\nis then incremented or decremented by an inﬁnitesimal amount in 2(b),\\nwhile all other coeﬃcients ˇ αk, k̸=k∗are left unchanged. In principle, this\\nprocess could be iterated until either all the residuals are zero, or β∗= 0.\\nThe latter case can occur if K < N , and at that point the coeﬃcient values\\nrepresent a least squares solution. This corresponds to λ= 0 in (16.2).\\nAfter applying Algorithm 16.1 with M <∞iterations, many of the coef-\\nﬁcients will be zero, namely, those that have yet to be incremented. The oth-\\ners will tend to have absolute values smaller than their corresponding least\\nsquares solution values, |ˇαk(M)|<|ˆαk(0)|. Therefore this M-iteration\\nsolution qualitatively resembles the lasso, with Minversely related to λ.\\nFigure 16.1 shows an example, using the prostate data studied in Chap-\\nter 3. Here, instead of using trees Tk(X) as basis functions, we use the origi-\\n2IfK > N , there is in general no unique “least squares value,” since i nﬁnitely many\\nsolutions will exist that ﬁt the data perfectly. We can pick t he minimum L1-norm solution\\namongst these, which is the unique lasso solution.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cd3e2d35-eb0f-4fe1-a62c-9a944ae15845', embedding=None, metadata={'page_label': '628', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.2 Boosting and Regularization Paths 609−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0.0 0.5 1.0 1.5 2.0\\n−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0 50 100 150 200\\nt=P\\nk|αk|\\nCoeﬃcientsCoeﬃcientsLasso Forward Stagewise\\nIteration\\nFIGURE 16.1. Proﬁles of estimated coeﬃcients from linear regression, for the\\nprostate data studied in Chapter 3. The left panel shows the re sults from the lasso,\\nfor diﬀerent values of the bound parameter t=P\\nk|αk|. The right panel shows\\nthe results of the stagewise linear regression Algorithm 16. 1, using M= 220\\nconsecutive steps of size ε=.01.\\nnal variables Xkthemselves; that is, a multiple linear regression model. The\\nleft panel displays the proﬁles of estimated coeﬃcients from the lasso, for\\ndiﬀerent values of the bound parameter t=∑\\nk|αk|. The right panel shows\\nthe results of the stagewise Algorithm 16.1, with M= 250 and ε= 0.01.\\n[The left and right panels of Figure 16.1 are the same as Figure 3.10 and\\nthe left panel of Figure 3.19, respectively.] The similarity between the two\\ngraphs is striking.\\nIn some situations the resemblance is more than qualitative. For example,\\nif all of the basis functions Tkare mutually uncorrelated, then as ε↓0,M↑\\nsuch that Mǫ→t, Algorithm 16.1 yields exactly the same solution as the\\nlasso for bound parameter t=∑\\nk|αk|(and likewise for all solutions along\\nthe path). Of course, tree-based regressors are not uncorrelated. However,\\nthe solution sets are also identical if the coeﬃcients ˆ αk(λ) are all monotone\\nfunctions of λ. This is often the case when the correlation between the\\nvariables is low. When the ˆ αk(λ) are not monotone in λ, then the solution\\nsets are not identical. The solution sets for Algorithm 16.1 tend to change\\nless rapidly with changing values of the regularization parameter than those\\nof the lasso.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c263f14-18b8-48a5-8ac0-b9f599a11495', embedding=None, metadata={'page_label': '629', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='610 16. Ensemble Learning\\nEfron et al. (2004) make the connections more precise, by characterizing\\nthe exact solution paths in the ε-limiting case. They show that the coeﬃ-\\ncient paths are piece-wise linear functions, both for the lasso and forward\\nstagewise. This facilitates eﬃcient algorithms which allow the entire pat hs\\nto be computed with the same cost as a single least-squares ﬁt. This least\\nangle regression algorithm is described in more detail in Section 3.8.1.\\nHastie et al. (2007) show that this inﬁnitesimal forward stagewise alg o-\\nrithm (FS 0) ﬁts a monotone version of the lasso, which optimally reduces\\nat each step the loss function for a given increase in the arc length of the\\ncoeﬃcient path (see Sections 16.2.3 and 3.8.1). The arc-length for the ǫ >0\\ncase is Mǫ, and hence proportional to the number of steps.\\nTree boosting (Algorithm 10.3) with shrinkage (10.41) closely resembl es\\nAlgorithm 16.1, with the learning rate parameter νcorresponding to ε. For\\nsquared error loss, the only diﬀerence is that the optimal tree to be selected\\nat each iteration Tk∗is approximated by the standard top-down greedy\\ntree-induction algorithm. For other loss functions, such as the exponential\\nloss of AdaBoost and the binomial deviance, Rosset et al. (2004a) show\\nsimilar results to what we see here. Thus, one can view tree boosting with\\nshrinkage as a form of monotone ill-posed regression on all possible ( J-\\nterminal node) trees, with the lasso penalty (16.4) as a regularizer. We\\nreturn to this topic in Section 16.2.3.\\nThe choice of no shrinkage [ ν= 1 in equation (10.41)] is analogous to\\nforward-stepwise regression, and its more aggressive cousin best-subset se-\\nlection, which penalizes the number of non zero coeﬃcients J(α) =∑\\nk|αk|0.\\nWith a small fraction of dominant variables, best subset approaches often\\nwork well. But with a moderate fraction of strong variables, it is well k nown\\nthat subset selection can be excessively greedy (Copas, 1983), often yielding\\npoor results when compared to less aggressive strategies such as the lasso\\nor ridge regression. The dramatic improvements often seen when shrinkage\\nis used with boosting are yet another conﬁrmation of this approach.\\n16.2.2 The “Bet on Sparsity” Principle\\nAs shown in the previous section, boosting’s forward stagewise strategy\\nwith shrinkage approximately minimizes the same loss function with a\\nlasso-style L1penalty. The model is built up slowly, searching through\\n“model space” and adding shrunken basis functions derived from impor-\\ntant predictors. In contrast, the L2penalty is computationally much easier\\nto deal with, as shown in Section 12.3.7. With the basis functions and L2\\npenalty chosen to match a particular positive-deﬁnite kernel, one can solve\\nthe corresponding optimization problem without explicitly searching over\\nindividual basis functions.\\nHowever, the sometimes superior performance of boosting over proce-\\ndures such as the support vector machine may be largely due to the im-\\nplicit use of the L1versus L2penalty. The shrinkage resulting from the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec0b3234-8df3-45d4-b7f2-349401250ad0', embedding=None, metadata={'page_label': '630', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.2 Boosting and Regularization Paths 611\\nL1penalty is better suited to sparse situations, where there are few basis\\nfunctions with nonzero coeﬃcients (among all possible choices).\\nWe can strengthen this argument through a simple example, taken from\\nFriedman et al. (2004). Suppose we have 10 ,000 data points and our model\\nis a linear combination of a million trees. If the true population coeﬃcients\\nof these trees arose from a Gaussian distribution, then we know that in a\\nBayesian sense the best predictor is ridge regression (Exercise 3.6). That is ,\\nwe should use an L2rather than an L1penalty when ﬁtting the coeﬃcients.\\nOn the other hand, if there are only a small number (e.g., 1000) coeﬃcients\\nthat are nonzero, the lasso ( L1penalty) will work better. We think of this\\nas asparse scenario, while the ﬁrst case (Gaussian coeﬃcients) is dense.\\nNote however that in the dense scenario, although the L2penalty is best,\\nneither method does very well since there is too little data from which to\\nestimate such a large number of nonzero coeﬃcients. This is the curse of\\ndimensionality taking its toll. In a sparse setting, we can potentially do\\nwell with the L1penalty, since the number of nonzero coeﬃcients is small.\\nTheL2penalty fails again.\\nIn other words, use of the L1penalty follows what we call the “bet on\\nsparsity” principle for high-dimensional problems:\\nUse a procedure that does well in sparse problems, since no pr o-\\ncedure does well in dense problems.\\nThese comments need some qualiﬁcation:\\n•For any given application, the degree of sparseness/denseness depends\\non the unknown true target function, and the chosen dictionary T.\\n•The notion of sparse versus dense is relative to the size of the train-\\ning data set and/or the noise-to-signal ratio (NSR). Larger training\\nsets allow us to estimate coeﬃcients with smaller standard errors.\\nLikewise in situations with small NSR, we can identify more nonzero\\ncoeﬃcients with a given sample size than in situations where the NSR\\nis larger.\\n•The size of the dictionary plays a role as well. Increasing the size of the\\ndictionary may lead to a sparser representation for our function, but\\nthe search problem becomes more diﬃcult leading to higher variance.\\nFigure 16.2 illustrates these points in the context of linear models us-\\ning simulation. We compare ridge regression and lasso, both for classiﬁ-\\ncation and regression problems. Each run has 50 observations with 300\\nindependent Gaussian predictors. In the top row all 300 coeﬃcients are\\nnonzero, generated from a Gaussian distribution. In the middle row, only\\n10 are nonzero and generated from a Gaussian, and the last row has 30\\nnon zero Gaussian coeﬃcients. For regression, standard Gaussian noise is', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee54f03e-1d94-42f1-b5c5-3f07ac06f16d', embedding=None, metadata={'page_label': '631', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='612 16. Ensemble Learning\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Gaussian\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 10\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Squared Prediction Error Explained\\nNoise−to−Signal RatioRegression\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Gaussian\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 10\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Misclassification Error Explained\\nNoise−to−Signal RatioClassification\\nFIGURE 16.2. Simulations that show the superiority of the L1(lasso) penalty\\noverL2(ridge) in regression and classiﬁcation. Each run has 50observations\\nwith300independent Gaussian predictors. In the top row all 300coeﬃcients are\\nnonzero, generated from a Gaussian distribution. In the middle r ow, only 10are\\nnonzero, and the last row has 30nonzero. Gaussian errors are added to the linear\\npredictor η(X)for the regression problems, and binary responses generated via the\\ninverse-logit transform for the classiﬁcation problems. Scali ng of η(X)resulted in\\nthe noise-to-signal ratios shown. Lasso is used in the left sub- columns, ridge in the\\nright. We report the optimal percentage of error explained on te st data (relative\\nto the error of a constant model), displayed as boxplots over 20realizations for\\neach combination. In the only situation where ridge beats lasso (top row), neither\\ndo well.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2e332e9c-d15f-4066-a59a-e02a6e12d9d9', embedding=None, metadata={'page_label': '632', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.2 Boosting and Regularization Paths 613\\nadded to the linear predictor η(X) =XTβto produce a continuous re-\\nsponse. For classiﬁcation the linear predictor is transformed via the inverse-\\nlogit to a probability, and a binary response is generated. Five diﬀer-\\nent noise-to-signal ratios are presented, obtained by scaling η(X) prior\\nto generating the response. In both cases this is deﬁned to be NSR =\\nVar(Y|η(X))/Var(η(X)). Both the ridge regression and lasso coeﬃcient\\npaths were ﬁt using a series of 50 values of λcorresponding to a range of\\ndffrom 1 to 50 (see Chapter 3 for details). The models were evaluated on\\na large test set (inﬁnite for Gaussian, 5000 for binary), and in each case the\\nvalue for λwas chosen to minimize the test-set error. We report percentage\\nvariance explained for the regression problems, and percentage misclassiﬁ-\\ncation error explained for the classiﬁcation problems (relative to a baseline\\nerror of 0 .5). There are 20 simulation runs for each scenario.\\nNote that for the classiﬁcation problems, we are using squared-error loss\\nto ﬁt the binary response. Note also that we do not using the training\\ndata to select λ, but rather are reporting the best possible behavior for\\neach method in the diﬀerent scenarios. The L2penalty performs poorly\\neverywhere. The Lasso performs reasonably well in the only two situations\\nwhere it can (sparse coeﬃcients). As expected the performance gets worse\\nas the NSR increases (less so for classiﬁcation), and as the model becomes\\ndenser. The diﬀerences are less marked for classiﬁcation than for regression.\\nThese empirical results are supported by a large body of theoretical\\nresults (Donoho and Johnstone, 1994; Donoho and Elad, 2003; Donoho,\\n2006b; Candes and Tao, 2007) that support the superiority of L1estimation\\nin sparse settings.\\n16.2.3 Regularization Paths, Over-ﬁtting and Margins\\nIt has often been observed that boosting “does not overﬁt,” or more as-\\ntutely is “slow to overﬁt.” Part of the explanation for this phenomenon was\\nmade earlier for random forests — misclassiﬁcation error is less sensitive to\\nvariance than is mean-squared error, and classiﬁcation is the major focus\\nin the boosting community. In this section we show that the regulariza-\\ntion paths of boosted models are “well behaved,” and that for certain loss\\nfunctions they have an appealing limiting form.\\nFigure 16.3 shows the coeﬃcient paths for lasso and inﬁnitesimal forward\\nstagewise (FS 0) in a simulated regression setting. The data consists of a\\ndictionary of 1000 Gaussian variables, strongly correlated ( ρ= 0.95) within\\nblocks of 20, but uncorrelated between blocks. The generating model has\\nnonzero coeﬃcients for 50 variables, one drawn from each block, and the\\ncoeﬃcient values are drawn from a standard Gaussian. Finally, Gaussian\\nnoise is added, with a noise-to-signal ratio of 0 .72 (Exercise 16.1.) The\\nFS0algorithm is a limiting form of algorithm 16.1, where the step size ε\\nis shrunk to zero (Section 3.8.1). The grouping of the variables is intended\\nto mimic the correlations of nearby trees, and with the forward-stagewise', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bda4aac0-3eff-4c93-825e-9a0b40fa594b', embedding=None, metadata={'page_label': '633', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='614 16. Ensemble Learning\\n0.0 0.2 0.4 0.6 0.8 1.0−20 −10 0 10 20 30Standardized CoefficientsLASSO\\n0.0 0.2 0.4 0.6 0.8 1.0−20 −10 0 10 20 30Standardized CoefficientsForward Stagewise\\n|α(m)|/|α(∞)| |α(m)|/|α(∞)|\\nFIGURE 16.3. Comparison of lasso and inﬁnitesimal forward stagewise paths\\non simulated regression data. The number of samples is 60and the number of\\nvariables is 1000. The forward-stagewise paths ﬂuctuate less than those of la sso\\nin the ﬁnal stages of the algorithms.\\nalgorithm, this setup is intended as an idealized version of gradient boosting\\nwith shrinkage. For both these algorithms, the coeﬃcient paths can be\\ncomputed exactly, since they are piecewise linear (see the LARS algorithm\\nin Section 3.8.1).\\nHere the coeﬃcient proﬁles are similar only in the early stages of the\\npaths. For the later stages, the forward stagewise paths tend to be mono-\\ntone and smoother, while those for the lasso ﬂuctuate widely. This is due\\nto the strong correlations among subsets of the variables —lasso suﬀers\\nsomewhat from the multi-collinearity problem (Exercise 3.28).\\nThe performance of the two models is rather similar (Figure 16.4), and\\nthey achieve about the same minimum. In the later stages forward stagewise\\ntakes longer to overﬁt, a likely consequence of the smoother paths.\\nHastie et al. (2007) show that FS 0solves a monotone version of the lasso\\nproblem for squared error loss. Let Ta=T ∪ {−T } be the augmented\\ndictionary obtained by including a negative copy of every basis element\\ninT. We consider models f(x) =∑\\nTk∈TaαkTk(x) with non-negative co-\\neﬃcients αk≥0. In this expanded space, the lasso coeﬃcient paths are\\npositive, while those of FS 0are monotone nondecreasing.\\nThe monotone lasso path is characterized by a diﬀerential equation\\n∂α\\n∂ℓ=ρml(α(ℓ)), (16.6)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13350912-af56-4e7c-af08-971ebb03025d', embedding=None, metadata={'page_label': '634', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.2 Boosting and Regularization Paths 615\\no\\noo\\no\\no\\noo\\noo\\noo\\noooo\\nooo\\nooooooooooooooooooooooooooooooo ooooooo ooooooooooooooooooooooooooooooooooooooooo ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo oo ooo\\noo\\no\\no o\\noo\\nooo\\nooo oo\\no ooooo\\noooooooo ooooo ooo ooooooooooo oooooo o ooo oooo oo ooooo oo oo ooooooo oo oo ooooo oo oo oo oo ooo oo oo oo ooooooo oo ooo oooooo oo ooo oo oooooooo oooo ooo ooooooo ooo ooo ooooo oooo ooo oooooo ooooooo oooooooooooooo ooooo ooooooooooo ooooooooo oo oooo oooooo oo ooooooooooo ooo oo ooo oo ooo oooooooooo ooooo ooooooo ooooooooo oooo oooooo oooo ooo oooooo oooo oo oo oooo ooooo ooooo ooooooo ooooo o o oooooooooooooooooo oo ooo oo oo oo oooo o oo o o o o o oo o o oooo oo o oo o o oo oo o ooo o o o ooo o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o o\\n0 10 20 30 40 50 60 7025 30 35 40 45 50 55Lasso\\nForward Stagewise\\n|α(m)|Mean Squared Error\\nFIGURE 16.4. Mean squared error for lasso and inﬁnitesimal forward stagewise\\non the simulated data. Despite the diﬀerence in the coeﬃcient p aths, the two\\nmodels perform similarly over the critical part of the regul arization path. In the\\nright tail, lasso appears to overﬁt more rapidly.\\nwith initial condition α(0) = 0, where ℓis the L1arc-length of the path\\nα(ℓ) (Exercise 16.2). The monotone lasso move direction (velocity vector)\\nρml(α(ℓ)) decreases the loss at the optimal quadratic rate per unit increase\\nin the L1arc-length of the path. Since ρml\\nk(α(ℓ))≥0∀k,ℓ, the solution\\npaths are monotone.\\nThe lasso can similarly be characterized as the solution to a diﬀerential\\nequation as in (16.6), except that the move directions decrease the loss\\noptimally per unit increase in the L1norm of the path. As a consequence,\\nthey are not necessarily positive, and hence the lasso paths need not be\\nmonotone.\\nIn this augmented dictionary, restricting the coeﬃcients to be positive is\\nnatural, since it avoids an obvious ambiguity. It also ties in more natura lly\\nwith tree boosting—we always ﬁnd trees positively correlated with the\\ncurrent residual.\\nThere have been suggestions that boosting performs well (for two-class\\nclassiﬁcation) because it exhibits maximal-margin properties, much like the\\nsupport-vector machines of Chapters 4.5.2 and 12. Schapire et al. (1998)\\ndeﬁne the normalized L1margin of a ﬁtted model f(x) =∑\\nkαkTk(x) as\\nm(f) = min\\niyif(xi)∑K\\nk=1|αk|. (16.7)\\nHere the minimum is taken over the training sample, and yi∈ {−1,+1}.\\nUnlike the L2margin (4.40) of support vector machines, the L1margin\\nm(f) measures the distance to the closest training point in L∞units (max-\\nimum coordinate distance).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4beb14a5-6908-4feb-93e8-7b812a677db4', embedding=None, metadata={'page_label': '635', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='616 16. Ensemble Learning\\n−0.3 −0.2 −0.1 0.0 0.1\\nNumber of TreesMargin\\n0 2K 4K 6K 8K 10K\\n0.25 0.26 0.27 0.28\\nNumber of TreesTest Error\\n0 2K 4K 6K 8K 10K\\nFIGURE 16.5. The left panel shows the L1margin m(f)for the Adaboost clas-\\nsiﬁer on the mixture data, as a function of the number of 4-node trees. The model\\nwas ﬁt using the R package gbm, with a shrinkage factor of 0.02. After 10,000\\ntrees, m(f)has settled down. Note that when the margin crosses zero, the t raining\\nerror becomes zero. The right panel shows the test error, whic h is minimized at\\n240trees. In this case, Adaboost overﬁts dramatically if run to c onvergence.\\nSchapire et al. (1998) prove that with separable data, Adaboost in-\\ncreases m(f) with each iteration, converging to a margin-symmetric so-\\nlution. R¨ atsch and Warmuth (2002) prove the asymptotic convergence of\\nAdaboost with shrinkage to a L1-margin-maximizing solution. Rosset et\\nal. (2004a) consider regularized models of the form (16.2) for general loss\\nfunctions. They show that as λ↓0, for particular loss functions the solution\\nconverges to a margin-maximizing conﬁguration. In particular they show\\nthis to be the case for the exponential loss of Adaboost, as well as binomia l\\ndeviance.\\nCollecting together the results of this section, we reach the following\\nsummary for boosted classiﬁers:\\nThe sequence of boosted classiﬁers form an L1-regularized mono-\\ntone path to a margin-maximizing solution.\\nOf course the margin-maximizing end of the path can be a very poor, overﬁt\\nsolution, as it is in the example in Figure 16.5. Early stopping amounts\\nto picking a point along the path, and should be done with the aid of a\\nvalidation dataset.\\n16.3 Learning Ensembles\\nThe insights learned from the previous sections can be harnessed to produce\\na more eﬀective and eﬃcient ensemble model. Again we consider functions', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e5ae5fb8-f8a0-43d8-bb90-ec607d23757a', embedding=None, metadata={'page_label': '636', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.3 Learning Ensembles 617\\nof the form\\nf(x) =α0+∑\\nTk∈TαkTk(x), (16.8)\\nwhere Tis a dictionary of basis functions, typically trees. For gradient\\nboosting and random forests, |T |is very large, and it is quite typical for the\\nﬁnal model to involve many thousands of trees. In the previous section we\\nargue that gradient boosting with shrinkage ﬁts an L1regularized monotone\\npath in this space of trees.\\nFriedman and Popescu (2003) propose a hybrid approach which breaks\\nthis process down into two stages:\\n•A ﬁnite dictionary TL={T1(x),T2(x),... ,T M(x)}of basis functions\\nis induced from the training data;\\n•A family of functions fλ(x) is built by ﬁtting a lasso path in this\\ndictionary:\\nα(λ) = arg min\\nαN∑\\ni=1L[yi,α0+M∑\\nm=1αmTm(xi)] +λM∑\\nm=1|αm|.(16.9)\\nIn its simplest form this model could be seen as a way of post-processing\\nboosting or random forests, taking for TLthe collection of trees produced\\nby the gradient boosting or random forest algorithms. By ﬁtting the lasso\\npath to these trees, we would typically use a much reduced set, which would\\nsave in computations and storage for future predictions. In the next section\\nwe describe modiﬁcations of this prescription that reduce the correlations in\\nthe ensemble TL, and improve the performance of the lasso post processor.\\nAs an initial illustration, we apply this procedure to a random forest\\nensemble grown on the spam data.\\nFigure 16.6 shows that a lasso post-processing oﬀers modest improve-\\nment over the random forest (blue curve), and reduces the forest to about\\n40 trees, rather than the original 1000. The post-processed performance\\nmatches that of gradient boosting. The orange curves represent a modiﬁed\\nversion of random forests, designed to reduce the correlations between trees\\neven more. Here a random sub-sample (without replacement) of 5% of the\\ntraining sample is used to grow each tree, and the trees are restricted to be\\nshallow (about six terminal nodes). The post-processing oﬀers more dra-\\nmatic improvements here, and the training costs are reduced by a factor\\nof about 100. However, the performance of the post-processed model falls\\nsomewhat short of the blue curves.\\n16.3.1 Learning a Good Ensemble\\nNot all ensembles TLwill perform well with post-processing. In terms of\\nbasis functions, we want a collection that covers the space well in places', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91c4080c-b29c-4cf1-8502-439da4871895', embedding=None, metadata={'page_label': '637', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='618 16. Ensemble Learning\\n0 100 200 300 400 5000.04 0.05 0.06 0.07 0.08 0.09Spam Data\\nNumber of TreesTest ErrorRandom Forest\\nRandom Forest (5%, 6)\\nGradient Boost (5 node)\\nFIGURE 16.6. Application of the lasso post-processing (16.9) to the spam d ata.\\nThe horizontal blue line is the test error of a random forest ﬁt to t he spam data,\\nusing 1000trees grown to maximum depth (with m= 7; see Algorithm 15.1).\\nThe jagged blue curve is the test error after post-processing the ﬁrst 500trees\\nusing the lasso, as a function of the number of trees with nonzero co eﬃcients.\\nThe orange curve/line use a modiﬁed form of random forest, where a random\\ndraw of 5% of the data are used to grow each tree, and the trees are forced to\\nbe shallow (typically six terminal nodes). Here the post-proc essing oﬀers much\\ngreater improvement over the random forest that generated the e nsemble.\\nwhere they are needed, and are suﬃciently diﬀerent from each other for\\nthe post-processor to be eﬀective.\\nFriedman and Popescu (2003) gain insights from numerical quadrature\\nand importance sampling. They view the unknown function as an integral\\nf(x) =∫\\nβ(γ)b(x;γ)dγ, (16.10)\\nwhere γ∈Γ indexes the basis functions b(x;γ). For example, if the basis\\nfunctions are trees, then γindexes the splitting variables, the split-points\\nand the values in the terminal nodes. Numerical quadrature amounts to\\nﬁnding a set of Mevaluation points γm∈Γ and corresponding weights\\nαmso that fM(x) =α0+∑M\\nm=1αmb(x;γm) approximates f(x) well over\\nthe domain of x. Importance sampling amounts to sampling γat random,\\nbut giving more weight to relevant regions of the space Γ. Friedman and\\nPopescu (2003) suggest a measure of (lack of) relevance that uses the loss\\nfunction (16.9):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbe47558-4095-49fa-ad68-32b82b5d4d38', embedding=None, metadata={'page_label': '638', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.3 Learning Ensembles 619\\nQ(γ) = min\\nc0,c1N∑\\ni=1L(yi,c0+c1b(xi;γ)), (16.11)\\nevaluated on the training data.\\nIf a single basis function were to be selected (e.g., a tree), it would be\\nthe global minimizer γ∗= arg min γ∈ΓQ(γ). Introducing randomness in the\\nselection of γwould necessarily produce less optimal values with Q(γ)≥\\nQ(γ∗). They propose a natural measure of the characteristic width σof the\\nsampling scheme S,\\nσ= ES[Q(γ)−Q(γ∗)]. (16.12)\\n•σtoo narrow suggests too many of the b(x;γm) look alike, and similar\\ntob(x;γ∗);\\n•σtoo wide implies a large spread in the b(x;γm), but possibly con-\\nsisting of many irrelevant cases.\\nFriedman and Popescu (2003) use sub-sampling as a mechanism for intro-\\nducing randomness, leading to their ensemble-generation algorithm 16.2.\\nAlgorithm 16.2 ISLE Ensemble Generation.\\n1.f0(x) = arg min c∑N\\ni=1L(yi,c)\\n2. For m= 1 to Mdo\\n(a)γm= arg min γ∑\\ni∈Sm(η)L(yi,fm−1(xi) +b(xi;γ))\\n(b)fm(x) =fm−1(x) +νb(x;γm)\\n3.TISLE={b(x;γ1),b(x;γ2),... ,b (x;γM)}.\\nSm(η) refers to a subsample of N≤η(η∈(0,1]) of the training obser-\\nvations, typically without replacement. Their simulations suggest picking\\nη≤1\\n2, and for large Npicking η∼1/√\\nN. Reducing ηincreases the\\nrandomness, and hence the width σ. The parameter ν∈[0,1] introduces\\nmemory into the randomization process; the larger ν, the more the pro-\\ncedure avoids b(x;γ) similar to those found before. A number of familiar\\nrandomization schemes are special cases of Algorithm 16.2:\\nBagging hasη= 1, but samples with replacement, and has ν= 0. Fried-\\nman and Hall (2007) argue that sampling without replacement with\\nη= 1/2 is equivalent to sampling with replacement with η= 1, and\\nthe former is much more eﬃcient.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91260070-89a4-474f-8546-8e7276d4095d', embedding=None, metadata={'page_label': '639', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='620 16. Ensemble Learning\\nRandom forest sampling is similar, with more randomness introduced by\\nthe selection of the splitting variable. Reducing η <1/2 in algo-\\nrithm 16.2 has a similar eﬀect to reducing min random forests, but\\ndoes not suﬀer from the potential biases discussed in Section 15.4.2.\\nGradient boosting with shrinkage (10.41) uses η= 1, but typically does\\nnot produce suﬃcient width σ.\\nStochastic gradient boosting (Friedman, 1999) follows the recipe exactly.\\nThe authors recommend values ν= 0.1 and η≤1\\n2, and call their combined\\nprocedure (ensemble generation and post processing) Importance sampled\\nlearning ensemble (ISLE).\\nFigure 16.7 shows the performance of an ISLE on the spam data. It does\\n0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060Spam Data\\nNumber of TreesTest ErrorGradient Boosting (5 Node)\\nLasso Post−processed\\nFIGURE 16.7. Importance sampling learning ensemble (ISLE) ﬁt to the spam\\ndata. Here we used η= 1/2,ν= 0.05, and trees with ﬁve terminal nodes. The\\nlasso post-processed ensemble does not improve the predictio n error in this case,\\nbut it reduces the number of trees by a factor of ﬁve.\\nnot improve the predictive performance, but is able to produce a more\\nparsimonious model. Note that in practice the post-processing includes\\nthe selection of the regularization parameter λin (16.9), which would be', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='70059e1b-bb1f-4ab9-ab71-2afd85347e5d', embedding=None, metadata={'page_label': '640', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.3 Learning Ensembles 621\\nchosen by cross-validation. Here we simply demonstrate the eﬀects of post-\\nprocessing by showing the entire path on the test data.\\nFigure 16.8 shows various ISLEs on a regression example. The generating\\n0 500 1000 1500 2000 25001.0 1.5 2.0 2.5 3.0 3.5\\nNumber of TreesMean Squared ErrorGBM (1, 0.01)\\nGBM (0.1, 0.01)\\nISLE  GB\\nISLE RF\\nRandom Forest\\nFIGURE 16.8. Demonstration of ensemble methods on a regression simulation\\nexample. The notation GBM (0.1, 0.01) refers to a gradient boost ed model, with\\nparameters (η, ν). We report mean-squared error from the true (known) function.\\nNote that the sub-sampled GBM model (green) outperforms the f ull GBM model\\n(orange). The lasso post-processed version achieves simila r error. The random\\nforest is outperformed by its post-processed version, but bo th fall short of the\\nother models.\\nfunction is\\nf(X) = 10 ≤5∏\\nj=1e−2X2\\nj+35∑\\nj=6Xj, (16.13)\\nwhere X∼U[0,1]100(the last 65 elements are noise variables). The re-\\nsponse Y=f(X) +εwhere ε∼N(0,σ2); we chose σ= 1.3 resulting in a\\nsignal-to-noise ratio of approximately 2. We used a training sample of size\\n1000, and estimated the mean squared error E( ˆf(X)−f(X))2by averaging\\nover a test set of 500 samples. The sub-sampled GBM curve (light blue)\\nis an instance of stochastic gradient boosting (Friedman, 1999) discussed in\\nSection 10.12, and it outperforms gradient boosting on this example.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='57f57430-ced7-4bef-b985-cc0cbe74d6e1', embedding=None, metadata={'page_label': '641', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='622 16. Ensemble Learning\\n16.3.2 Rule Ensembles\\nHere we describe a modiﬁcation of the tree-ensemble method that focuses\\non individual rules (Friedman and Popescu, 2003). We encountered rules\\nin Section 9.3 in the discussion of the PRIM method. The idea is to enlarge\\nan ensemble of trees by constructing a set of rules from each of the trees\\nin the collection.\\n1 2\\n30\\n4\\n5 6X1<2.1 X1≥2.1\\nX3∈ {M,L} X3∈ {S}\\nX7<4.5 X7≥4.5\\nFIGURE 16.9. A typical tree in an ensemble, from which rules can be derived.\\nFigure 16.9 depicts a small tree, with numbered nodes. The following\\nrules can be derived from this tree:\\nR1(X) = I(X1<2.1)\\nR2(X) = I(X1≥2.1)\\nR3(X) = I(X1≥2.1)≤I(X3∈ {S})\\nR4(X) = I(X1≥2.1)≤I(X3∈ {M, L})\\nR5(X) = I(X1≥2.1)≤I(X3∈ {S})≤I(X7<4.5)\\nR6(X) = I(X1≥2.1)≤I(X3∈ {S})≤I(X7≥4.5)(16.14)\\nA linear expansion in rules 1, 4, 5 and 6 is equivalent to the tree itself\\n(Exercise 16.3); hence (16.14) is an over-complete basis for the tree.\\nFor each tree Tmin an ensemble T, we can construct its mini-ensemble\\nof rules Tm\\nRULE, and then combine them all to form a larger ensemble\\nTRULE=M⋃\\nm=1Tm\\nRULE. (16.15)\\nThis is then treated like any other ensemble, and post-processed via the\\nlasso or similar regularized procedure.\\nThere are several advantages to this approach of deriving rules from the\\nmore complex trees:\\n•The space of models is enlarged, and can lead to improved perfor-\\nmance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e0aa099-0722-4e5f-b7df-0ba01874d39a', embedding=None, metadata={'page_label': '642', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.3 Learning Ensembles 623\\nRules Rules + Linear0.9 1.0 1.1 1.2 1.3Mean Squared Error\\nFIGURE 16.10. Mean squared error for rule ensembles, using 20realizations\\nof the simulation example (16.13).\\n•Rules are easier to interpret than trees, so there is the potential for\\na simpliﬁed model.\\n•It is often natural to augment TRULEby including each variable Xj\\nseparately as well, thus allowing the ensemble to model linear func-\\ntions well.\\nFriedman and Popescu (2008) demonstrate the power of this procedure on a\\nnumber of illustrative examples, including the simulation example (16.13).\\nFigure 16.10 shows boxplots of the mean-squared error from the true model\\nfor twenty realizations from this model. The models were all ﬁt using the\\nRulefit software, available on the ESL homepage3, which runs in an auto-\\nmatic mode.\\nOn the same training set as used in Figure 16.8, the rule based model\\nachieved a mean-squared error of 1.06. Although slightly worse than the\\nbest achieved in that ﬁgure, the results are not comparable because cross-\\nvalidation was used here to select the ﬁnal model.\\nBibliographic Notes\\nAs noted in the introduction, many of the new methods in machine learning\\nhave been dubbed “ensemble” methods. These include neural networks\\nboosting, bagging and random forests; Dietterich (2000a) gives a survey o f\\ntree-based ensemble methods. Neural networks (Chapter 11) are perhaps\\nmore deserving of the name, since they simultaneously learn the parameters\\n3ESL homepage: www-stat.stanford.edu/ElemStatLearn', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='463b67e1-deda-430c-a092-a92a483b7533', embedding=None, metadata={'page_label': '643', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='624 16. Ensemble Learning\\nof the hidden units (basis functions), along with how to combine them.\\nBishop (2006) discusses neural networks in some detail, along with the\\nBayesian perspective (MacKay, 1992; Neal, 1996). Support vector machines\\n(Chapter 12) can also be regarded as an ensemble method; they perform\\nL2regularized model ﬁtting in high-dimensional feature spaces. Boosting\\nand lasso exploit sparsity through L1regularization to overcome the high-\\ndimensionality, while SVMs rely on the “kernel trick” characteristic of L2\\nregularization.\\nC5.0 (Quinlan, 2004) is a commercial tree and rule generation package,\\nwith some goals in common with Rulefit .\\nThere is a vast and varied literature often referred to as “combining clas-\\nsiﬁers” which abounds in ad-hoc schemes for mixing methods of diﬀerent\\ntypes to achieve better performance. For a principled approach, see Kittler\\net al. (1998).\\nExercises\\nEx. 16.1 Describe exactly how to generate the block correlated data used\\nin the simulation in Section 16.2.3.\\nEx. 16.2 Letα(t)∈IRpbe a piecewise-diﬀerentiable and continuous coef-\\nﬁcient proﬁle, with α(0) = 0. The L1arc-length of αfrom time 0 to tis\\ndeﬁned by\\nΛ(t) =∫t\\n0|˙α(t)|1dt. (16.16)\\nShow that Λ( t)≥ |α(t)|1, with equality iﬀ α(t) is monotone.\\nEx. 16.3 Show that ﬁtting a linear regression model using rules 1, 4, 5 and\\n6 in equation (16.14) gives the same ﬁt as the regression tree corresponding\\nto this tree. Show the same is true for classiﬁcation, if a logistic regressi on\\nmodel is ﬁt.\\nEx. 16.4 Program and run the simulation study described in Figure 16.2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d3b4c195-8f8e-46a8-8bc0-bd729781d461', embedding=None, metadata={'page_label': '644', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 625\\nPrinter: Opaque this\\n17\\nUndirected Graphical Models\\n17.1 Introduction\\nA graph consists of a set of vertices (nodes), along with a set of edges join-\\ning some pairs of the vertices. In graphical models, each vertex represents\\na random variable, and the graph gives a visual way of understanding the\\njoint distribution of the entire set of random variables. They can be use-\\nful for either unsupervised or supervised learning. In an undirected graph ,\\nthe edges have no directional arrows. We restrict our discussion to undi-\\nrected graphical models, also known as Markov random ﬁelds orMarkov\\nnetworks . In these graphs, the absence of an edge between two vertices has\\na special meaning: the corresponding random variables are conditionally\\nindependent, given the other variables.\\nFigure 17.1 shows an example of a graphical model for a ﬂow-cytometry\\ndataset with p= 11 proteins measured on N= 7466 cells, from Sachs\\net al. (2003). Each vertex in the graph corresponds to the real-valued ex-\\npression level of a protein. The network structure was estimated assuming\\na multivariate Gaussian distribution, using the graphical lasso procedure\\ndiscussed later in this chapter.\\nSparse graphs have a relatively small number of edges, and are convenient\\nfor interpretation. They are useful in a variety of domains, including ge-\\nnomics and proteomics, where they provide rough models of cell pathways.\\nMuch work has been done in deﬁning and understanding the structure of\\ngraphical models; see the Bibliographic Notes for references.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e1e0346-4dae-42ce-a11d-cab36299760c', embedding=None, metadata={'page_label': '645', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='626 17. Undirected Graphical Models\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnk\\nFIGURE 17.1. Example of a sparse undirected graph, estimated from a ﬂow\\ncytometry dataset, with p= 11proteins measured on N= 7466 cells. The net-\\nwork structure was estimated using the graphical lasso proce dure discussed in this\\nchapter.\\nAs we will see, the edges in a graph are parametrized by values or po-\\ntentials that encode the strength of the conditional dependence between\\nthe random variables at the corresponding vertices. The main challenges in\\nworking with graphical models are model selection (choosing the structure\\nof the graph), estimation of the edge parameters from data, and compu-\\ntation of marginal vertex probabilities and expectations, from their joint\\ndistribution. The last two tasks are sometimes called learning andinference\\nin the computer science literature.\\nWe do not attempt a comprehensive treatment of this interesting area.\\nInstead, we introduce some basic concepts, and then discuss a few sim-\\nple methods for estimation of the parameters and structure of undirected\\ngraphical models; methods that relate to the techniques already discussed\\nin this book. The estimation approaches that we present for continuous\\nand discrete-valued vertices are diﬀerent, so we treat them separately. Sec-\\ntions 17.3.1 and 17.3.2 may be of particular interest, as they describe new,\\nregression-based procedures for estimating graphical models.\\nThere is a large and active literature on directed graphical models or\\nBayesian networks ; these are graphical models in which the edges have\\ndirectional arrows (but no directed cycles). Directed graphical models rep-\\nresent probability distributions that can be factored into products of condi-\\ntional distributions, and have the potential for causal interpretations. We\\nrefer the reader to Wasserman (2004) for a brief overview of both undi-\\nrected and directed graphs; the next section follows closely his Chapter 18.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e20eb4fe-5f1c-4cc4-beb1-56f56929616a', embedding=None, metadata={'page_label': '646', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.2 Markov Graphs and Their Properties 627\\nX\\nXXX\\nYYYY\\nZ\\nZZ\\nZ\\nWWW\\n(a) (b)\\n(c) (d)\\nFIGURE 17.2. Examples of undirected graphical models or Markov networks.\\nEach node or vertex represents a random variable, and the lack of a n edge between\\ntwo nodes indicates conditional independence. For example, in graph (a),Xand\\nZare conditionally independent, given Y. In graph (b), Zis independent of each\\nofX,Y, and W.\\nA longer list of useful references is given in the Bibliographic Notes on\\npage 645.\\n17.2 Markov Graphs and Their Properties\\nIn this section we discuss the basic properties of graphs as models for the\\njoint distribution of a set of random variables. We defer discussion of (a)\\nparametrization and estimation of the edge parameters from data, and (b)\\nestimation of the topology of a graph, to later sections.\\nFigure 17.2 shows four examples of undirected graphs. A graph Gconsists\\nof a pair ( V,E), where Vis a set of vertices and Ethe set of edges (deﬁned\\nby pairs of vertices). Two vertices XandYare called adjacent if there\\nis a edge joining them; this is denoted by X∼Y. ApathX1,X2,... ,X n\\nis a set of vertices that are joined, that is Xi−1∼Xifori= 2,... ,n . A\\ncomplete graph is a graph with every pair of vertices joined by an edge.\\nAsubgraph U∈Vis a subset of vertices together with their edges. For\\nexample, ( X,Y,Z ) in Figure 17.2(a) form a path but not a complete graph.\\nSuppose that we have a graph Gwhose vertex set Vrepresents a set of\\nrandom variables having joint distribution P. In a Markov graph G, the\\nabsence of an edge implies that the corresponding random variables are\\nconditionally independent given the variables at the other vertices. This is\\nexpressed with the following notation:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de3bbe74-373a-49c6-b4ba-92dbd2cfbd48', embedding=None, metadata={'page_label': '647', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='628 17. Undirected Graphical Models\\nNo edge joining XandY⇐⇒X⊥Y|rest (17.1)\\nwhere “rest” refers to all of the other vertices in the graph. For example\\nin Figure 17.2(a) X⊥Z|Y. These are known as the pairwise Markov\\nindependencies ofG.\\nIfA,BandCare subgraphs, then Cis said to separate AandBif every\\npath between AandBintersects a node in C. For example, Yseparates\\nXandZin Figures 17.2(a) and (d), and Zseparates YandWin (d). In\\nFigure 17.2(b) Zis not connected to X,Y,W so we say that the two sets\\nare separated by the empty set. In Figure 17.2(c), C={X,Z}separates\\nYandW.\\nSeparators have the nice property that they break the graph into con-\\nditionally independent pieces. Speciﬁcally, in a Markov graph Gwith sub-\\ngraphs A,BandC,\\nifCseparates AandBthenA⊥B|C. (17.2)\\nThese are known as the global Markov properties ofG. It turns out that the\\npairwise and global Markov properties of a graph are equivalent (for gra phs\\nwith positive distributions). That is, the set of graphs with associated prob-\\nability distributions that satisfy the pairwise Markov independencies and\\nglobal Markov assumptions are the same. This result is useful for inferring\\nglobal independence relations from simple pairwise properties. For example\\nin Figure 17.2(d) X⊥Z|{Y,W}since it is a Markov graph and there is no\\nlink joining XandZ. But Yalso separates XfromZandWand hence by\\nthe global Markov assumption we conclude that X⊥Z|YandX⊥W|Y.\\nSimilarly we have Y⊥W|Z.\\nThe global Markov property allows us to decompose graphs into smaller\\nmore manageable pieces and thus leads to essential simpliﬁcations in com-\\nputation and interpretation. For this purpose we separate the graph into\\ncliques. A clique is a complete subgraph— a set of vertices that are all\\nadjacent to one another; it is called maximal if it is a clique and no other\\nvertices can be added to it and still yield a clique. The maximal cliques for\\nthe graphs of Figure 17.2 are\\n(a){X,Y},{Y,Z},\\n(b){X,Y,W },{Z},\\n(c){X,Y},{Y,Z},{Z,W},{X,W}, and\\n(d){X,Y},{Y,Z},{Z,W}.\\nAlthough the following applies to both continuous and discrete distri-\\nbutions, much of the development has been for the latter. A probability\\ndensity function fover a Markov graph Gcan be can represented as', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='86cb55e2-5870-444a-972d-0afe4cdb79e2', embedding=None, metadata={'page_label': '648', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.2 Markov Graphs and Their Properties 629\\nf(x) =1\\nZ∏\\nC∈CψC(xC) (17.3)\\nwhere Cis the set of maximal cliques, and the positive functions ψC(≤) are\\ncalled clique potentials . These are not in general density functions1, but\\nrather are aﬃnities that capture the dependence in XCby scoring certain\\ninstances xChigher than others. The quantity\\nZ=∑\\nx∈X∏\\nC∈CψC(xC) (17.4)\\nis the normalizing constant, also known as the partition function. Alterna-\\ntively, the representation (17.3) implies a graph with independence prop-\\nerties deﬁned by the cliques in the product. This result holds for Markov\\nnetworks Gwith positive distributions, and is known as the Hammersley-\\nCliﬀord theorem (Hammersley and Cliﬀord, 1971; Cliﬀord, 1990).\\nMany of the methods for estimation and computation on graphs ﬁrst de-\\ncompose the graph into its maximal cliques. Relevant quantities are com-\\nputed in the individual cliques and then accumulated across the entire\\ngraph. A prominent example is the join tree orjunction tree algorithm for\\ncomputing marginal and low order probabilities from the joint distribution\\non a graph. Details can be found in Pearl (1986), Lauritzen and Spiegel-\\nhalter (1988), Pearl (1988), Shenoy and Shafer (1988), Jensen et al. (1990),\\nor Koller and Friedman (2007).\\nXY\\nZ\\nFIGURE 17.3. A complete graph does not uniquely specify the higher-order\\ndependence structure in the joint distribution of the variable s.\\nA graphical model does not always uniquely specify the higher-order\\ndependence structure of a joint probability distribution. Consider the com-\\nplete three-node graph in Figure 17.3. It could represent the dependence\\nstructure of either of the following distributions:\\nf(2)(x,y,z) =1\\nZψ(x,y)ψ(x,z)ψ(y,z);\\nf(3)(x,y,z) =1\\nZψ(x,y,z).(17.5)\\nThe ﬁrst speciﬁes only second order dependence (and can be represented\\nwith fewer parameters). Graphical models for discrete data are a special\\n1If the cliques are separated, then the potentials can be dens ities, but this is in general\\nnot the case.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7e34a463-da83-43e5-9af2-af12027c28ee', embedding=None, metadata={'page_label': '649', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='630 17. Undirected Graphical Models\\ncase of loglinear models for multiway contingency tables (Bishop et al.,\\n1975, e.g.); in that language f(2)is referred to as the “no second-order\\ninteraction” model.\\nFor the remainder of this chapter we focus on pairwise Markov graphs\\n(Koller and Friedman, 2007). Here there is a potential function for each\\nedge (pair of variables as in f(2)above), and at most second–order interac-\\ntions are represented. These are more parsimonious in terms of parameters,\\neasier to work with, and give the minimal complexity implied by the graph\\nstructure. The models for both continuous and discrete data are functions\\nof only the pairwise marginal distributions of the variables represented in\\nthe edge set.\\n17.3 Undirected Graphical Models for Continuous\\nVariables\\nHere we consider Markov networks where all the variables are continuous.\\nThe Gaussian distribution is almost always used for such graphical models,\\nbecause of its convenient analytical properties. We assume that the observa-\\ntions have a multivariate Gaussian distribution with mean θand covariance\\nmatrix Σ. Since the Gaussian distribution represents at most second-order\\nrelationships, it automatically encodes a pairwise Markov graph. The graph\\nin Figure 17.1 is an example of a Gaussian graphical model.\\nThe Gaussian distribution has the property that all conditional distri-\\nbutions are also Gaussian. The inverse covariance matrix Σ−1contains\\ninformation about the partial covariances between the variables; that is,\\nthe covariances between pairs iandj, conditioned on all other variables.\\nIn particular, if the ijth component of Θ=Σ−1is zero, then variables iand\\njare conditionally independent, given the other variables (Exercise 17.3).\\nIt is instructive to examine the conditional distribution of one variable\\nversus the rest, where the role of Θis explicit. Suppose we partition X=\\n(Z,Y) where Z= (X1,... ,X p−1) consists of the ﬁrst p−1 variables and\\nY=Xpis the last. Then we have the conditional distribution of YgiveZ\\n(Mardia et al., 1979, e.g.)\\nY|Z=z∼N(\\nθY+ (z−θZ)TΣ−1\\nZZσZY, σY Y−σT\\nZYΣ−1\\nZZσZY)\\n,(17.6)\\nwhere we have partitioned Σas\\nΣ=(ΣZZσZY\\nσT\\nZYσY Y)\\n. (17.7)\\nThe conditional mean in (17.6) has exactly the same form as the pop-\\nulation multiple linear regression of YonZ, with regression coeﬃcient\\nβ=Σ−1\\nZZσZY[see (2.16) on page 19]. If we partition Θin the same way,\\nsinceΣΘ=Istandard formulas for partitioned inverses give', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b2bd2622-be4c-4834-b912-4f9ccb839a18', embedding=None, metadata={'page_label': '650', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.3 Undirected Graphical Models for Continuous Variables 631\\nθZY=−θY Y≤Σ−1\\nZZσZY, (17.8)\\nwhere 1 /θY Y=σY Y−σT\\nZYΣ−1\\nZZσZY>0. Hence\\nβ=Σ−1\\nZZσZY\\n=−θZY/θY Y.(17.9)\\nWe have learned two things here:\\n•The dependence of YonZin (17.6) is in the mean term alone. Here\\nwe see explicitly that zero elements in βand hence θZYmean that\\nthe corresponding elements of Zare conditionally independent of Y,\\ngiven the rest.\\n•We can learn about this dependence structure through multiple linear\\nregression.\\nThusΘcaptures all the second-order information (both structural and\\nquantitative) needed to describe the conditional distribution of each node\\ngiven the rest, and is the so-called “natural” parameter for the Gaussian\\ngraphical model2.\\nAnother (diﬀerent) kind of graphical model is the covariance graph orrel-\\nevance network , in which vertices are connected by bidirectional edges if the\\ncovariance (rather than the partial covariance) between the corresponding\\nvariables is nonzero. These are popular in genomics, see especially Butte\\net al. (2000). The negative log-likelihood from these models is not convex,\\nmaking the computations more challenging (Chaudhuri et al., 2007).\\n17.3.1 Estimation of the Parameters when the Graph\\nStructure is Known\\nGiven some realizations of X, we would like to estimate the parameters\\nof an undirected graph that approximates their joint distribution. Suppose\\nﬁrst that the graph is complete (fully connected). We assume that we have\\nNmultivariate normal realizations xi, i= 1,... ,N with population mean\\nθand covariance Σ. Let\\nS=1\\nNN∑\\ni=1(xi−¯x)(xi−¯x)T(17.10)\\nbe the empirical covariance matrix, with ¯ xthe sample mean vector. Ignoring\\nconstants, the log-likelihood of the data can be written as\\n2The distribution arising from a Gaussian graphical model is a Wishart distribution.\\nThis is a member of the exponential family, with canonical or “natural” parameter\\nΘ=Σ−1. Indeed, the partially maximized log-likelihood (17.11) i s (up to constants)\\nthe Wishart log-likelihood.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f550c71-c1bd-4e31-84cd-82d968510ed5', embedding=None, metadata={'page_label': '651', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='632 17. Undirected Graphical Models\\nℓ(Θ) = log det Θ−trace(SΘ). (17.11)\\nIn (17.11) we have partially maximized with respect to the mean parameter\\nθ. The quantity −ℓ(Θ) is a convex function of Θ. It is easy to show that\\nthe maximum likelihood estimate of Σis simply S.\\nNow to make the graph more useful (especially in high-dimensional set-\\ntings) let’s assume that some of the edges are missing; for example, the\\nedge between PIP3andErkis one of several missing in Figure 17.1. As we\\nhave seen, for the Gaussian distribution this implies that the correspond-\\ning entries of Θ=Σ−1are zero. Hence we now would like to maximize\\n(17.11) under the constraints that some pre-deﬁned subset of the parame-\\nters are zero. This is an equality-constrained convex optimization problem,\\nand a number of methods have been proposed for solving it, in particular\\nthe iterative proportional ﬁtting procedure (Speed and Kiiveri, 1986). This\\nand other methods are summarized for example in Whittaker (1990) and\\nLauritzen (1996). These methods exploit the simpliﬁcations that arise from\\ndecomposing the graph into its maximal cliques, as described in the previ-\\nous section. Here we outline a simple alternate approach, that exploits the\\nsparsity in a diﬀerent way. The fruits of this approach will become apparent\\nlater when we discuss the problem of estimation of the graph structure.\\nThe idea is based on linear regression, as inspired by (17.6) and (17.9).\\nIn particular, suppose that we want to estimate the edge parameters θijfor\\nthe vertices that are joined to a given vertex i, restricting those that are not\\njoined to be zero. Then it would seem that the linear regression of the node\\nivalues on the other relevant vertices might provide a reasonable estimate.\\nBut this ignores the dependence structure among the predictors in this\\nregression. It turns out that if instead we use our current (model-based)\\nestimate of the cross-product matrix of the predictors when we perform\\nour regressions, this gives the correct solutions and solves the constrained\\nmaximum-likelihood problem exactly. We now give details.\\nTo constrain the log-likelihood (17.11), we add Lagrange constants for\\nall missing edges\\nℓC(Θ) = log det Θ−trace(SΘ)−∑\\n(j,k)̸∈Eγjkθjk. (17.12)\\nThe gradient equation for maximizing (17.12) can be written as\\nΘ−1−S−Γ=0, (17.13)\\nusing the fact that the derivative of log det Θequals Θ−1(Boyd and Van-\\ndenberghe, 2004, for example, page 641). Γis a matrix of Lagrange param-\\neters with nonzero values for all pairs with edges absent.\\nWe will show how we can use regression to solve for Θand its inverse\\nW=Θ−1one row and column at a time. For simplicity let’s focus on the\\nlast row and column. Then the upper right block of equation (17.13) can\\nbe written as', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4de2143e-0522-4d8e-95ea-a4bd7fce1b19', embedding=None, metadata={'page_label': '652', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.3 Undirected Graphical Models for Continuous Variables 633\\nw12−s12−γ12= 0. (17.14)\\nHere we have partitioned the matrices into two parts as in (17.7): part 1\\nbeing the ﬁrst p−1 rows and columns, and part 2 the pth row and column.\\nWithWand its inverse Θpartitioned in a similar fashion, we have\\n(W11w12\\nwT\\n12w22)(Θ11θ12\\nθT\\n12θ22)\\n=(I0\\n0T1)\\n. (17.15)\\nThis implies\\nw12=−W11θ12/θ22 (17.16)\\n=W11β (17.17)\\nwhere β=−θ12/θ22as in (17.9). Now substituting (17.17) into (17.14)\\ngives\\nW11β−s12−γ12= 0. (17.18)\\nThese can be interpreted as the p−1 estimating equations for the con-\\nstrained regression of Xpon the other predictors, except that the observed\\nmean cross-products matrix S11is replaced by W11, the current estimated\\ncovariance matrix from the model.\\nNow we can solve (17.18) by simple subset regression. Suppose there are\\np−qnonzero elements in γ12—i.e., p−qedges constrained to be zero. These\\np−qrows carry no information and can be removed. Furthermore we can\\nreduce βtoβ∗by removing its p−qzero elements, yielding the reduced\\nq×qsystem of equations\\nW∗\\n11β∗−s∗\\n12= 0, (17.19)\\nwith solution ˆβ∗=W∗\\n11−1s∗\\n12. This is padded with p−qzeros to give ˆβ.\\nAlthough it appears from (17.16) that we only recover the elements θ12\\nup to a scale factor 1 /θ22, it is easy to show that\\n1\\nθ22=w22−wT\\n12β (17.20)\\n(using partitioned inverse formulas). Also w22=s22, since the diagonal of\\nΓin (17.13) is zero.\\nThis leads to the simple iterative procedure given in Algorithm 17.1 for\\nestimating both ˆWand its inverse ˆΘ, subject to the constraints of the\\nmissing edges.\\nNote that this algorithm makes conceptual sense. The graph estimation\\nproblem is not pseparate regression problems, but rather pcoupled prob-\\nlems. The use of the common Win step (b), in place of the observed\\ncross-products matrix, couples the problems together in the appropriate\\nfashion. Surprisingly, we were not able to ﬁnd this procedure in the lit-\\nerature. However it is related to the covariance selection procedures of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0942bd02-8d41-4f09-9763-f491695488c0', embedding=None, metadata={'page_label': '653', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='634 17. Undirected Graphical Models\\nAlgorithm 17.1 A Modiﬁed Regression Algorithm for Estimation of an\\nUndirected Gaussian Graphical Model with Known Structure.\\n1. Initialize W=S.\\n2. Repeat for j= 1,2,... ,p, 1,2,... ,p,... until convergence:\\n(a) Partition the matrix Winto part 1: all but the jth row and\\ncolumn, and part 2: the jth row and column.\\n(b) Solve W∗\\n11β∗−s∗\\n12= 0 for the unconstrained edge parameters\\nβ∗, using the reduced system of equations as in (17.19). Obtain\\nˆβby padding ˆβ∗with zeros in the appropriate positions.\\n(c) Update w12=W11ˆβ\\n3. In the ﬁnal cycle (for each j) solve for ˆθ12=−ˆβ≤ˆθ22, with 1 /ˆθ22=\\ns22−wT\\n12ˆβ.\\nX1X2 X3\\nX4S=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed10 1 5 4\\n1 10 2 6\\n5 2 10 3\\n4 6 3 10\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8\\nFIGURE 17.4. A simple graph for illustration, along with the empirical cova ri-\\nance matrix.\\nDempster (1972), and is similar in ﬂavor to the iterative conditional ﬁtti ng\\nprocedure for covariance graphs, proposed by Chaudhuri et al. (2007).\\nHere is a little example, borrowed from Whittaker (1990). Suppose that\\nour model is as depicted in Figure 17.4, along with its empirical covariance\\nmatrix S. We apply algorithm (17.1) to this problem; for example, in the\\nmodiﬁed regression for variable 1 in step (b), variable 3 is left out. The\\nprocedure quickly converged to the solutions:\\nˆΣ=\\uf8eb\\n\\uf8ec\\uf8ed10.00 1 .00 1.31 4.00\\n1.00 10 .00 2 .00 0.87\\n1.31 2.00 10 .00 3 .00\\n4.00 0.87 3.00 10 .00\\uf8f6\\n\\uf8f7\\uf8f8,ˆΣ−1=\\uf8eb\\n\\uf8ec\\uf8ed0.12−0.010.00−0.05\\n−0.01 0 .11−0.020.00\\n0.00−0.02 0 .11−0.03\\n−0.050.00−0.03 0 .13\\uf8f6\\n\\uf8f7\\uf8f8.\\nNote the zeroes in ˆΣ−1, corresponding to the missing edges (1,3) and (2,4).\\nNote also that the corresponding elements in ˆΣare the only elements dif-\\nferent from S. The estimation of ˆΣis an example of what is sometimes\\ncalled the positive deﬁnite “completion” of S.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='63019566-3b2a-47f4-8169-2bccb9f4e9e1', embedding=None, metadata={'page_label': '654', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.3 Undirected Graphical Models for Continuous Variables 635\\n17.3.2 Estimation of the Graph Structure\\nIn most cases we do not know which edges to omit from our graph, and\\nso would like to try to discover this from the data itself. In recent years a\\nnumber of authors have proposed the use of L1(lasso) regularization for\\nthis purpose.\\nMeinshausen and B¨ uhlmann (2006) take a simple approach to the prob-\\nlem: rather than trying to fully estimate ΣorΘ=Σ−1, they only estimate\\nwhich components of θijare nonzero. To do this, they ﬁt a lasso regression\\nusing each variable as the response and the others as predictors. The com-\\nponent θijis then estimated to be nonzero if either the estimated coeﬃcient\\nof variable ionjis nonzero, orthe estimated coeﬃcient of variable jon\\niis nonzero (alternatively they use an andrule). They show that asymp-\\ntotically this procedure consistently estimates the set of nonzero elements\\nofΘ.\\nWe can take a more systematic approach with the lasso penalty, follow ing\\nthe development of the previous section. Consider maximizing the penalized\\nlog-likelihood\\nlog det Θ−trace(SΘ)−λ||Θ||1, (17.21)\\nwhere ||Θ||1is the L1norm—the sum of the absolute values of the elements\\nofΣ−1, and we have ignored constants. The negative of this penalized\\nlikelihood is a convex function of Θ.\\nIt turns out that one can adapt the lasso to give the exact maximizer of\\nthe penalized log-likelihood. In particular, we simply replace the modiﬁed\\nregression step (b) in Algorithm 17.1 by a modiﬁed lasso step. Here are the\\ndetails.\\nThe analog of the gradient equation (17.13) is now\\nΘ−1−S−λ≤Sign(Θ) =0. (17.22)\\nHere we use sub-gradient notation, with Sign( θjk) = sign( θjk) ifθjk̸= 0,\\nelse Sign( θjk)∈[−1,1] ifθjk= 0. Continuing the development in the\\nprevious section, we reach the analog of (17.18)\\nW11β−s12+λ≤Sign(β) = 0 (17.23)\\n(recall that βandθ12have opposite signs). We will now see that this system\\nis exactly equivalent to the estimating equations for a lasso regression.\\nConsider the usual regression setup with outcome variables yand pre-\\ndictor matrix Z. There the lasso minimizes\\n1\\n2(y−Zβ)T(y−Zβ) +λ≤ ||β||1 (17.24)\\n[see (3.52) on page 68; here we have added a factor1\\n2for convenience]. The\\ngradient of this expression is', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='abff12ed-591e-4cc3-9e26-e62b29489d2f', embedding=None, metadata={'page_label': '655', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='636 17. Undirected Graphical Models\\nAlgorithm 17.2 Graphical Lasso.\\n1. Initialize W=S+λI. The diagonal of Wremains unchanged in\\nwhat follows.\\n2. Repeat for j= 1,2,... p, 1,2,... p,... until convergence:\\n(a) Partition the matrix Winto part 1: all but the jth row and\\ncolumn, and part 2: the jth row and column.\\n(b) Solve the estimating equations W11β−s12+λ≤Sign(β) = 0\\nusing the cyclical coordinate-descent algorithm (17.26) for the\\nmodiﬁed lasso.\\n(c) Update w12=W11ˆβ\\n3. In the ﬁnal cycle (for each j) solve for ˆθ12=−ˆβ≤ˆθ22, with 1 /ˆθ22=\\nw22−wT\\n12ˆβ.\\nZTZβ−ZTy+λ≤Sign(β) = 0 (17.25)\\nSo up to a factor 1 /N,ZTyis the analog of s12, and we replace ZTZby\\nW11, the estimated cross-product matrix from our current model.\\nThe resulting procedure is called the graphical lasso , proposed by Fried-\\nman et al. (2008b) building on the work of Banerjee et al. (2008). It is\\nsummarized in Algorithm 17.2.\\nFriedman et al. (2008b) use the pathwise coordinate descent method\\n(Section 3.8.6) to solve the modiﬁed lasso problem at each stage. Here are\\nthe details of pathwise coordinate descent for the graphical lasso algorithm.\\nLetting V=W11, the update has the form\\nˆβj←S(\\ns12j−∑\\nk̸=jVkjˆβk,λ)\\n/Vjj (17.26)\\nforj= 1,2,... ,p −1,1,2,... ,p −1,..., where Sis the soft-threshold\\noperator:\\nS(x,t) = sign( x)(|x| −t)+. (17.27)\\nThe procedure cycles through the predictors until convergence.\\nIt is easy to show that the diagonal elements wjjof the solution matrix\\nWare simply sjj+λ, and these are ﬁxed in step 1 of Algorithm 17.23.\\nThe graphical lasso algorithm is extremely fast, and can solve a moder-\\nately sparse problem with 1000 nodes in less than a minute. It is easy to\\nmodify the algorithm to have edge-speciﬁc penalty parameters λjk; since\\n3An alternative formulation of the problem (17.21) can be pos ed, where we don’t\\npenalize the diagonal of Θ. Then the diagonal elements wjjof the solution matrix are\\nsjj, and the rest of the algorithm is unchanged.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2113e76c-8e97-4342-9739-ccba336eba33', embedding=None, metadata={'page_label': '656', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.3 Undirected Graphical Models for Continuous Variables 637\\nλjk=∞will force ˆθjkto be zero, this algorithm subsumes Algorithm 17.1.\\nBy casting the sparse inverse-covariance problem as a series of regressions,\\none can also quickly compute and examine the solution paths as a function\\nof the penalty parameter λ. More details can be found in Friedman et al.\\n(2008b).\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38JnkRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnk\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38JnkRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnkλ= 0 λ= 7λ= 27 λ= 36\\nFIGURE 17.5. Four diﬀerent graphical-lasso solutions for the ﬂow-cytomet ry\\ndata.\\nFigure 17.1 shows the result of applying the graphical lasso to the ﬂow-\\ncytometry dataset. Here the lasso penalty parameter λwas set at 14. In\\npractice it is informative to examine the diﬀerent sets of graphs that are\\nobtained as λis varied. Figure 17.5 shows four diﬀerent solutions. The\\ngraph becomes more sparse as the penalty parameter is increased.\\nFinally note that the values at some of the nodes in a graphical model can\\nbe unobserved; that is, missing or hidden. If only some values are missing\\nat a node, the EM algorithm can be used to impute the missing values', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fdff7909-20d9-43cc-baf1-b6e0217fba23', embedding=None, metadata={'page_label': '657', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='638 17. Undirected Graphical Models\\n(Exercise 17.9). However, sometimes the entire node is hidden or latent.\\nIn the Gaussian model, if a node has all missing values, due to linearity\\none can simply average over the missing nodes to yield another Gaussian\\nmodel over the observed nodes. Hence the inclusion of hidden nodes does\\nnot enrich the resulting model for the observed nodes; in fact, it imposes\\nadditional structure on its covariance matrix. However in the discrete model\\n(described next) the inherent nonlinearities make hidden units a powerful\\nway of expanding the model.\\n17.4 Undirected Graphical Models for Discrete\\nVariables\\nUndirected Markov networks with all discrete variables are popular, and\\nin particular pairwise Markov networks with binary variables being the\\nmost common. They are sometimes called Ising models in the statistical\\nmechanics literature, and Boltzmann machines in the machine learning lit-\\nerature, where the vertices are referred to as “nodes” or “units” and are\\nbinary-valued.\\nIn addition, the values at each node can be observed (“visible”) or un-\\nobserved (“hidden”). The nodes are often organized in layers, similar to a\\nneural network. Boltzmann machines are useful both for unsupervised and\\nsupervised learning, especially for structured input data such as images,\\nbut have been hampered by computational diﬃculties. Figure 17.6 shows\\na restricted Boltzmann machine (discussed later), in which some variables\\nare hidden, and only some pairs of nodes are connected. We ﬁrst consider\\nthe simpler case in which all pnodes are visible with edge pairs ( j,k) enu-\\nmerated in E.\\nDenoting the binary valued variable at node jbyXj, the Ising model\\nfor their joint probabilities is given by\\np(X,Θ) = exp[∑\\n(j,k)∈EθjkXjXk−Φ(Θ)]\\nforX∈ X, (17.28)\\nwithX={0,1}p. As with the Gaussian model of the previous section,\\nonly pairwise interactions are modeled. The Ising model was developed in\\nstatistical mechanics, and is now used more generally to model the joint\\neﬀects of pairwise interactions. Φ( Θ) is the log of the partition function,\\nand is deﬁned by\\nΦ(Θ) = log∑\\nx∈X[\\nexp(∑\\n(j,k)∈Eθjkxjxk)]\\n. (17.29)\\nThe partition function ensures that the probabilities add to one over the\\nsample space. The terms θjkXjXkrepresent a particular parametrization', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='090ed743-9cf3-484f-b9ce-ac8a6aaf2191', embedding=None, metadata={'page_label': '658', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.4 Undirected Graphical Models for Discrete Variables 639\\nof the (log) potential functions (17.5), and for technical reasons requires\\naconstant nodeX0≡1 to be included (Exercise 17.10), with “edges” to\\nall the other nodes. In the statistics literature, this model is equivalent\\nto a ﬁrst-order-interaction Poisson log-linear model for multiway tables of\\ncounts (Bishop et al., 1975; McCullagh and Nelder, 1989; Agresti, 2002).\\nThe Ising model implies a logistic form for each node conditional on the\\nothers (exercise 17.11):\\nPr(Xj= 1|X−j=x−j) =1\\n1 + exp( −θj0−∑\\n(j,k)∈Eθjkxk),(17.30)\\nwhere X−jdenotes all of the nodes except j. Hence the parameter θjk\\nmeasures the dependence of XjonXk, conditional on the other nodes.\\n17.4.1 Estimation of the Parameters when the Graph\\nStructure is Known\\nGiven some data from this model, how can we estimate the parameters?\\nSuppose we have observations xi= (xi1,xi2,... ,x ip)∈ {0,1}p, i= 1,... ,N .\\nThe log-likelihood is\\nℓ(Θ) =N∑\\ni=1log Pr Θ(Xi=xi)\\n=N∑\\ni=1\\uf8ee\\n\\uf8f0∑\\n(j,k)∈Eθjkxijxik−Φ(Θ)\\uf8f9\\n\\uf8fb (17.31)\\nThe gradient of the log-likelihood is\\n∂ℓ(Θ)\\n∂θjk=N∑\\ni=1xijxik−N∂Φ(Θ)\\n∂θjk(17.32)\\nand\\n∂Φ(Θ)\\n∂θjk=∑\\nx∈Xxjxk≤p(x,Θ)\\n= E Θ(XjXk) (17.33)\\nSetting the gradient to zero gives\\nˆE(XjXk)−EΘ(XjXk) = 0 (17.34)\\nwhere we have deﬁned', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='285686d4-3441-47a6-8913-e3caa8ed92db', embedding=None, metadata={'page_label': '659', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='640 17. Undirected Graphical Models\\nˆE(XjXk) =1\\nNN∑\\ni=1xijxik, (17.35)\\nthe expectation taken with respect to the empirical distribution of the data.\\nLooking at (17.34), we see that the maximum likelihood estimates simply\\nmatch the estimated inner products between the nodes to their observed\\ninner products. This is a standard form for the score (gradient) equation\\nfor exponential family models, in which suﬃcient statistics are set equal t o\\ntheir expectations under the model.\\nTo ﬁnd the maximum likelihood estimates, we can use gradient search\\nor Newton methods. However the computation of E Θ(XjXk) involves enu-\\nmeration of p(X,Θ) over 2p−2of the |X|= 2ppossible values of X, and is\\nnot generally feasible for large p(e.g., larger than about 30). For smaller\\np, a number of standard statistical approaches are available:\\nPoisson log-linear modeling , where we treat the problem as a large regres-\\nsion problem (Exercise 17.12). The response vector yis the vector of\\n2pcounts in each of the cells of the multiway tabulation of the data4.\\nThe predictor matrix Zhas 2prows and up to 1+ p+p2columns that\\ncharacterize each of the cells, although this number depends on the\\nsparsity of the graph. The computational cost is essentially that of a\\nregression problem of this size, which is O(p42p) and is manageable\\nforp <20. The Newton updates are typically computed by iteratively\\nreweighted least squares, and the number of steps is usually in the\\nsingle digits. See Agresti (2002) and McCullagh and Nelder (1989) for\\ndetails. Standard software (such as the Rpackageglm) can be used\\nto ﬁt this model.\\nGradient descent requires at most O(p22p−2) computations to compute\\nthe gradient, but may require many more gradient steps than the\\nsecond–order Newton methods. Nevertheless, it can handle slightly\\nlarger problems with p≤30. These computations can be reduced\\nby exploiting the special clique structure in sparse graphs, using the\\njunction-tree algorithm. Details are not given here.\\nIterative proportional ﬁtting (IPF) performs cyclical coordinate descent on\\nthe gradient equations (17.34). At each step a parameter is updated\\nso that its gradient equation is exactly zero. This is done in a cyclical\\nfashion until all the gradients are zero. One complete cycle costs the\\nsame as a gradient evaluation, but may be more eﬃcient. Jirou´ sek and\\nPˇ reuˇ cil (1995) implement an eﬃcient version of IPF, using junction\\ntrees.\\n4Each of the cell counts is treated as an independent Poisson v ariable. We get the\\nmultinomial model corresponding to (17.28) by conditionin g on the total count N(which\\nis also Poisson under this framework).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='46de85e3-533b-4d0a-b15a-4b67c8417775', embedding=None, metadata={'page_label': '660', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.4 Undirected Graphical Models for Discrete Variables 641\\nWhen pis large ( >30) other approaches have been used to approximate\\nthe gradient.\\n•The mean ﬁeld approximation (Peterson and Anderson, 1987) esti-\\nmates E Θ(XjXk) by E Θ(Xj)EΘ(Xj), and replaces the input vari-\\nables by their means, leading to a set of nonlinear equations for the\\nparameters θjk.\\n•To obtain near-exact solutions, Gibbs sampling (Section 8.6) is used\\nto approximate E Θ(XjXk) by successively sampling from the esti-\\nmated model probabilities Pr Θ(Xj|X−j) (see e.g. Ripley (1996)).\\nWe have not discussed decomposable models , for which the maximum\\nlikelihood estimates can be found in closed form without any iteration\\nwhatsoever. These models arise, for example, in trees: special graphs with\\ntree-structured topology. When computational tractability is a concern,\\ntrees represent a useful class of models and they sidestep the computational\\nconcerns raised in this section. For details, see for example Chapter 12 of\\nWhittaker (1990).\\n17.4.2 Hidden Nodes\\nWe can increase the complexity of a discrete Markov network by including\\nlatent or hidden nodes. Suppose that a subset of the variables XHare\\nunobserved or “hidden”, and the remainder XVare observed or “visible.”\\nThen the log-likelihood of the observed data is\\nℓ(Θ) =N∑\\ni=1log[Pr Θ(XV=xiV)]\\n=N∑\\ni=1[\\nlog∑\\nxH∈XHexp∑\\n(j,k)∈E(θjkxijxik−Φ(Θ))]\\n.(17.36)\\nThe sum over xHmeans that we are summing over all possible {0,1}values\\nfor the hidden units. The gradient works out to be\\ndℓ(Θ)\\ndθjk=ˆEVEΘ(XjXk|XV)−EΘ(XjXk) (17.37)\\nThe ﬁrst term is an empirical average of XjXkif both are visible; if one\\nor both are hidden, they are ﬁrst imputed given the visible data, and then\\naveraged over the hidden variables. The second term is the unconditional\\nexpectation of XjXk.\\nThe inner expectation in the ﬁrst term can be evaluated using basic rules\\nof conditional expectation and properties of Bernoulli random variables. In\\ndetail, for observation i', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2f07dd39-8d99-4a2f-9fcf-3dacf3134271', embedding=None, metadata={'page_label': '661', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='642 17. Undirected Graphical Models\\nEΘ(XjXk|XV=xiV) ={xijxik ifj,k∈ V\\nxijPrΘ(Xk= 1|XV=xiV) if j∈ V,k∈ H\\nPrΘ(Xj= 1,Xk= 1|XV=xiV) ifj,k∈ H.\\n(17.38)\\nNow two separate runs of Gibbs sampling are required; the ﬁrst to estimate\\nEΘ(XjXk) by sampling from the model as above, and the second to esti-\\nmate E Θ(XjXk|XV=xiV). In this latter run, the visible units are ﬁxed\\n(“clamped”) at their observed values and only the hidden variables are\\nsampled. Gibbs sampling must be done for each observation in the training\\nset, at each stage of the gradient search. As a result this procedure can be\\nvery slow, even for moderate-sized models. In Section 17.4.4 we consider\\nfurther model restrictions to make these computations manageable.\\n17.4.3 Estimation of the Graph Structure\\nThe use of a lasso penalty with binary pairwise Markov networks has been\\nsuggested by Lee et al. (2007) and Wainwright et al. (2007). The ﬁrst au-\\nthors investigate a conjugate gradient procedure for exact maximization of\\na penalized log-likelihood. The bottleneck is the computation of E Θ(XjXk)\\nin the gradient; exact computation via the junction tree algorithm is man-\\nageable for sparse graphs but becomes unwieldy for dense graphs.\\nThe second authors propose an approximate solution, analogous to the\\nMeinshausen and B¨ uhlmann (2006) approach for the Gaussian graphical\\nmodel. They ﬁt an L1-penalized logistic regression model to each node as\\na function of the other nodes, and then symmetrize the edge parameter\\nestimates in some fashion. For example if ˜θjkis the estimate of the j-k\\nedge parameter from the logistic model for outcome node j, the “min”\\nsymmetrization sets ˆθjkto either ˜θjkor˜θkj, whichever is smallest in abso-\\nlute value. The “max” criterion is deﬁned similarly. They show that under\\ncertain conditions either approximation estimates the nonzero edges cor-\\nrectly as the sample size goes to inﬁnity. Hoeﬂing and Tibshirani (2008)\\nextend the graphical lasso to discrete Markov networks, obtaining a pro-\\ncedure which is somewhat faster than conjugate gradients, but still must\\ndeal with computation of E Θ(XjXk). They also compare the exact and\\napproximate solutions in an extensive simulation study and ﬁnd the “min”\\nor “max” approximations are only slightly less accurate than the exact pro-\\ncedure, both for estimating the nonzero edges and for estimating the actual\\nvalues of the edge parameters, and are much faster. Furthermore, they can\\nhandle denser graphs because they never need to compute the quantities\\nEΘ(XjXk).\\nFinally, we point out a key diﬀerence between the Gaussian and binary\\nmodels. In the Gaussian case, both Σand its inverse will often be of interest,\\nand the graphical lasso procedure delivers estimates for both of these quan-\\ntities. However, the approximation of Meinshausen and B¨ uhlmann (2006)\\nfor Gaussian graphical models, analogous to the Wainwright et al. (2007 )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b3d5a8f6-05a1-4a5c-8ac2-1cf128f0088d', embedding=None, metadata={'page_label': '662', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.4 Undirected Graphical Models for Discrete Variables 643\\nXjXk\\nXℓ\\nVisible V1 Visible V2Hidden H\\nθjk\\nFIGURE 17.6. A restricted Boltzmann machine (RBM) in which there are no\\nconnections between nodes in the same layer. The visible units are subdivided to\\nallow the RBM to model the joint density of feature V1and their labels V2.\\napproximation for the binary case, only yields an estimate of Σ−1. In con-\\ntrast, in the Markov model for binary data, Θis the object of interest, and\\nits inverse is not of interest. The approximate method of Wainwright et a l.\\n(2007) estimates Θeﬃciently and hence is an attractive solution for the\\nbinary problem.\\n17.4.4 Restricted Boltzmann Machines\\nIn this section we consider a particular architecture for graphical models\\ninspired by neural networks, where the units are organized in layers. A\\nrestricted Boltzmann machine (RBM) consists of one layer of visible units\\nand one layer of hidden units with no connections within each layer. It\\nis much simpler to compute the conditional expectations (as in 17.37 and\\n17.38) if the connections between hidden units are removed5. Figure 17.6\\nshows an example; the visible layer is divided into input variables V1and\\noutput variables V2, and there is a hidden layer H. We denote such a\\nnetwork by\\nV1↔ H ↔ V 2. (17.39)\\nFor example, V1could be the binary pixels of an image of a handwritten\\ndigit, and V2could have 10 units, one for each of the observed class labels\\n0-9.\\nThe restricted form of this model simpliﬁes the Gibbs sampling for es-\\ntimating the expectations in (17.37), since the variables in each layer are\\nindependent of one another, given the variables in the other layers. Hence\\nthey can be sampled together, using the conditional probabilities given by\\nexpression (17.30).\\nThe resulting model is less general than a Boltzmann machine, but is still\\nuseful; for example it can learn to extract interesting features from images.\\n5We thank Geoﬀrey Hinton for assistance in the preparation of the material on RBMs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='78cce5d3-aaa4-4918-8536-12c52b89ef64', embedding=None, metadata={'page_label': '663', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='644 17. Undirected Graphical Models\\nBy alternately sampling the variables in each layer of the RBM shown\\nin Figure 17.6, it is possible to generate samples from the joint density\\nmodel. If the V1part of the visible layer is clamped at a particular feature\\nvector during the alternating sampling, it is possible to sample from the\\ndistribution over labels given V1. Alternatively classiﬁcation of test items\\ncan also be achieved by comparing the unnormalized joint densities of each\\nlabel category with the observed features. We do not need to compute the\\npartition function as it is the same for all of these combinations.\\nAs noted the restricted Boltzmann machine has the same generic form\\nas a single hidden layer neural network (Section 11.3). The edges in the\\nlatter model are directed, the hidden units are usually real-valued, and the\\nﬁtting criterion is diﬀerent. The neural network minimizes the error (cross-\\nentropy) between the targets and their model predictions, conditional on\\nthe input features. In contrast, the restricted Boltzmann machine maxi-\\nmizes the log-likelihood for the joint distribution of all visible units—tha t\\nis, the features and targets. It can extract information from the input fea-\\ntures that is useful for predicting the labels, but, unlike supervised learning\\nmethods, it may also use some of its hidden units to model structure in the\\nfeature vectors that is not immediately relevant for predicting the labels.\\nThese features may turn out to be useful, however, when combined with\\nfeatures derived from other hidden layers.\\nUnfortunately, Gibbs sampling in a restricted Boltzmann machine can\\nbe very slow, as it can take a long time to reach stationarity. As the net -\\nwork weights get larger, the chain mixes more slowly and we need to run\\nmore steps to get the unconditional estimates. Hinton (2002) noticed em-\\npirically that learning still works well if we estimate the second expectatio n\\nin (17.37) by starting the Markov chain at the data and only running for a\\nfew steps (instead of to convergence). He calls this contrastive divergence :\\nwe sample HgivenV1,V2, then V1,V2givenHand ﬁnally HgivenV1,V2\\nagain. The idea is that when the parameters are far from the solution, it\\nmay be wasteful to iterate the Gibbs sampler to stationarity, as just a si ngle\\niteration will reveal a good direction for moving the estimates.\\nWe now give an example to illustrate the use of an RBM. Using con-\\ntrastive divergence, it is possible to train an RBM to recognize hand-written\\ndigits from the MNIST dataset (LeCun et al., 1998). With 2000 hidden\\nunits, 784 visible units for representing binary pixel intensities and one\\n10-way multinomial visible unit for representing labels, the RBM achieves\\nan error rate of 1.9% on the test set. This is a little higher than the 1.4%\\nachieved by a support vector machine and comparable to the error rate\\nachieved by a neural network trained with backpropagation. The error rate\\nof the RBM, however, can be reduced to 1.25% by replacing the 784 pixel\\nintensities by 500 features that are produced from the images without using\\nany label information. First, an RBM with 784 visible units and 500 hidden\\nunits is trained, using contrastive divergence, to model the set of images.\\nThen the hidden states of the ﬁrst RBM are used as data for training a', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c188f8dc-2f30-4825-8339-d94a2a415838', embedding=None, metadata={'page_label': '664', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 645\\nFIGURE 17.7. Example of a restricted Boltzmann machine for handwritten\\ndigit classiﬁcation. The network is depicted in the schematic o n the left. Displayed\\non the right are some diﬃcult test images that the model class iﬁes correctly.\\nsecond RBM that has 500 visible units and 500 hidden units. Finally, the\\nhidden states of the second RBM are used as the features for training an\\nRBM with 2000 hidden units as a joint density model. The details and\\njustiﬁcation for learning features in this greedy, layer-by-layer way are de-\\nscribed in Hinton et al. (2006). Figure 17.7 gives a representation of t he\\ncomposite model that is learned in this way and also shows some examples\\nof the types of distortion that it can cope with.\\nBibliographic Notes\\nMuch work has been done in deﬁning and understanding the structure of\\ngraphical models. Comprehensive treatments of graphical models can be\\nfound in Whittaker (1990), Lauritzen (1996), Cox and Wermuth (1996),\\nEdwards (2000), Pearl (2000), Anderson (2003), Jordan (2004), and Kol ler\\nand Friedman (2007). Wasserman (2004) gives a brief introduction, and\\nChapter 8 of Bishop (2006) gives a more detailed overview. Boltzmann\\nmachines were proposed in Ackley et al. (1985). Ripley (1996) has a detailed\\nchapter on topics in graphical models that relate to machine learning. We\\nfound this particularly useful for its discussion of Boltzmann machines.\\nExercises\\nEx. 17.1 For the Markov graph of Figure 17.8, list all of the implied condi-\\ntional independence relations and ﬁnd the maximal cliques.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4cef7271-c4bf-4e72-8489-c4d82c49a637', embedding=None, metadata={'page_label': '665', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='646 17. Undirected Graphical Models\\nX1\\nX2X3X4\\nX5\\nX6\\nFIGURE 17.8.\\nEx. 17.2 Consider random variables X1,X2,X3,X4. In each of the following\\ncases draw a graph that has the given independence relations:\\n(a)X1⊥X3|X2andX2⊥X4|X3.\\n(b)X1⊥X4|X2,X3andX2⊥X4|X1,X3.\\n(c)X1⊥X4|X2,X3,X1⊥X3|X2,X4andX3⊥X4|X1,X2.\\nEx. 17.3 LetΣbe the covariance matrix of a set of pvariables X. Consider\\nthe partial covariance matrix Σa.b=Σaa−ΣabΣ−1\\nbbΣbabetween the two\\nsubsets of variables Xa= (X1,X2) consisting of the ﬁrst two, and Xb\\nthe rest. This is the covariance matrix between these two variables, after\\nlinear adjustment for all the rest. In the Gaussian distribution, this is the\\ncovariance matrix of the conditional distribution of Xa|Xb. The partial\\ncorrelation coeﬃcient ρjk|restbetween the pair Xaconditional on the rest\\nXb, is simply computed from this partial covariance. Deﬁne Θ=Σ−1.\\n1. Show that Σa.b=Θ−1\\naa.\\n2. Show that if any oﬀ-diagonal element of Θ is zero, then the partial\\ncorrelation coeﬃcient between the corresponding variables is zero.\\n3. Show that if we treat Θas if it were a covariance matrix, and compute\\nthe corresponding “correlation” matrix\\nR= diag(Θ)−1/2≤Θ≤diag(Θ)−1/2, (17.40)\\nthenrjk=−ρjk|rest\\nEx. 17.4 Denote by\\nf(X1|X2,X3,... ,X p)\\nthe conditional density of X1given X2,... ,X p. If\\nf(X1|X2,X3,... ,X p) =f(X1|X3,... ,X p),\\nshow that X1⊥X2|X3,... ,X p.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='816df729-324c-4c13-b4e9-1a4dcd8a709d', embedding=None, metadata={'page_label': '666', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 647\\nEx. 17.5 Consider the setup in Section 17.4.1 with no missing edges. Show\\nthat\\nS11β−s12= 0\\nare the estimating equations for the multiple regression coeﬃcients of the\\nlast variable on the rest.\\nEx. 17.6 Recovery of ˆΘ=ˆΣ−1from Algorithm 17.1 . Use expression (17.16)\\nto derive the standard partitioned inverse expressions\\nθ12=−W−1\\n11w12θ22 (17.41)\\nθ22= 1/(w22−wT\\n12W−1\\n11w12). (17.42)\\nSince ˆβ=W−1\\n11w12, show that ˆθ22= 1/(w22−wT\\n12ˆβ) and ˆθ12=−ˆβˆθ22.\\nThus ˆθ12is a simply rescaling of ˆβby−ˆθ22.\\nEx. 17.7 Write a program to implement the modiﬁed regression procedure\\n(17.1) for ﬁtting the Gaussian graphical model with pre-speciﬁed edges\\nmissing. Test it on the ﬂow cytometry data from the book website, using\\nthe graph of Figure 17.1.\\nEx. 17.8\\n(a) Write a program to ﬁt the lasso using the coordinate descent procedure\\n(17.26). Compare its results to those from the larsprogram or some\\nother convex optimizer, to check that it is working correctly.\\n(b) Using the program from (a), write code to implement the graphical\\nlasso algorithm (17.2). Apply it to the ﬂow cytometry data from the\\nbook website. Vary the regularization parameter and examine the\\nresulting networks.\\nEx. 17.9 Suppose that we have a Gaussian graphical model in which some\\nor all of the data at some vertices are missing.\\n(a) Consider the EM algorithm for a dataset of Ni.i.d. multivariate ob-\\nservations xi∈IRpwith mean θand covariance matrix Σ. For each\\nsample i, letoiandmiindex the predictors that are observed and\\nmissing, respectively. Show that in the E step, the observations are\\nimputed from the current estimates of θandΣ:\\nˆxi,mi= E(xi,mi|xi,oi,θ) = ˆθmi+ˆΣmi,oiˆΣ−1\\noi,oi(xi,oi−ˆθoi)\\n(17.43)\\nwhile in the M step, θandΣare re-estimated from the empirical\\nmean and (modiﬁed) covariance of the imputed data:\\nˆθj=N∑\\ni=1ˆxij/N', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c010411b-e03d-4395-83de-6d45e2153c66', embedding=None, metadata={'page_label': '667', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='648 17. Undirected Graphical Models\\nˆΣjj′=N∑\\ni=1[(ˆxij−ˆθj)(ˆxij;−ˆθj′) +ci,jj′]/N (17.44)\\nwhere ci,jj′=ˆΣjj′ifj,j′∈miand zero otherwise. Explain the reason\\nfor the correction term ci,jj′(Little and Rubin, 2002).\\n(b) Implement the EM algorithm for the Gaussian graphical model using\\nthe modiﬁed regression procedure from Exercise 17.7 for the M-step.\\n(c) For the ﬂow cytometry data on the book website, set the data for the\\nlast protein Jnkin the ﬁrst 1000 observations to missing, ﬁt the model\\nof Figure 17.1, and compare the predicted values to the actual values\\nforJnk. Compare the results to those obtained from a regression of\\nJnkon the other vertices with edges to Jnkin Figure 17.1, using only\\nthe non-missing data.\\nEx. 17.10 Using a simple binary graphical model with just two variables,\\nshow why it is essential to include a constant node X0≡1 in the model.\\nEx. 17.11 Show that the Ising model (17.28) (17.28) for the joint probabili-\\nties in a discrete graphical model implies that the conditional distributions\\nhave the logistic form (17.30).\\nEx. 17.12 Consider a Poisson regression problem with pbinary variables\\nxij, j= 1,... ,p and response variable yiwhich measures the number of\\nobservations with predictor xi∈ {0,1}p. The design is balanced, in that all\\nn= 2ppossible combinations are measured. We assume a log-linear model\\nfor the Poisson mean in each cell\\nlogθ(X) =θ00+∑\\n(j,k)∈Exijxikθjk, (17.45)\\nusing the same notation as in Section 17.4.1 (including the constant variable\\nxi0= 1∀i). We assume the response is distributed as\\nPr(Y=y|X=x) =e−θ(x)θ(x)y\\ny!. (17.46)\\nWrite down the conditional log-likelihood for the observed responses yi,\\nand compute the gradient.\\n(a) Show that the gradient equation for θ00computes the partition func-\\ntion (17.29).\\n(b) Show that the gradient equations for the remainder of the parameters\\nare equivalent to the gradient (17.34).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0a366ef8-f671-4493-b4dd-3da3e1011ca3', embedding=None, metadata={'page_label': '668', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the number of\\nfeatures pis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, especially in\\ngenomics and other areas of computational biology. We will see that high\\nvariance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the methods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses the more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that applies when p≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhere εwas generated from a standard Gaussian distribution. For each\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\\nstandard deviation σwas chosen in each case so that the signal-to-noise\\nratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d1ba1ce-e4d9-46c0-a8f0-01494a3a2dbe', embedding=None, metadata={'page_label': '669', 'file_name': 'ESLII.pdf', 'file_path': '/content/data/ESLII.pdf', 'file_type': 'application/pdf', 'file_size': 21644344, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21', 'last_accessed_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='650 18. High-Dimensional Problems: p≫N1.0 1.5 2.0 2.5 3.0Relative error\\n20 9 220 featuresTest Error\\n1.0 1.5 2.0 2.5 3.0\\n99 35 7100 features\\n1.0 1.5 2.0 2.5 3.0\\n99 87 431000 features\\nEffective Degrees of Freedom\\nFIGURE 18.1. Test-error results for simulation experiments. Shown are box-\\nplots of the relative test errors over 100simulations, for three diﬀerent values\\nofp, the number of features. The relative error is the test error d ivided by the\\nBayes error, σ2. From left to right, results are shown for ridge regression w ith\\nthree diﬀerent values of the regularization parameter λ:0.001,100and1000. The\\n(average) eﬀective degrees of freedom in the ﬁt is indicated be low each plot.\\nvariate regression coeﬃcients1was 9, 33 and 331, respectively, averaged\\nover the 100 simulation runs. The p= 1000 case is designed to mimic the\\nkind of data that we might see in a high-dimensional genomic or proteomic\\ndataset, for example.\\nWe ﬁt a ridge regression to the data, with three diﬀerent values for the\\nregularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\\nis nearly the same as least squares regression, with a little regularizati on\\njust to ensure that the problem is non-singular when p > N . Figure 18.1\\nshows boxplots of the relative test error achieved by the diﬀerent estimator s\\nin each scenario. The corresponding average degrees of freedom used in\\neach ridge-regression ﬁt is indicated (computed using formula (3.50) on\\npage 682). The degrees of freedom is a more interpretable parameter than\\nλ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;\\nλ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when\\np= 1000,\\nHere is an explanation for these results. When p= 20, we ﬁt all the way\\nand we can identify as many of the signiﬁcant coeﬃcients as possible with\\n1We call a regression coeﬃcient signiﬁcant if |bβj/bsej| ≥2, where ˆβjis the estimated\\n(univariate) coeﬃcient and bsejis its estimated standard error.\\n2For a ﬁxed value of the regularization parameter λ, the degrees of freedom depends\\non the observed predictor values in each simulation. Hence w e compute the average\\ndegrees of freedom over simulations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "documents=SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5-mOhoIhfo0",
        "outputId": "0d7cca68-1577-426f-a413-b6967dea9bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "## Default format supportable by LLama2\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "4Gw_a6aA4kZZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "07d43027b05d43a284931d1606cf82f3",
            "79e5e8be0ce9498db58a0d1efa3151b4",
            "3b5e9e0ed5774bdfa5206024a6e4bc6d",
            "d1bd269861354d32b8c2e88f093bf2fe",
            "1876c0261ab547099cade84aae76730c",
            "a5f8bcb2788b498190be11c600019076",
            "2058a3f47ccd4e0ba96154f179ad718c",
            "2fb476544c784c00a1c5488b0d931b41",
            "0c97885f61214f9194b6466989acf8b8",
            "a9ff720cf3b14d068caca4197d7faa3d",
            "ac1209b611024814813d13e375b0ba69",
            "ca879187f07746f78beed164029f01ae",
            "568d2f119c60410f83a6e98c2ca1578b",
            "ff7b310010e6456280216f6ef715deeb",
            "f98feb2d4d834051a46cc526ff08159c",
            "0aab9d867c7c438e84c5ac08763be89d",
            "f248a639c5a845739f3b74b2697e870d",
            "3f3e5c46dfa1415ca87360695506c42a",
            "7b238587f900456ebe652b0eca5c3ed3",
            "75647d2ccace479481dbf16438bb4cfe",
            "43131f5040b74fe7add51fe37d01d0ed",
            "34a0b1dd1ac14192b04ca577eec2fe9b",
            "a0323df87245407cbe66ee30e73454ec",
            "27248eb8bfa04736b785c516bda5cab2",
            "d4b61ce85d6e4933bace9851ea7a8e92",
            "41ab5b349bff4a518d1dcc5dfc536fe9",
            "f10c3d17547b4c1ab7e8cd168e2bade5",
            "16dcf4a1e5bb4cc9959e48fdf6d48177",
            "44c96b5494ab4776af1549989836a2e8",
            "f04aa16b5be14d77bfca03e4ca65706a",
            "8b7cd332ed854665bab1c7f64e49fa22",
            "bfa65675994345a3a22a48670e8b0683",
            "eeaa9858848448a294e7f39447b90d1d",
            "4e62c8fd519c42cbae355fc4440dbe6b",
            "f882dfde00a54c09a4100368ba2b2f29",
            "8e2d6af843404b63ba831adfa419d437",
            "5b8f9d3959194da5a1c6eafe246f3297",
            "834fbc008cf3447d9e1046710d170bd5",
            "6caa37ea5e504738b2eb721dc85698f5",
            "797608f208f644dbadc3b2f08b0d79f9",
            "579e8101cefd477abed87bd7023be6ad",
            "d033a4bbb04347ec807a5d5ed59b64b5",
            "50c74f3561cc49458ddbf6d3dfec1391",
            "f2e01ecdd47345adb9cb0b4364519aa3",
            "010bceb725e04458b6b71f4704d09f72",
            "f17badb293914896ae23760c835f1130",
            "a2faf4864c7842acb94d2b928f6293bc",
            "01b42b9acefc473ba6cd6cc783c46694",
            "27fd94ed17df4c1aa61ffa1738d2ed32",
            "894bc4c819a24f92a7b53b466e0a0301",
            "79a2bb52e1f74cc2b8bee35726de3b2b",
            "165ca9bcfd344fb5b213a5f9b0556811",
            "f8450e23aad34bcfbc92fa304f35899c",
            "c3dad70fd77a42fa92436ab47427fceb",
            "c62edcacab11475b85c785916205575b",
            "1e3a38f6d1bd42599d7a25332b29e61e",
            "d37f9f734fcb4547914d90b55be5e67c",
            "d92a530ec11f49e4b3fc0e4267a76e4b",
            "cdb0b6c2c50b4a75a0b3497a585d01d4",
            "bcef3620e5ca4b098849459dbd2fe834",
            "2acb93867d78497080cc6fd53e87ff3f",
            "b83cebbde5ec40958034931b7f36cda4",
            "2fa2fdcc55a84aa9b36b33b6dd187a3d",
            "04c56444617c455ca9e7e967d0977711",
            "31dd757433fb41c39ca45f7a5199a5ee",
            "f893c422b2444e3db4f29882a54c9c95",
            "89e5f7b29bfb454bb15ba3eed5916d37",
            "c50b5b5b10a1400da988015ddaf5d792",
            "0f59042505f644aca092f0f79bc54b4c",
            "5717cd9ffed842aa87d9faa5b32a87a8",
            "6535e5cb00f64e76b5af54b16ca75b9c",
            "57bb8b5642b64c538ee59bbb4b8e1270",
            "6f583050a41b4fd98aa0d559801e1172",
            "08cf56b4e3924dc3bccee23231f3519d",
            "97b15daf291c4f288e2d76c9d3a400c8",
            "d9e9d61790844f468257bc63d349d249",
            "36151ede110a4f83a6055cb5c1d415c9",
            "48c4a8076ffe49448a3d8b343c08fac9",
            "29c78294bee844d59f40f2962436ba72",
            "d85435d60d02442d820ebafc575f3d3b",
            "f53c10e87e3e436f83ad7d590a5399a6",
            "1dd3a2d864b44a47a5dda1f36f5da1c6",
            "e5a8a92ed5df44d8b8e6781882a98dcf",
            "226c59af24b949e09dc9b0ad9d65206e",
            "aa76d669fb5640c4bb12626d10b9b4f5",
            "581b808654ba415a88b09c1d771ddcdb",
            "58af3bef99514d259e69bed2309f0511",
            "3cab9876ad3644edb442077138c0b0bd",
            "5197f875ea784ee9a1d773fd0ebccfad",
            "f37c4a2850d049ce89b31238e05929e9",
            "c622e895f23943d0acbc78bdb8b4ecdf",
            "6b0a3dc0278043d1b3461804e5aee0f9",
            "73692a9c7f6e47518d6983d95130f4e7",
            "4c919ef97da14876b5c7205f0bd8ba0a",
            "df0a1f72e853476bb9b05f5b48d5e096",
            "8278ee3fcbdc48f7b8aac3183e6eb2bf",
            "57f38417574f4ea4b05d8415fd6100ed",
            "3c7e28bc2b624bb8aed0ba2f0aab8be0",
            "42d6d8ed9a6c4eff9b1eeb27e6534087",
            "5ef266b4bb83445eab0c0407decdc987",
            "8b865e5f72e74f61a320999660662b1b",
            "282b762072cc4980b0f3364feebc3239",
            "d46f99424236455bb85fae0a662c6a0d",
            "2ae01fe894134a6cb338b6cce687c6cc",
            "7adf753e509b43f88b765737f6cd587e",
            "e76da2bd8fea4392b81f3f0e90fd9044",
            "f9384b919d584731ad42c804f267d123",
            "4605e74bcd884742baf831bcfb076559",
            "9d7469c6ff304a8eacecad88d1fd641c",
            "2b65a7c25d6940afa091a611398f73ed",
            "9f5849a2a96b45ddb1a34001b8f8fb3f",
            "729461b3470745b7883afab091911bd2",
            "dcaebd472e96465b9043eba19ed71960",
            "5537a71394164594a3563b8ea703fc90",
            "c174c167dff64f7099d31c08b0b9aacb",
            "ad307c054be747d8b72b53257c833faa",
            "218af710143746fb8d67ca819e1d114f",
            "49ea4f1f9ed84dddb26ee71d803370aa",
            "024ac37e88b04554bf675944f3d4bfba",
            "f620edd1613b40cf9d7e649e4c78b970",
            "1cb1d6c9eec84018873819956838ef8f"
          ]
        },
        "id": "jyVOhSuhhqdb",
        "outputId": "47117d9d-5f72-43e9-ba4e-463f6e4509a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07d43027b05d43a284931d1606cf82f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca879187f07746f78beed164029f01ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0323df87245407cbe66ee30e73454ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e62c8fd519c42cbae355fc4440dbe6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "010bceb725e04458b6b71f4704d09f72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e3a38f6d1bd42599d7a25332b29e61e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89e5f7b29bfb454bb15ba3eed5916d37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48c4a8076ffe49448a3d8b343c08fac9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5197f875ea784ee9a1d773fd0ebccfad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ef266b4bb83445eab0c0407decdc987"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f5849a2a96b45ddb1a34001b8f8fb3f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "e3fdd9c511454ded8b74218bd5800a7d",
            "93ec2acaa67a4fa2b052fd1149f14220",
            "a06982d10d8347b5ab809e5ebb845046",
            "83f1aeb88ead41798060e300043bf148",
            "91525ef4c5b84cd2a50f5c80311bfb84",
            "353645d53f824396b5e442e706d32169",
            "dc33195b4d8c480180a219c1538c5885",
            "29b4bb995c2d43428863992b18a45bb2",
            "27743385c0764033bb2fdb25c1736277",
            "8fe37e125bd945d7925924f83f517477",
            "20934dd736bf4b51b148e2eadb93083e",
            "a357ca735f73486caf30ea7b1021ae71",
            "38846f83812a43bda75a257d67b6db5d",
            "d0faf7d47832442cb50c386c43773606",
            "32c9322c51424d0a886e9887053e65fe",
            "5e748779468643f3bff3a66b36aa3a47",
            "a735b6fd091b44fda2f7164f3cf60926",
            "301cff2ddd184963a9d7e465297b41fd",
            "7d50260506d649f1b7a560979c429748",
            "35392f92b1444d61afa73c32c82aa6bd",
            "efe0194e2ee74ea19273a7701feaba41",
            "96e6d2c5da574b08acc96b36b81995ff",
            "040d475e55994caea51c9ddab4c9e930",
            "4c672fc900924aa18c921b37dd49b0f2",
            "6c48e6ba4b9946a19a8992fff936287d",
            "b1cf57721c704515a530cfc6783ddc60",
            "72ff618c98094202b9bfc4ae20d45c74",
            "ee79f70571f84c2091e7b09125bbd804",
            "b151982c25a74e98930156d2de709494",
            "633425dea2c14c0994cd866b324771ce",
            "2098ce7fe16c4227953be6d6818836a0",
            "19f8160962ab4924a4e8e72fbe0bfa26",
            "9e3bc23a966f4fbe9a65ee56c6af8610",
            "630d75101d5a4142a8a202793050c6c5",
            "bd2aa09e6e794530a94dc09317ba2ab8",
            "6c5298940f434bdebdb286879b8b4ca9",
            "d7969b627bdc40b7b2d5e5004b26aefe",
            "c1365c2514f84c97984c20a9c97fc489",
            "8c779df9d1d447bd87b7b9d614acca61",
            "12a85dae2ab6489abbd7f0383722f18e",
            "0505b2903b9f42c1be6ccbcf36aed6c3",
            "3b043062c20440dc814dd6a6893dee90",
            "824848de21954a4ea6dafc9c12c5882f",
            "d7e1e49a15894b8f89f3e61909740e65",
            "fdeaf3a981d445259374d252f7f329f7",
            "34ce545499ef44968a53d32a4846a316",
            "6ae078a3de5045d49b9564a21b41eb37",
            "4baf8a8291e04b86bc8e76952e3f3704",
            "1eabc0fabe5e457a8f8f1058811c6396",
            "fc16765aa33243fcabbc37d53a904aff",
            "ce2fadf4c0464489bbf3dc96f2a2d603",
            "48862232c1de4f8d96ebeb5ae240b5d6",
            "18904f526bb94fcaa3f658ca2a78e636",
            "a8e259d77c7346eda27c757207d5c649",
            "8c005cce347644aabbd07ad9ec703f3f",
            "1a60acd175f542578814902c1690190c",
            "94ad068866f54505a06bb66ee6b2fb6e",
            "297ec93c66f14bda9ca79b57bd228586",
            "e6f8f68ffb3343e6af16b67d0137b78f",
            "80ad62cfe14c423a9efdd03fdf876980",
            "7b410b8d17e64425829ff80bb6763a27",
            "83febf5162f34205b1a1481a4c1b5c03",
            "31933be30a3b48f693a2cb4ac6089d67",
            "9259981511f74bb29ba513e6deda25a7",
            "e7862bfb7e154bbb9c9d001b05756037",
            "f4ac46e42a9f4a0eb0c63bfbdc668f03",
            "d7219887a3e74de6ba96484b529e92cd",
            "cc3eb16c320b4be1a228a4bf33e5484d",
            "2145473863f3487699c22ac670fb435a",
            "769d85aaa349453cb00a5350b27b41ac",
            "828227a285e4473ca88a19cd3dd1fb47",
            "c179f49a15e3439b8d1dbbf26a78638e",
            "4cb6b8f547ed44daab403c2c1ce2f3d0",
            "986a4ff27b0c468ba638945c4e3aee0e",
            "68579f56ec1f4e4285c20a9bc4eda852",
            "6d84e572e1dd42cabb38406e26b55e96",
            "a6c4629c485e4eb6b1fad915949e9045",
            "7b3de4782f8d4ba1bf716cea18857cd7",
            "39ceab543685438f9839338871baaa82",
            "65fa75e17e65418b8dae505d05fc5861",
            "ba53d785bbd848bbabbb2875803abdc4",
            "8e3c135e386946d5b9a483998b5dfe55",
            "ffde76cce4a24acc952ced645e129e0f",
            "3018f4680d844a318557448c6d0ddf6f",
            "f71b009735764f369b59cc8c1b2b9fb5",
            "f31838c2824741ea8d7f6fa31ec90e22",
            "8c8df161bf7c471790065f2010ff07e0",
            "8387fa91177a45a7b491454e541a6064",
            "5a8f104672584882ad1a3bdf123f4307",
            "617000c8698d477d940619180546905a",
            "99de76ebcf40410097b216d9d36be3d2",
            "65adce2ef9284e4191c665c238e78e2b",
            "bb63cd2408fe41209fbb257171a65d99",
            "a0b2df6cbdf947b8ac39db18ea3d92ca",
            "d9ceccca2b974890a8180cf8d3f12066",
            "907910ddc59a468b97e1b6a340d367e8",
            "12ef29bbb7fb456d9ea54edcef3f338c",
            "525c66d8194c44cab4e6ef6291c81eeb",
            "6e97859b8eed4821adb67a99163cfd10",
            "f2b6a122926340e7b13cfd02d3328902",
            "a3dba4df7a3c4beca935f38fe9dfe162",
            "cb421bc362ab42fca85b1c7337e34f2b",
            "6dce5be5861f4effa4e87c61c4093878",
            "330d06eea19f4a789c826eb82746c6ab",
            "68aaf091ef5f4ea09a512034ce765297",
            "e4c348f1f12b4630bb7ccbd3e4c5ddf9",
            "809252b64854419d88e9c061e8683cca",
            "420dae5d49e041aeba04a4bfb84f9ce9",
            "25e1bfde506640be8356fca924600f57",
            "c32f454335754188a1cf2b8ca468e637",
            "44db732bf2484d37a4d9eaf7bb5323fb",
            "9d7b0b2e32724dbf8435b5322265f95b",
            "d66f81c2463e4009b092e099e8f56147",
            "0fc8ce54ddee42488c945b25ec841e10",
            "1a07d4f9d0dc4280b192be42a5c6a15a",
            "cec44136e6bd466799710b289e671a45",
            "27c24691f6494d1d985d249834c865f0",
            "a5bf8b6bff6147c58452aeeb5055f2a6",
            "1b077daf756f42088b0f72cdafeb5cd7",
            "cc23032c40634703abb2827de0f18308",
            "23cedbf47f914dcb95c989385be6a3e4"
          ]
        },
        "id": "pr1EN5sViQm9",
        "outputId": "36014415-c85f-43dc-a465-09c46aa21d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "<ipython-input-13-ffac2e45ddeb>:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3fdd9c511454ded8b74218bd5800a7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a357ca735f73486caf30ea7b1021ae71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "040d475e55994caea51c9ddab4c9e930"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "630d75101d5a4142a8a202793050c6c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdeaf3a981d445259374d252f7f329f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a60acd175f542578814902c1690190c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7219887a3e74de6ba96484b529e92cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b3de4782f8d4ba1bf716cea18857cd7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a8f104672584882ad1a3bdf123f4307"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2b6a122926340e7b13cfd02d3328902"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44db732bf2484d37a4d9eaf7bb5323fb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "\n",
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LduiJD2ajpy4"
      },
      "outputs": [],
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=512,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ_jtoK3kCeT",
        "outputId": "b3a04fcd-94fc-4178-ef7c-93543047e048"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7d17cde95510>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7d17cde95510>, id_func=<function default_id_func at 0x7d18fba05870>, chunk_size=512, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7d18f46ee8f0>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7d17cde95510>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "service_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "31jvrW2BkFEL"
      },
      "outputs": [],
      "source": [
        "index=VectorStoreIndex.from_documents(documents,service_context=service_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM-gNZ0-kRnO",
        "outputId": "5ae0aedf-be23-4f79-e1f2-70cf4a47f047"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7d18f8265870>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uSJzrMm6kTxf"
      },
      "outputs": [],
      "source": [
        "query_engine=index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ZIg5iUaxSE",
        "outputId": "8e74c17f-2793-4fc9-9c8d-776afc98611d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"How many diagonals can you draw in a decagon?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XKsLsgWSkfGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a90dc1-0d68-443e-f02a-4353343d3a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A decagon has 10 sides, and each side can be connected to 3 other sides, so you can draw 30 diagonals in a decagon.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L4lmVBNDJs1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'data/train-00000-of-00001.parquet', 'val': 'data/val-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
        "df = pd.read_parquet(\"hf://datasets/rvv-karma/Math-QA/\" + splits[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "xL5X-clmJtu1",
        "outputId": "e3ccebdd-378f-40ed-d5c3-10f4123c5279"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              topic                                        sub_topic  \\\n",
              "0           Algebra                           Evaluating expressions   \n",
              "1           Algebra                           Evaluating expressions   \n",
              "2           Algebra                           Evaluating expressions   \n",
              "3           Algebra                           Evaluating expressions   \n",
              "4           Algebra                           Evaluating expressions   \n",
              "...             ...                                              ...   \n",
              "34995  Trigonometry  Using trigonometry to solve real-world problems   \n",
              "34996  Trigonometry  Using trigonometry to solve real-world problems   \n",
              "34997  Trigonometry  Using trigonometry to solve real-world problems   \n",
              "34998  Trigonometry  Using trigonometry to solve real-world problems   \n",
              "34999  Trigonometry  Using trigonometry to solve real-world problems   \n",
              "\n",
              "                                                question  \\\n",
              "0          Solve the expression 2x + 5 for x when x = 3.   \n",
              "1      Evaluate the expression when x = 2 and y = 5:\\...   \n",
              "2      Evaluate the expression 2x^2 - 3x + 5 when x = 4.   \n",
              "3      If x = 4 and y = 2, evaluate the expression 3x...   \n",
              "4      If x = 3 and y = 4, evaluate the expression 2x...   \n",
              "...                                                  ...   \n",
              "34995  A flagpole stands vertically on level ground, ...   \n",
              "34996  A ladder leaning against a wall makes an angle...   \n",
              "34997  A flagpole stands vertically on a hillside tha...   \n",
              "34998  A student is standing 50 meters away from the ...   \n",
              "34999  A 10-meter pole is leaning against a wall with...   \n",
              "\n",
              "                                                  answer  \n",
              "0      To solve the expression 2x + 5 for x when x = ...  \n",
              "1      To evaluate the expression when x = 2 and y = ...  \n",
              "2      To evaluate the expression 2x^2 - 3x + 5 when ...  \n",
              "3      Given x = 4 and y = 2, we can substitute these...  \n",
              "4      To evaluate the expression 2x² - 3y + 5, we wi...  \n",
              "...                                                  ...  \n",
              "34995  To find the height of the flagpole, we can use...  \n",
              "34996  We can use the trigonometric function to solve...  \n",
              "34997  To find the length of the flagpole's shadow on...  \n",
              "34998  To find the height of the building, we can use...  \n",
              "34999  We can use the right triangle trigonometry to ...  \n",
              "\n",
              "[35000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff52695c-2fbe-449b-9bf3-8f304b6894a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>sub_topic</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Algebra</td>\n",
              "      <td>Evaluating expressions</td>\n",
              "      <td>Solve the expression 2x + 5 for x when x = 3.</td>\n",
              "      <td>To solve the expression 2x + 5 for x when x = ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Algebra</td>\n",
              "      <td>Evaluating expressions</td>\n",
              "      <td>Evaluate the expression when x = 2 and y = 5:\\...</td>\n",
              "      <td>To evaluate the expression when x = 2 and y = ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Algebra</td>\n",
              "      <td>Evaluating expressions</td>\n",
              "      <td>Evaluate the expression 2x^2 - 3x + 5 when x = 4.</td>\n",
              "      <td>To evaluate the expression 2x^2 - 3x + 5 when ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Algebra</td>\n",
              "      <td>Evaluating expressions</td>\n",
              "      <td>If x = 4 and y = 2, evaluate the expression 3x...</td>\n",
              "      <td>Given x = 4 and y = 2, we can substitute these...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Algebra</td>\n",
              "      <td>Evaluating expressions</td>\n",
              "      <td>If x = 3 and y = 4, evaluate the expression 2x...</td>\n",
              "      <td>To evaluate the expression 2x² - 3y + 5, we wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34995</th>\n",
              "      <td>Trigonometry</td>\n",
              "      <td>Using trigonometry to solve real-world problems</td>\n",
              "      <td>A flagpole stands vertically on level ground, ...</td>\n",
              "      <td>To find the height of the flagpole, we can use...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34996</th>\n",
              "      <td>Trigonometry</td>\n",
              "      <td>Using trigonometry to solve real-world problems</td>\n",
              "      <td>A ladder leaning against a wall makes an angle...</td>\n",
              "      <td>We can use the trigonometric function to solve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34997</th>\n",
              "      <td>Trigonometry</td>\n",
              "      <td>Using trigonometry to solve real-world problems</td>\n",
              "      <td>A flagpole stands vertically on a hillside tha...</td>\n",
              "      <td>To find the length of the flagpole's shadow on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34998</th>\n",
              "      <td>Trigonometry</td>\n",
              "      <td>Using trigonometry to solve real-world problems</td>\n",
              "      <td>A student is standing 50 meters away from the ...</td>\n",
              "      <td>To find the height of the building, we can use...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34999</th>\n",
              "      <td>Trigonometry</td>\n",
              "      <td>Using trigonometry to solve real-world problems</td>\n",
              "      <td>A 10-meter pole is leaning against a wall with...</td>\n",
              "      <td>We can use the right triangle trigonometry to ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35000 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff52695c-2fbe-449b-9bf3-8f304b6894a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff52695c-2fbe-449b-9bf3-8f304b6894a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff52695c-2fbe-449b-9bf3-8f304b6894a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c7c58cad-9bc0-4330-bcb3-98c340c8ef86\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c7c58cad-9bc0-4330-bcb3-98c340c8ef86')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c7c58cad-9bc0-4330-bcb3-98c340c8ef86 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b9bc9af6-07f6-4bb1-bd56-526e295dc0b9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b9bc9af6-07f6-4bb1-bd56-526e295dc0b9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 35000,\n  \"fields\": [\n    {\n      \"column\": \"topic\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"Differential geometry\",\n          \"Logic\",\n          \"Algebra\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sub_topic\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 620,\n        \"samples\": [\n          \"Theta functions and their applications\",\n          \"Manifolds and their classification\",\n          \"The R\\u00f6ssler system and its chaotic attractor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34316,\n        \"samples\": [\n          \"What are the critical points of the function \\nf(x) = 3x^4 - 16x^3 + 18x^2 + 24x - 5?\",\n          \"Determine whether the group G = {e,a,b,c,d} with the following Cayley table is isomorphic to either the cyclic group C10, the dihedral group D5, or the non-abelian group of order 10:\\n\\n|   | e | a | b | c | d |\\n|---|---|---|---|---|---|\\n| e | e | a | b | c | d |\\n| a | a | e | d | b | c |\\n| b | b | c | e | d | a |\\n| c | c | d | a | e | b |\\n| d | d | b | c | a | e |\",\n          \"Consider the function f(x) = 2x^3 - 9x^2 + 12x - 6. Find the inflection points of this function.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34818,\n        \"samples\": [\n          \"To use the Schwarz reflection principle, we first need to find an analytic continuation of the function $f(z)$ to a larger domain that includes the point $z=\\\\frac{1}{4}$. Since $f(z)$ has singularities at the cube roots of unity, we can consider the function $g(z) = (z^3-1)f(z) = \\\\frac{z^3-1}{z^3-1} = 1$ for $z \\\\neq 1, e^{2\\\\pi i/3}, e^{4\\\\pi i/3}$. This function $g(z)$ is holomorphic on the punctured complex plane $\\\\mathbb{C} \\\\setminus \\\\{1, e^{2\\\\pi i/3}, e^{4\\\\pi i/3}\\\\}$.\\n\\nNow, let's find an analytic continuation of $f(z)$ to the region $|z| < 1$. We can use the reflection formula $f(1/\\\\bar{z}) = \\\\overline{f(z)}$ for $|z|=1$. Since $f(z)$ is defined on the unit circle, we can define a new function $h(z) = f(1/\\\\bar{z})$ for $|z| > 1$. Then, we can define the function $F(z) = \\\\begin{cases} f(z) & |z| < 1 \\\\\\\\ h(z) & |z| > 1 \\\\end{cases}$. This function $F(z)$ is holomorphic on the punctured complex plane $\\\\mathbb{C} \\\\setminus \\\\{1, e^{2\\\\pi i/3}, e^{4\\\\pi i/3}\\\\}$.\\n\\nNow, we can find the value of $f(\\\\frac{1}{4})$ using the reflection formula. Since $\\\\left|\\\\frac{1}{4}\\\\right| < 1$, we have $f\\\\left(\\\\frac{1}{4}\\\\right) = F\\\\left(\\\\frac{1}{4}\\\\right) = h\\\\left(\\\\frac{1}{4}\\\\right) = f\\\\left(\\\\frac{1}{\\\\frac{1}{4}}\\\\right) = f(4)$.\\n\\nFinally, we can compute $f(4) = \\\\frac{1}{4^3-1} = \\\\frac{1}{64-1} = \\\\frac{1}{63}$.\\n\\nSo, $f\\\\left(\\\\frac{1}{4}\\\\right) = \\\\frac{1}{63}$.\",\n          \"To prove that G is isomorphic to either C6 or D3, we will use the following facts:\\n\\n1. The order of a group is the number of its elements.\\n2. The order of an element a in a group is the smallest positive integer n such that a^n = e, where e is the identity element.\\n3. Lagrange's theorem: The order of a subgroup H of a group G divides the order of G.\\n4. A group of prime order is cyclic.\\n\\nNow let's consider the possible orders of elements in G. Since the order of G is 6, by Lagrange's theorem, the possible orders of elements in G are 1, 2, 3, and 6. The identity element has order 1, so we are left with orders 2, 3, and 6.\\n\\nCase 1: G has an element of order 6.\\nIf G has an element a of order 6, then the subgroup generated by a, denoted by <a>, has order 6. Since the order of <a> is equal to the order of G, we have G = <a>. This means G is a cyclic group generated by a single element of order 6, so G is isomorphic to C6.\\n\\nCase 2: G has no elements of order 6.\\nIn this case, G must have elements of orders 2 and 3, as the identity element has order 1. Let a be an element of order 2 and b be an element of order 3. Then a^2 = e and b^3 = e.\\n\\nNow consider the element ab. Since (ab)^6 = a^6b^6 = e, the order of ab divides 6. The order of ab cannot be 1, as ab \\u2260 e. If the order of ab is 6, then G would have an element of order 6, which contradicts our assumption. Therefore, the order of ab must be either 2 or 3.\\n\\nIf the order of ab is 2, then (ab)^2 = a^2b^2 = e, which implies that a = b^(-1), and G = {e, a, b, b^2, ab, b^(-1)ab}. In this case, G has the same structure as the dihedral group D3, which has 6 elements and is generated by a rotation (b) and a reflection (a). Thus, G is isomorphic to D3.\\n\\nIf the order of ab is 3, then (ab)^3 = a^3b^3 = e, which implies that a = b^2. In this case, G = {e, a, b, b^2, ab, b^2ab} = {e, b^2, b, b^2, b^3, b^4}. However, this is a contradiction because G would be a cyclic group generated by b, which means G would have an element of order 6. This contradicts our assumption that G has no elements of order 6.\\n\\nTherefore, the only possible case is that G is isomorphic to D3.\\n\\nIn conclusion, G must be isomorphic to either the cyclic group C6 or the dihedral group D3. To determine which group is isomorphic to G, we can find all the possible subgroups for G and check their properties. If G has an element of order 6, it is isomorphic to C6. If G has elements of orders 2 and 3 but no elements of order 6, it is isomorphic to D3.\",\n          \"To find out how many units the factory will produce per day with the new machine, we first need to calculate the increase in production. \\n\\nThe increase in production is 20% of the current production, which is 5000 units per day. \\n\\n20% of 5000 = (20/100) * 5000 = 1000 units\\n\\nNow, we add this increase to the current production to find the total production with the new machine:\\n\\nTotal production with the new machine = Current production + Increase in production\\nTotal production with the new machine = 5000 units + 1000 units = 6000 units\\n\\nSo, the factory will produce 6000 units per day with the new machine.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_mjHXhbIrpy",
        "outputId": "3d3c208f-ab10-4919-ad95-fd3b3a18e38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            topic                     sub_topic  \\\n",
            "31580  Statistics        Monte Carlo simulation   \n",
            "31825  Statistics     Probability distributions   \n",
            "30986  Statistics         Central limit theorem   \n",
            "31277  Statistics         Discriminant analysis   \n",
            "31618  Statistics         Multivariate analysis   \n",
            "31553  Statistics        Monte Carlo simulation   \n",
            "31203  Statistics                   Data mining   \n",
            "31771  Statistics  Principal component analysis   \n",
            "31070  Statistics              Cluster analysis   \n",
            "32016  Statistics           Sampling techniques   \n",
            "31418  Statistics            Hypothesis testing   \n",
            "31125  Statistics          Confidence intervals   \n",
            "31030  Statistics              Cluster analysis   \n",
            "32028  Statistics           Sampling techniques   \n",
            "31610  Statistics         Multivariate analysis   \n",
            "31481  Statistics                 Markov chains   \n",
            "31528  Statistics        Monte Carlo simulation   \n",
            "31749  Statistics  Principal component analysis   \n",
            "31974  Statistics           Sampling techniques   \n",
            "31990  Statistics           Sampling techniques   \n",
            "31567  Statistics        Monte Carlo simulation   \n",
            "32147  Statistics          Time series analysis   \n",
            "31912  Statistics           Regression analysis   \n",
            "31702  Statistics             Outlier detection   \n",
            "30889  Statistics           Bayesian statistics   \n",
            "30922  Statistics                 Bootstrapping   \n",
            "30895  Statistics           Bayesian statistics   \n",
            "31961  Statistics             Robust statistics   \n",
            "32085  Statistics             Survival analysis   \n",
            "32037  Statistics         Statistical inference   \n",
            "\n",
            "                                                question  \\\n",
            "31580  A company wants to determine the expected numb...   \n",
            "31825  Suppose the length of time it takes a student ...   \n",
            "30986  A manufacturer produces light bulbs with a mea...   \n",
            "31277  In discriminant analysis, a student is given a...   \n",
            "31618  Here's a precise numerical problem for a math ...   \n",
            "31553  Suppose you are simulating the roll of a six-s...   \n",
            "31203  Suppose we have a dataset containing the ages ...   \n",
            "31771  What is the principal component for a dataset ...   \n",
            "31070  A retail store recorded the sales from 10 diff...   \n",
            "32016  A company wants to determine the average salar...   \n",
            "31418  Suppose a car manufacturer claims that their c...   \n",
            "31125  A coffee company wants to estimate the mean ca...   \n",
            "31030  What is the average distance between the centr...   \n",
            "32028  A problem could be: \\n\\nA company wants to sur...   \n",
            "31610  Suppose a survey is conducted among a group of...   \n",
            "31481  In a town, it has been observed that during su...   \n",
            "31528  Suppose that the probability of winning a game...   \n",
            "31749  Consider the following dataset of six observat...   \n",
            "31974  Suppose a factory produces light bulbs with an...   \n",
            "31990  A company produces batteries and they claim th...   \n",
            "31567  Assume you have a fair six-sided die. Use Mont...   \n",
            "32147  What is the forecasted value for the number of...   \n",
            "31912  In a regression analysis, the data set include...   \n",
            "31702  In a statistics class, a professor gives a mid...   \n",
            "30889  Suppose you want to estimate the probability t...   \n",
            "30922  Suppose that you collected a sample of 25 scor...   \n",
            "30895  A company produces light bulbs, and they claim...   \n",
            "31961  A math student wants to calculate the mean of ...   \n",
            "32085  A group of 50 patients were tested for the eff...   \n",
            "32037  A factory produces light bulbs and claims that...   \n",
            "\n",
            "                                                  answer  \\\n",
            "31580  To estimate the mean number of days it will ta...   \n",
            "31825  To find the probability that a randomly select...   \n",
            "30986  To solve this problem, we can use the Central ...   \n",
            "31277  To perform discriminant analysis, we first nee...   \n",
            "31618  To find the covariance and correlation coeffic...   \n",
            "31553  To find the probability of rolling at least on...   \n",
            "31203  To find the average age of the students in the...   \n",
            "31771  To find the principal component, we first need...   \n",
            "31070  To perform k-means clustering with 3 clusters,...   \n",
            "32016  To calculate the probability that one particul...   \n",
            "31418  To perform a hypothesis test at a 5% level of ...   \n",
            "31125  To create a 95% confidence interval for the me...   \n",
            "31030  To find the average distance between the centr...   \n",
            "32028  In simple random sampling, each employee has a...   \n",
            "31610  To determine the correlation between the score...   \n",
            "31481  Let S represent a sunny day and R represent a ...   \n",
            "31528  To perform a Monte Carlo simulation, we will g...   \n",
            "31749  To perform PCA on the given dataset, we first ...   \n",
            "31974  To solve this problem, we will use the Central...   \n",
            "31990  To calculate the 95% confidence interval for t...   \n",
            "31567  To estimate the probability using Monte Carlo ...   \n",
            "32147  To forecast the number of daily website visito...   \n",
            "31912  To calculate the expected score of a student w...   \n",
            "31702  To determine if there are any outliers using t...   \n",
            "30889  To solve this problem using Bayesian statistic...   \n",
            "30922  To estimate the 95% confidence interval for th...   \n",
            "30895  To solve this problem using Bayesian statistic...   \n",
            "31961  First, we need to find the median of the datas...   \n",
            "32085  To estimate the probability that a patient sur...   \n",
            "32037  To find the probability that the sample will h...   \n",
            "\n",
            "                                        generated_answer  \n",
            "31580  \\nTo estimate the mean number of days it will ...  \n",
            "31825  The probability that a randomly selected stude...  \n",
            "30986  The probability that the average lifetime of t...  \n",
            "31277  \\nThank you for asking! Based on the context i...  \n",
            "31618  \\n\\nPlease provide the answer in a clear and c...  \n",
            "31553  To calculate the probability of rolling at lea...  \n",
            "31203  The average age of the students in the classro...  \n",
            "31771  The principal component for the given covarian...  \n",
            "31070  Sure, I can help you with that! Based on the c...  \n",
            "32016  The probability of selecting a particular empl...  \n",
            "31418  The manufacturer's claim that their cars get a...  \n",
            "31125  \\nTo construct a 95% confidence interval for t...  \n",
            "31030  The average distance between the centroids of ...  \n",
            "32028  The probability of choosing a sample of 30 emp...  \n",
            "31610  The correlation between the scores of Mathemat...  \n",
            "31481  The probability that it will be sunny three da...  \n",
            "31528  To estimate the probability that a contestant ...  \n",
            "31749  The first principal component of the dataset i...  \n",
            "31974  The probability that the sample mean lifespan ...  \n",
            "31990  The 95% confidence interval for the true avera...  \n",
            "31567  To estimate the probability of rolling a total...  \n",
            "32147  \\n\\nPlease provide the forecasted value for th...  \n",
            "31912  The expected score of a student who studies fo...  \n",
            "31702  The scores of 93, 94, and 95 are outliers.\\n\\n...  \n",
            "30889  To estimate the probability that a new student...  \n",
            "30922  To estimate the 95% confidence interval for th...  \n",
            "30895  The probability that the company's claim is tr...  \n",
            "31961  \\nThe MAD is calculated as follows:\\n\\nMAD = √...  \n",
            "32085  I can help you with that! To estimate the prob...  \n",
            "32037  To determine the probability that the sample w...  \n"
          ]
        }
      ],
      "source": [
        "# Filter the DataFrame by the 'Statistics' topic\n",
        "filtered_df = df[df['topic'] == 'Statistics']\n",
        "\n",
        "# Sample 2 rows from the filtered DataFrame\n",
        "test_df = filtered_df.sample(n=30)\n",
        "\n",
        "# Initialize a list to store the answers\n",
        "answers = []\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in test_df.iterrows():\n",
        "    # Get the question\n",
        "    question = row['question']\n",
        "\n",
        "    # Use the RAG model to generate an answer\n",
        "    answer = query_engine.query(question)\n",
        "\n",
        "    # Store the generated answer\n",
        "    answers.append(answer)\n",
        "\n",
        "# Add the answers to the DataFrame\n",
        "test_df['generated_answer'] = answers\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rTUcq7FIIrrz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "072254d8-f49a-4295-cb6e-e4ba18899b58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            topic                     sub_topic  \\\n",
              "31580  Statistics        Monte Carlo simulation   \n",
              "31825  Statistics     Probability distributions   \n",
              "30986  Statistics         Central limit theorem   \n",
              "31277  Statistics         Discriminant analysis   \n",
              "31618  Statistics         Multivariate analysis   \n",
              "31553  Statistics        Monte Carlo simulation   \n",
              "31203  Statistics                   Data mining   \n",
              "31771  Statistics  Principal component analysis   \n",
              "31070  Statistics              Cluster analysis   \n",
              "32016  Statistics           Sampling techniques   \n",
              "31418  Statistics            Hypothesis testing   \n",
              "31125  Statistics          Confidence intervals   \n",
              "31030  Statistics              Cluster analysis   \n",
              "32028  Statistics           Sampling techniques   \n",
              "31610  Statistics         Multivariate analysis   \n",
              "31481  Statistics                 Markov chains   \n",
              "31528  Statistics        Monte Carlo simulation   \n",
              "31749  Statistics  Principal component analysis   \n",
              "31974  Statistics           Sampling techniques   \n",
              "31990  Statistics           Sampling techniques   \n",
              "31567  Statistics        Monte Carlo simulation   \n",
              "32147  Statistics          Time series analysis   \n",
              "31912  Statistics           Regression analysis   \n",
              "31702  Statistics             Outlier detection   \n",
              "30889  Statistics           Bayesian statistics   \n",
              "30922  Statistics                 Bootstrapping   \n",
              "30895  Statistics           Bayesian statistics   \n",
              "31961  Statistics             Robust statistics   \n",
              "32085  Statistics             Survival analysis   \n",
              "32037  Statistics         Statistical inference   \n",
              "\n",
              "                                                question  \\\n",
              "31580  A company wants to determine the expected numb...   \n",
              "31825  Suppose the length of time it takes a student ...   \n",
              "30986  A manufacturer produces light bulbs with a mea...   \n",
              "31277  In discriminant analysis, a student is given a...   \n",
              "31618  Here's a precise numerical problem for a math ...   \n",
              "31553  Suppose you are simulating the roll of a six-s...   \n",
              "31203  Suppose we have a dataset containing the ages ...   \n",
              "31771  What is the principal component for a dataset ...   \n",
              "31070  A retail store recorded the sales from 10 diff...   \n",
              "32016  A company wants to determine the average salar...   \n",
              "31418  Suppose a car manufacturer claims that their c...   \n",
              "31125  A coffee company wants to estimate the mean ca...   \n",
              "31030  What is the average distance between the centr...   \n",
              "32028  A problem could be: \\n\\nA company wants to sur...   \n",
              "31610  Suppose a survey is conducted among a group of...   \n",
              "31481  In a town, it has been observed that during su...   \n",
              "31528  Suppose that the probability of winning a game...   \n",
              "31749  Consider the following dataset of six observat...   \n",
              "31974  Suppose a factory produces light bulbs with an...   \n",
              "31990  A company produces batteries and they claim th...   \n",
              "31567  Assume you have a fair six-sided die. Use Mont...   \n",
              "32147  What is the forecasted value for the number of...   \n",
              "31912  In a regression analysis, the data set include...   \n",
              "31702  In a statistics class, a professor gives a mid...   \n",
              "30889  Suppose you want to estimate the probability t...   \n",
              "30922  Suppose that you collected a sample of 25 scor...   \n",
              "30895  A company produces light bulbs, and they claim...   \n",
              "31961  A math student wants to calculate the mean of ...   \n",
              "32085  A group of 50 patients were tested for the eff...   \n",
              "32037  A factory produces light bulbs and claims that...   \n",
              "\n",
              "                                                  answer  \\\n",
              "31580  To estimate the mean number of days it will ta...   \n",
              "31825  To find the probability that a randomly select...   \n",
              "30986  To solve this problem, we can use the Central ...   \n",
              "31277  To perform discriminant analysis, we first nee...   \n",
              "31618  To find the covariance and correlation coeffic...   \n",
              "31553  To find the probability of rolling at least on...   \n",
              "31203  To find the average age of the students in the...   \n",
              "31771  To find the principal component, we first need...   \n",
              "31070  To perform k-means clustering with 3 clusters,...   \n",
              "32016  To calculate the probability that one particul...   \n",
              "31418  To perform a hypothesis test at a 5% level of ...   \n",
              "31125  To create a 95% confidence interval for the me...   \n",
              "31030  To find the average distance between the centr...   \n",
              "32028  In simple random sampling, each employee has a...   \n",
              "31610  To determine the correlation between the score...   \n",
              "31481  Let S represent a sunny day and R represent a ...   \n",
              "31528  To perform a Monte Carlo simulation, we will g...   \n",
              "31749  To perform PCA on the given dataset, we first ...   \n",
              "31974  To solve this problem, we will use the Central...   \n",
              "31990  To calculate the 95% confidence interval for t...   \n",
              "31567  To estimate the probability using Monte Carlo ...   \n",
              "32147  To forecast the number of daily website visito...   \n",
              "31912  To calculate the expected score of a student w...   \n",
              "31702  To determine if there are any outliers using t...   \n",
              "30889  To solve this problem using Bayesian statistic...   \n",
              "30922  To estimate the 95% confidence interval for th...   \n",
              "30895  To solve this problem using Bayesian statistic...   \n",
              "31961  First, we need to find the median of the datas...   \n",
              "32085  To estimate the probability that a patient sur...   \n",
              "32037  To find the probability that the sample will h...   \n",
              "\n",
              "                                        generated_answer  \n",
              "31580  \\nTo estimate the mean number of days it will ...  \n",
              "31825  The probability that a randomly selected stude...  \n",
              "30986  The probability that the average lifetime of t...  \n",
              "31277  \\nThank you for asking! Based on the context i...  \n",
              "31618  \\n\\nPlease provide the answer in a clear and c...  \n",
              "31553  To calculate the probability of rolling at lea...  \n",
              "31203  The average age of the students in the classro...  \n",
              "31771  The principal component for the given covarian...  \n",
              "31070  Sure, I can help you with that! Based on the c...  \n",
              "32016  The probability of selecting a particular empl...  \n",
              "31418  The manufacturer's claim that their cars get a...  \n",
              "31125  \\nTo construct a 95% confidence interval for t...  \n",
              "31030  The average distance between the centroids of ...  \n",
              "32028  The probability of choosing a sample of 30 emp...  \n",
              "31610  The correlation between the scores of Mathemat...  \n",
              "31481  The probability that it will be sunny three da...  \n",
              "31528  To estimate the probability that a contestant ...  \n",
              "31749  The first principal component of the dataset i...  \n",
              "31974  The probability that the sample mean lifespan ...  \n",
              "31990  The 95% confidence interval for the true avera...  \n",
              "31567  To estimate the probability of rolling a total...  \n",
              "32147  \\n\\nPlease provide the forecasted value for th...  \n",
              "31912  The expected score of a student who studies fo...  \n",
              "31702  The scores of 93, 94, and 95 are outliers.\\n\\n...  \n",
              "30889  To estimate the probability that a new student...  \n",
              "30922  To estimate the 95% confidence interval for th...  \n",
              "30895  The probability that the company's claim is tr...  \n",
              "31961  \\nThe MAD is calculated as follows:\\n\\nMAD = √...  \n",
              "32085  I can help you with that! To estimate the prob...  \n",
              "32037  To determine the probability that the sample w...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a829326-e5e3-402b-9258-655e5a049314\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>sub_topic</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>generated_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31580</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Monte Carlo simulation</td>\n",
              "      <td>A company wants to determine the expected numb...</td>\n",
              "      <td>To estimate the mean number of days it will ta...</td>\n",
              "      <td>\\nTo estimate the mean number of days it will ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31825</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Probability distributions</td>\n",
              "      <td>Suppose the length of time it takes a student ...</td>\n",
              "      <td>To find the probability that a randomly select...</td>\n",
              "      <td>The probability that a randomly selected stude...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30986</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Central limit theorem</td>\n",
              "      <td>A manufacturer produces light bulbs with a mea...</td>\n",
              "      <td>To solve this problem, we can use the Central ...</td>\n",
              "      <td>The probability that the average lifetime of t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31277</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Discriminant analysis</td>\n",
              "      <td>In discriminant analysis, a student is given a...</td>\n",
              "      <td>To perform discriminant analysis, we first nee...</td>\n",
              "      <td>\\nThank you for asking! Based on the context i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31618</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Multivariate analysis</td>\n",
              "      <td>Here's a precise numerical problem for a math ...</td>\n",
              "      <td>To find the covariance and correlation coeffic...</td>\n",
              "      <td>\\n\\nPlease provide the answer in a clear and c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31553</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Monte Carlo simulation</td>\n",
              "      <td>Suppose you are simulating the roll of a six-s...</td>\n",
              "      <td>To find the probability of rolling at least on...</td>\n",
              "      <td>To calculate the probability of rolling at lea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31203</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Data mining</td>\n",
              "      <td>Suppose we have a dataset containing the ages ...</td>\n",
              "      <td>To find the average age of the students in the...</td>\n",
              "      <td>The average age of the students in the classro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31771</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Principal component analysis</td>\n",
              "      <td>What is the principal component for a dataset ...</td>\n",
              "      <td>To find the principal component, we first need...</td>\n",
              "      <td>The principal component for the given covarian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31070</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Cluster analysis</td>\n",
              "      <td>A retail store recorded the sales from 10 diff...</td>\n",
              "      <td>To perform k-means clustering with 3 clusters,...</td>\n",
              "      <td>Sure, I can help you with that! Based on the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32016</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Sampling techniques</td>\n",
              "      <td>A company wants to determine the average salar...</td>\n",
              "      <td>To calculate the probability that one particul...</td>\n",
              "      <td>The probability of selecting a particular empl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31418</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Hypothesis testing</td>\n",
              "      <td>Suppose a car manufacturer claims that their c...</td>\n",
              "      <td>To perform a hypothesis test at a 5% level of ...</td>\n",
              "      <td>The manufacturer's claim that their cars get a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31125</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Confidence intervals</td>\n",
              "      <td>A coffee company wants to estimate the mean ca...</td>\n",
              "      <td>To create a 95% confidence interval for the me...</td>\n",
              "      <td>\\nTo construct a 95% confidence interval for t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31030</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Cluster analysis</td>\n",
              "      <td>What is the average distance between the centr...</td>\n",
              "      <td>To find the average distance between the centr...</td>\n",
              "      <td>The average distance between the centroids of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32028</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Sampling techniques</td>\n",
              "      <td>A problem could be: \\n\\nA company wants to sur...</td>\n",
              "      <td>In simple random sampling, each employee has a...</td>\n",
              "      <td>The probability of choosing a sample of 30 emp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31610</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Multivariate analysis</td>\n",
              "      <td>Suppose a survey is conducted among a group of...</td>\n",
              "      <td>To determine the correlation between the score...</td>\n",
              "      <td>The correlation between the scores of Mathemat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31481</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Markov chains</td>\n",
              "      <td>In a town, it has been observed that during su...</td>\n",
              "      <td>Let S represent a sunny day and R represent a ...</td>\n",
              "      <td>The probability that it will be sunny three da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31528</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Monte Carlo simulation</td>\n",
              "      <td>Suppose that the probability of winning a game...</td>\n",
              "      <td>To perform a Monte Carlo simulation, we will g...</td>\n",
              "      <td>To estimate the probability that a contestant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31749</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Principal component analysis</td>\n",
              "      <td>Consider the following dataset of six observat...</td>\n",
              "      <td>To perform PCA on the given dataset, we first ...</td>\n",
              "      <td>The first principal component of the dataset i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31974</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Sampling techniques</td>\n",
              "      <td>Suppose a factory produces light bulbs with an...</td>\n",
              "      <td>To solve this problem, we will use the Central...</td>\n",
              "      <td>The probability that the sample mean lifespan ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31990</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Sampling techniques</td>\n",
              "      <td>A company produces batteries and they claim th...</td>\n",
              "      <td>To calculate the 95% confidence interval for t...</td>\n",
              "      <td>The 95% confidence interval for the true avera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31567</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Monte Carlo simulation</td>\n",
              "      <td>Assume you have a fair six-sided die. Use Mont...</td>\n",
              "      <td>To estimate the probability using Monte Carlo ...</td>\n",
              "      <td>To estimate the probability of rolling a total...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32147</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Time series analysis</td>\n",
              "      <td>What is the forecasted value for the number of...</td>\n",
              "      <td>To forecast the number of daily website visito...</td>\n",
              "      <td>\\n\\nPlease provide the forecasted value for th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31912</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Regression analysis</td>\n",
              "      <td>In a regression analysis, the data set include...</td>\n",
              "      <td>To calculate the expected score of a student w...</td>\n",
              "      <td>The expected score of a student who studies fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31702</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Outlier detection</td>\n",
              "      <td>In a statistics class, a professor gives a mid...</td>\n",
              "      <td>To determine if there are any outliers using t...</td>\n",
              "      <td>The scores of 93, 94, and 95 are outliers.\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30889</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Bayesian statistics</td>\n",
              "      <td>Suppose you want to estimate the probability t...</td>\n",
              "      <td>To solve this problem using Bayesian statistic...</td>\n",
              "      <td>To estimate the probability that a new student...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30922</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Bootstrapping</td>\n",
              "      <td>Suppose that you collected a sample of 25 scor...</td>\n",
              "      <td>To estimate the 95% confidence interval for th...</td>\n",
              "      <td>To estimate the 95% confidence interval for th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30895</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Bayesian statistics</td>\n",
              "      <td>A company produces light bulbs, and they claim...</td>\n",
              "      <td>To solve this problem using Bayesian statistic...</td>\n",
              "      <td>The probability that the company's claim is tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31961</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Robust statistics</td>\n",
              "      <td>A math student wants to calculate the mean of ...</td>\n",
              "      <td>First, we need to find the median of the datas...</td>\n",
              "      <td>\\nThe MAD is calculated as follows:\\n\\nMAD = √...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32085</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Survival analysis</td>\n",
              "      <td>A group of 50 patients were tested for the eff...</td>\n",
              "      <td>To estimate the probability that a patient sur...</td>\n",
              "      <td>I can help you with that! To estimate the prob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32037</th>\n",
              "      <td>Statistics</td>\n",
              "      <td>Statistical inference</td>\n",
              "      <td>A factory produces light bulbs and claims that...</td>\n",
              "      <td>To find the probability that the sample will h...</td>\n",
              "      <td>To determine the probability that the sample w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a829326-e5e3-402b-9258-655e5a049314')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1a829326-e5e3-402b-9258-655e5a049314 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1a829326-e5e3-402b-9258-655e5a049314');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-be2bcc75-eae5-4086-8ed0-4917fb65c31b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be2bcc75-eae5-4086-8ed0-4917fb65c31b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-be2bcc75-eae5-4086-8ed0-4917fb65c31b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f2372a43-e10e-4513-a889-22d1a0cd6a47\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f2372a43-e10e-4513-a889-22d1a0cd6a47 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"topic\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Statistics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sub_topic\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"Monte Carlo simulation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"A math student wants to calculate the mean of a set of data which includes some outliers. The student decides to use the Median Absolute Deviation (MAD) as a robust measure of dispersion to estimate the population standard deviation. The student collects the following data: \\n\\n10, 12, 12, 14, 10, 20, 14, 10, 50\\n\\nWhat is the MAD and the estimated population standard deviation?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"First, we need to find the median of the dataset. To do this, we'll sort the data in ascending order:\\n\\n10, 10, 10, 12, 12, 14, 14, 20, 50\\n\\nThe median is the middle value, which in this case is 12.\\n\\nNext, we'll find the absolute deviations from the median:\\n\\n|10 - 12| = 2\\n|10 - 12| = 2\\n|10 - 12| = 2\\n|12 - 12| = 0\\n|12 - 12| = 0\\n|14 - 12| = 2\\n|14 - 12| = 2\\n|20 - 12| = 8\\n|50 - 12| = 38\\n\\nNow, we'll find the median of these absolute deviations:\\n\\n0, 2, 2, 2, 2, 2, 2, 8, 38\\n\\nThe median of the absolute deviations is 2, so the MAD is 2.\\n\\nTo estimate the population standard deviation using the MAD, we can use the following formula:\\n\\nStandard Deviation \\u2248 1.4826 * MAD\\n\\nStandard Deviation \\u2248 1.4826 * 2 \\u2248 2.9652\\n\\nSo, the estimated population standard deviation is approximately 2.9652.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated_answer\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "acsy1nMxCS_6"
      },
      "outputs": [],
      "source": [
        "test_df.drop(columns=\"topic\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "N26z_cXiPU0y"
      },
      "outputs": [],
      "source": [
        "test_df.rename(columns={'sub_topic': 'context'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tRBvpLWRPpZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "080f80c6-d828-4e10-c007-b476eb077f90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            context  \\\n",
              "31580        Monte Carlo simulation   \n",
              "31825     Probability distributions   \n",
              "30986         Central limit theorem   \n",
              "31277         Discriminant analysis   \n",
              "31618         Multivariate analysis   \n",
              "31553        Monte Carlo simulation   \n",
              "31203                   Data mining   \n",
              "31771  Principal component analysis   \n",
              "31070              Cluster analysis   \n",
              "32016           Sampling techniques   \n",
              "31418            Hypothesis testing   \n",
              "31125          Confidence intervals   \n",
              "31030              Cluster analysis   \n",
              "32028           Sampling techniques   \n",
              "31610         Multivariate analysis   \n",
              "31481                 Markov chains   \n",
              "31528        Monte Carlo simulation   \n",
              "31749  Principal component analysis   \n",
              "31974           Sampling techniques   \n",
              "31990           Sampling techniques   \n",
              "31567        Monte Carlo simulation   \n",
              "32147          Time series analysis   \n",
              "31912           Regression analysis   \n",
              "31702             Outlier detection   \n",
              "30889           Bayesian statistics   \n",
              "30922                 Bootstrapping   \n",
              "30895           Bayesian statistics   \n",
              "31961             Robust statistics   \n",
              "32085             Survival analysis   \n",
              "32037         Statistical inference   \n",
              "\n",
              "                                                question  \\\n",
              "31580  A company wants to determine the expected numb...   \n",
              "31825  Suppose the length of time it takes a student ...   \n",
              "30986  A manufacturer produces light bulbs with a mea...   \n",
              "31277  In discriminant analysis, a student is given a...   \n",
              "31618  Here's a precise numerical problem for a math ...   \n",
              "31553  Suppose you are simulating the roll of a six-s...   \n",
              "31203  Suppose we have a dataset containing the ages ...   \n",
              "31771  What is the principal component for a dataset ...   \n",
              "31070  A retail store recorded the sales from 10 diff...   \n",
              "32016  A company wants to determine the average salar...   \n",
              "31418  Suppose a car manufacturer claims that their c...   \n",
              "31125  A coffee company wants to estimate the mean ca...   \n",
              "31030  What is the average distance between the centr...   \n",
              "32028  A problem could be: \\n\\nA company wants to sur...   \n",
              "31610  Suppose a survey is conducted among a group of...   \n",
              "31481  In a town, it has been observed that during su...   \n",
              "31528  Suppose that the probability of winning a game...   \n",
              "31749  Consider the following dataset of six observat...   \n",
              "31974  Suppose a factory produces light bulbs with an...   \n",
              "31990  A company produces batteries and they claim th...   \n",
              "31567  Assume you have a fair six-sided die. Use Mont...   \n",
              "32147  What is the forecasted value for the number of...   \n",
              "31912  In a regression analysis, the data set include...   \n",
              "31702  In a statistics class, a professor gives a mid...   \n",
              "30889  Suppose you want to estimate the probability t...   \n",
              "30922  Suppose that you collected a sample of 25 scor...   \n",
              "30895  A company produces light bulbs, and they claim...   \n",
              "31961  A math student wants to calculate the mean of ...   \n",
              "32085  A group of 50 patients were tested for the eff...   \n",
              "32037  A factory produces light bulbs and claims that...   \n",
              "\n",
              "                                                  answer  \\\n",
              "31580  To estimate the mean number of days it will ta...   \n",
              "31825  To find the probability that a randomly select...   \n",
              "30986  To solve this problem, we can use the Central ...   \n",
              "31277  To perform discriminant analysis, we first nee...   \n",
              "31618  To find the covariance and correlation coeffic...   \n",
              "31553  To find the probability of rolling at least on...   \n",
              "31203  To find the average age of the students in the...   \n",
              "31771  To find the principal component, we first need...   \n",
              "31070  To perform k-means clustering with 3 clusters,...   \n",
              "32016  To calculate the probability that one particul...   \n",
              "31418  To perform a hypothesis test at a 5% level of ...   \n",
              "31125  To create a 95% confidence interval for the me...   \n",
              "31030  To find the average distance between the centr...   \n",
              "32028  In simple random sampling, each employee has a...   \n",
              "31610  To determine the correlation between the score...   \n",
              "31481  Let S represent a sunny day and R represent a ...   \n",
              "31528  To perform a Monte Carlo simulation, we will g...   \n",
              "31749  To perform PCA on the given dataset, we first ...   \n",
              "31974  To solve this problem, we will use the Central...   \n",
              "31990  To calculate the 95% confidence interval for t...   \n",
              "31567  To estimate the probability using Monte Carlo ...   \n",
              "32147  To forecast the number of daily website visito...   \n",
              "31912  To calculate the expected score of a student w...   \n",
              "31702  To determine if there are any outliers using t...   \n",
              "30889  To solve this problem using Bayesian statistic...   \n",
              "30922  To estimate the 95% confidence interval for th...   \n",
              "30895  To solve this problem using Bayesian statistic...   \n",
              "31961  First, we need to find the median of the datas...   \n",
              "32085  To estimate the probability that a patient sur...   \n",
              "32037  To find the probability that the sample will h...   \n",
              "\n",
              "                                        generated_answer  \n",
              "31580  \\nTo estimate the mean number of days it will ...  \n",
              "31825  The probability that a randomly selected stude...  \n",
              "30986  The probability that the average lifetime of t...  \n",
              "31277  \\nThank you for asking! Based on the context i...  \n",
              "31618  \\n\\nPlease provide the answer in a clear and c...  \n",
              "31553  To calculate the probability of rolling at lea...  \n",
              "31203  The average age of the students in the classro...  \n",
              "31771  The principal component for the given covarian...  \n",
              "31070  Sure, I can help you with that! Based on the c...  \n",
              "32016  The probability of selecting a particular empl...  \n",
              "31418  The manufacturer's claim that their cars get a...  \n",
              "31125  \\nTo construct a 95% confidence interval for t...  \n",
              "31030  The average distance between the centroids of ...  \n",
              "32028  The probability of choosing a sample of 30 emp...  \n",
              "31610  The correlation between the scores of Mathemat...  \n",
              "31481  The probability that it will be sunny three da...  \n",
              "31528  To estimate the probability that a contestant ...  \n",
              "31749  The first principal component of the dataset i...  \n",
              "31974  The probability that the sample mean lifespan ...  \n",
              "31990  The 95% confidence interval for the true avera...  \n",
              "31567  To estimate the probability of rolling a total...  \n",
              "32147  \\n\\nPlease provide the forecasted value for th...  \n",
              "31912  The expected score of a student who studies fo...  \n",
              "31702  The scores of 93, 94, and 95 are outliers.\\n\\n...  \n",
              "30889  To estimate the probability that a new student...  \n",
              "30922  To estimate the 95% confidence interval for th...  \n",
              "30895  The probability that the company's claim is tr...  \n",
              "31961  \\nThe MAD is calculated as follows:\\n\\nMAD = √...  \n",
              "32085  I can help you with that! To estimate the prob...  \n",
              "32037  To determine the probability that the sample w...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7714d105-782a-4b87-86d8-a7308059339a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>generated_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31580</th>\n",
              "      <td>Monte Carlo simulation</td>\n",
              "      <td>A company wants to determine the expected numb...</td>\n",
              "      <td>To estimate the mean number of days it will ta...</td>\n",
              "      <td>\\nTo estimate the mean number of days it will ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31825</th>\n",
              "      <td>Probability distributions</td>\n",
              "      <td>Suppose the length of time it takes a student ...</td>\n",
              "      <td>To find the probability that a randomly select...</td>\n",
              "      <td>The probability that a randomly selected stude...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30986</th>\n",
              "      <td>Central limit theorem</td>\n",
              "      <td>A manufacturer produces light bulbs with a mea...</td>\n",
              "      <td>To solve this problem, we can use the Central ...</td>\n",
              "      <td>The probability that the average lifetime of t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31277</th>\n",
              "      <td>Discriminant analysis</td>\n",
              "      <td>In discriminant analysis, a student is given a...</td>\n",
              "      <td>To perform discriminant analysis, we first nee...</td>\n",
              "      <td>\\nThank you for asking! Based on the context i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31618</th>\n",
              "      <td>Multivariate analysis</td>\n",
              "      <td>Here's a precise numerical problem for a math ...</td>\n",
              "      <td>To find the covariance and correlation coeffic...</td>\n",
              "      <td>\\n\\nPlease provide the answer in a clear and c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31553</th>\n",
              "      <td>Monte Carlo simulation</td>\n",
              "      <td>Suppose you are simulating the roll of a six-s...</td>\n",
              "      <td>To find the probability of rolling at least on...</td>\n",
              "      <td>To calculate the probability of rolling at lea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31203</th>\n",
              "      <td>Data mining</td>\n",
              "      <td>Suppose we have a dataset containing the ages ...</td>\n",
              "      <td>To find the average age of the students in the...</td>\n",
              "      <td>The average age of the students in the classro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31771</th>\n",
              "      <td>Principal component analysis</td>\n",
              "      <td>What is the principal component for a dataset ...</td>\n",
              "      <td>To find the principal component, we first need...</td>\n",
              "      <td>The principal component for the given covarian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31070</th>\n",
              "      <td>Cluster analysis</td>\n",
              "      <td>A retail store recorded the sales from 10 diff...</td>\n",
              "      <td>To perform k-means clustering with 3 clusters,...</td>\n",
              "      <td>Sure, I can help you with that! Based on the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32016</th>\n",
              "      <td>Sampling techniques</td>\n",
              "      <td>A company wants to determine the average salar...</td>\n",
              "      <td>To calculate the probability that one particul...</td>\n",
              "      <td>The probability of selecting a particular empl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31418</th>\n",
              "      <td>Hypothesis testing</td>\n",
              "      <td>Suppose a car manufacturer claims that their c...</td>\n",
              "      <td>To perform a hypothesis test at a 5% level of ...</td>\n",
              "      <td>The manufacturer's claim that their cars get a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31125</th>\n",
              "      <td>Confidence intervals</td>\n",
              "      <td>A coffee company wants to estimate the mean ca...</td>\n",
              "      <td>To create a 95% confidence interval for the me...</td>\n",
              "      <td>\\nTo construct a 95% confidence interval for t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31030</th>\n",
              "      <td>Cluster analysis</td>\n",
              "      <td>What is the average distance between the centr...</td>\n",
              "      <td>To find the average distance between the centr...</td>\n",
              "      <td>The average distance between the centroids of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32028</th>\n",
              "      <td>Sampling techniques</td>\n",
              "      <td>A problem could be: \\n\\nA company wants to sur...</td>\n",
              "      <td>In simple random sampling, each employee has a...</td>\n",
              "      <td>The probability of choosing a sample of 30 emp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31610</th>\n",
              "      <td>Multivariate analysis</td>\n",
              "      <td>Suppose a survey is conducted among a group of...</td>\n",
              "      <td>To determine the correlation between the score...</td>\n",
              "      <td>The correlation between the scores of Mathemat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31481</th>\n",
              "      <td>Markov chains</td>\n",
              "      <td>In a town, it has been observed that during su...</td>\n",
              "      <td>Let S represent a sunny day and R represent a ...</td>\n",
              "      <td>The probability that it will be sunny three da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31528</th>\n",
              "      <td>Monte Carlo simulation</td>\n",
              "      <td>Suppose that the probability of winning a game...</td>\n",
              "      <td>To perform a Monte Carlo simulation, we will g...</td>\n",
              "      <td>To estimate the probability that a contestant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31749</th>\n",
              "      <td>Principal component analysis</td>\n",
              "      <td>Consider the following dataset of six observat...</td>\n",
              "      <td>To perform PCA on the given dataset, we first ...</td>\n",
              "      <td>The first principal component of the dataset i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31974</th>\n",
              "      <td>Sampling techniques</td>\n",
              "      <td>Suppose a factory produces light bulbs with an...</td>\n",
              "      <td>To solve this problem, we will use the Central...</td>\n",
              "      <td>The probability that the sample mean lifespan ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31990</th>\n",
              "      <td>Sampling techniques</td>\n",
              "      <td>A company produces batteries and they claim th...</td>\n",
              "      <td>To calculate the 95% confidence interval for t...</td>\n",
              "      <td>The 95% confidence interval for the true avera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31567</th>\n",
              "      <td>Monte Carlo simulation</td>\n",
              "      <td>Assume you have a fair six-sided die. Use Mont...</td>\n",
              "      <td>To estimate the probability using Monte Carlo ...</td>\n",
              "      <td>To estimate the probability of rolling a total...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32147</th>\n",
              "      <td>Time series analysis</td>\n",
              "      <td>What is the forecasted value for the number of...</td>\n",
              "      <td>To forecast the number of daily website visito...</td>\n",
              "      <td>\\n\\nPlease provide the forecasted value for th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31912</th>\n",
              "      <td>Regression analysis</td>\n",
              "      <td>In a regression analysis, the data set include...</td>\n",
              "      <td>To calculate the expected score of a student w...</td>\n",
              "      <td>The expected score of a student who studies fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31702</th>\n",
              "      <td>Outlier detection</td>\n",
              "      <td>In a statistics class, a professor gives a mid...</td>\n",
              "      <td>To determine if there are any outliers using t...</td>\n",
              "      <td>The scores of 93, 94, and 95 are outliers.\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30889</th>\n",
              "      <td>Bayesian statistics</td>\n",
              "      <td>Suppose you want to estimate the probability t...</td>\n",
              "      <td>To solve this problem using Bayesian statistic...</td>\n",
              "      <td>To estimate the probability that a new student...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30922</th>\n",
              "      <td>Bootstrapping</td>\n",
              "      <td>Suppose that you collected a sample of 25 scor...</td>\n",
              "      <td>To estimate the 95% confidence interval for th...</td>\n",
              "      <td>To estimate the 95% confidence interval for th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30895</th>\n",
              "      <td>Bayesian statistics</td>\n",
              "      <td>A company produces light bulbs, and they claim...</td>\n",
              "      <td>To solve this problem using Bayesian statistic...</td>\n",
              "      <td>The probability that the company's claim is tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31961</th>\n",
              "      <td>Robust statistics</td>\n",
              "      <td>A math student wants to calculate the mean of ...</td>\n",
              "      <td>First, we need to find the median of the datas...</td>\n",
              "      <td>\\nThe MAD is calculated as follows:\\n\\nMAD = √...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32085</th>\n",
              "      <td>Survival analysis</td>\n",
              "      <td>A group of 50 patients were tested for the eff...</td>\n",
              "      <td>To estimate the probability that a patient sur...</td>\n",
              "      <td>I can help you with that! To estimate the prob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32037</th>\n",
              "      <td>Statistical inference</td>\n",
              "      <td>A factory produces light bulbs and claims that...</td>\n",
              "      <td>To find the probability that the sample will h...</td>\n",
              "      <td>To determine the probability that the sample w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7714d105-782a-4b87-86d8-a7308059339a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7714d105-782a-4b87-86d8-a7308059339a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7714d105-782a-4b87-86d8-a7308059339a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7fb855a7-943a-41ba-8bc8-a0222d4b8baa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7fb855a7-943a-41ba-8bc8-a0222d4b8baa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7fb855a7-943a-41ba-8bc8-a0222d4b8baa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_101af269-a1da-4e19-a0e8-2f2b6620e35c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_101af269-a1da-4e19-a0e8-2f2b6620e35c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"Monte Carlo simulation\",\n          \"Robust statistics\",\n          \"Bayesian statistics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"A math student wants to calculate the mean of a set of data which includes some outliers. The student decides to use the Median Absolute Deviation (MAD) as a robust measure of dispersion to estimate the population standard deviation. The student collects the following data: \\n\\n10, 12, 12, 14, 10, 20, 14, 10, 50\\n\\nWhat is the MAD and the estimated population standard deviation?\",\n          \"In a town, it has been observed that during summer, 70% of days are sunny and 30% of days are rainy. If it is sunny today, there is a 60% chance that tomorrow will also be sunny. If it is rainy today, there is a 70% chance that tomorrow will also be rainy. What is the probability that it will be sunny three days from now, given that it is sunny today?\",\n          \"In a statistics class, a professor gives a midterm exam with a maximum score of 100. The scores received by a group of 20 students are:\\n\\n60, 70, 75, 80, 82, 84, 86, 88, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 100, 100\\n\\nUsing the rule of thumb method for outlier detection, determine if any scores are outliers.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"First, we need to find the median of the dataset. To do this, we'll sort the data in ascending order:\\n\\n10, 10, 10, 12, 12, 14, 14, 20, 50\\n\\nThe median is the middle value, which in this case is 12.\\n\\nNext, we'll find the absolute deviations from the median:\\n\\n|10 - 12| = 2\\n|10 - 12| = 2\\n|10 - 12| = 2\\n|12 - 12| = 0\\n|12 - 12| = 0\\n|14 - 12| = 2\\n|14 - 12| = 2\\n|20 - 12| = 8\\n|50 - 12| = 38\\n\\nNow, we'll find the median of these absolute deviations:\\n\\n0, 2, 2, 2, 2, 2, 2, 8, 38\\n\\nThe median of the absolute deviations is 2, so the MAD is 2.\\n\\nTo estimate the population standard deviation using the MAD, we can use the following formula:\\n\\nStandard Deviation \\u2248 1.4826 * MAD\\n\\nStandard Deviation \\u2248 1.4826 * 2 \\u2248 2.9652\\n\\nSo, the estimated population standard deviation is approximately 2.9652.\",\n          \"Let S represent a sunny day and R represent a rainy day. We are given the following probabilities:\\n\\nP(S) = 0.7\\nP(R) = 0.3\\nP(S|S) = 0.6\\nP(R|R) = 0.7\\n\\nWe want to find the probability P(S_3|S_1), where S_1 represents that it is sunny today, and S_3 represents that it is sunny three days from now.\\n\\nWe can use the law of total probability to find P(S_3|S_1):\\n\\nP(S_3|S_1) = P(S_3|S_1, S_2) * P(S_2|S_1) + P(S_3|S_1, R_2) * P(R_2|S_1)\\n\\nWe know the following probabilities:\\n\\nP(S_2|S_1) = 0.6 (given)\\nP(R_2|S_1) = 1 - P(S_2|S_1) = 1 - 0.6 = 0.4\\nP(S_3|S_1, S_2) = P(S_3|S_2) = 0.6 (given)\\nP(S_3|S_1, R_2) = P(S_3|R_2) = 1 - P(R_3|R_2) = 1 - 0.7 = 0.3\\n\\nNow we can plug these probabilities into the equation:\\n\\nP(S_3|S_1) = P(S_3|S_1, S_2) * P(S_2|S_1) + P(S_3|S_1, R_2) * P(R_2|S_1)\\nP(S_3|S_1) = (0.6 * 0.6) + (0.3 * 0.4)\\nP(S_3|S_1) = 0.36 + 0.12\\nP(S_3|S_1) = 0.48\\n\\nSo, the probability that it will be sunny three days from now, given that it is sunny today, is 0.48 or 48%.\",\n          \"To determine if there are any outliers using the rule of thumb method, we first need to calculate the interquartile range (IQR) and then use it to find the lower and upper bounds for the scores.\\n\\nStep 1: Calculate the quartiles.\\nQ1 (first quartile) is the median of the lower half of the data. In this case, the lower half is 60, 70, 75, 80, 82, 84, 86, 88, 90, 92. The median is the average of 82 and 84, so Q1 = 83.\\nQ3 (third quartile) is the median of the upper half of the data. In this case, the upper half is 93, 94, 95, 96, 97, 98, 99, 100, 100, 100. The median is the average of 96 and 97, so Q3 = 96.5.\\n\\nStep 2: Calculate the interquartile range (IQR).\\nIQR = Q3 - Q1 = 96.5 - 83 = 13.5\\n\\nStep 3: Calculate the lower and upper bounds.\\nLower bound = Q1 - 1.5 * IQR = 83 - 1.5 * 13.5 = 63.25\\nUpper bound = Q3 + 1.5 * IQR = 96.5 + 1.5 * 13.5 = 116.25\\n\\nStep 4: Check for outliers.\\nNow we check if any scores are below the lower bound or above the upper bound. In this case, all scores are between 63.25 and 116.25, so there are no outliers.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated_answer\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Basic preprocessing: ensure all entries are strings\n",
        "for column in ['context', 'question', 'answer', 'generated_answer']:\n",
        "    test_df[column] = test_df[column].astype(str).str.lower()\n",
        "\n",
        "# Debugging: Print the initial DataFrame\n",
        "print(\"Initial DataFrame:\")\n",
        "print(test_df)\n",
        "\n",
        "# Combine the relevant columns for vectorization\n",
        "combined_series = pd.concat([test_df['context'], test_df['answer'], test_df['generated_answer']], ignore_index=True)\n",
        "\n",
        "# Debugging: Print the combined series\n",
        "print(\"\\nCombined Series for Vectorization:\")\n",
        "print(combined_series)\n",
        "\n",
        "# Vectorize using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(combined_series)\n",
        "vectors_array = vectors.toarray()\n",
        "\n",
        "# Debugging: Print the vectors\n",
        "print(\"\\nTF-IDF Vectors:\")\n",
        "print(vectors_array)\n",
        "\n",
        "# Split the vectors\n",
        "num_rows = len(test_df)\n",
        "context_vectors = vectors_array[:num_rows]\n",
        "answer_vectors = vectors_array[num_rows:2*num_rows]\n",
        "generated_vectors = vectors_array[2*num_rows:]\n",
        "\n",
        "# Ensure that the vectors are correctly sized\n",
        "assert len(context_vectors) == num_rows, \"Mismatch in context vectors length\"\n",
        "assert len(answer_vectors) == num_rows, \"Mismatch in answer vectors length\"\n",
        "assert len(generated_vectors) == num_rows, \"Mismatch in generated vectors length\"\n",
        "\n",
        "# Debugging: Print the split vectors\n",
        "print(\"\\nContext Vectors:\")\n",
        "print(context_vectors)\n",
        "print(\"\\nAnswer Vectors:\")\n",
        "print(answer_vectors)\n",
        "print(\"\\nGenerated Vectors:\")\n",
        "print(generated_vectors)\n",
        "\n",
        "# Calculate similarities\n",
        "faithfulness = [cosine_similarity([context_vectors[i]], [generated_vectors[i]])[0][0] for i in range(num_rows)]\n",
        "answer_relevancy = [cosine_similarity([answer_vectors[i]], [generated_vectors[i]])[0][0] for i in range(num_rows)]\n",
        "\n",
        "# Add similarities to DataFrame\n",
        "test_df['faithfulness'] = faithfulness\n",
        "test_df['answer_relevancy'] = answer_relevancy\n",
        "\n",
        "# Debugging: Print the DataFrame with similarities\n",
        "print(\"\\nDataFrame with Similarities:\")\n",
        "print(test_df)\n",
        "\n",
        "# Calculate context recall and precision\n",
        "def compute_recall_precision(context, generated):\n",
        "    context_terms = set(context.split())\n",
        "    generated_terms = set(generated.split())\n",
        "\n",
        "    common_terms = context_terms.intersection(generated_terms)\n",
        "\n",
        "    recall = len(common_terms) / len(context_terms) if len(context_terms) > 0 else 0\n",
        "    precision = len(common_terms) / len(generated_terms) if len(generated_terms) > 0 else 0\n",
        "\n",
        "    return recall, precision\n",
        "\n",
        "# Using try-except block to capture indexing issues\n",
        "context_recall = []\n",
        "context_precision = []\n",
        "\n",
        "for i in range(num_rows):\n",
        "    try:\n",
        "        recall, precision = compute_recall_precision(test_df['context'].iloc[i], test_df['generated_answer'].iloc[i])\n",
        "        context_recall.append(recall)\n",
        "        context_precision.append(precision)\n",
        "    except KeyError as e:\n",
        "        print(f\"KeyError at index {i}: {e}\")\n",
        "\n",
        "test_df['context_recall'] = context_recall\n",
        "test_df['context_precision'] = context_precision\n",
        "\n",
        "# Calculate answer correctness\n",
        "threshold = 0.8\n",
        "test_df['answer_correctness'] = test_df['answer_relevancy'] >= threshold\n",
        "\n",
        "# Final DataFrame output\n",
        "print(\"\\nFinal DataFrame with all metrics:\")\n",
        "print(test_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHzPr5lmBM8J",
        "outputId": "5d23f97b-dda2-4859-ad8e-1cc512c4b6ba"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame:\n",
            "                            context  \\\n",
            "31580        monte carlo simulation   \n",
            "31825     probability distributions   \n",
            "30986         central limit theorem   \n",
            "31277         discriminant analysis   \n",
            "31618         multivariate analysis   \n",
            "31553        monte carlo simulation   \n",
            "31203                   data mining   \n",
            "31771  principal component analysis   \n",
            "31070              cluster analysis   \n",
            "32016           sampling techniques   \n",
            "31418            hypothesis testing   \n",
            "31125          confidence intervals   \n",
            "31030              cluster analysis   \n",
            "32028           sampling techniques   \n",
            "31610         multivariate analysis   \n",
            "31481                 markov chains   \n",
            "31528        monte carlo simulation   \n",
            "31749  principal component analysis   \n",
            "31974           sampling techniques   \n",
            "31990           sampling techniques   \n",
            "31567        monte carlo simulation   \n",
            "32147          time series analysis   \n",
            "31912           regression analysis   \n",
            "31702             outlier detection   \n",
            "30889           bayesian statistics   \n",
            "30922                 bootstrapping   \n",
            "30895           bayesian statistics   \n",
            "31961             robust statistics   \n",
            "32085             survival analysis   \n",
            "32037         statistical inference   \n",
            "\n",
            "                                                question  \\\n",
            "31580  a company wants to determine the expected numb...   \n",
            "31825  suppose the length of time it takes a student ...   \n",
            "30986  a manufacturer produces light bulbs with a mea...   \n",
            "31277  in discriminant analysis, a student is given a...   \n",
            "31618  here's a precise numerical problem for a math ...   \n",
            "31553  suppose you are simulating the roll of a six-s...   \n",
            "31203  suppose we have a dataset containing the ages ...   \n",
            "31771  what is the principal component for a dataset ...   \n",
            "31070  a retail store recorded the sales from 10 diff...   \n",
            "32016  a company wants to determine the average salar...   \n",
            "31418  suppose a car manufacturer claims that their c...   \n",
            "31125  a coffee company wants to estimate the mean ca...   \n",
            "31030  what is the average distance between the centr...   \n",
            "32028  a problem could be: \\n\\na company wants to sur...   \n",
            "31610  suppose a survey is conducted among a group of...   \n",
            "31481  in a town, it has been observed that during su...   \n",
            "31528  suppose that the probability of winning a game...   \n",
            "31749  consider the following dataset of six observat...   \n",
            "31974  suppose a factory produces light bulbs with an...   \n",
            "31990  a company produces batteries and they claim th...   \n",
            "31567  assume you have a fair six-sided die. use mont...   \n",
            "32147  what is the forecasted value for the number of...   \n",
            "31912  in a regression analysis, the data set include...   \n",
            "31702  in a statistics class, a professor gives a mid...   \n",
            "30889  suppose you want to estimate the probability t...   \n",
            "30922  suppose that you collected a sample of 25 scor...   \n",
            "30895  a company produces light bulbs, and they claim...   \n",
            "31961  a math student wants to calculate the mean of ...   \n",
            "32085  a group of 50 patients were tested for the eff...   \n",
            "32037  a factory produces light bulbs and claims that...   \n",
            "\n",
            "                                                  answer  \\\n",
            "31580  to estimate the mean number of days it will ta...   \n",
            "31825  to find the probability that a randomly select...   \n",
            "30986  to solve this problem, we can use the central ...   \n",
            "31277  to perform discriminant analysis, we first nee...   \n",
            "31618  to find the covariance and correlation coeffic...   \n",
            "31553  to find the probability of rolling at least on...   \n",
            "31203  to find the average age of the students in the...   \n",
            "31771  to find the principal component, we first need...   \n",
            "31070  to perform k-means clustering with 3 clusters,...   \n",
            "32016  to calculate the probability that one particul...   \n",
            "31418  to perform a hypothesis test at a 5% level of ...   \n",
            "31125  to create a 95% confidence interval for the me...   \n",
            "31030  to find the average distance between the centr...   \n",
            "32028  in simple random sampling, each employee has a...   \n",
            "31610  to determine the correlation between the score...   \n",
            "31481  let s represent a sunny day and r represent a ...   \n",
            "31528  to perform a monte carlo simulation, we will g...   \n",
            "31749  to perform pca on the given dataset, we first ...   \n",
            "31974  to solve this problem, we will use the central...   \n",
            "31990  to calculate the 95% confidence interval for t...   \n",
            "31567  to estimate the probability using monte carlo ...   \n",
            "32147  to forecast the number of daily website visito...   \n",
            "31912  to calculate the expected score of a student w...   \n",
            "31702  to determine if there are any outliers using t...   \n",
            "30889  to solve this problem using bayesian statistic...   \n",
            "30922  to estimate the 95% confidence interval for th...   \n",
            "30895  to solve this problem using bayesian statistic...   \n",
            "31961  first, we need to find the median of the datas...   \n",
            "32085  to estimate the probability that a patient sur...   \n",
            "32037  to find the probability that the sample will h...   \n",
            "\n",
            "                                        generated_answer  \n",
            "31580  \\nto estimate the mean number of days it will ...  \n",
            "31825  the probability that a randomly selected stude...  \n",
            "30986  the probability that the average lifetime of t...  \n",
            "31277  \\nthank you for asking! based on the context i...  \n",
            "31618  \\n\\nplease provide the answer in a clear and c...  \n",
            "31553  to calculate the probability of rolling at lea...  \n",
            "31203  the average age of the students in the classro...  \n",
            "31771  the principal component for the given covarian...  \n",
            "31070  sure, i can help you with that! based on the c...  \n",
            "32016  the probability of selecting a particular empl...  \n",
            "31418  the manufacturer's claim that their cars get a...  \n",
            "31125  \\nto construct a 95% confidence interval for t...  \n",
            "31030  the average distance between the centroids of ...  \n",
            "32028  the probability of choosing a sample of 30 emp...  \n",
            "31610  the correlation between the scores of mathemat...  \n",
            "31481  the probability that it will be sunny three da...  \n",
            "31528  to estimate the probability that a contestant ...  \n",
            "31749  the first principal component of the dataset i...  \n",
            "31974  the probability that the sample mean lifespan ...  \n",
            "31990  the 95% confidence interval for the true avera...  \n",
            "31567  to estimate the probability of rolling a total...  \n",
            "32147  \\n\\nplease provide the forecasted value for th...  \n",
            "31912  the expected score of a student who studies fo...  \n",
            "31702  the scores of 93, 94, and 95 are outliers.\\n\\n...  \n",
            "30889  to estimate the probability that a new student...  \n",
            "30922  to estimate the 95% confidence interval for th...  \n",
            "30895  the probability that the company's claim is tr...  \n",
            "31961  \\nthe mad is calculated as follows:\\n\\nmad = √...  \n",
            "32085  i can help you with that! to estimate the prob...  \n",
            "32037  to determine the probability that the sample w...  \n",
            "\n",
            "Combined Series for Vectorization:\n",
            "0                                monte carlo simulation\n",
            "1                             probability distributions\n",
            "2                                 central limit theorem\n",
            "3                                 discriminant analysis\n",
            "4                                 multivariate analysis\n",
            "                            ...                        \n",
            "85    to estimate the 95% confidence interval for th...\n",
            "86    the probability that the company's claim is tr...\n",
            "87    \\nthe mad is calculated as follows:\\n\\nmad = √...\n",
            "88    i can help you with that! to estimate the prob...\n",
            "89    to determine the probability that the sample w...\n",
            "Length: 90, dtype: object\n",
            "\n",
            "TF-IDF Vectors:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "Context Vectors:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "Answer Vectors:\n",
            "[[0.0724491  0.         0.09385037 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            "\n",
            "Generated Vectors:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "DataFrame with Similarities:\n",
            "                            context  \\\n",
            "31580        monte carlo simulation   \n",
            "31825     probability distributions   \n",
            "30986         central limit theorem   \n",
            "31277         discriminant analysis   \n",
            "31618         multivariate analysis   \n",
            "31553        monte carlo simulation   \n",
            "31203                   data mining   \n",
            "31771  principal component analysis   \n",
            "31070              cluster analysis   \n",
            "32016           sampling techniques   \n",
            "31418            hypothesis testing   \n",
            "31125          confidence intervals   \n",
            "31030              cluster analysis   \n",
            "32028           sampling techniques   \n",
            "31610         multivariate analysis   \n",
            "31481                 markov chains   \n",
            "31528        monte carlo simulation   \n",
            "31749  principal component analysis   \n",
            "31974           sampling techniques   \n",
            "31990           sampling techniques   \n",
            "31567        monte carlo simulation   \n",
            "32147          time series analysis   \n",
            "31912           regression analysis   \n",
            "31702             outlier detection   \n",
            "30889           bayesian statistics   \n",
            "30922                 bootstrapping   \n",
            "30895           bayesian statistics   \n",
            "31961             robust statistics   \n",
            "32085             survival analysis   \n",
            "32037         statistical inference   \n",
            "\n",
            "                                                question  \\\n",
            "31580  a company wants to determine the expected numb...   \n",
            "31825  suppose the length of time it takes a student ...   \n",
            "30986  a manufacturer produces light bulbs with a mea...   \n",
            "31277  in discriminant analysis, a student is given a...   \n",
            "31618  here's a precise numerical problem for a math ...   \n",
            "31553  suppose you are simulating the roll of a six-s...   \n",
            "31203  suppose we have a dataset containing the ages ...   \n",
            "31771  what is the principal component for a dataset ...   \n",
            "31070  a retail store recorded the sales from 10 diff...   \n",
            "32016  a company wants to determine the average salar...   \n",
            "31418  suppose a car manufacturer claims that their c...   \n",
            "31125  a coffee company wants to estimate the mean ca...   \n",
            "31030  what is the average distance between the centr...   \n",
            "32028  a problem could be: \\n\\na company wants to sur...   \n",
            "31610  suppose a survey is conducted among a group of...   \n",
            "31481  in a town, it has been observed that during su...   \n",
            "31528  suppose that the probability of winning a game...   \n",
            "31749  consider the following dataset of six observat...   \n",
            "31974  suppose a factory produces light bulbs with an...   \n",
            "31990  a company produces batteries and they claim th...   \n",
            "31567  assume you have a fair six-sided die. use mont...   \n",
            "32147  what is the forecasted value for the number of...   \n",
            "31912  in a regression analysis, the data set include...   \n",
            "31702  in a statistics class, a professor gives a mid...   \n",
            "30889  suppose you want to estimate the probability t...   \n",
            "30922  suppose that you collected a sample of 25 scor...   \n",
            "30895  a company produces light bulbs, and they claim...   \n",
            "31961  a math student wants to calculate the mean of ...   \n",
            "32085  a group of 50 patients were tested for the eff...   \n",
            "32037  a factory produces light bulbs and claims that...   \n",
            "\n",
            "                                                  answer  \\\n",
            "31580  to estimate the mean number of days it will ta...   \n",
            "31825  to find the probability that a randomly select...   \n",
            "30986  to solve this problem, we can use the central ...   \n",
            "31277  to perform discriminant analysis, we first nee...   \n",
            "31618  to find the covariance and correlation coeffic...   \n",
            "31553  to find the probability of rolling at least on...   \n",
            "31203  to find the average age of the students in the...   \n",
            "31771  to find the principal component, we first need...   \n",
            "31070  to perform k-means clustering with 3 clusters,...   \n",
            "32016  to calculate the probability that one particul...   \n",
            "31418  to perform a hypothesis test at a 5% level of ...   \n",
            "31125  to create a 95% confidence interval for the me...   \n",
            "31030  to find the average distance between the centr...   \n",
            "32028  in simple random sampling, each employee has a...   \n",
            "31610  to determine the correlation between the score...   \n",
            "31481  let s represent a sunny day and r represent a ...   \n",
            "31528  to perform a monte carlo simulation, we will g...   \n",
            "31749  to perform pca on the given dataset, we first ...   \n",
            "31974  to solve this problem, we will use the central...   \n",
            "31990  to calculate the 95% confidence interval for t...   \n",
            "31567  to estimate the probability using monte carlo ...   \n",
            "32147  to forecast the number of daily website visito...   \n",
            "31912  to calculate the expected score of a student w...   \n",
            "31702  to determine if there are any outliers using t...   \n",
            "30889  to solve this problem using bayesian statistic...   \n",
            "30922  to estimate the 95% confidence interval for th...   \n",
            "30895  to solve this problem using bayesian statistic...   \n",
            "31961  first, we need to find the median of the datas...   \n",
            "32085  to estimate the probability that a patient sur...   \n",
            "32037  to find the probability that the sample will h...   \n",
            "\n",
            "                                        generated_answer  faithfulness  \\\n",
            "31580  \\nto estimate the mean number of days it will ...      0.056756   \n",
            "31825  the probability that a randomly selected stude...      0.067104   \n",
            "30986  the probability that the average lifetime of t...      0.000000   \n",
            "31277  \\nthank you for asking! based on the context i...      0.265817   \n",
            "31618  \\n\\nplease provide the answer in a clear and c...      0.000000   \n",
            "31553  to calculate the probability of rolling at lea...      0.000000   \n",
            "31203  the average age of the students in the classro...      0.000000   \n",
            "31771  the principal component for the given covarian...      0.249793   \n",
            "31070  sure, i can help you with that! based on the c...      0.139087   \n",
            "32016  the probability of selecting a particular empl...      0.000000   \n",
            "31418  the manufacturer's claim that their cars get a...      0.067147   \n",
            "31125  \\nto construct a 95% confidence interval for t...      0.117161   \n",
            "31030  the average distance between the centroids of ...      0.000000   \n",
            "32028  the probability of choosing a sample of 30 emp...      0.141620   \n",
            "31610  the correlation between the scores of mathemat...      0.000000   \n",
            "31481  the probability that it will be sunny three da...      0.000000   \n",
            "31528  to estimate the probability that a contestant ...      0.186260   \n",
            "31749  the first principal component of the dataset i...      0.397887   \n",
            "31974  the probability that the sample mean lifespan ...      0.000000   \n",
            "31990  the 95% confidence interval for the true avera...      0.000000   \n",
            "31567  to estimate the probability of rolling a total...      0.086574   \n",
            "32147  \\n\\nplease provide the forecasted value for th...      0.292595   \n",
            "31912  the expected score of a student who studies fo...      0.143955   \n",
            "31702  the scores of 93, 94, and 95 are outliers.\\n\\n...      0.053657   \n",
            "30889  to estimate the probability that a new student...      0.069772   \n",
            "30922  to estimate the 95% confidence interval for th...      0.148400   \n",
            "30895  the probability that the company's claim is tr...      0.109494   \n",
            "31961  \\nthe mad is calculated as follows:\\n\\nmad = √...      0.060435   \n",
            "32085  i can help you with that! to estimate the prob...      0.438570   \n",
            "32037  to determine the probability that the sample w...      0.000000   \n",
            "\n",
            "       answer_relevancy  \n",
            "31580          0.421500  \n",
            "31825          0.612493  \n",
            "30986          0.662880  \n",
            "31277          0.521626  \n",
            "31618          0.032957  \n",
            "31553          0.851130  \n",
            "31203          0.626776  \n",
            "31771          0.433935  \n",
            "31070          0.389440  \n",
            "32016          0.388205  \n",
            "31418          0.494657  \n",
            "31125          0.669276  \n",
            "31030          0.545167  \n",
            "32028          0.682295  \n",
            "31610          0.637452  \n",
            "31481          0.167844  \n",
            "31528          0.460557  \n",
            "31749          0.321445  \n",
            "31974          0.600499  \n",
            "31990          0.491713  \n",
            "31567          0.452309  \n",
            "32147          0.424310  \n",
            "31912          0.744068  \n",
            "31702          0.481395  \n",
            "30889          0.586975  \n",
            "30922          0.581815  \n",
            "30895          0.576801  \n",
            "31961          0.591347  \n",
            "32085          0.321155  \n",
            "32037          0.816314  \n",
            "\n",
            "Final DataFrame with all metrics:\n",
            "                            context  \\\n",
            "31580        monte carlo simulation   \n",
            "31825     probability distributions   \n",
            "30986         central limit theorem   \n",
            "31277         discriminant analysis   \n",
            "31618         multivariate analysis   \n",
            "31553        monte carlo simulation   \n",
            "31203                   data mining   \n",
            "31771  principal component analysis   \n",
            "31070              cluster analysis   \n",
            "32016           sampling techniques   \n",
            "31418            hypothesis testing   \n",
            "31125          confidence intervals   \n",
            "31030              cluster analysis   \n",
            "32028           sampling techniques   \n",
            "31610         multivariate analysis   \n",
            "31481                 markov chains   \n",
            "31528        monte carlo simulation   \n",
            "31749  principal component analysis   \n",
            "31974           sampling techniques   \n",
            "31990           sampling techniques   \n",
            "31567        monte carlo simulation   \n",
            "32147          time series analysis   \n",
            "31912           regression analysis   \n",
            "31702             outlier detection   \n",
            "30889           bayesian statistics   \n",
            "30922                 bootstrapping   \n",
            "30895           bayesian statistics   \n",
            "31961             robust statistics   \n",
            "32085             survival analysis   \n",
            "32037         statistical inference   \n",
            "\n",
            "                                                question  \\\n",
            "31580  a company wants to determine the expected numb...   \n",
            "31825  suppose the length of time it takes a student ...   \n",
            "30986  a manufacturer produces light bulbs with a mea...   \n",
            "31277  in discriminant analysis, a student is given a...   \n",
            "31618  here's a precise numerical problem for a math ...   \n",
            "31553  suppose you are simulating the roll of a six-s...   \n",
            "31203  suppose we have a dataset containing the ages ...   \n",
            "31771  what is the principal component for a dataset ...   \n",
            "31070  a retail store recorded the sales from 10 diff...   \n",
            "32016  a company wants to determine the average salar...   \n",
            "31418  suppose a car manufacturer claims that their c...   \n",
            "31125  a coffee company wants to estimate the mean ca...   \n",
            "31030  what is the average distance between the centr...   \n",
            "32028  a problem could be: \\n\\na company wants to sur...   \n",
            "31610  suppose a survey is conducted among a group of...   \n",
            "31481  in a town, it has been observed that during su...   \n",
            "31528  suppose that the probability of winning a game...   \n",
            "31749  consider the following dataset of six observat...   \n",
            "31974  suppose a factory produces light bulbs with an...   \n",
            "31990  a company produces batteries and they claim th...   \n",
            "31567  assume you have a fair six-sided die. use mont...   \n",
            "32147  what is the forecasted value for the number of...   \n",
            "31912  in a regression analysis, the data set include...   \n",
            "31702  in a statistics class, a professor gives a mid...   \n",
            "30889  suppose you want to estimate the probability t...   \n",
            "30922  suppose that you collected a sample of 25 scor...   \n",
            "30895  a company produces light bulbs, and they claim...   \n",
            "31961  a math student wants to calculate the mean of ...   \n",
            "32085  a group of 50 patients were tested for the eff...   \n",
            "32037  a factory produces light bulbs and claims that...   \n",
            "\n",
            "                                                  answer  \\\n",
            "31580  to estimate the mean number of days it will ta...   \n",
            "31825  to find the probability that a randomly select...   \n",
            "30986  to solve this problem, we can use the central ...   \n",
            "31277  to perform discriminant analysis, we first nee...   \n",
            "31618  to find the covariance and correlation coeffic...   \n",
            "31553  to find the probability of rolling at least on...   \n",
            "31203  to find the average age of the students in the...   \n",
            "31771  to find the principal component, we first need...   \n",
            "31070  to perform k-means clustering with 3 clusters,...   \n",
            "32016  to calculate the probability that one particul...   \n",
            "31418  to perform a hypothesis test at a 5% level of ...   \n",
            "31125  to create a 95% confidence interval for the me...   \n",
            "31030  to find the average distance between the centr...   \n",
            "32028  in simple random sampling, each employee has a...   \n",
            "31610  to determine the correlation between the score...   \n",
            "31481  let s represent a sunny day and r represent a ...   \n",
            "31528  to perform a monte carlo simulation, we will g...   \n",
            "31749  to perform pca on the given dataset, we first ...   \n",
            "31974  to solve this problem, we will use the central...   \n",
            "31990  to calculate the 95% confidence interval for t...   \n",
            "31567  to estimate the probability using monte carlo ...   \n",
            "32147  to forecast the number of daily website visito...   \n",
            "31912  to calculate the expected score of a student w...   \n",
            "31702  to determine if there are any outliers using t...   \n",
            "30889  to solve this problem using bayesian statistic...   \n",
            "30922  to estimate the 95% confidence interval for th...   \n",
            "30895  to solve this problem using bayesian statistic...   \n",
            "31961  first, we need to find the median of the datas...   \n",
            "32085  to estimate the probability that a patient sur...   \n",
            "32037  to find the probability that the sample will h...   \n",
            "\n",
            "                                        generated_answer  faithfulness  \\\n",
            "31580  \\nto estimate the mean number of days it will ...      0.056756   \n",
            "31825  the probability that a randomly selected stude...      0.067104   \n",
            "30986  the probability that the average lifetime of t...      0.000000   \n",
            "31277  \\nthank you for asking! based on the context i...      0.265817   \n",
            "31618  \\n\\nplease provide the answer in a clear and c...      0.000000   \n",
            "31553  to calculate the probability of rolling at lea...      0.000000   \n",
            "31203  the average age of the students in the classro...      0.000000   \n",
            "31771  the principal component for the given covarian...      0.249793   \n",
            "31070  sure, i can help you with that! based on the c...      0.139087   \n",
            "32016  the probability of selecting a particular empl...      0.000000   \n",
            "31418  the manufacturer's claim that their cars get a...      0.067147   \n",
            "31125  \\nto construct a 95% confidence interval for t...      0.117161   \n",
            "31030  the average distance between the centroids of ...      0.000000   \n",
            "32028  the probability of choosing a sample of 30 emp...      0.141620   \n",
            "31610  the correlation between the scores of mathemat...      0.000000   \n",
            "31481  the probability that it will be sunny three da...      0.000000   \n",
            "31528  to estimate the probability that a contestant ...      0.186260   \n",
            "31749  the first principal component of the dataset i...      0.397887   \n",
            "31974  the probability that the sample mean lifespan ...      0.000000   \n",
            "31990  the 95% confidence interval for the true avera...      0.000000   \n",
            "31567  to estimate the probability of rolling a total...      0.086574   \n",
            "32147  \\n\\nplease provide the forecasted value for th...      0.292595   \n",
            "31912  the expected score of a student who studies fo...      0.143955   \n",
            "31702  the scores of 93, 94, and 95 are outliers.\\n\\n...      0.053657   \n",
            "30889  to estimate the probability that a new student...      0.069772   \n",
            "30922  to estimate the 95% confidence interval for th...      0.148400   \n",
            "30895  the probability that the company's claim is tr...      0.109494   \n",
            "31961  \\nthe mad is calculated as follows:\\n\\nmad = √...      0.060435   \n",
            "32085  i can help you with that! to estimate the prob...      0.438570   \n",
            "32037  to determine the probability that the sample w...      0.000000   \n",
            "\n",
            "       answer_relevancy  context_recall  context_precision  answer_correctness  \n",
            "31580          0.421500        0.666667           0.030769               False  \n",
            "31825          0.612493        0.500000           0.020833               False  \n",
            "30986          0.662880        0.000000           0.000000               False  \n",
            "31277          0.521626        0.500000           0.011111               False  \n",
            "31618          0.032957        0.000000           0.000000               False  \n",
            "31553          0.851130        0.000000           0.000000                True  \n",
            "31203          0.626776        0.000000           0.000000               False  \n",
            "31771          0.433935        0.666667           0.040000               False  \n",
            "31070          0.389440        1.000000           0.025641               False  \n",
            "32016          0.388205        0.000000           0.000000               False  \n",
            "31418          0.494657        0.500000           0.012658               False  \n",
            "31125          0.669276        0.500000           0.015873               False  \n",
            "31030          0.545167        0.000000           0.000000               False  \n",
            "32028          0.682295        0.500000           0.050000               False  \n",
            "31610          0.637452        0.000000           0.000000               False  \n",
            "31481          0.167844        0.000000           0.000000               False  \n",
            "31528          0.460557        1.000000           0.045455               False  \n",
            "31749          0.321445        0.666667           0.057143               False  \n",
            "31974          0.600499        0.000000           0.000000               False  \n",
            "31990          0.491713        0.000000           0.000000               False  \n",
            "31567          0.452309        1.000000           0.038961               False  \n",
            "32147          0.424310        0.666667           0.100000               False  \n",
            "31912          0.744068        0.500000           0.035714               False  \n",
            "31702          0.481395        0.500000           0.014286               False  \n",
            "30889          0.586975        0.500000           0.012500               False  \n",
            "30922          0.581815        1.000000           0.012346               False  \n",
            "30895          0.576801        1.000000           0.029851               False  \n",
            "31961          0.591347        0.500000           0.016667               False  \n",
            "32085          0.321155        0.500000           0.012500               False  \n",
            "32037          0.816314        0.000000           0.000000                True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up the figure and axis\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot 'answer_relevancy'\n",
        "plt.scatter(test_df.index, test_df['answer_relevancy'], color='teal', alpha=0.7, marker='o')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Answer Relevancy')\n",
        "plt.title('Scatter Plot of Answer Relevancy for Each Entry')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "XdIz2dvfBNDC",
        "outputId": "7172d76b-6877-42b1-9ff9-ea76e3e40a24"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the overall performance\n",
        "average_faithfulness = test_df['faithfulness'].mean()\n",
        "average_answer_relevancy = test_df['answer_relevancy'].mean()\n",
        "average_context_recall = test_df['context_recall'].mean()\n",
        "average_context_precision = test_df['context_precision'].mean()\n",
        "accuracy_answer_correctness = test_df['answer_correctness'].mean()\n",
        "\n",
        "# Print the overall performance metrics\n",
        "print(\"\\nOverall Performance Metrics:\")\n",
        "print(f\"Average Faithfulness: {average_faithfulness:.4f}\")\n",
        "print(f\"Average Answer Relevancy: {average_answer_relevancy:.4f}\")\n",
        "print(f\"Average Context Recall: {average_context_recall:.4f}\")\n",
        "print(f\"Average Context Precision: {average_context_precision:.4f}\")\n",
        "print(f\"Answer Correctness Accuracy: {accuracy_answer_correctness:.4f}\")\n",
        "\n",
        "# If you want a performance summary in percentage terms, you could format them as such:\n",
        "performance_summary = {\n",
        "    'Average Faithfulness (%)': average_faithfulness * 100,\n",
        "    'Average Answer Relevancy (%)': average_answer_relevancy * 100,\n",
        "    'Average Context Recall (%)': average_context_recall * 100,\n",
        "    'Average Context Precision (%)': average_context_precision * 100,\n",
        "    'Answer Correctness Accuracy (%)': accuracy_answer_correctness * 100\n",
        "}\n",
        "\n",
        "# Print the performance summary as percentages\n",
        "print(\"\\nPerformance Summary (in %):\")\n",
        "for metric, value in performance_summary.items():\n",
        "    print(f\"{metric}: {value:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMlOSPyOJ37y",
        "outputId": "763ee213-cc7b-439a-ccc3-8bc154c343d6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall Performance Metrics:\n",
            "Average Faithfulness: 0.1031\n",
            "Average Answer Relevancy: 0.5196\n",
            "Average Context Recall: 0.4222\n",
            "Average Context Precision: 0.0194\n",
            "Answer Correctness Accuracy: 0.0667\n",
            "\n",
            "Performance Summary (in %):\n",
            "Average Faithfulness (%): 10.31%\n",
            "Average Answer Relevancy (%): 51.96%\n",
            "Average Context Recall (%): 42.22%\n",
            "Average Context Precision (%): 1.94%\n",
            "Answer Correctness Accuracy (%): 6.67%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07d43027b05d43a284931d1606cf82f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79e5e8be0ce9498db58a0d1efa3151b4",
              "IPY_MODEL_3b5e9e0ed5774bdfa5206024a6e4bc6d",
              "IPY_MODEL_d1bd269861354d32b8c2e88f093bf2fe"
            ],
            "layout": "IPY_MODEL_1876c0261ab547099cade84aae76730c"
          }
        },
        "79e5e8be0ce9498db58a0d1efa3151b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5f8bcb2788b498190be11c600019076",
            "placeholder": "​",
            "style": "IPY_MODEL_2058a3f47ccd4e0ba96154f179ad718c",
            "value": "config.json: 100%"
          }
        },
        "3b5e9e0ed5774bdfa5206024a6e4bc6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fb476544c784c00a1c5488b0d931b41",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c97885f61214f9194b6466989acf8b8",
            "value": 614
          }
        },
        "d1bd269861354d32b8c2e88f093bf2fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9ff720cf3b14d068caca4197d7faa3d",
            "placeholder": "​",
            "style": "IPY_MODEL_ac1209b611024814813d13e375b0ba69",
            "value": " 614/614 [00:00&lt;00:00, 43.0kB/s]"
          }
        },
        "1876c0261ab547099cade84aae76730c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f8bcb2788b498190be11c600019076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2058a3f47ccd4e0ba96154f179ad718c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fb476544c784c00a1c5488b0d931b41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c97885f61214f9194b6466989acf8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9ff720cf3b14d068caca4197d7faa3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac1209b611024814813d13e375b0ba69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca879187f07746f78beed164029f01ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_568d2f119c60410f83a6e98c2ca1578b",
              "IPY_MODEL_ff7b310010e6456280216f6ef715deeb",
              "IPY_MODEL_f98feb2d4d834051a46cc526ff08159c"
            ],
            "layout": "IPY_MODEL_0aab9d867c7c438e84c5ac08763be89d"
          }
        },
        "568d2f119c60410f83a6e98c2ca1578b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f248a639c5a845739f3b74b2697e870d",
            "placeholder": "​",
            "style": "IPY_MODEL_3f3e5c46dfa1415ca87360695506c42a",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "ff7b310010e6456280216f6ef715deeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b238587f900456ebe652b0eca5c3ed3",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75647d2ccace479481dbf16438bb4cfe",
            "value": 26788
          }
        },
        "f98feb2d4d834051a46cc526ff08159c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43131f5040b74fe7add51fe37d01d0ed",
            "placeholder": "​",
            "style": "IPY_MODEL_34a0b1dd1ac14192b04ca577eec2fe9b",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 1.17MB/s]"
          }
        },
        "0aab9d867c7c438e84c5ac08763be89d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f248a639c5a845739f3b74b2697e870d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f3e5c46dfa1415ca87360695506c42a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b238587f900456ebe652b0eca5c3ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75647d2ccace479481dbf16438bb4cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43131f5040b74fe7add51fe37d01d0ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a0b1dd1ac14192b04ca577eec2fe9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0323df87245407cbe66ee30e73454ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27248eb8bfa04736b785c516bda5cab2",
              "IPY_MODEL_d4b61ce85d6e4933bace9851ea7a8e92",
              "IPY_MODEL_41ab5b349bff4a518d1dcc5dfc536fe9"
            ],
            "layout": "IPY_MODEL_f10c3d17547b4c1ab7e8cd168e2bade5"
          }
        },
        "27248eb8bfa04736b785c516bda5cab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16dcf4a1e5bb4cc9959e48fdf6d48177",
            "placeholder": "​",
            "style": "IPY_MODEL_44c96b5494ab4776af1549989836a2e8",
            "value": "Downloading shards: 100%"
          }
        },
        "d4b61ce85d6e4933bace9851ea7a8e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f04aa16b5be14d77bfca03e4ca65706a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b7cd332ed854665bab1c7f64e49fa22",
            "value": 2
          }
        },
        "41ab5b349bff4a518d1dcc5dfc536fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfa65675994345a3a22a48670e8b0683",
            "placeholder": "​",
            "style": "IPY_MODEL_eeaa9858848448a294e7f39447b90d1d",
            "value": " 2/2 [01:09&lt;00:00, 31.56s/it]"
          }
        },
        "f10c3d17547b4c1ab7e8cd168e2bade5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16dcf4a1e5bb4cc9959e48fdf6d48177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44c96b5494ab4776af1549989836a2e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f04aa16b5be14d77bfca03e4ca65706a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7cd332ed854665bab1c7f64e49fa22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfa65675994345a3a22a48670e8b0683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeaa9858848448a294e7f39447b90d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e62c8fd519c42cbae355fc4440dbe6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f882dfde00a54c09a4100368ba2b2f29",
              "IPY_MODEL_8e2d6af843404b63ba831adfa419d437",
              "IPY_MODEL_5b8f9d3959194da5a1c6eafe246f3297"
            ],
            "layout": "IPY_MODEL_834fbc008cf3447d9e1046710d170bd5"
          }
        },
        "f882dfde00a54c09a4100368ba2b2f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6caa37ea5e504738b2eb721dc85698f5",
            "placeholder": "​",
            "style": "IPY_MODEL_797608f208f644dbadc3b2f08b0d79f9",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "8e2d6af843404b63ba831adfa419d437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_579e8101cefd477abed87bd7023be6ad",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d033a4bbb04347ec807a5d5ed59b64b5",
            "value": 9976576152
          }
        },
        "5b8f9d3959194da5a1c6eafe246f3297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50c74f3561cc49458ddbf6d3dfec1391",
            "placeholder": "​",
            "style": "IPY_MODEL_f2e01ecdd47345adb9cb0b4364519aa3",
            "value": " 9.98G/9.98G [00:50&lt;00:00, 232MB/s]"
          }
        },
        "834fbc008cf3447d9e1046710d170bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6caa37ea5e504738b2eb721dc85698f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "797608f208f644dbadc3b2f08b0d79f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "579e8101cefd477abed87bd7023be6ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d033a4bbb04347ec807a5d5ed59b64b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50c74f3561cc49458ddbf6d3dfec1391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e01ecdd47345adb9cb0b4364519aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "010bceb725e04458b6b71f4704d09f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f17badb293914896ae23760c835f1130",
              "IPY_MODEL_a2faf4864c7842acb94d2b928f6293bc",
              "IPY_MODEL_01b42b9acefc473ba6cd6cc783c46694"
            ],
            "layout": "IPY_MODEL_27fd94ed17df4c1aa61ffa1738d2ed32"
          }
        },
        "f17badb293914896ae23760c835f1130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_894bc4c819a24f92a7b53b466e0a0301",
            "placeholder": "​",
            "style": "IPY_MODEL_79a2bb52e1f74cc2b8bee35726de3b2b",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "a2faf4864c7842acb94d2b928f6293bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_165ca9bcfd344fb5b213a5f9b0556811",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8450e23aad34bcfbc92fa304f35899c",
            "value": 3500296424
          }
        },
        "01b42b9acefc473ba6cd6cc783c46694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3dad70fd77a42fa92436ab47427fceb",
            "placeholder": "​",
            "style": "IPY_MODEL_c62edcacab11475b85c785916205575b",
            "value": " 3.50G/3.50G [00:17&lt;00:00, 255MB/s]"
          }
        },
        "27fd94ed17df4c1aa61ffa1738d2ed32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "894bc4c819a24f92a7b53b466e0a0301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79a2bb52e1f74cc2b8bee35726de3b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "165ca9bcfd344fb5b213a5f9b0556811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8450e23aad34bcfbc92fa304f35899c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3dad70fd77a42fa92436ab47427fceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62edcacab11475b85c785916205575b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e3a38f6d1bd42599d7a25332b29e61e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d37f9f734fcb4547914d90b55be5e67c",
              "IPY_MODEL_d92a530ec11f49e4b3fc0e4267a76e4b",
              "IPY_MODEL_cdb0b6c2c50b4a75a0b3497a585d01d4"
            ],
            "layout": "IPY_MODEL_bcef3620e5ca4b098849459dbd2fe834"
          }
        },
        "d37f9f734fcb4547914d90b55be5e67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2acb93867d78497080cc6fd53e87ff3f",
            "placeholder": "​",
            "style": "IPY_MODEL_b83cebbde5ec40958034931b7f36cda4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d92a530ec11f49e4b3fc0e4267a76e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fa2fdcc55a84aa9b36b33b6dd187a3d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04c56444617c455ca9e7e967d0977711",
            "value": 2
          }
        },
        "cdb0b6c2c50b4a75a0b3497a585d01d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31dd757433fb41c39ca45f7a5199a5ee",
            "placeholder": "​",
            "style": "IPY_MODEL_f893c422b2444e3db4f29882a54c9c95",
            "value": " 2/2 [01:01&lt;00:00, 28.16s/it]"
          }
        },
        "bcef3620e5ca4b098849459dbd2fe834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2acb93867d78497080cc6fd53e87ff3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b83cebbde5ec40958034931b7f36cda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fa2fdcc55a84aa9b36b33b6dd187a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04c56444617c455ca9e7e967d0977711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31dd757433fb41c39ca45f7a5199a5ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f893c422b2444e3db4f29882a54c9c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89e5f7b29bfb454bb15ba3eed5916d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c50b5b5b10a1400da988015ddaf5d792",
              "IPY_MODEL_0f59042505f644aca092f0f79bc54b4c",
              "IPY_MODEL_5717cd9ffed842aa87d9faa5b32a87a8"
            ],
            "layout": "IPY_MODEL_6535e5cb00f64e76b5af54b16ca75b9c"
          }
        },
        "c50b5b5b10a1400da988015ddaf5d792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57bb8b5642b64c538ee59bbb4b8e1270",
            "placeholder": "​",
            "style": "IPY_MODEL_6f583050a41b4fd98aa0d559801e1172",
            "value": "generation_config.json: 100%"
          }
        },
        "0f59042505f644aca092f0f79bc54b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08cf56b4e3924dc3bccee23231f3519d",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97b15daf291c4f288e2d76c9d3a400c8",
            "value": 188
          }
        },
        "5717cd9ffed842aa87d9faa5b32a87a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e9d61790844f468257bc63d349d249",
            "placeholder": "​",
            "style": "IPY_MODEL_36151ede110a4f83a6055cb5c1d415c9",
            "value": " 188/188 [00:00&lt;00:00, 13.4kB/s]"
          }
        },
        "6535e5cb00f64e76b5af54b16ca75b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57bb8b5642b64c538ee59bbb4b8e1270": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f583050a41b4fd98aa0d559801e1172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08cf56b4e3924dc3bccee23231f3519d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97b15daf291c4f288e2d76c9d3a400c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9e9d61790844f468257bc63d349d249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36151ede110a4f83a6055cb5c1d415c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48c4a8076ffe49448a3d8b343c08fac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29c78294bee844d59f40f2962436ba72",
              "IPY_MODEL_d85435d60d02442d820ebafc575f3d3b",
              "IPY_MODEL_f53c10e87e3e436f83ad7d590a5399a6"
            ],
            "layout": "IPY_MODEL_1dd3a2d864b44a47a5dda1f36f5da1c6"
          }
        },
        "29c78294bee844d59f40f2962436ba72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5a8a92ed5df44d8b8e6781882a98dcf",
            "placeholder": "​",
            "style": "IPY_MODEL_226c59af24b949e09dc9b0ad9d65206e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d85435d60d02442d820ebafc575f3d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa76d669fb5640c4bb12626d10b9b4f5",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_581b808654ba415a88b09c1d771ddcdb",
            "value": 1618
          }
        },
        "f53c10e87e3e436f83ad7d590a5399a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58af3bef99514d259e69bed2309f0511",
            "placeholder": "​",
            "style": "IPY_MODEL_3cab9876ad3644edb442077138c0b0bd",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 72.9kB/s]"
          }
        },
        "1dd3a2d864b44a47a5dda1f36f5da1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a8a92ed5df44d8b8e6781882a98dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "226c59af24b949e09dc9b0ad9d65206e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa76d669fb5640c4bb12626d10b9b4f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581b808654ba415a88b09c1d771ddcdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58af3bef99514d259e69bed2309f0511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cab9876ad3644edb442077138c0b0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5197f875ea784ee9a1d773fd0ebccfad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f37c4a2850d049ce89b31238e05929e9",
              "IPY_MODEL_c622e895f23943d0acbc78bdb8b4ecdf",
              "IPY_MODEL_6b0a3dc0278043d1b3461804e5aee0f9"
            ],
            "layout": "IPY_MODEL_73692a9c7f6e47518d6983d95130f4e7"
          }
        },
        "f37c4a2850d049ce89b31238e05929e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c919ef97da14876b5c7205f0bd8ba0a",
            "placeholder": "​",
            "style": "IPY_MODEL_df0a1f72e853476bb9b05f5b48d5e096",
            "value": "tokenizer.model: 100%"
          }
        },
        "c622e895f23943d0acbc78bdb8b4ecdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8278ee3fcbdc48f7b8aac3183e6eb2bf",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57f38417574f4ea4b05d8415fd6100ed",
            "value": 499723
          }
        },
        "6b0a3dc0278043d1b3461804e5aee0f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c7e28bc2b624bb8aed0ba2f0aab8be0",
            "placeholder": "​",
            "style": "IPY_MODEL_42d6d8ed9a6c4eff9b1eeb27e6534087",
            "value": " 500k/500k [00:00&lt;00:00, 35.4MB/s]"
          }
        },
        "73692a9c7f6e47518d6983d95130f4e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c919ef97da14876b5c7205f0bd8ba0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df0a1f72e853476bb9b05f5b48d5e096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8278ee3fcbdc48f7b8aac3183e6eb2bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f38417574f4ea4b05d8415fd6100ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c7e28bc2b624bb8aed0ba2f0aab8be0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42d6d8ed9a6c4eff9b1eeb27e6534087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ef266b4bb83445eab0c0407decdc987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b865e5f72e74f61a320999660662b1b",
              "IPY_MODEL_282b762072cc4980b0f3364feebc3239",
              "IPY_MODEL_d46f99424236455bb85fae0a662c6a0d"
            ],
            "layout": "IPY_MODEL_2ae01fe894134a6cb338b6cce687c6cc"
          }
        },
        "8b865e5f72e74f61a320999660662b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7adf753e509b43f88b765737f6cd587e",
            "placeholder": "​",
            "style": "IPY_MODEL_e76da2bd8fea4392b81f3f0e90fd9044",
            "value": "tokenizer.json: 100%"
          }
        },
        "282b762072cc4980b0f3364feebc3239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9384b919d584731ad42c804f267d123",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4605e74bcd884742baf831bcfb076559",
            "value": 1842767
          }
        },
        "d46f99424236455bb85fae0a662c6a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d7469c6ff304a8eacecad88d1fd641c",
            "placeholder": "​",
            "style": "IPY_MODEL_2b65a7c25d6940afa091a611398f73ed",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 6.88MB/s]"
          }
        },
        "2ae01fe894134a6cb338b6cce687c6cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7adf753e509b43f88b765737f6cd587e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e76da2bd8fea4392b81f3f0e90fd9044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9384b919d584731ad42c804f267d123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4605e74bcd884742baf831bcfb076559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d7469c6ff304a8eacecad88d1fd641c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b65a7c25d6940afa091a611398f73ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f5849a2a96b45ddb1a34001b8f8fb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_729461b3470745b7883afab091911bd2",
              "IPY_MODEL_dcaebd472e96465b9043eba19ed71960",
              "IPY_MODEL_5537a71394164594a3563b8ea703fc90"
            ],
            "layout": "IPY_MODEL_c174c167dff64f7099d31c08b0b9aacb"
          }
        },
        "729461b3470745b7883afab091911bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad307c054be747d8b72b53257c833faa",
            "placeholder": "​",
            "style": "IPY_MODEL_218af710143746fb8d67ca819e1d114f",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "dcaebd472e96465b9043eba19ed71960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49ea4f1f9ed84dddb26ee71d803370aa",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_024ac37e88b04554bf675944f3d4bfba",
            "value": 414
          }
        },
        "5537a71394164594a3563b8ea703fc90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f620edd1613b40cf9d7e649e4c78b970",
            "placeholder": "​",
            "style": "IPY_MODEL_1cb1d6c9eec84018873819956838ef8f",
            "value": " 414/414 [00:00&lt;00:00, 28.2kB/s]"
          }
        },
        "c174c167dff64f7099d31c08b0b9aacb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad307c054be747d8b72b53257c833faa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "218af710143746fb8d67ca819e1d114f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49ea4f1f9ed84dddb26ee71d803370aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "024ac37e88b04554bf675944f3d4bfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f620edd1613b40cf9d7e649e4c78b970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cb1d6c9eec84018873819956838ef8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3fdd9c511454ded8b74218bd5800a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93ec2acaa67a4fa2b052fd1149f14220",
              "IPY_MODEL_a06982d10d8347b5ab809e5ebb845046",
              "IPY_MODEL_83f1aeb88ead41798060e300043bf148"
            ],
            "layout": "IPY_MODEL_91525ef4c5b84cd2a50f5c80311bfb84"
          }
        },
        "93ec2acaa67a4fa2b052fd1149f14220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_353645d53f824396b5e442e706d32169",
            "placeholder": "​",
            "style": "IPY_MODEL_dc33195b4d8c480180a219c1538c5885",
            "value": "modules.json: 100%"
          }
        },
        "a06982d10d8347b5ab809e5ebb845046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b4bb995c2d43428863992b18a45bb2",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27743385c0764033bb2fdb25c1736277",
            "value": 349
          }
        },
        "83f1aeb88ead41798060e300043bf148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fe37e125bd945d7925924f83f517477",
            "placeholder": "​",
            "style": "IPY_MODEL_20934dd736bf4b51b148e2eadb93083e",
            "value": " 349/349 [00:00&lt;00:00, 16.4kB/s]"
          }
        },
        "91525ef4c5b84cd2a50f5c80311bfb84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353645d53f824396b5e442e706d32169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc33195b4d8c480180a219c1538c5885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29b4bb995c2d43428863992b18a45bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27743385c0764033bb2fdb25c1736277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fe37e125bd945d7925924f83f517477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20934dd736bf4b51b148e2eadb93083e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a357ca735f73486caf30ea7b1021ae71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38846f83812a43bda75a257d67b6db5d",
              "IPY_MODEL_d0faf7d47832442cb50c386c43773606",
              "IPY_MODEL_32c9322c51424d0a886e9887053e65fe"
            ],
            "layout": "IPY_MODEL_5e748779468643f3bff3a66b36aa3a47"
          }
        },
        "38846f83812a43bda75a257d67b6db5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a735b6fd091b44fda2f7164f3cf60926",
            "placeholder": "​",
            "style": "IPY_MODEL_301cff2ddd184963a9d7e465297b41fd",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "d0faf7d47832442cb50c386c43773606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d50260506d649f1b7a560979c429748",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35392f92b1444d61afa73c32c82aa6bd",
            "value": 116
          }
        },
        "32c9322c51424d0a886e9887053e65fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efe0194e2ee74ea19273a7701feaba41",
            "placeholder": "​",
            "style": "IPY_MODEL_96e6d2c5da574b08acc96b36b81995ff",
            "value": " 116/116 [00:00&lt;00:00, 9.16kB/s]"
          }
        },
        "5e748779468643f3bff3a66b36aa3a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a735b6fd091b44fda2f7164f3cf60926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301cff2ddd184963a9d7e465297b41fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d50260506d649f1b7a560979c429748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35392f92b1444d61afa73c32c82aa6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efe0194e2ee74ea19273a7701feaba41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e6d2c5da574b08acc96b36b81995ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "040d475e55994caea51c9ddab4c9e930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c672fc900924aa18c921b37dd49b0f2",
              "IPY_MODEL_6c48e6ba4b9946a19a8992fff936287d",
              "IPY_MODEL_b1cf57721c704515a530cfc6783ddc60"
            ],
            "layout": "IPY_MODEL_72ff618c98094202b9bfc4ae20d45c74"
          }
        },
        "4c672fc900924aa18c921b37dd49b0f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee79f70571f84c2091e7b09125bbd804",
            "placeholder": "​",
            "style": "IPY_MODEL_b151982c25a74e98930156d2de709494",
            "value": "README.md: 100%"
          }
        },
        "6c48e6ba4b9946a19a8992fff936287d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_633425dea2c14c0994cd866b324771ce",
            "max": 10621,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2098ce7fe16c4227953be6d6818836a0",
            "value": 10621
          }
        },
        "b1cf57721c704515a530cfc6783ddc60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f8160962ab4924a4e8e72fbe0bfa26",
            "placeholder": "​",
            "style": "IPY_MODEL_9e3bc23a966f4fbe9a65ee56c6af8610",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 639kB/s]"
          }
        },
        "72ff618c98094202b9bfc4ae20d45c74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee79f70571f84c2091e7b09125bbd804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b151982c25a74e98930156d2de709494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "633425dea2c14c0994cd866b324771ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2098ce7fe16c4227953be6d6818836a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19f8160962ab4924a4e8e72fbe0bfa26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e3bc23a966f4fbe9a65ee56c6af8610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "630d75101d5a4142a8a202793050c6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd2aa09e6e794530a94dc09317ba2ab8",
              "IPY_MODEL_6c5298940f434bdebdb286879b8b4ca9",
              "IPY_MODEL_d7969b627bdc40b7b2d5e5004b26aefe"
            ],
            "layout": "IPY_MODEL_c1365c2514f84c97984c20a9c97fc489"
          }
        },
        "bd2aa09e6e794530a94dc09317ba2ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c779df9d1d447bd87b7b9d614acca61",
            "placeholder": "​",
            "style": "IPY_MODEL_12a85dae2ab6489abbd7f0383722f18e",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "6c5298940f434bdebdb286879b8b4ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0505b2903b9f42c1be6ccbcf36aed6c3",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b043062c20440dc814dd6a6893dee90",
            "value": 53
          }
        },
        "d7969b627bdc40b7b2d5e5004b26aefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_824848de21954a4ea6dafc9c12c5882f",
            "placeholder": "​",
            "style": "IPY_MODEL_d7e1e49a15894b8f89f3e61909740e65",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.27kB/s]"
          }
        },
        "c1365c2514f84c97984c20a9c97fc489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c779df9d1d447bd87b7b9d614acca61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12a85dae2ab6489abbd7f0383722f18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0505b2903b9f42c1be6ccbcf36aed6c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b043062c20440dc814dd6a6893dee90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "824848de21954a4ea6dafc9c12c5882f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e1e49a15894b8f89f3e61909740e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdeaf3a981d445259374d252f7f329f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34ce545499ef44968a53d32a4846a316",
              "IPY_MODEL_6ae078a3de5045d49b9564a21b41eb37",
              "IPY_MODEL_4baf8a8291e04b86bc8e76952e3f3704"
            ],
            "layout": "IPY_MODEL_1eabc0fabe5e457a8f8f1058811c6396"
          }
        },
        "34ce545499ef44968a53d32a4846a316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc16765aa33243fcabbc37d53a904aff",
            "placeholder": "​",
            "style": "IPY_MODEL_ce2fadf4c0464489bbf3dc96f2a2d603",
            "value": "config.json: 100%"
          }
        },
        "6ae078a3de5045d49b9564a21b41eb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48862232c1de4f8d96ebeb5ae240b5d6",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18904f526bb94fcaa3f658ca2a78e636",
            "value": 571
          }
        },
        "4baf8a8291e04b86bc8e76952e3f3704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8e259d77c7346eda27c757207d5c649",
            "placeholder": "​",
            "style": "IPY_MODEL_8c005cce347644aabbd07ad9ec703f3f",
            "value": " 571/571 [00:00&lt;00:00, 39.6kB/s]"
          }
        },
        "1eabc0fabe5e457a8f8f1058811c6396": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc16765aa33243fcabbc37d53a904aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce2fadf4c0464489bbf3dc96f2a2d603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48862232c1de4f8d96ebeb5ae240b5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18904f526bb94fcaa3f658ca2a78e636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8e259d77c7346eda27c757207d5c649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c005cce347644aabbd07ad9ec703f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a60acd175f542578814902c1690190c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94ad068866f54505a06bb66ee6b2fb6e",
              "IPY_MODEL_297ec93c66f14bda9ca79b57bd228586",
              "IPY_MODEL_e6f8f68ffb3343e6af16b67d0137b78f"
            ],
            "layout": "IPY_MODEL_80ad62cfe14c423a9efdd03fdf876980"
          }
        },
        "94ad068866f54505a06bb66ee6b2fb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b410b8d17e64425829ff80bb6763a27",
            "placeholder": "​",
            "style": "IPY_MODEL_83febf5162f34205b1a1481a4c1b5c03",
            "value": "model.safetensors: 100%"
          }
        },
        "297ec93c66f14bda9ca79b57bd228586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31933be30a3b48f693a2cb4ac6089d67",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9259981511f74bb29ba513e6deda25a7",
            "value": 437971872
          }
        },
        "e6f8f68ffb3343e6af16b67d0137b78f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7862bfb7e154bbb9c9d001b05756037",
            "placeholder": "​",
            "style": "IPY_MODEL_f4ac46e42a9f4a0eb0c63bfbdc668f03",
            "value": " 438M/438M [00:01&lt;00:00, 246MB/s]"
          }
        },
        "80ad62cfe14c423a9efdd03fdf876980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b410b8d17e64425829ff80bb6763a27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83febf5162f34205b1a1481a4c1b5c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31933be30a3b48f693a2cb4ac6089d67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9259981511f74bb29ba513e6deda25a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7862bfb7e154bbb9c9d001b05756037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4ac46e42a9f4a0eb0c63bfbdc668f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7219887a3e74de6ba96484b529e92cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc3eb16c320b4be1a228a4bf33e5484d",
              "IPY_MODEL_2145473863f3487699c22ac670fb435a",
              "IPY_MODEL_769d85aaa349453cb00a5350b27b41ac"
            ],
            "layout": "IPY_MODEL_828227a285e4473ca88a19cd3dd1fb47"
          }
        },
        "cc3eb16c320b4be1a228a4bf33e5484d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c179f49a15e3439b8d1dbbf26a78638e",
            "placeholder": "​",
            "style": "IPY_MODEL_4cb6b8f547ed44daab403c2c1ce2f3d0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "2145473863f3487699c22ac670fb435a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_986a4ff27b0c468ba638945c4e3aee0e",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68579f56ec1f4e4285c20a9bc4eda852",
            "value": 363
          }
        },
        "769d85aaa349453cb00a5350b27b41ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d84e572e1dd42cabb38406e26b55e96",
            "placeholder": "​",
            "style": "IPY_MODEL_a6c4629c485e4eb6b1fad915949e9045",
            "value": " 363/363 [00:00&lt;00:00, 17.8kB/s]"
          }
        },
        "828227a285e4473ca88a19cd3dd1fb47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c179f49a15e3439b8d1dbbf26a78638e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb6b8f547ed44daab403c2c1ce2f3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "986a4ff27b0c468ba638945c4e3aee0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68579f56ec1f4e4285c20a9bc4eda852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d84e572e1dd42cabb38406e26b55e96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c4629c485e4eb6b1fad915949e9045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b3de4782f8d4ba1bf716cea18857cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39ceab543685438f9839338871baaa82",
              "IPY_MODEL_65fa75e17e65418b8dae505d05fc5861",
              "IPY_MODEL_ba53d785bbd848bbabbb2875803abdc4"
            ],
            "layout": "IPY_MODEL_8e3c135e386946d5b9a483998b5dfe55"
          }
        },
        "39ceab543685438f9839338871baaa82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffde76cce4a24acc952ced645e129e0f",
            "placeholder": "​",
            "style": "IPY_MODEL_3018f4680d844a318557448c6d0ddf6f",
            "value": "vocab.txt: 100%"
          }
        },
        "65fa75e17e65418b8dae505d05fc5861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f71b009735764f369b59cc8c1b2b9fb5",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f31838c2824741ea8d7f6fa31ec90e22",
            "value": 231536
          }
        },
        "ba53d785bbd848bbabbb2875803abdc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c8df161bf7c471790065f2010ff07e0",
            "placeholder": "​",
            "style": "IPY_MODEL_8387fa91177a45a7b491454e541a6064",
            "value": " 232k/232k [00:00&lt;00:00, 3.49MB/s]"
          }
        },
        "8e3c135e386946d5b9a483998b5dfe55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffde76cce4a24acc952ced645e129e0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3018f4680d844a318557448c6d0ddf6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f71b009735764f369b59cc8c1b2b9fb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f31838c2824741ea8d7f6fa31ec90e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c8df161bf7c471790065f2010ff07e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8387fa91177a45a7b491454e541a6064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a8f104672584882ad1a3bdf123f4307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_617000c8698d477d940619180546905a",
              "IPY_MODEL_99de76ebcf40410097b216d9d36be3d2",
              "IPY_MODEL_65adce2ef9284e4191c665c238e78e2b"
            ],
            "layout": "IPY_MODEL_bb63cd2408fe41209fbb257171a65d99"
          }
        },
        "617000c8698d477d940619180546905a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0b2df6cbdf947b8ac39db18ea3d92ca",
            "placeholder": "​",
            "style": "IPY_MODEL_d9ceccca2b974890a8180cf8d3f12066",
            "value": "tokenizer.json: 100%"
          }
        },
        "99de76ebcf40410097b216d9d36be3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_907910ddc59a468b97e1b6a340d367e8",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12ef29bbb7fb456d9ea54edcef3f338c",
            "value": 466021
          }
        },
        "65adce2ef9284e4191c665c238e78e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_525c66d8194c44cab4e6ef6291c81eeb",
            "placeholder": "​",
            "style": "IPY_MODEL_6e97859b8eed4821adb67a99163cfd10",
            "value": " 466k/466k [00:00&lt;00:00, 3.57MB/s]"
          }
        },
        "bb63cd2408fe41209fbb257171a65d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b2df6cbdf947b8ac39db18ea3d92ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9ceccca2b974890a8180cf8d3f12066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "907910ddc59a468b97e1b6a340d367e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12ef29bbb7fb456d9ea54edcef3f338c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "525c66d8194c44cab4e6ef6291c81eeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e97859b8eed4821adb67a99163cfd10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2b6a122926340e7b13cfd02d3328902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3dba4df7a3c4beca935f38fe9dfe162",
              "IPY_MODEL_cb421bc362ab42fca85b1c7337e34f2b",
              "IPY_MODEL_6dce5be5861f4effa4e87c61c4093878"
            ],
            "layout": "IPY_MODEL_330d06eea19f4a789c826eb82746c6ab"
          }
        },
        "a3dba4df7a3c4beca935f38fe9dfe162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68aaf091ef5f4ea09a512034ce765297",
            "placeholder": "​",
            "style": "IPY_MODEL_e4c348f1f12b4630bb7ccbd3e4c5ddf9",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "cb421bc362ab42fca85b1c7337e34f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_809252b64854419d88e9c061e8683cca",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_420dae5d49e041aeba04a4bfb84f9ce9",
            "value": 239
          }
        },
        "6dce5be5861f4effa4e87c61c4093878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25e1bfde506640be8356fca924600f57",
            "placeholder": "​",
            "style": "IPY_MODEL_c32f454335754188a1cf2b8ca468e637",
            "value": " 239/239 [00:00&lt;00:00, 7.42kB/s]"
          }
        },
        "330d06eea19f4a789c826eb82746c6ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68aaf091ef5f4ea09a512034ce765297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4c348f1f12b4630bb7ccbd3e4c5ddf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "809252b64854419d88e9c061e8683cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "420dae5d49e041aeba04a4bfb84f9ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25e1bfde506640be8356fca924600f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c32f454335754188a1cf2b8ca468e637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44db732bf2484d37a4d9eaf7bb5323fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d7b0b2e32724dbf8435b5322265f95b",
              "IPY_MODEL_d66f81c2463e4009b092e099e8f56147",
              "IPY_MODEL_0fc8ce54ddee42488c945b25ec841e10"
            ],
            "layout": "IPY_MODEL_1a07d4f9d0dc4280b192be42a5c6a15a"
          }
        },
        "9d7b0b2e32724dbf8435b5322265f95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cec44136e6bd466799710b289e671a45",
            "placeholder": "​",
            "style": "IPY_MODEL_27c24691f6494d1d985d249834c865f0",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "d66f81c2463e4009b092e099e8f56147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5bf8b6bff6147c58452aeeb5055f2a6",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b077daf756f42088b0f72cdafeb5cd7",
            "value": 190
          }
        },
        "0fc8ce54ddee42488c945b25ec841e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc23032c40634703abb2827de0f18308",
            "placeholder": "​",
            "style": "IPY_MODEL_23cedbf47f914dcb95c989385be6a3e4",
            "value": " 190/190 [00:00&lt;00:00, 7.10kB/s]"
          }
        },
        "1a07d4f9d0dc4280b192be42a5c6a15a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec44136e6bd466799710b289e671a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27c24691f6494d1d985d249834c865f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5bf8b6bff6147c58452aeeb5055f2a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b077daf756f42088b0f72cdafeb5cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc23032c40634703abb2827de0f18308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23cedbf47f914dcb95c989385be6a3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}